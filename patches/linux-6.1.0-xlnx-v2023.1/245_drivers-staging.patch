--- /dev/null
+++ linux-xlnx-2023.1/Documentation/devicetree/bindings/misc/xlnx,fclk.txt	2023-07-05 08:33:03.965584000 +0900
@@ -0,0 +1,12 @@
+* Xilinx fclk clock enable
+Temporary solution for enabling the PS_PL clocks.
+
+Required properties:
+- compatible: "xlnx,fclk"
+
+Example:
+++++++++
+fclk0: fclk0 {
+	compatible = "xlnx,fclk";
+	clocks = <&clkc 71>;
+};
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/Documentation/devicetree/bindings/staging/net/xilinx-tsn-ethernet.txt
--- /dev/null
+++ linux-xlnx-2023.1/Documentation/devicetree/bindings/staging/net/xilinx-tsn-ethernet.txt	2023-07-05 08:33:04.829544500 +0900
@@ -0,0 +1,54 @@
+Xilinx TSN (time sensitive networking) TEMAC axi ethernet driver (xilinx_axienet)
+-----------------------------------------------------------------------
+
+Required properties:
+- compatible		: Should be "xlnx,tsn-ethernet-1.00.a".
+- reg			: Physical base address and size of the TSN registers map.
+- interrupts		: Property with a value describing the interrupt
+			  number.
+- interrupts-names	: Property denotes the interrupt names.
+- interrupt-parent	: Must be core interrupt controller.
+- phy-handle		: See ethernet.txt file [1].
+- local-mac-address	: See ethernet.txt file [1].
+- phy-mode		: see ethernet.txt file [1].
+
+Optional properties:
+- xlnx,tsn		: Denotes a ethernet with TSN capabilities.
+- xlnx,tsn-slave	: Denotes a TSN slave port.
+- xlnx,txcsum		: Tx checksum mode (Full, Partial and None).
+- xlnx,rxcsum		: Rx checksum mode (Full, Partial and None).
+- xlnx,phy-type		: Xilinx phy device type. See xilinx-phy.txt [2].
+- xlnx,eth-hasnobuf	: Used when 1G MAC is configured in non-processor mode.
+- xlnx,num-queue	: Number of queue supported in current design, range is
+			  2 to 5 and default value is 5.
+- xlnx,num-tc		: Number of traffic class supported in current design,
+			  range is 2,3 and default value is 3. It denotes
+			  the traffic classes based on VLAN-PCP value.
+- xlnx,qbv-addr		: Denotes mac scheduler physical base address.
+- xlnx,qbv-size		: Denotes mac scheduler address space size.
+
+[1] Documentation/devicetree/bindings/net/ethernet.txt
+[2] Documentation/devicetree/bindings/net/xilinx-phy.txt
+
+Example:
+
+	tsn_emac_0: tsn_mac@80040000 {
+		compatible = "xlnx,tsn-ethernet-1.00.a";
+		interrupt-parent = <&gic>;
+		interrupts = <0 104 4 0 106 4 0 91 4 0 110 4>;
+		interrupt-names = "interrupt_ptp_rx_1", "interrupt_ptp_tx_1", "mac_irq_1", "interrupt_ptp_timer";
+		local-mac-address = [ 00 0A 35 00 01 0e ];
+		phy-mode = "rgmii";
+		reg = <0x0 0x80040000 0x0 0x14000>;
+		tsn,endpoint = <&tsn_ep>;
+		xlnx,tsn;
+		xlnx,tsn-slave;
+		xlnx,phy-type = <0x3>;
+		xlnx,eth-hasnobuf;
+		xlnx,num-queue = <0x2>;
+		xlnx,num-tc = <0x3>;
+		xlnx,qbv-addr = <0x80054000>;
+		xlnx,qbv-size = <0x2000>;
+		xlnx,txsum = <0>;
+		xlnx,rxsum = <0>;
+	};
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/Documentation/devicetree/bindings/staging/net/xilinx_tsn.txt
--- /dev/null
+++ linux-xlnx-2023.1/Documentation/devicetree/bindings/staging/net/xilinx_tsn.txt	2023-07-05 08:33:04.829544500 +0900
@@ -0,0 +1,17 @@
+Xilinx TSN (time sensitive networking) IP driver (xilinx_tsn_ip)
+-----------------------------------------------------------------------
+
+Required properties:
+- compatible		: Should be one of "xlnx,tsn-endpoint-ethernet-mac-1.0",
+			  "xlnx,tsn-endpoint-ethernet-mac-2.0" for TSN.
+- reg			: Physical base address and size of the TSN registers map.
+- ranges		: Specifies child address ranges of TSN IP subsystem including
+			  TEMACs, endpoint and switch. Leave this property as empty
+			  because parent node has the same mapping as all the child nodes.
+Example:
+
+	tsn_endpoint_ip_0: tsn_endpoint_ip_0 {
+		compatible = "xlnx,tsn-endpoint-ethernet-mac-2.0";
+		reg = <0x0 0x80040000 0x0 0x40000>;
+		ranges;
+	};
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/Documentation/devicetree/bindings/staging/net/xilinx_tsn_ep.txt
--- /dev/null
+++ linux-xlnx-2023.1/Documentation/devicetree/bindings/staging/net/xilinx_tsn_ep.txt	2023-07-05 08:33:04.829544500 +0900
@@ -0,0 +1,55 @@
+Xilinx TSN (time sensitive networking) EndPoint Driver (xilinx_tsn_ep)
+-------------------------------------------------------------------------------
+
+Required properties:
+- compatible		: Should be "xlnx,tsn-ep"
+- reg			: Physical base address and size of the TSN Endpoint
+				registers map
+- interrupts		: Property with a value describing the interrupt
+- interrupts-names	: Property denotes the interrupt names.
+- interrupt-parent	: Must be core interrupt controller.
+- local-mac-address	: See ethernet.txt [1].
+
+Optional properties:
+- xlnx,num-tc		: Number of traffic class supported in current design,
+			  range is 2,3 and default value is 3. It denotes
+			  the traffic classes based on VLAN-PCP value.
+- xlnx,channel-ids 	: Queue Identifier associated with the MCDMA Channel, range
+			  is Tx: "1 to 2" and Rx: "2 to 5", default value is "1 to 5".
+- xlnx,eth-hasnobuf	: Used when 1G MAC is configured in non processor mode.
+- axistream-connected-rx	: Reference to another node which contains the
+				  resources for the MCDMA controller used by
+				  this device. The MCDMA related resources
+				  (registers and interrupts) will be used from
+				  this node.
+
+Optional TADMA properties:
+- xlnx,num-buffers-per-stream	: Number of TADMA buffers per stream in design. Default is 64.
+- xlnx,num-streams		: Number of streams. Default is 8.
+- xlnx,num-fetch-entries	: Maximum number of entries that can be programmed to be
+				  fetched by TADMA. Default is 8.
+- axistream-connected-tx	: Reference to another node which contains the
+				  resources for the TADMA controller used by
+				  this device. The TADMA-related resources
+				  (registers and interrupts) will be used from
+				  this node.
+
+Optional MCDMA properties:
+- xlnx,num-mm2s-channels	: Number of MM2S(read) channels in MCDMA
+- xlnx,num-s2mm-channels	: Number of S2MM(write) channels in MCDMA
+
+[1] Documentation/devicetree/bindings/net/ethernet.txt
+
+Example:
+
+	tsn_ep: tsn_ep@80056000 {
+		compatible = "xlnx,tsn-ep";
+		reg = <0x0 0x80056000 0x0 0xA000>;
+		xlnx,num-tc = <0x3>;
+		interrupt-names = "tsn_ep_scheduler_irq";
+		interrupt-parent = <&gic>;
+		interrupts = <0 111 4>;
+		local-mac-address = [00 0A 35 00 01 10];
+		xlnx,channel-ids = "1","2","3","4","5";
+		xlnx,eth-hasnobuf ;
+	};
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/Documentation/devicetree/bindings/staging/net/xilinx_tsn_ep_ex.txt
--- /dev/null
+++ linux-xlnx-2023.1/Documentation/devicetree/bindings/staging/net/xilinx_tsn_ep_ex.txt	2023-07-05 08:33:04.829544500 +0900
@@ -0,0 +1,26 @@
+Xilinx TSN (time sensitive networking) Extended EndPoint Driver (xilinx_tsn_ep_ex)
+-------------------------------------------------------------------------------
+
+Required properties:
+- compatible		: Should be "xlnx,tsn-ex-ep"
+- reg			: Physical base address and size of the TSN Endpoint
+				registers map
+- local-mac-address	: See ethernet.txt [1].
+
+Optional properties:
+- packet-switch		: set to 1 when packet switching on ex-ep is
+			  enabled in the design.
+- tsn,endpoint		: This is a handle to the endpoint node.
+			  The necessary ep resource details are obtained
+			  from this reference.
+
+[1] Documentation/devicetree/bindings/net/ethernet.txt
+
+Example:
+
+	tsn_ep_ex: tsn_ep_ex@80056000 {
+		compatible = "xlnx,tsn-ex-ep";
+		reg = <0x0 0x80056000 0x0 0xA000>;
+		local-mac-address = [00 0A 35 00 01 20];
+		tsn,endpoint = <&tsn_ep>;
+	};
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/Documentation/devicetree/bindings/staging/net/xilinx_tsn_switch.txt
--- /dev/null
+++ linux-xlnx-2023.1/Documentation/devicetree/bindings/staging/net/xilinx_tsn_switch.txt	2023-07-05 08:33:04.845166200 +0900
@@ -0,0 +1,26 @@
+Xilinx TSN (time sensitive networking) Switch Driver (xilinx_tsn_switch)
+-----------------------------------------------------------------------------
+
+Required properties:
+- compatible		: Should be "xlnx,tsn-switch"
+- reg			: Physical base address and size of the TSN registers map.
+- xlnx,num-ports	: Number of network ports in subsystems. For ex., for an
+			  EP + Switch system to two TEMACs, this value should be 3
+			  (ep, temac1 and temac 2).
+
+Optional properties:
+- xlnx,num-tc		: Number of traffic class supported in current design,
+			  range is 2,3 and default value is 3. It denotes
+			  the traffic classes based on VLAN-PCP value.
+- xlnx,has-hwaddr-learning	: Denotes hardware address learning support
+- xlnx,has-inband-mgmt-tag	: Denotes inband management support
+
+Example:
+
+	epswitch: tsn_switch@80078000 {
+		compatible = "xlnx,tsn-switch";
+		reg = <0x0 0x80078000 0x0 0x4000>;
+		xlnx,num-tc = <0x3>;
+		xlnx,has-hwaddr-learning ;
+		xlnx,has-inband-mgmt-tag ;
+	};
--- linux-6.1.0/drivers/staging/Kconfig	2023-07-05 08:37:34.297918500 +0900
+++ linux-xlnx-2023.1/drivers/staging/Kconfig	2023-07-05 08:33:31.212635900 +0900
@@ -80,4 +80,22 @@
 
 source "drivers/staging/vme_user/Kconfig"
 
+source "drivers/staging/fclk/Kconfig"
+
+source "drivers/staging/xlnxsync/Kconfig"
+
+source "drivers/staging/xlnx_tsmux/Kconfig"
+
+source "drivers/staging/xroeframer/Kconfig"
+
+source "drivers/staging/xroetrafficgen/Kconfig"
+
+source "drivers/staging/uartlite-rs485/Kconfig"
+
+source "drivers/staging/xilinx-tsn/Kconfig"
+
+source "drivers/staging/xlnx_hdcp1x/Kconfig"
+
+source "drivers/staging/xilinx_hdcp/Kconfig"
+
 endif # STAGING
--- linux-6.1.0/drivers/staging/Makefile	2023-07-05 08:37:34.297918500 +0900
+++ linux-xlnx-2023.1/drivers/staging/Makefile	2023-07-05 08:33:31.228230600 +0900
@@ -29,3 +29,11 @@
 obj-$(CONFIG_XIL_AXIS_FIFO)	+= axis-fifo/
 obj-$(CONFIG_FIELDBUS_DEV)     += fieldbus/
 obj-$(CONFIG_QLGE)		+= qlge/
+obj-$(CONFIG_XILINX_FCLK)	+= fclk/
+obj-$(CONFIG_XLNX_HDCP1X_CIPHER)	+= xlnx_hdcp1x/
+obj-$(CONFIG_XLNX_SYNC)		+= xlnxsync/
+obj-$(CONFIG_XLNX_TSMUX)	+= xlnx_tsmux/
+obj-$(CONFIG_XROE_FRAMER)	+= xroeframer/
+obj-$(CONFIG_SERIAL_UARTLITE_RS485)	+= uartlite-rs485/
+obj-$(CONFIG_XILINX_TSN)	+= xilinx-tsn/
+obj-$(CONFIG_XILINX_HDCP_COMMON)	+= xilinx_hdcp/
--- linux-6.1.0/drivers/staging/fbtft/fbtft-core.c	2023-07-05 08:37:34.329157500 +0900
+++ linux-xlnx-2023.1/drivers/staging/fbtft/fbtft-core.c	2023-07-05 08:33:31.243851800 +0900
@@ -840,7 +840,7 @@
 		sprintf(text1, ", %zu KiB buffer memory", par->txbuf.len >> 10);
 	if (spi)
 		sprintf(text2, ", spi%d.%d at %d MHz", spi->master->bus_num,
-			spi->chip_select, spi->max_speed_hz / 1000000);
+			spi_get_chipselect(spi, 0), spi->max_speed_hz / 1000000);
 	dev_info(fb_info->dev,
 		 "%s frame buffer, %dx%d, %d KiB video memory%s, fps=%lu%s\n",
 		 fb_info->fix.id, fb_info->var.xres, fb_info->var.yres,
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/fclk/Kconfig
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/fclk/Kconfig	2023-07-05 08:33:31.260478400 +0900
@@ -0,0 +1,9 @@
+#
+# Xilinx PL clk enabler
+#
+
+config XILINX_FCLK
+	tristate "Xilinx PL clock enabler"
+	depends on COMMON_CLK && OF
+	help
+	  Support for the Xilinx fclk clock enabler.
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/fclk/Makefile
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/fclk/Makefile	2023-07-05 08:33:31.260478400 +0900
@@ -0,0 +1 @@
+obj-$(CONFIG_XILINX_FCLK)	+= xilinx_fclk.o
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/fclk/TODO
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/fclk/TODO	2023-07-05 08:33:31.260478400 +0900
@@ -0,0 +1,2 @@
+TODO:
+	- Remove this hack and clock adapt all the drivers.
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/fclk/dt-binding.txt
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/fclk/dt-binding.txt	2023-07-05 08:33:31.260478400 +0900
@@ -0,0 +1,16 @@
+Binding for Xilinx pl clocks
+
+This binding uses the common clock binding[1].
+
+[1] Documentation/devicetree/bindings/clock/clock-bindings.txt
+
+Required properties:
+ - compatible: Must be 'xlnx,fclk'
+ - clocks: Handle to input clock
+
+Example:
+	fclk3: fclk3 {
+		status = "disabled";
+		compatible = "xlnx,fclk";
+		clocks = <&clkc 71>;
+	};
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/fclk/xilinx_fclk.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/fclk/xilinx_fclk.c	2023-07-05 08:33:31.260478400 +0900
@@ -0,0 +1,115 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx fclk clock driver.
+ * Copyright (c) 2017 - 2020 Xilinx Inc.
+ */
+
+#include <linux/clk.h>
+#include <linux/clk-provider.h>
+#include <linux/errno.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+
+struct fclk_state {
+	struct device	*dev;
+	struct clk	*pl;
+};
+
+/* Match table for of_platform binding */
+static const struct of_device_id fclk_of_match[] = {
+	{ .compatible = "xlnx,fclk",},
+	{ /* end of list */ },
+};
+MODULE_DEVICE_TABLE(of, fclk_of_match);
+
+static ssize_t set_rate_show(struct device *dev,
+			     struct device_attribute *attr, char *buf)
+{
+	struct fclk_state *st = dev_get_drvdata(dev);
+
+	return scnprintf(buf, PAGE_SIZE, "%lu\n", clk_get_rate(st->pl));
+}
+
+static ssize_t set_rate_store(struct device *dev,
+			      struct device_attribute *attr,
+			      const char *buf, size_t count)
+{
+	int ret = 0;
+	unsigned long rate;
+	struct fclk_state *st = dev_get_drvdata(dev);
+
+	ret = kstrtoul(buf, 0, &rate);
+	if (ret)
+		return -EINVAL;
+
+	rate = clk_round_rate(st->pl, rate);
+	ret = clk_set_rate(st->pl, rate);
+
+	return ret ? ret : count;
+}
+
+static DEVICE_ATTR_RW(set_rate);
+
+static const struct attribute *fclk_ctrl_attrs[] = {
+	&dev_attr_set_rate.attr,
+	NULL,
+};
+
+static const struct attribute_group fclk_ctrl_attr_grp = {
+	.attrs = (struct attribute **)fclk_ctrl_attrs,
+};
+
+static int fclk_probe(struct platform_device *pdev)
+{
+	struct fclk_state *st;
+	int ret;
+	struct device *dev = &pdev->dev;
+
+	st = devm_kzalloc(&pdev->dev, sizeof(*st), GFP_KERNEL);
+	if (!st)
+		return -ENOMEM;
+
+	st->dev = dev;
+	platform_set_drvdata(pdev, st);
+
+	st->pl = devm_clk_get(&pdev->dev, NULL);
+	if (IS_ERR(st->pl))
+		return PTR_ERR(st->pl);
+
+	ret = clk_prepare_enable(st->pl);
+	if (ret) {
+		dev_err(&pdev->dev, "Unable to enable clock.\n");
+		return ret;
+	}
+
+	ret = sysfs_create_group(&dev->kobj, &fclk_ctrl_attr_grp);
+	if (ret)
+		return ret;
+
+	return 0;
+}
+
+static int fclk_remove(struct platform_device *pdev)
+{
+	struct fclk_state *st = platform_get_drvdata(pdev);
+
+	clk_disable_unprepare(st->pl);
+	return 0;
+}
+
+static struct platform_driver fclk_driver = {
+	.driver = {
+		.name = KBUILD_MODNAME,
+		.of_match_table = fclk_of_match,
+	},
+	.probe		= fclk_probe,
+	.remove		= fclk_remove,
+};
+
+module_platform_driver(fclk_driver);
+
+MODULE_AUTHOR("Shubhrajyoti Datta <shubhrajyoti.datta@xilinx.com>");
+MODULE_DESCRIPTION("fclk enable");
+MODULE_LICENSE("GPL v2");
--- linux-6.1.0/drivers/staging/greybus/spilib.c	2023-07-05 08:37:34.360400500 +0900
+++ linux-xlnx-2023.1/drivers/staging/greybus/spilib.c	2023-07-05 08:33:31.295229400 +0900
@@ -237,7 +237,7 @@
 	request = operation->request->payload;
 	request->count = cpu_to_le16(count);
 	request->mode = dev->mode;
-	request->chip_select = dev->chip_select;
+	request->chip_select = spi_get_chipselect(dev, 0);
 
 	gb_xfer = &request->transfers[0];
 	tx_data = gb_xfer + count;	/* place tx data after last gb_xfer */
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/uartlite-rs485/Kconfig
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/uartlite-rs485/Kconfig	2023-07-05 08:33:32.140562600 +0900
@@ -0,0 +1,15 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Uartlite RS485 Serial device configuration
+#
+
+config SERIAL_UARTLITE_RS485
+	tristate "Xilinx uartlite rs485 serial port support"
+	depends on HAS_IOMEM
+	select SERIAL_CORE
+	help
+	  Say Y here if you want to use the Xilinx uartlite with rs485 serial controller.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called uartlite_485.
+
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/uartlite-rs485/Makefile
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/uartlite-rs485/Makefile	2023-07-05 08:33:32.140562600 +0900
@@ -0,0 +1,6 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Makefile for the kernel serial device drivers.
+#
+
+obj-$(CONFIG_SERIAL_UARTLITE_RS485) += uartlite-rs485.o
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/uartlite-rs485/uartlite-rs485.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/uartlite-rs485/uartlite-rs485.c	2023-07-05 08:33:32.140562600 +0900
@@ -0,0 +1,718 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * uartlite.c: Serial driver for Xilinx uartlite serial controller
+ *
+ * Copyright (C) 2006 Peter Korsgaard <jacmet@sunsite.dk>
+ * Copyright (C) 2007 Secret Lab Technologies Ltd.
+ */
+
+#include <linux/platform_device.h>
+#include <linux/module.h>
+#include <linux/console.h>
+#include <linux/serial.h>
+#include <linux/serial_core.h>
+#include <linux/tty.h>
+#include <linux/tty_flip.h>
+#include <linux/delay.h>
+#include <linux/interrupt.h>
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/iopoll.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/of_platform.h>
+#include <linux/clk.h>
+#include <linux/pm_runtime.h>
+
+#define ULITE_NAME		"ttyULR"
+#define ULITE_DRV_NAME		"uartlite_rs485"
+#define ULITE_MAJOR		205
+#define ULITE_MINOR		187
+#define ULITE_NR_UARTS		CONFIG_SERIAL_UARTLITE_NR_UARTS
+
+/* ---------------------------------------------------------------------
+ * Register definitions
+ *
+ * For register details see datasheet:
+ * https://www.xilinx.com/support/documentation/ip_documentation/opb_uartlite.pdf
+ */
+
+#define ULITE_RX		0x00
+#define ULITE_TX		0x04
+#define ULITE_STATUS		0x08
+#define ULITE_CONTROL		0x0c
+
+#define ULITE_REGION		16
+
+#define ULITE_STATUS_RXVALID	0x01
+#define ULITE_STATUS_RXFULL	0x02
+#define ULITE_STATUS_TXEMPTY	0x04
+#define ULITE_STATUS_TXFULL	0x08
+#define ULITE_STATUS_IE		0x10
+#define ULITE_STATUS_OVERRUN	0x20
+#define ULITE_STATUS_FRAME	0x40
+#define ULITE_STATUS_PARITY	0x80
+
+#define ULITE_CONTROL_RST_TX	0x01
+#define ULITE_CONTROL_RST_RX	0x02
+#define ULITE_CONTROL_IE	0x10
+#define UART_AUTOSUSPEND_TIMEOUT	3000	/* ms */
+
+struct uartlite_data {
+	const struct uartlite_reg_ops *reg_ops;
+	struct clk *clk;
+};
+
+struct uartlite_reg_ops {
+	u32 (*in)(void __iomem *addr);
+	void (*out)(u32 val, void __iomem *addr);
+};
+
+static u32 uartlite_inbe32(void __iomem *addr)
+{
+	return ioread32be(addr);
+}
+
+static void uartlite_outbe32(u32 val, void __iomem *addr)
+{
+	iowrite32be(val, addr);
+}
+
+static const struct uartlite_reg_ops uartlite_be = {
+	.in = uartlite_inbe32,
+	.out = uartlite_outbe32,
+};
+
+static u32 uartlite_inle32(void __iomem *addr)
+{
+	return ioread32(addr);
+}
+
+static void uartlite_outle32(u32 val, void __iomem *addr)
+{
+	iowrite32(val, addr);
+}
+
+static const struct uartlite_reg_ops uartlite_le = {
+	.in = uartlite_inle32,
+	.out = uartlite_outle32,
+};
+
+static inline u32 uart_in32(u32 offset, struct uart_port *port)
+{
+	struct uartlite_data *pdata = port->private_data;
+
+	return pdata->reg_ops->in(port->membase + offset);
+}
+
+static inline void uart_out32(u32 val, u32 offset, struct uart_port *port)
+{
+	struct uartlite_data *pdata = port->private_data;
+
+	pdata->reg_ops->out(val, port->membase + offset);
+}
+
+static struct uart_port ulite_ports[ULITE_NR_UARTS];
+
+/* ---------------------------------------------------------------------
+ * Core UART driver operations
+ */
+
+static int ulite_receive(struct uart_port *port, int stat)
+{
+	struct tty_port *tport = &port->state->port;
+	unsigned char ch = 0;
+	char flag = TTY_NORMAL;
+
+	if ((stat & (ULITE_STATUS_RXVALID | ULITE_STATUS_OVERRUN
+		     | ULITE_STATUS_FRAME)) == 0)
+		return 0;
+
+	/* stats */
+	if (stat & ULITE_STATUS_RXVALID) {
+		port->icount.rx++;
+		ch = uart_in32(ULITE_RX, port);
+
+		if (stat & ULITE_STATUS_PARITY)
+			port->icount.parity++;
+	}
+
+	if (stat & ULITE_STATUS_OVERRUN)
+		port->icount.overrun++;
+
+	if (stat & ULITE_STATUS_FRAME)
+		port->icount.frame++;
+
+	/* drop byte with parity error if IGNPAR specified */
+	if (stat & port->ignore_status_mask & ULITE_STATUS_PARITY)
+		stat &= ~ULITE_STATUS_RXVALID;
+
+	stat &= port->read_status_mask;
+
+	if (stat & ULITE_STATUS_PARITY)
+		flag = TTY_PARITY;
+
+	stat &= ~port->ignore_status_mask;
+
+	if (stat & ULITE_STATUS_RXVALID)
+		tty_insert_flip_char(tport, ch, flag);
+
+	if (stat & ULITE_STATUS_FRAME)
+		tty_insert_flip_char(tport, 0, TTY_FRAME);
+
+	if (stat & ULITE_STATUS_OVERRUN)
+		tty_insert_flip_char(tport, 0, TTY_OVERRUN);
+
+	return 1;
+}
+
+static int ulite_transmit(struct uart_port *port, int stat)
+{
+	struct circ_buf *xmit  = &port->state->xmit;
+
+	if (stat & ULITE_STATUS_TXFULL)
+		return 0;
+
+	if (port->x_char) {
+		uart_out32(port->x_char, ULITE_TX, port);
+		port->x_char = 0;
+		port->icount.tx++;
+		return 1;
+	}
+
+	if (uart_circ_empty(xmit) || uart_tx_stopped(port))
+		return 0;
+
+	uart_out32(xmit->buf[xmit->tail], ULITE_TX, port);
+	xmit->tail = (xmit->tail + 1) & (UART_XMIT_SIZE - 1);
+	port->icount.tx++;
+
+	/* wake up */
+	if (uart_circ_chars_pending(xmit) < WAKEUP_CHARS)
+		uart_write_wakeup(port);
+
+	return 1;
+}
+
+static irqreturn_t ulite_isr(int irq, void *dev_id)
+{
+	struct uart_port *port = dev_id;
+	int stat, busy, n = 0;
+	unsigned long flags;
+
+	do {
+		spin_lock_irqsave(&port->lock, flags);
+		stat = uart_in32(ULITE_STATUS, port);
+		busy  = ulite_receive(port, stat);
+		busy |= ulite_transmit(port, stat);
+		spin_unlock_irqrestore(&port->lock, flags);
+		n++;
+	} while (busy);
+
+	/* work done? */
+	if (n > 1) {
+		tty_flip_buffer_push(&port->state->port);
+		return IRQ_HANDLED;
+	} else {
+		return IRQ_NONE;
+	}
+}
+
+static unsigned int ulite_tx_empty(struct uart_port *port)
+{
+	unsigned long flags;
+	unsigned int ret;
+
+	spin_lock_irqsave(&port->lock, flags);
+	ret = uart_in32(ULITE_STATUS, port);
+	spin_unlock_irqrestore(&port->lock, flags);
+
+	return ret & ULITE_STATUS_TXEMPTY ? TIOCSER_TEMT : 0;
+}
+
+static unsigned int ulite_get_mctrl(struct uart_port *port)
+{
+	return TIOCM_CTS | TIOCM_DSR | TIOCM_CAR;
+}
+
+static void ulite_set_mctrl(struct uart_port *port, unsigned int mctrl)
+{
+	/* N/A */
+}
+
+static void ulite_stop_tx(struct uart_port *port)
+{
+	/* N/A */
+}
+
+static void ulite_start_tx(struct uart_port *port)
+{
+	ulite_transmit(port, uart_in32(ULITE_STATUS, port));
+}
+
+static void ulite_stop_rx(struct uart_port *port)
+{
+	/* don't forward any more data (like !CREAD) */
+	port->ignore_status_mask = ULITE_STATUS_RXVALID | ULITE_STATUS_PARITY
+		| ULITE_STATUS_FRAME | ULITE_STATUS_OVERRUN;
+}
+
+static void ulite_break_ctl(struct uart_port *port, int ctl)
+{
+	/* N/A */
+}
+
+static int ulite_startup(struct uart_port *port)
+{
+	struct uartlite_data *pdata = port->private_data;
+	int ret;
+
+	ret = clk_enable(pdata->clk);
+	if (ret) {
+		dev_err(port->dev, "Failed to enable clock\n");
+		return ret;
+	}
+
+	ret = request_irq(port->irq, ulite_isr, IRQF_SHARED | IRQF_TRIGGER_RISING,
+			  ULITE_DRV_NAME, port);
+	if (ret)
+		return ret;
+
+	uart_out32(ULITE_CONTROL_RST_RX | ULITE_CONTROL_RST_TX,
+		   ULITE_CONTROL, port);
+	uart_out32(ULITE_CONTROL_IE, ULITE_CONTROL, port);
+
+	return 0;
+}
+
+static void ulite_shutdown(struct uart_port *port)
+{
+	struct uartlite_data *pdata = port->private_data;
+
+	uart_out32(0, ULITE_CONTROL, port);
+	free_irq(port->irq, port);
+	clk_disable(pdata->clk);
+}
+
+static void ulite_set_termios(struct uart_port *port, struct ktermios *termios,
+			      const struct ktermios *old)
+{
+	unsigned long flags;
+	unsigned int baud;
+
+	spin_lock_irqsave(&port->lock, flags);
+
+	port->read_status_mask = ULITE_STATUS_RXVALID | ULITE_STATUS_OVERRUN
+		| ULITE_STATUS_TXFULL;
+
+	if (termios->c_iflag & INPCK)
+		port->read_status_mask |=
+			ULITE_STATUS_PARITY | ULITE_STATUS_FRAME;
+
+	port->ignore_status_mask = 0;
+	if (termios->c_iflag & IGNPAR)
+		port->ignore_status_mask |= ULITE_STATUS_PARITY
+			| ULITE_STATUS_FRAME | ULITE_STATUS_OVERRUN;
+
+	/* ignore all characters if CREAD is not set */
+	if ((termios->c_cflag & CREAD) == 0)
+		port->ignore_status_mask |=
+			ULITE_STATUS_RXVALID | ULITE_STATUS_PARITY
+			| ULITE_STATUS_FRAME | ULITE_STATUS_OVERRUN;
+
+	/* update timeout */
+	baud = uart_get_baud_rate(port, termios, old, 0, 460800);
+	uart_update_timeout(port, termios->c_cflag, baud);
+
+	spin_unlock_irqrestore(&port->lock, flags);
+}
+
+static const char *ulite_type(struct uart_port *port)
+{
+	return port->type == PORT_UARTLITE ? ULITE_DRV_NAME : NULL;
+}
+
+static void ulite_release_port(struct uart_port *port)
+{
+	release_mem_region(port->mapbase, ULITE_REGION);
+	iounmap(port->membase);
+	port->membase = NULL;
+}
+
+static int ulite_request_port(struct uart_port *port)
+{
+	struct uartlite_data *pdata = port->private_data;
+	int ret;
+
+	pr_debug("ulite console: port=%p; port->mapbase=%llx\n",
+		 port, (unsigned long long)port->mapbase);
+
+	if (!request_mem_region(port->mapbase, ULITE_REGION, ULITE_DRV_NAME)) {
+		dev_err(port->dev, "Memory region busy\n");
+		return -EBUSY;
+	}
+
+	port->membase = ioremap(port->mapbase, ULITE_REGION);
+	if (!port->membase) {
+		dev_err(port->dev, "Unable to map registers\n");
+		release_mem_region(port->mapbase, ULITE_REGION);
+		return -EBUSY;
+	}
+
+	pdata->reg_ops = &uartlite_be;
+	uart_out32(ULITE_CONTROL_RST_TX, ULITE_CONTROL, port);
+	ret = uart_in32(ULITE_STATUS, port);
+	/* Endianness detection */
+	if ((ret & ULITE_STATUS_TXEMPTY) != ULITE_STATUS_TXEMPTY)
+		pdata->reg_ops = &uartlite_le;
+
+	return 0;
+}
+
+static void ulite_config_port(struct uart_port *port, int flags)
+{
+	if (!ulite_request_port(port))
+		port->type = PORT_UARTLITE;
+}
+
+static int ulite_verify_port(struct uart_port *port, struct serial_struct *ser)
+{
+	/* we don't want the core code to modify any port params */
+	return -EINVAL;
+}
+
+static void ulite_pm(struct uart_port *port, unsigned int state,
+		     unsigned int oldstate)
+{
+	int ret;
+
+	if (!state) {
+		ret = pm_runtime_get_sync(port->dev);
+		if (ret < 0)
+			dev_err(port->dev, "Failed to enable clocks\n");
+	} else {
+		pm_runtime_mark_last_busy(port->dev);
+		pm_runtime_put_autosuspend(port->dev);
+	}
+}
+
+static int ulite_config_rs485(struct uart_port *port,
+			      struct ktermios *termios,
+			      struct serial_rs485 *rs485conf)
+{
+	port->rs485 = *rs485conf;
+
+	if (rs485conf->flags & SER_RS485_ENABLED)
+		dev_dbg(port->dev, "Setting UART to RS485\n");
+	else
+		dev_dbg(port->dev, "Setting UART to RS232\n");
+
+	return 0;
+}
+
+static const struct uart_ops ulite_ops = {
+	.tx_empty	= ulite_tx_empty,
+	.set_mctrl	= ulite_set_mctrl,
+	.get_mctrl	= ulite_get_mctrl,
+	.stop_tx	= ulite_stop_tx,
+	.start_tx	= ulite_start_tx,
+	.stop_rx	= ulite_stop_rx,
+	.break_ctl	= ulite_break_ctl,
+	.startup	= ulite_startup,
+	.shutdown	= ulite_shutdown,
+	.set_termios	= ulite_set_termios,
+	.type		= ulite_type,
+	.release_port	= ulite_release_port,
+	.request_port	= ulite_request_port,
+	.config_port	= ulite_config_port,
+	.verify_port	= ulite_verify_port,
+	.pm		= ulite_pm,
+};
+
+static struct uart_driver ulite_uart_driver = {
+	.owner		= THIS_MODULE,
+	.driver_name	= ULITE_DRV_NAME,
+	.dev_name	= ULITE_NAME,
+	.major		= ULITE_MAJOR,
+	.minor		= ULITE_MINOR,
+	.nr		= ULITE_NR_UARTS,
+};
+
+/* ---------------------------------------------------------------------
+ * Port assignment functions (mapping devices to uart_port structures)
+ */
+
+/** ulite_assign: register a uartlite device with the driver
+ *
+ * @dev: pointer to device structure
+ * @id: requested id number.  Pass -1 for automatic port assignment
+ * @base: base address of uartlite registers
+ * @irq: irq number for uartlite
+ * @pdata: private data for uartlite
+ *
+ * Returns: 0 on success, <0 otherwise
+ */
+static int ulite_assign(struct device *dev, int id, phys_addr_t base, int irq,
+			struct uartlite_data *pdata)
+{
+	struct uart_port *port;
+	int rc;
+
+	/* if id = -1; then scan for a free id and use that */
+	if (id < 0) {
+		for (id = 0; id < ULITE_NR_UARTS; id++)
+			if (ulite_ports[id].mapbase == 0)
+				break;
+	}
+	if (id < 0 || id >= ULITE_NR_UARTS) {
+		dev_err(dev, "%s%i too large\n", ULITE_NAME, id);
+		return -EINVAL;
+	}
+
+	if ((ulite_ports[id].mapbase) && (ulite_ports[id].mapbase != base)) {
+		dev_err(dev, "cannot assign to %s%i; it is already in use\n",
+			ULITE_NAME, id);
+		return -EBUSY;
+	}
+
+	port = &ulite_ports[id];
+
+	spin_lock_init(&port->lock);
+	port->fifosize = 16;
+	port->regshift = 2;
+	port->iotype = UPIO_MEM;
+	port->iobase = 1; /* mark port in use */
+	port->mapbase = base;
+	port->membase = NULL;
+	port->ops = &ulite_ops;
+	port->irq = irq;
+	port->flags = UPF_BOOT_AUTOCONF;
+	port->dev = dev;
+	port->type = PORT_UNKNOWN;
+	port->line = id;
+	port->private_data = pdata;
+
+	port->rs485.flags |= SER_RS485_ENABLED;
+	port->rs485_config = ulite_config_rs485;
+
+	dev_set_drvdata(dev, port);
+
+	/* Register the port */
+	rc = uart_add_one_port(&ulite_uart_driver, port);
+	if (rc) {
+		dev_err(dev, "uart_add_one_port() failed; err=%i\n", rc);
+		port->mapbase = 0;
+		dev_set_drvdata(dev, NULL);
+		return rc;
+	}
+
+	return 0;
+}
+
+/** ulite_release: register a uartlite device with the driver
+ *
+ * @dev: pointer to device structure
+ */
+static int ulite_release(struct device *dev)
+{
+	struct uart_port *port = dev_get_drvdata(dev);
+	int rc = 0;
+
+	if (port) {
+		rc = uart_remove_one_port(&ulite_uart_driver, port);
+		dev_set_drvdata(dev, NULL);
+		port->mapbase = 0;
+	}
+
+	return rc;
+}
+
+/**
+ * ulite_suspend - Stop the device.
+ *
+ * @dev: handle to the device structure.
+ * Return: 0 always.
+ */
+static int __maybe_unused ulite_suspend(struct device *dev)
+{
+	struct uart_port *port = dev_get_drvdata(dev);
+
+	if (port)
+		uart_suspend_port(&ulite_uart_driver, port);
+
+	return 0;
+}
+
+/**
+ * ulite_resume - Resume the device.
+ *
+ * @dev: handle to the device structure.
+ * Return: 0 on success, errno otherwise.
+ */
+static int __maybe_unused ulite_resume(struct device *dev)
+{
+	struct uart_port *port = dev_get_drvdata(dev);
+
+	if (port)
+		uart_resume_port(&ulite_uart_driver, port);
+
+	return 0;
+}
+
+static int __maybe_unused ulite_runtime_suspend(struct device *dev)
+{
+	struct uart_port *port = dev_get_drvdata(dev);
+	struct uartlite_data *pdata = port->private_data;
+
+	clk_disable(pdata->clk);
+	return 0;
+};
+
+static int __maybe_unused ulite_runtime_resume(struct device *dev)
+{
+	struct uart_port *port = dev_get_drvdata(dev);
+	struct uartlite_data *pdata = port->private_data;
+	int ret;
+
+	ret = clk_enable(pdata->clk);
+	if (ret) {
+		dev_err(dev, "Cannot enable clock.\n");
+		return ret;
+	}
+	return 0;
+}
+
+/* ---------------------------------------------------------------------
+ * Platform bus binding
+ */
+
+static const struct dev_pm_ops ulite_pm_ops = {
+	SET_SYSTEM_SLEEP_PM_OPS(ulite_suspend, ulite_resume)
+	SET_RUNTIME_PM_OPS(ulite_runtime_suspend,
+			   ulite_runtime_resume, NULL)
+};
+
+#if defined(CONFIG_OF)
+/* Match table for of_platform binding */
+static const struct of_device_id ulite_of_match[] = {
+	{ .compatible = "xlnx,axi-uartlite-rs485", },
+	{}
+};
+MODULE_DEVICE_TABLE(of, ulite_of_match);
+#endif /* CONFIG_OF */
+
+static int ulite_probe(struct platform_device *pdev)
+{
+	struct resource *res;
+	struct uartlite_data *pdata;
+	int irq, ret;
+	int id = pdev->id;
+#ifdef CONFIG_OF
+	const __be32 *prop;
+
+	prop = of_get_property(pdev->dev.of_node, "port-number", NULL);
+	if (prop)
+		id = be32_to_cpup(prop);
+#endif
+	pdata = devm_kzalloc(&pdev->dev, sizeof(struct uartlite_data),
+			     GFP_KERNEL);
+	if (!pdata)
+		return -ENOMEM;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res)
+		return -ENODEV;
+
+	irq = platform_get_irq(pdev, 0);
+	if (irq < 0)
+		return irq;
+
+	pdata->clk = devm_clk_get(&pdev->dev, "s_axi_aclk");
+	if (IS_ERR(pdata->clk)) {
+		if (PTR_ERR(pdata->clk) != -ENOENT)
+			return PTR_ERR(pdata->clk);
+
+		/*
+		 * Clock framework support is optional, continue on
+		 * anyways if we don't find a matching clock.
+		 */
+		pdata->clk = NULL;
+	}
+
+	ret = clk_prepare_enable(pdata->clk);
+	if (ret) {
+		dev_err(&pdev->dev, "Failed to prepare clock\n");
+		return ret;
+	}
+
+	pm_runtime_use_autosuspend(&pdev->dev);
+	pm_runtime_set_autosuspend_delay(&pdev->dev, UART_AUTOSUSPEND_TIMEOUT);
+	pm_runtime_set_active(&pdev->dev);
+	pm_runtime_enable(&pdev->dev);
+
+	if (!ulite_uart_driver.state) {
+		dev_dbg(&pdev->dev, "uartlite: calling uart_register_driver()\n");
+		ret = uart_register_driver(&ulite_uart_driver);
+		if (ret < 0) {
+			dev_err(&pdev->dev, "Failed to register driver\n");
+			clk_disable_unprepare(pdata->clk);
+			return ret;
+		}
+	}
+
+	ret = ulite_assign(&pdev->dev, id, res->start, irq, pdata);
+
+	pm_runtime_mark_last_busy(&pdev->dev);
+	pm_runtime_put_autosuspend(&pdev->dev);
+
+	return ret;
+}
+
+static int ulite_remove(struct platform_device *pdev)
+{
+	struct uart_port *port = dev_get_drvdata(&pdev->dev);
+	struct uartlite_data *pdata = port->private_data;
+	int rc;
+
+	clk_disable_unprepare(pdata->clk);
+	rc = ulite_release(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+	pm_runtime_set_suspended(&pdev->dev);
+	pm_runtime_dont_use_autosuspend(&pdev->dev);
+	return rc;
+}
+
+static struct platform_driver ulite_platform_driver = {
+	.probe = ulite_probe,
+	.remove = ulite_remove,
+	.driver = {
+		.name  = ULITE_DRV_NAME,
+		.of_match_table = of_match_ptr(ulite_of_match),
+		.pm = &ulite_pm_ops,
+	},
+};
+
+/* ---------------------------------------------------------------------
+ * Module setup/teardown
+ */
+
+static int __init ulite_init(void)
+{
+	pr_debug("uartlite: calling platform_driver_register()\n");
+	return platform_driver_register(&ulite_platform_driver);
+}
+
+static void __exit ulite_exit(void)
+{
+	platform_driver_unregister(&ulite_platform_driver);
+	if (ulite_uart_driver.state)
+		uart_unregister_driver(&ulite_uart_driver);
+}
+
+module_init(ulite_init);
+module_exit(ulite_exit);
+
+MODULE_AUTHOR("Peter Korsgaard <jacmet@sunsite.dk>");
+MODULE_DESCRIPTION("Xilinx uartlite serial driver");
+MODULE_LICENSE("GPL");
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/Kconfig
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/Kconfig	2023-07-05 08:33:32.276559500 +0900
@@ -0,0 +1,68 @@
+# SPDX-License-Identifier: GPL-2.0-only
+#
+# Xilinx device configuration
+#
+
+if NET_VENDOR_XILINX
+
+config AXIENET_HAS_TADMA
+	bool "AxiEthernet is configured with TADMA"
+	depends on XILINX_TSN
+	help
+	  When hardware is generated with Axi Ethernet with TADMA select this option.
+	  This driver enables Time Aware DMA to support TSN Qbv clause accurately.
+	  It is used to fetch schedule traffic.
+	  It contains one queue.
+
+config XILINX_TSN
+	bool "Enable Xilinx's TSN IP"
+	select PHYLIB
+	help
+	  Enable Xilinx's TSN IP.
+
+config XILINX_TSN_PTP
+	bool "Generate hardware packet timestamps using Xilinx's TSN IP"
+	depends on XILINX_TSN
+	select PTP_1588_CLOCK
+	default y
+	help
+	  Generate hardware packet timestamps. This is to facilitate IEEE 1588.
+
+config XILINX_TSN_QBV
+	bool "Support Qbv protocol in TSN"
+	depends on XILINX_TSN_PTP
+	select PTP_1588_CLOCK
+	default y
+	help
+	  Enables TSN Qbv protocol.
+
+config XILINX_TSN_SWITCH
+	bool "Support TSN switch"
+	depends on XILINX_TSN
+	select NET_SWITCHDEV
+	default y
+	help
+	  Enable Xilinx's TSN Switch support.
+
+config XILINX_TSN_QCI
+	bool "Support Qci protocol in TSN"
+	depends on XILINX_TSN_SWITCH
+	default y
+	help
+	  Enable TSN QCI protocol.
+
+config XILINX_TSN_CB
+	bool "Support CB protocol in TSN"
+	depends on XILINX_TSN_SWITCH
+	default y
+	help
+	  Enable TSN CB protocol support.
+
+config XILINX_TSN_QBR
+	bool "Support QBR protocol in TSN"
+	depends on XILINX_TSN_SWITCH
+	default y
+	help
+	  Enable TSN QBR protocol support.
+
+endif # NET_VENDOR_XILINX
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/Makefile
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/Makefile	2023-07-05 08:33:32.276559500 +0900
@@ -0,0 +1,14 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Makefile for the Xilinx network device drivers.
+#
+
+xilinx_emac_tsn-objs := xilinx_axienet_main_tsn.o xilinx_axienet_mcdma_tsn.o xilinx_axienet_mdio_tsn.o
+obj-$(CONFIG_XILINX_TSN) += xilinx_tsn_ep.o xilinx_tsn_ep_ex.o xilinx_tsn_ip.o xilinx_emac_tsn.o
+obj-$(CONFIG_XILINX_TSN_PTP) += xilinx_tsn_ptp_xmit.o xilinx_tsn_ptp_clock.o
+obj-$(CONFIG_XILINX_TSN_QBV) += xilinx_tsn_shaper.o
+obj-$(CONFIG_XILINX_TSN_QCI) += xilinx_tsn_qci.o
+obj-$(CONFIG_XILINX_TSN_CB) += xilinx_tsn_cb.o
+obj-$(CONFIG_XILINX_TSN_SWITCH) += xilinx_tsn_switch.o xilinx_tsn_switchdev.o
+obj-$(CONFIG_XILINX_TSN_QBR) += xilinx_tsn_preemption.o
+obj-$(CONFIG_AXIENET_HAS_TADMA) += xilinx_tsn_tadma.o
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_axienet_main_tsn.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_axienet_main_tsn.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,2032 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx Axi Ethernet device driver
+ *
+ * Copyright (c) 2008 Nissin Systems Co., Ltd.,  Yoshio Kashiwagi
+ * Copyright (c) 2005-2008 DLA Systems,  David H. Lynch Jr. <dhlii@dlasys.net>
+ * Copyright (c) 2008-2009 Secret Lab Technologies Ltd.
+ * Copyright (c) 2010 - 2011 Michal Simek <monstr@monstr.eu>
+ * Copyright (c) 2010 - 2011 PetaLogix
+ * Copyright (c) 2019 SED Systems, a division of Calian Ltd.
+ * Copyright (c) 2010 - 2012 Xilinx, Inc. All rights reserved.
+ *
+ * This is a driver for the Xilinx Axi Ethernet which is used in the Virtex6
+ * and Spartan6.
+ *
+ * TODO:
+ *  - Add Axi Fifo support.
+ *  - Factor out Axi DMA code into separate driver.
+ *  - Test and fix basic multicast filtering.
+ *  - Add support for extended multicast filtering.
+ *  - Test basic VLAN support.
+ *  - Add support for extended VLAN support.
+ */
+
+#include <linux/clk.h>
+#include <linux/circ_buf.h>
+#include <linux/delay.h>
+#include <linux/etherdevice.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/of_mdio.h>
+#include <linux/of_net.h>
+#include <linux/of_platform.h>
+#include <linux/of_irq.h>
+#include <linux/of_address.h>
+#include <linux/skbuff.h>
+#include <linux/spinlock.h>
+#include <linux/phy.h>
+#include <linux/mii.h>
+#include <linux/ethtool.h>
+#include <linux/iopoll.h>
+#include <linux/ptp_classify.h>
+#include <linux/net_tstamp.h>
+#include <linux/random.h>
+#include <net/sock.h>
+#include <linux/xilinx_phy.h>
+#include <linux/clk.h>
+
+#include "xilinx_axienet_tsn.h"
+
+/* Descriptors defines for Tx and Rx DMA */
+#define TX_BD_NUM_DEFAULT		64
+#define RX_BD_NUM_DEFAULT		128
+#define TX_BD_NUM_MAX			4096
+#define RX_BD_NUM_MAX			4096
+
+/* Must be shorter than length of ethtool_drvinfo.driver field to fit */
+#define DRIVER_NAME		"xaxienet"
+#define DRIVER_DESCRIPTION	"Xilinx Axi Ethernet driver"
+#define DRIVER_VERSION		"1.00a"
+
+#define AXIENET_REGS_N		40
+#define AXIENET_TS_HEADER_LEN	8
+#define XXVENET_TS_HEADER_LEN	4
+#define MRMAC_TS_HEADER_LEN		16
+#define MRMAC_TS_HEADER_WORDS   (MRMAC_TS_HEADER_LEN / 4)
+#define NS_PER_SEC              1000000000ULL /* Nanoseconds per second */
+
+#define MRMAC_RESET_DELAY	1 /* Delay in msecs*/
+
+/* IEEE1588 Message Type field values  */
+#define PTP_TYPE_SYNC		0
+#define PTP_TYPE_PDELAY_REQ	2
+#define PTP_TYPE_PDELAY_RESP	3
+#define PTP_TYPE_OFFSET		42
+/* SW flags used to convey message type for command FIFO handling */
+#define MSG_TYPE_SHIFT			4
+#define MSG_TYPE_SYNC_FLAG		((PTP_TYPE_SYNC + 1) << MSG_TYPE_SHIFT)
+#define MSG_TYPE_PDELAY_RESP_FLAG	((PTP_TYPE_PDELAY_RESP + 1) << \
+									 MSG_TYPE_SHIFT)
+
+#define FILTER_SELECT		0x100	   /* Filter select */
+#define ETHERTYPE_FILTER_IPV4	0x00000008 /* Ethertype field 0x08 for IPv4 packets */
+#define ETHERTYPE_FILTER_PTP    0x0000F788 /* Ethertype field 0x88F7 for PTP packets */
+#define PROTO_FILTER_UDP	0x11000000 /* protocol field 0x11 for UDP packets */
+#define PTP_UDP_PORT		0x00003F01 /* dest port field 0x013F for PTP over UDP packes */
+#define PTP_VERSION		0x02000000 /* PTPv2 */
+
+#define DESTMAC_FILTER_ENABLE_MASK_MSB		0xFFFFFFFF /* Enable filtering for bytes 0-3 in a
+							    * packet corresponding to 4 Most
+							    * significant bytes of
+							    * Destination MAC address
+							    */
+#define DESTMAC_FILTER_ENABLE_MASK_LSB		0xFF000000 /* Enable filtering for bytes
+							    * 4-5 in a packet
+							    * corresponding to 2 Least
+							    * significant bytes of
+							    * Destination MAC address
+							    */
+#define PROTO_FILTER_DISABLE_MASK		0x0 /* Disable protocol based filtering */
+#define PORT_NUM_FILTER_DISABLE_MASK		0x0 /* Disable port number based filtering */
+#define VERSION_FILTER_DISABLE_MASK		0x0 /* Disable filtering based on PTP version */
+
+#define DESTMAC_FILTER_DISABLE_MASK_MSB		0 /* Disable Dest MAC address filtering(4 MSB'S) */
+#define DESTMAC_FILTER_DISABLE_MASK_LSB		0 /* Disable Dest MAC address filtering(2 LSB's) */
+#define PROTO_FILTER_ENABLE_MASK		0xFF000000 /* Enable protocol based filtering */
+#define PORT_NUM_FILTER_ENABLE_MASK		0x0000FFFF /* Enable port number based filtering */
+#define VERSION_FILTER_ENABLE_MASK		0xFF000000 /* Enable PTP version based filtering */
+
+#ifdef CONFIG_XILINX_TSN_PTP
+int axienet_phc_index = -1;
+EXPORT_SYMBOL(axienet_phc_index);
+#endif
+
+/* Option table for setting up Axi Ethernet hardware options */
+static struct axienet_option axienet_options[] = {
+	/* Turn on jumbo packet support for both Rx and Tx */
+	{
+		.opt = XAE_OPTION_JUMBO,
+		.reg = XAE_TC_OFFSET,
+		.m_or = XAE_TC_JUM_MASK,
+	}, {
+		.opt = XAE_OPTION_JUMBO,
+		.reg = XAE_RCW1_OFFSET,
+		.m_or = XAE_RCW1_JUM_MASK,
+	}, { /* Turn on VLAN packet support for both Rx and Tx */
+		.opt = XAE_OPTION_VLAN,
+		.reg = XAE_TC_OFFSET,
+		.m_or = XAE_TC_VLAN_MASK,
+	}, {
+		.opt = XAE_OPTION_VLAN,
+		.reg = XAE_RCW1_OFFSET,
+		.m_or = XAE_RCW1_VLAN_MASK,
+	}, { /* Turn on FCS stripping on receive packets */
+		.opt = XAE_OPTION_FCS_STRIP,
+		.reg = XAE_RCW1_OFFSET,
+		.m_or = XAE_RCW1_FCS_MASK,
+	}, { /* Turn on FCS insertion on transmit packets */
+		.opt = XAE_OPTION_FCS_INSERT,
+		.reg = XAE_TC_OFFSET,
+		.m_or = XAE_TC_FCS_MASK,
+	}, { /* Turn off length/type field checking on receive packets */
+		.opt = XAE_OPTION_LENTYPE_ERR,
+		.reg = XAE_RCW1_OFFSET,
+		.m_or = XAE_RCW1_LT_DIS_MASK,
+	}, { /* Turn on Rx flow control */
+		.opt = XAE_OPTION_FLOW_CONTROL,
+		.reg = XAE_FCC_OFFSET,
+		.m_or = XAE_FCC_FCRX_MASK,
+	}, { /* Turn on Tx flow control */
+		.opt = XAE_OPTION_FLOW_CONTROL,
+		.reg = XAE_FCC_OFFSET,
+		.m_or = XAE_FCC_FCTX_MASK,
+	}, { /* Turn on promiscuous frame filtering */
+		.opt = XAE_OPTION_PROMISC,
+		.reg = XAE_FMC_OFFSET,
+		.m_or = XAE_FMC_PM_MASK,
+	}, { /* Enable transmitter */
+		.opt = XAE_OPTION_TXEN,
+		.reg = XAE_TC_OFFSET,
+		.m_or = XAE_TC_TX_MASK,
+	}, { /* Enable receiver */
+		.opt = XAE_OPTION_RXEN,
+		.reg = XAE_RCW1_OFFSET,
+		.m_or = XAE_RCW1_RX_MASK,
+	},
+	{}
+};
+
+struct axienet_ethtools_stat {
+	const char *name;
+};
+
+static struct axienet_ethtools_stat axienet_get_ethtools_strings_stats[] = {
+	{ "tx_packets" },
+	{ "rx_packets" },
+	{ "tx_bytes" },
+	{ "rx_bytes" },
+	{ "tx_errors" },
+	{ "rx_errors" },
+};
+
+/**
+ * axienet_dma_bd_release_tsn - Release buffer descriptor rings
+ * @ndev:	Pointer to the net_device structure
+ *
+ * This function is used to release the descriptors allocated in
+ * axienet_dma_bd_init. axienet_dma_bd_release is called when Axi Ethernet
+ * driver stop api is called.
+ */
+void axienet_dma_bd_release_tsn(struct net_device *ndev)
+{
+	int i;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	for_each_tx_dma_queue(lp, i) {
+		axienet_mcdma_tx_bd_free_tsn(ndev, lp->dq[i]);
+	}
+	for_each_rx_dma_queue(lp, i) {
+		axienet_mcdma_rx_bd_free_tsn(ndev, lp->dq[i]);
+	}
+}
+
+/**
+ * axienet_set_mac_address_tsn - Write the MAC address
+ * @ndev:	Pointer to the net_device structure
+ * @address:	6 byte Address to be written as MAC address
+ *
+ * This function is called to initialize the MAC address of the Axi Ethernet
+ * core. It writes to the UAW0 and UAW1 registers of the core.
+ */
+void axienet_set_mac_address_tsn(struct net_device *ndev,
+				 const void *address)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (address)
+		eth_hw_addr_set(ndev, address);
+	if (!is_valid_ether_addr(ndev->dev_addr))
+		eth_hw_addr_random(ndev);
+
+	if (lp->axienet_config->mactype != XAXIENET_1G &&
+	    lp->axienet_config->mactype != XAXIENET_2_5G)
+		return;
+
+	/* Set up unicast MAC address filter set its mac address */
+	axienet_iow(lp, XAE_UAW0_OFFSET,
+		    (ndev->dev_addr[0]) |
+		    (ndev->dev_addr[1] << 8) |
+		    (ndev->dev_addr[2] << 16) |
+		    (ndev->dev_addr[3] << 24));
+	axienet_iow(lp, XAE_UAW1_OFFSET,
+		    (((axienet_ior(lp, XAE_UAW1_OFFSET)) &
+		      ~XAE_UAW1_UNICASTADDR_MASK) |
+		     (ndev->dev_addr[4] |
+		     (ndev->dev_addr[5] << 8))));
+}
+
+/**
+ * netdev_set_mac_address - Write the MAC address (from outside the driver)
+ * @ndev:	Pointer to the net_device structure
+ * @p:		6 byte Address to be written as MAC address
+ *
+ * Return: 0 for all conditions. Presently, there is no failure case.
+ *
+ * This function is called to initialize the MAC address of the Axi Ethernet
+ * core. It calls the core specific axienet_set_mac_address_tsn. This is the
+ * function that goes into net_device_ops structure entry ndo_set_mac_address.
+ */
+static int netdev_set_mac_address(struct net_device *ndev, void *p)
+{
+	struct sockaddr *addr = p;
+
+	axienet_set_mac_address_tsn(ndev, addr->sa_data);
+	return 0;
+}
+
+/**
+ * axienet_set_multicast_list_tsn - Prepare the multicast table
+ * @ndev:	Pointer to the net_device structure
+ *
+ * This function is called to initialize the multicast table during
+ * initialization. The Axi Ethernet basic multicast support has a four-entry
+ * multicast table which is initialized here. Additionally this function
+ * goes into the net_device_ops structure entry ndo_set_multicast_list. This
+ * means whenever the multicast table entries need to be updated this
+ * function gets called.
+ */
+void axienet_set_multicast_list_tsn(struct net_device *ndev)
+{
+	int i;
+	u32 reg, af0reg, af1reg;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (lp->axienet_config->mactype != XAXIENET_1G || lp->eth_hasnobuf)
+		return;
+
+	if (ndev->flags & (IFF_ALLMULTI | IFF_PROMISC) ||
+	    netdev_mc_count(ndev) > XAE_MULTICAST_CAM_TABLE_NUM) {
+		/* We must make the kernel realize we had to move into
+		 * promiscuous mode. If it was a promiscuous mode request
+		 * the flag is already set. If not we set it.
+		 */
+		ndev->flags |= IFF_PROMISC;
+		reg = axienet_ior(lp, XAE_FMC_OFFSET);
+		reg |= XAE_FMC_PM_MASK;
+		axienet_iow(lp, XAE_FMC_OFFSET, reg);
+		dev_info(&ndev->dev, "Promiscuous mode enabled.\n");
+	} else if (!netdev_mc_empty(ndev)) {
+		struct netdev_hw_addr *ha;
+
+		i = 0;
+		netdev_for_each_mc_addr(ha, ndev) {
+			if (i >= XAE_MULTICAST_CAM_TABLE_NUM)
+				break;
+
+			af0reg = (ha->addr[0]);
+			af0reg |= (ha->addr[1] << 8);
+			af0reg |= (ha->addr[2] << 16);
+			af0reg |= (ha->addr[3] << 24);
+
+			af1reg = (ha->addr[4]);
+			af1reg |= (ha->addr[5] << 8);
+
+			reg = axienet_ior(lp, XAE_FMC_OFFSET) & 0xFFFFFF00;
+			reg |= i;
+
+			axienet_iow(lp, XAE_FMC_OFFSET, reg);
+			axienet_iow(lp, XAE_AF0_OFFSET, af0reg);
+			axienet_iow(lp, XAE_AF1_OFFSET, af1reg);
+			i++;
+		}
+	} else {
+		reg = axienet_ior(lp, XAE_FMC_OFFSET);
+		reg &= ~XAE_FMC_PM_MASK;
+
+		axienet_iow(lp, XAE_FMC_OFFSET, reg);
+
+		for (i = 0; i < XAE_MULTICAST_CAM_TABLE_NUM; i++) {
+			reg = axienet_ior(lp, XAE_FMC_OFFSET) & 0xFFFFFF00;
+			reg |= i;
+
+			axienet_iow(lp, XAE_FMC_OFFSET, reg);
+			axienet_iow(lp, XAE_AF0_OFFSET, 0);
+			axienet_iow(lp, XAE_AF1_OFFSET, 0);
+		}
+
+		dev_info(&ndev->dev, "Promiscuous mode disabled.\n");
+	}
+}
+
+/**
+ * axienet_setoptions_tsn - Set an Axi Ethernet option
+ * @ndev:	Pointer to the net_device structure
+ * @options:	Option to be enabled/disabled
+ *
+ * The Axi Ethernet core has multiple features which can be selectively turned
+ * on or off. The typical options could be jumbo frame option, basic VLAN
+ * option, promiscuous mode option etc. This function is used to set or clear
+ * these options in the Axi Ethernet hardware. This is done through
+ * axienet_option structure .
+ */
+void axienet_setoptions_tsn(struct net_device *ndev, u32 options)
+{
+	int reg;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_option *tp = &axienet_options[0];
+
+	while (tp->opt) {
+		reg = ((axienet_ior(lp, tp->reg)) & ~(tp->m_or));
+		if (options & tp->opt)
+			reg |= tp->m_or;
+		axienet_iow(lp, tp->reg, reg);
+		tp++;
+	}
+
+	lp->options |= options;
+}
+
+void __axienet_device_reset_tsn(struct axienet_dma_q *q)
+{
+	u32 timeout;
+	/* Reset Axi DMA. This would reset Axi Ethernet core as well. The reset
+	 * process of Axi DMA takes a while to complete as all pending
+	 * commands/transfers will be flushed or completed during this
+	 * reset process.
+	 * Note that even though both TX and RX have their own reset register,
+	 * they both reset the entire DMA core, so only one needs to be used.
+	 */
+	axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET, XAXIDMA_CR_RESET_MASK);
+	timeout = DELAY_OF_ONE_MILLISEC;
+	while (axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET) &
+				XAXIDMA_CR_RESET_MASK) {
+		udelay(1);
+		if (--timeout == 0) {
+			netdev_err(q->lp->ndev, "%s: DMA reset timeout!\n",
+				   __func__);
+			break;
+		}
+	}
+}
+
+/**
+ * axienet_adjust_link_tsn - Adjust the PHY link speed/duplex.
+ * @ndev:	Pointer to the net_device structure
+ *
+ * This function is called to change the speed and duplex setting after
+ * auto negotiation is done by the PHY. This is the function that gets
+ * registered with the PHY interface through the "of_phy_connect" call.
+ */
+void axienet_adjust_link_tsn(struct net_device *ndev)
+{
+	u32 emmc_reg;
+	u32 link_state;
+	u32 setspeed = 1;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct phy_device *phy = ndev->phydev;
+
+	link_state = phy->speed | (phy->duplex << 1) | phy->link;
+	if (lp->last_link != link_state) {
+		if (phy->speed == SPEED_10 || phy->speed == SPEED_100) {
+			if (lp->phy_mode == PHY_INTERFACE_MODE_1000BASEX)
+				setspeed = 0;
+		} else {
+			if (phy->speed == SPEED_1000 &&
+			    lp->phy_mode == PHY_INTERFACE_MODE_MII)
+				setspeed = 0;
+		}
+
+		if (setspeed == 1) {
+			emmc_reg = axienet_ior(lp, XAE_EMMC_OFFSET);
+			emmc_reg &= ~XAE_EMMC_LINKSPEED_MASK;
+
+			switch (phy->speed) {
+			case SPEED_2500:
+				emmc_reg |= XAE_EMMC_LINKSPD_2500;
+				break;
+			case SPEED_1000:
+				emmc_reg |= XAE_EMMC_LINKSPD_1000;
+				break;
+			case SPEED_100:
+				emmc_reg |= XAE_EMMC_LINKSPD_100;
+				break;
+			case SPEED_10:
+				emmc_reg |= XAE_EMMC_LINKSPD_10;
+				break;
+			default:
+				dev_err(&ndev->dev, "Speed other than 10, 100 ");
+				dev_err(&ndev->dev, "or 1Gbps is not supported\n");
+				break;
+			}
+
+			axienet_iow(lp, XAE_EMMC_OFFSET, emmc_reg);
+			phy_print_status(phy);
+		} else {
+			netdev_err(ndev,
+				   "Error setting Axi Ethernet mac speed\n");
+		}
+
+		lp->last_link = link_state;
+	}
+}
+
+/**
+ * axienet_start_xmit_done_tsn - Invoked once a transmit is completed by the
+ * Axi DMA Tx channel.
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * This function is invoked from the Axi DMA Tx isr to notify the completion
+ * of transmit operation. It clears fields in the corresponding Tx BDs and
+ * unmaps the corresponding buffer so that CPU can regain ownership of the
+ * buffer. It finally invokes "netif_wake_queue" to restart transmission if
+ * required.
+ */
+void axienet_start_xmit_done_tsn(struct net_device *ndev,
+				 struct axienet_dma_q *q)
+{
+	u32 size = 0;
+	u32 packets = 0;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct aximcdma_bd *cur_p;
+	unsigned int status = 0;
+
+	cur_p = &q->txq_bd_v[q->tx_bd_ci];
+	status = cur_p->sband_stats;
+	while (status & XAXIDMA_BD_STS_COMPLETE_MASK) {
+		if (cur_p->tx_desc_mapping == DESC_DMA_MAP_PAGE)
+			dma_unmap_page(ndev->dev.parent, cur_p->phys,
+				       cur_p->cntrl &
+				       XAXIDMA_BD_CTRL_LENGTH_MASK,
+				       DMA_TO_DEVICE);
+		else
+			dma_unmap_single(ndev->dev.parent, cur_p->phys,
+					 cur_p->cntrl &
+					 XAXIDMA_BD_CTRL_LENGTH_MASK,
+					 DMA_TO_DEVICE);
+		if (cur_p->tx_skb)
+			dev_kfree_skb_irq((struct sk_buff *)cur_p->tx_skb);
+		/*cur_p->phys = 0;*/
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		cur_p->app2 = 0;
+		cur_p->app4 = 0;
+		cur_p->status = 0;
+		cur_p->tx_skb = 0;
+		cur_p->sband_stats = 0;
+
+		size += status & XAXIDMA_BD_STS_ACTUAL_LEN_MASK;
+		packets++;
+
+		if (++q->tx_bd_ci >= lp->tx_bd_num)
+			q->tx_bd_ci = 0;
+		cur_p = &q->txq_bd_v[q->tx_bd_ci];
+		status = cur_p->sband_stats;
+	}
+
+	ndev->stats.tx_packets += packets;
+	ndev->stats.tx_bytes += size;
+	q->tx_packets += packets;
+	q->tx_bytes += size;
+
+	/* Matches barrier in axienet_start_xmit */
+	smp_mb();
+
+	/* Fixme: With the existing multiqueue implementation
+	 * in the driver it is difficult to get the exact queue info.
+	 * We should wake only the particular queue
+	 * instead of waking all ndev queues.
+	 */
+	netif_tx_wake_all_queues(ndev);
+}
+
+/**
+ * axienet_check_tx_bd_space - Checks if a BD/group of BDs are currently busy
+ * @q:		Pointer to DMA queue structure
+ * @num_frag:	The number of BDs to check for
+ *
+ * Return: 0, on success
+ *	    NETDEV_TX_BUSY, if any of the descriptors are not free
+ *
+ * This function is invoked before BDs are allocated and transmission starts.
+ * This function returns 0 if a BD or group of BDs can be allocated for
+ * transmission. If the BD or any of the BDs are not free the function
+ * returns a busy status. This is invoked from axienet_start_xmit.
+ */
+static inline int axienet_check_tx_bd_space(struct axienet_dma_q *q,
+					    int num_frag)
+{
+	struct axienet_local *lp = q->lp;
+	struct aximcdma_bd *cur_p;
+
+	if (CIRC_SPACE(q->tx_bd_tail, q->tx_bd_ci, lp->tx_bd_num) < (num_frag + 1))
+		return NETDEV_TX_BUSY;
+
+	cur_p = &q->txq_bd_v[(q->tx_bd_tail + num_frag) % lp->tx_bd_num];
+	if (cur_p->sband_stats & XMCDMA_BD_STS_ALL_MASK)
+		return NETDEV_TX_BUSY;
+	return 0;
+}
+
+int axienet_queue_xmit_tsn(struct sk_buff *skb,
+			   struct net_device *ndev, u16 map)
+{
+	u32 ii;
+	u32 num_frag;
+	u32 csum_start_off;
+	u32 csum_index_off;
+	dma_addr_t tail_p;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct aximcdma_bd *cur_p;
+	unsigned long flags;
+	struct axienet_dma_q *q;
+
+	num_frag = skb_shinfo(skb)->nr_frags;
+
+	q = lp->dq[map];
+
+	cur_p = &q->txq_bd_v[q->tx_bd_tail];
+	spin_lock_irqsave(&q->tx_lock, flags);
+	if (axienet_check_tx_bd_space(q, num_frag)) {
+		if (netif_queue_stopped(ndev)) {
+			spin_unlock_irqrestore(&q->tx_lock, flags);
+			return NETDEV_TX_BUSY;
+		}
+
+		netif_stop_queue(ndev);
+
+		/* Matches barrier in axienet_start_xmit_done_tsn */
+		smp_mb();
+
+		/* Space might have just been freed - check again */
+		if (axienet_check_tx_bd_space(q, num_frag)) {
+			spin_unlock_irqrestore(&q->tx_lock, flags);
+			return NETDEV_TX_BUSY;
+		}
+
+		netif_wake_queue(ndev);
+	}
+
+	if (skb->ip_summed == CHECKSUM_PARTIAL && !lp->eth_hasnobuf &&
+	    lp->axienet_config->mactype == XAXIENET_1G) {
+		if (lp->features & XAE_FEATURE_FULL_TX_CSUM) {
+			/* Tx Full Checksum Offload Enabled */
+			cur_p->app0 |= 2;
+		} else if (lp->features & XAE_FEATURE_PARTIAL_RX_CSUM) {
+			csum_start_off = skb_transport_offset(skb);
+			csum_index_off = csum_start_off + skb->csum_offset;
+			/* Tx Partial Checksum Offload Enabled */
+			cur_p->app0 |= 1;
+			cur_p->app1 = (csum_start_off << 16) | csum_index_off;
+		}
+	} else if (skb->ip_summed == CHECKSUM_UNNECESSARY &&
+		   !lp->eth_hasnobuf &&
+		   (lp->axienet_config->mactype == XAXIENET_1G)) {
+		cur_p->app0 |= 2; /* Tx Full Checksum Offload Enabled */
+	}
+
+	cur_p->cntrl = (skb_headlen(skb) | XMCDMA_BD_CTRL_TXSOF_MASK);
+
+	if (!q->eth_hasdre &&
+	    (((phys_addr_t)skb->data & 0x3) || num_frag > 0)) {
+		skb_copy_and_csum_dev(skb, q->tx_buf[q->tx_bd_tail]);
+
+		cur_p->phys = q->tx_bufs_dma +
+			      (q->tx_buf[q->tx_bd_tail] - q->tx_bufs);
+
+		cur_p->cntrl = skb_pagelen(skb) | XMCDMA_BD_CTRL_TXSOF_MASK;
+		goto out;
+	} else {
+		cur_p->phys = dma_map_single(ndev->dev.parent, skb->data,
+					     skb_headlen(skb), DMA_TO_DEVICE);
+		if (unlikely(dma_mapping_error(ndev->dev.parent, cur_p->phys))) {
+			cur_p->phys = 0;
+			spin_unlock_irqrestore(&q->tx_lock, flags);
+			dev_err(&ndev->dev, "TX buffer map failed\n");
+			return NETDEV_TX_BUSY;
+		}
+	}
+	cur_p->tx_desc_mapping = DESC_DMA_MAP_SINGLE;
+
+	for (ii = 0; ii < num_frag; ii++) {
+		u32 len;
+		skb_frag_t *frag;
+
+		if (++q->tx_bd_tail >= lp->tx_bd_num)
+			q->tx_bd_tail = 0;
+
+		cur_p = &q->txq_bd_v[q->tx_bd_tail];
+		frag = &skb_shinfo(skb)->frags[ii];
+		len = skb_frag_size(frag);
+		cur_p->phys = skb_frag_dma_map(ndev->dev.parent, frag, 0, len,
+					       DMA_TO_DEVICE);
+		cur_p->cntrl = len;
+		cur_p->tx_desc_mapping = DESC_DMA_MAP_PAGE;
+	}
+
+out:
+	cur_p->cntrl |= XMCDMA_BD_CTRL_TXEOF_MASK;
+	tail_p = q->tx_bd_p + sizeof(*q->txq_bd_v) * q->tx_bd_tail;
+	cur_p->tx_skb = (phys_addr_t)skb;
+	cur_p->tx_skb = (phys_addr_t)skb;
+
+	tail_p = q->tx_bd_p + sizeof(*q->tx_bd_v) * q->tx_bd_tail;
+	/* Ensure BD write before starting transfer */
+	wmb();
+
+	/* Start the transfer */
+	axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id),
+			  tail_p);
+	if (++q->tx_bd_tail >= lp->tx_bd_num)
+		q->tx_bd_tail = 0;
+
+	spin_unlock_irqrestore(&q->tx_lock, flags);
+
+	return NETDEV_TX_OK;
+}
+
+/**
+ * axienet_recv - Is called from Axi DMA Rx Isr to complete the received
+ *		  BD processing.
+ * @ndev:	Pointer to net_device structure.
+ * @budget:	NAPI budget
+ * @q:		Pointer to axienet DMA queue structure
+ *
+ * This function is invoked from the Axi DMA Rx isr(poll) to process the Rx BDs
+ * It does minimal processing and invokes "netif_receive_skb" to complete
+ * further processing.
+ * Return: Number of BD's processed.
+ */
+static int axienet_recv(struct net_device *ndev, int budget,
+			struct axienet_dma_q *q)
+{
+	u32 length;
+	u32 csumstatus;
+	u32 size = 0;
+	u32 packets = 0;
+	dma_addr_t tail_p = 0;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct sk_buff *skb, *new_skb;
+	u32 sband_status = 0;
+	struct net_device *temp_ndev = NULL;
+	struct aximcdma_bd *cur_p;
+	unsigned int numbdfree = 0;
+
+	/* Get relevat BD status value */
+	rmb();
+	cur_p = &q->rxq_bd_v[q->rx_bd_ci];
+	sband_status = cur_p->sband_stats;
+
+	while ((numbdfree < budget) &&
+	       (cur_p->status & XAXIDMA_BD_STS_COMPLETE_MASK)) {
+		new_skb = netdev_alloc_skb(ndev, lp->max_frm_size);
+		if (!new_skb)
+			break;
+		tail_p = q->rx_bd_p + sizeof(*q->rxq_bd_v) * q->rx_bd_ci;
+
+		dma_unmap_single(ndev->dev.parent, cur_p->phys,
+				 lp->max_frm_size,
+				 DMA_FROM_DEVICE);
+
+		skb = (struct sk_buff *)(cur_p->sw_id_offset);
+
+		if (lp->eth_hasnobuf ||
+		    lp->axienet_config->mactype != XAXIENET_1G)
+			length = cur_p->status & XAXIDMA_BD_STS_ACTUAL_LEN_MASK;
+		else
+			length = cur_p->app4 & 0x0000FFFF;
+
+		skb_put(skb, length);
+		skb->protocol = eth_type_trans(skb, ndev);
+		/*skb_checksum_none_assert(skb);*/
+		skb->ip_summed = CHECKSUM_NONE;
+
+		/* if we're doing Rx csum offload, set it up */
+		if (lp->features & XAE_FEATURE_FULL_RX_CSUM &&
+		    lp->axienet_config->mactype == XAXIENET_1G &&
+		    !lp->eth_hasnobuf) {
+			csumstatus = (cur_p->app2 &
+				      XAE_FULL_CSUM_STATUS_MASK) >> 3;
+			if (csumstatus == XAE_IP_TCP_CSUM_VALIDATED ||
+			    csumstatus == XAE_IP_UDP_CSUM_VALIDATED) {
+				skb->ip_summed = CHECKSUM_UNNECESSARY;
+			}
+		} else if ((lp->features & XAE_FEATURE_PARTIAL_RX_CSUM) != 0 &&
+			   skb->protocol == htons(ETH_P_IP) &&
+			   skb->len > 64 && !lp->eth_hasnobuf &&
+			   (lp->axienet_config->mactype == XAXIENET_1G)) {
+			skb->csum = be32_to_cpu(cur_p->app3 & 0xFFFF);
+			skb->ip_summed = CHECKSUM_COMPLETE;
+		}
+		if (unlikely(q->flags & MCDMA_MGMT_CHAN)) {
+			/* received packet on mgmt channel */
+			if ((sband_status & XMCDMA_BD_SD_STS_ALL_MASK)
+			    == XMCDMA_BD_SD_STS_TUSER_MAC_1) {
+				temp_ndev = lp->slaves[0];
+			} else if ((sband_status & XMCDMA_BD_SD_STS_ALL_MASK)
+				 == XMCDMA_BD_SD_STS_TUSER_MAC_2) {
+				temp_ndev = lp->slaves[1];
+			} else if ((sband_status & XMCDMA_BD_SD_STS_ALL_MASK)
+				 == XMCDMA_BD_SD_STS_TUSER_EP) {
+				temp_ndev = lp->ndev;
+			} else if (lp->ex_ep && ((sband_status &
+				XMCDMA_BD_SD_STS_ALL_MASK) ==
+				XMCDMA_BD_SD_STS_TUSER_EX_EP)) {
+				temp_ndev = lp->ex_ep;
+			}
+
+			/* send to one of the front panel port */
+			if (temp_ndev && netif_running(temp_ndev)) {
+				skb->dev = temp_ndev;
+				netif_receive_skb(skb);
+			} else {
+				kfree(skb); /* dont send up the stack */
+			}
+		} else if (unlikely(q->flags & MCDMA_EP_EX_CHAN)) {
+			temp_ndev = lp->ex_ep;
+			if (temp_ndev && netif_running(temp_ndev)) {
+				skb->dev = temp_ndev;
+				netif_receive_skb(skb);
+			} else {
+				kfree(skb); /* dont send up the stack */
+			}
+		} else {
+			netif_receive_skb(skb); /* send on normal data path */
+		}
+
+		size += length;
+		packets++;
+
+		/* Ensure that the skb is completely updated
+		 * prio to mapping the DMA
+		 */
+		wmb();
+
+		cur_p->phys = dma_map_single(ndev->dev.parent, new_skb->data,
+					     lp->max_frm_size,
+					   DMA_FROM_DEVICE);
+		if (unlikely(dma_mapping_error(ndev->dev.parent, cur_p->phys))) {
+			cur_p->phys = 0;
+			dev_kfree_skb(new_skb);
+			dev_err(lp->dev, "RX buffer map failed\n");
+			break;
+		}
+		cur_p->cntrl = lp->max_frm_size;
+		cur_p->status = 0;
+		cur_p->sw_id_offset = (phys_addr_t)new_skb;
+
+		if (++q->rx_bd_ci >= lp->rx_bd_num)
+			q->rx_bd_ci = 0;
+
+		/* Get relevat BD status value */
+		rmb();
+		cur_p = &q->rxq_bd_v[q->rx_bd_ci];
+		numbdfree++;
+	}
+
+	ndev->stats.rx_packets += packets;
+	ndev->stats.rx_bytes += size;
+	q->rx_packets += packets;
+	q->rx_bytes += size;
+
+	if (tail_p) {
+		axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id) +
+				  q->rx_offset, tail_p);
+	}
+
+	return numbdfree;
+}
+
+/**
+ * xaxienet_rx_poll_tsn - Poll routine for rx packets (NAPI)
+ * @napi:	napi structure pointer
+ * @quota:	Max number of rx packets to be processed.
+ *
+ * This is the poll routine for rx part.
+ * It will process the packets maximux quota value.
+ *
+ * Return: number of packets received
+ */
+int xaxienet_rx_poll_tsn(struct napi_struct *napi, int quota)
+{
+	struct net_device *ndev = napi->dev;
+	struct axienet_local *lp = netdev_priv(ndev);
+	int work_done = 0;
+	unsigned int status, cr;
+
+	int map = napi - lp->napi;
+
+	struct axienet_dma_q *q = lp->dq[map];
+
+	spin_lock(&q->rx_lock);
+	status = axienet_dma_in32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset);
+	while ((status & (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK)) &&
+	       (work_done < quota)) {
+		axienet_dma_out32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset, status);
+		if (status & XMCDMA_IRQ_ERR_MASK) {
+			dev_err(lp->dev, "Rx error 0x%x\n\r", status);
+			break;
+		}
+		work_done += axienet_recv(lp->ndev, quota - work_done, q);
+		status = axienet_dma_in32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+					  q->rx_offset);
+	}
+	spin_unlock(&q->rx_lock);
+
+	if (work_done < quota) {
+		napi_complete(napi);
+		/* Enable the interrupts again */
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      XMCDMA_RX_OFFSET);
+		cr |= (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK);
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				  XMCDMA_RX_OFFSET, cr);
+	}
+
+	return work_done;
+}
+
+/**
+ * axienet_change_mtu - Driver change mtu routine.
+ * @ndev:	Pointer to net_device structure
+ * @new_mtu:	New mtu value to be applied
+ *
+ * Return: Always returns 0 (success).
+ *
+ * This is the change mtu driver routine. It checks if the Axi Ethernet
+ * hardware supports jumbo frames before changing the mtu. This can be
+ * called only when the device is not up.
+ */
+static int axienet_change_mtu(struct net_device *ndev, int new_mtu)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (netif_running(ndev))
+		return -EBUSY;
+
+	if ((new_mtu + VLAN_ETH_HLEN +
+		XAE_TRL_SIZE) > lp->rxmem)
+		return -EINVAL;
+
+	ndev->mtu = new_mtu;
+
+	return 0;
+}
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+/**
+ * axienet_poll_controller - Axi Ethernet poll mechanism.
+ * @ndev:	Pointer to net_device structure
+ *
+ * This implements Rx/Tx ISR poll mechanisms. The interrupts are disabled prior
+ * to polling the ISRs and are enabled back after the polling is done.
+ */
+static void axienet_poll_controller(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int i;
+
+	for_each_tx_dma_queue(lp, i)
+		disable_irq(lp->dq[i]->tx_irq);
+	for_each_rx_dma_queue(lp, i)
+		disable_irq(lp->dq[i]->rx_irq);
+
+	for_each_rx_dma_queue(lp, i)
+		axienet_mcdma_rx_irq_tsn(lp->dq[i]->rx_irq, ndev);
+	for_each_tx_dma_queue(lp, i)
+		axienet_mcdma_tx_irq_tsn(lp->dq[i]->tx_irq, ndev);
+	for_each_tx_dma_queue(lp, i)
+		enable_irq(lp->dq[i]->tx_irq);
+	for_each_rx_dma_queue(lp, i)
+		enable_irq(lp->dq[i]->rx_irq);
+}
+#endif
+
+#if defined(CONFIG_XILINX_TSN_PTP)
+/**
+ *  axienet_set_timestamp_mode - sets up the hardware for the requested mode
+ *  @lp: Pointer to axienet local structure
+ *  @config: the hwtstamp configuration requested
+ *
+ * Return: 0 on success, Negative value on errors
+ */
+static int axienet_set_timestamp_mode(struct axienet_local *lp,
+				      struct hwtstamp_config *config)
+{
+	u32 regval;
+
+#ifdef CONFIG_XILINX_TSN_PTP
+	/* reserved for future extensions */
+	if (config->flags)
+		return -EINVAL;
+
+	if (config->tx_type < HWTSTAMP_TX_OFF ||
+	    config->tx_type > HWTSTAMP_TX_ONESTEP_SYNC)
+		return -ERANGE;
+
+	lp->ptp_ts_type = config->tx_type;
+
+	/* On RX always timestamp everything */
+	switch (config->rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		break;
+	default:
+		config->rx_filter = lp->current_rx_filter;
+	}
+	return 0;
+
+#endif
+
+	/* reserved for future extensions */
+	if (config->flags)
+		return -EINVAL;
+
+	/* Read the current value in the MAC TX CTRL register */
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		regval = axienet_ior(lp, XAE_TC_OFFSET);
+
+	switch (config->tx_type) {
+	case HWTSTAMP_TX_OFF:
+		regval &= ~XAE_TC_INBAND1588_MASK;
+		break;
+	case HWTSTAMP_TX_ON:
+		config->tx_type = HWTSTAMP_TX_ON;
+		regval |= XAE_TC_INBAND1588_MASK;
+		if (lp->axienet_config->mactype == XAXIENET_MRMAC)
+			axienet_iow(lp, MRMAC_CFG1588_OFFSET, 0x0);
+		break;
+	case HWTSTAMP_TX_ONESTEP_SYNC:
+		config->tx_type = HWTSTAMP_TX_ONESTEP_SYNC;
+		regval |= XAE_TC_INBAND1588_MASK;
+		if (lp->axienet_config->mactype == XAXIENET_MRMAC)
+			axienet_iow(lp, MRMAC_CFG1588_OFFSET, MRMAC_ONE_STEP_EN);
+		break;
+	case HWTSTAMP_TX_ONESTEP_P2P:
+		if (lp->axienet_config->mactype == XAXIENET_MRMAC) {
+			config->tx_type = HWTSTAMP_TX_ONESTEP_P2P;
+			axienet_iow(lp, MRMAC_CFG1588_OFFSET, MRMAC_ONE_STEP_EN);
+		} else {
+			return -ERANGE;
+		}
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		axienet_iow(lp, XAE_TC_OFFSET, regval);
+
+	/* Read the current value in the MAC RX RCW1 register */
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		regval = axienet_ior(lp, XAE_RCW1_OFFSET);
+
+	/* On RX always timestamp everything */
+	switch (config->rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		regval &= ~XAE_RCW1_INBAND1588_MASK;
+		break;
+	default:
+		config->rx_filter = HWTSTAMP_FILTER_ALL;
+		regval |= XAE_RCW1_INBAND1588_MASK;
+	}
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		axienet_iow(lp, XAE_RCW1_OFFSET, regval);
+
+	return 0;
+}
+
+static void change_filter_values_to_udp(struct axienet_local *lp)
+{
+	axienet_iow(lp, XAE_FMC_OFFSET, FILTER_SELECT);
+	/**
+	 * axienet_iow(lp, 0x70C, 0x0); values may not
+	 * be written on to the specified address if this is not given
+	 */
+	axienet_iow(lp, XAE_FF_3_OFFSET, ETHERTYPE_FILTER_IPV4);
+	axienet_iow(lp, XAE_FF_5_OFFSET, PROTO_FILTER_UDP);
+	axienet_iow(lp, XAE_FF_9_OFFSET, PTP_UDP_PORT);
+	axienet_iow(lp, XAE_FF_10_OFFSET, PTP_VERSION);
+
+	axienet_iow(lp, XAE_AF0_MASK_OFFSET, DESTMAC_FILTER_DISABLE_MASK_MSB);
+	axienet_iow(lp, XAE_AF1_MASK_OFFSET, DESTMAC_FILTER_DISABLE_MASK_LSB);
+	axienet_iow(lp, XAE_FF_5_MASK_OFFSET, PROTO_FILTER_ENABLE_MASK);
+	axienet_iow(lp, XAE_FF_9_MASK_OFFSET, PORT_NUM_FILTER_ENABLE_MASK);
+	axienet_iow(lp, XAE_FF_10_MASK_OFFSET, VERSION_FILTER_DISABLE_MASK);
+}
+
+static void change_filter_values_to_gptp(struct axienet_local *lp)
+{
+	axienet_iow(lp, XAE_FF_3_OFFSET, ETHERTYPE_FILTER_PTP);
+	axienet_iow(lp, XAE_AF0_MASK_OFFSET, DESTMAC_FILTER_ENABLE_MASK_MSB);
+	axienet_iow(lp, XAE_AF1_MASK_OFFSET, DESTMAC_FILTER_ENABLE_MASK_LSB);
+	axienet_iow(lp, XAE_FF_5_MASK_OFFSET, PROTO_FILTER_DISABLE_MASK);
+	axienet_iow(lp, XAE_FF_9_MASK_OFFSET, PORT_NUM_FILTER_ENABLE_MASK);
+	axienet_iow(lp, XAE_FF_10_MASK_OFFSET, VERSION_FILTER_DISABLE_MASK);
+}
+
+/**
+ * axienet_set_ts_config - user entry point for timestamp mode
+ * @lp: Pointer to axienet local structure
+ * @ifr: ioctl data
+ *
+ * Set hardware to the requested more. If unsupported return an error
+ * with no changes. Otherwise, store the mode for future reference
+ *
+ * Return: 0 on success, Negative value on errors
+ */
+static int axienet_set_ts_config(struct axienet_local *lp, struct ifreq *ifr)
+{
+	struct hwtstamp_config config;
+	int err;
+
+	if (copy_from_user(&config, ifr->ifr_data, sizeof(config)))
+		return -EFAULT;
+	if (config.rx_filter == HWTSTAMP_FILTER_PTP_V2_L2_EVENT &&
+	    lp->current_rx_filter == HWTSTAMP_FILTER_PTP_V2_L4_EVENT) {
+		lp->current_rx_filter = HWTSTAMP_FILTER_PTP_V2_L2_EVENT;
+		change_filter_values_to_gptp(lp);
+	}
+	if (config.rx_filter == HWTSTAMP_FILTER_PTP_V2_L4_EVENT &&
+	    lp->current_rx_filter == HWTSTAMP_FILTER_PTP_V2_L2_EVENT) {
+		lp->current_rx_filter = HWTSTAMP_FILTER_PTP_V2_L4_EVENT;
+		change_filter_values_to_udp(lp);
+	}
+	err = axienet_set_timestamp_mode(lp, &config);
+	if (err)
+		return err;
+
+	/* save these settings for future reference */
+	memcpy(&lp->tstamp_config, &config, sizeof(lp->tstamp_config));
+
+	return copy_to_user(ifr->ifr_data, &config,
+			    sizeof(config)) ? -EFAULT : 0;
+}
+
+/**
+ * axienet_get_ts_config - return the current timestamp configuration
+ * to the user
+ * @lp: pointer to axienet local structure
+ * @ifr: ioctl data
+ *
+ * Return: 0 on success, Negative value on errors
+ */
+static int axienet_get_ts_config(struct axienet_local *lp, struct ifreq *ifr)
+{
+	struct hwtstamp_config *config = &lp->tstamp_config;
+
+	return copy_to_user(ifr->ifr_data, config,
+			    sizeof(*config)) ? -EFAULT : 0;
+}
+#endif
+
+/* Ioctl MII Interface */
+static int axienet_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
+{
+#if defined(CONFIG_XILINX_TSN_PTP)
+	struct axienet_local *lp = netdev_priv(dev);
+#endif
+
+	if (!netif_running(dev))
+		return -EINVAL;
+
+	switch (cmd) {
+	case SIOCGMIIPHY:
+	case SIOCGMIIREG:
+	case SIOCSMIIREG:
+		if (!dev->phydev)
+			return -EOPNOTSUPP;
+		return phy_mii_ioctl(dev->phydev, rq, cmd);
+#if defined(CONFIG_XILINX_TSN_PTP)
+	case SIOCSHWTSTAMP:
+		return axienet_set_ts_config(lp, rq);
+	case SIOCGHWTSTAMP:
+		return axienet_get_ts_config(lp, rq);
+#endif
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static int axienet_ioctl_siocdevprivate(struct net_device *dev,
+					struct ifreq *rq, void __user *data, int cmd)
+{
+	struct axienet_local *lp = netdev_priv(dev);
+
+	switch (cmd) {
+#ifdef CONFIG_XILINX_TSN_QBV
+	case SIOCCHIOCTL:
+		if (lp->qbv_regs)
+			return axienet_set_schedule(dev, data);
+		return -EINVAL;
+	case SIOC_GET_SCHED:
+		if (lp->qbv_regs)
+			return axienet_get_schedule(dev, data);
+		return -EINVAL;
+#endif
+#ifdef CONFIG_AXIENET_HAS_TADMA
+	case SIOC_TADMA_OFF:
+		if (!(lp->abl_reg & TSN_BRIDGEEP_EPONLY))
+			return -ENOENT;
+		return axienet_tadma_off(dev, data);
+	case SIOC_TADMA_STR_ADD:
+		if (!(lp->abl_reg & TSN_BRIDGEEP_EPONLY))
+			return -ENOENT;
+		return axienet_tadma_add_stream(dev, data);
+	case SIOC_TADMA_PROG_ALL:
+		if (!(lp->abl_reg & TSN_BRIDGEEP_EPONLY))
+			return -ENOENT;
+		return axienet_tadma_program(dev, data);
+	case SIOC_TADMA_STR_FLUSH:
+		if (!(lp->abl_reg & TSN_BRIDGEEP_EPONLY))
+			return -ENOENT;
+		return axienet_tadma_flush_stream(dev, data);
+#endif
+#ifdef CONFIG_XILINX_TSN_QBR
+	case SIOC_PREEMPTION_CFG:
+		return axienet_preemption(dev, data);
+	case SIOC_PREEMPTION_CTRL:
+		return axienet_preemption_ctrl(dev, data);
+	case SIOC_PREEMPTION_STS:
+		return axienet_preemption_sts(dev, data);
+	case SIOC_PREEMPTION_RECEIVE:
+		return axienet_preemption_receive(dev);
+	case SIOC_PREEMPTION_COUNTER:
+		return axienet_preemption_cnt(dev, data);
+#ifdef CONFIG_XILINX_TSN_QBV
+	case SIOC_QBU_USER_OVERRIDE:
+		return axienet_qbu_user_override(dev, data);
+	case SIOC_QBU_STS:
+		return axienet_qbu_sts(dev, data);
+#endif
+#endif
+
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+static const struct net_device_ops axienet_netdev_ops = {
+	.ndo_open = axienet_tsn_open,
+	.ndo_stop = axienet_tsn_stop,
+	.ndo_start_xmit = axienet_tsn_xmit,
+	.ndo_change_mtu	= axienet_change_mtu,
+	.ndo_set_mac_address = netdev_set_mac_address,
+	.ndo_validate_addr = eth_validate_addr,
+	.ndo_eth_ioctl = axienet_ioctl,
+	.ndo_siocdevprivate = axienet_ioctl_siocdevprivate,
+	.ndo_set_rx_mode = axienet_set_multicast_list_tsn,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller = axienet_poll_controller,
+#endif
+#ifdef CONFIG_XILINX_TSN
+	.ndo_select_queue = axienet_tsn_select_queue,
+#if defined(CONFIG_XILINX_TSN_SWITCH)
+	.ndo_get_port_parent_id = tsn_switch_get_port_parent_id,
+#endif
+#endif
+};
+
+/**
+ * axienet_ethtools_get_drvinfo - Get various Axi Ethernet driver information.
+ * @ndev:	Pointer to net_device structure
+ * @ed:		Pointer to ethtool_drvinfo structure
+ *
+ * This implements ethtool command for getting the driver information.
+ * Issue "ethtool -i ethX" under linux prompt to execute this function.
+ */
+static void axienet_ethtools_get_drvinfo(struct net_device *ndev,
+					 struct ethtool_drvinfo *ed)
+{
+	strscpy(ed->driver, DRIVER_NAME, sizeof(ed->driver));
+	strscpy(ed->version, DRIVER_VERSION, sizeof(ed->version));
+}
+
+/**
+ * axienet_ethtools_get_regs_len - Get the total regs length present in the
+ *				   AxiEthernet core.
+ * @ndev:	Pointer to net_device structure
+ *
+ * This implements ethtool command for getting the total register length
+ * information.
+ *
+ * Return: the total regs length
+ */
+static int axienet_ethtools_get_regs_len(struct net_device *ndev)
+{
+	return sizeof(u32) * AXIENET_REGS_N;
+}
+
+/**
+ * axienet_ethtools_get_regs - Dump the contents of all registers present
+ *			       in AxiEthernet core.
+ * @ndev:	Pointer to net_device structure
+ * @regs:	Pointer to ethtool_regs structure
+ * @ret:	Void pointer used to return the contents of the registers.
+ *
+ * This implements ethtool command for getting the Axi Ethernet register dump.
+ * Issue "ethtool -d ethX" to execute this function.
+ */
+static void axienet_ethtools_get_regs(struct net_device *ndev,
+				      struct ethtool_regs *regs, void *ret)
+{
+	u32 *data = (u32 *)ret;
+	size_t len = sizeof(u32) * AXIENET_REGS_N;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	regs->version = 0;
+	regs->len = len;
+
+	memset(data, 0, len);
+	data[13] = axienet_ior(lp, XAE_RCW0_OFFSET);
+	data[14] = axienet_ior(lp, XAE_RCW1_OFFSET);
+	data[15] = axienet_ior(lp, XAE_TC_OFFSET);
+	data[16] = axienet_ior(lp, XAE_FCC_OFFSET);
+	data[17] = axienet_ior(lp, XAE_EMMC_OFFSET);
+	data[18] = axienet_ior(lp, XAE_RMFC_OFFSET);
+	data[19] = axienet_ior(lp, XAE_MDIO_MC_OFFSET);
+	data[20] = axienet_ior(lp, XAE_MDIO_MCR_OFFSET);
+	data[21] = axienet_ior(lp, XAE_MDIO_MWD_OFFSET);
+	data[22] = axienet_ior(lp, XAE_MDIO_MRD_OFFSET);
+	data[23] = axienet_ior(lp, XAE_TEMAC_IS_OFFSET);
+	data[24] = axienet_ior(lp, XAE_TEMAC_IP_OFFSET);
+	data[25] = axienet_ior(lp, XAE_TEMAC_IE_OFFSET);
+	data[26] = axienet_ior(lp, XAE_TEMAC_IC_OFFSET);
+	data[27] = axienet_ior(lp, XAE_UAW0_OFFSET);
+	data[28] = axienet_ior(lp, XAE_UAW1_OFFSET);
+	data[29] = axienet_ior(lp, XAE_FMC_OFFSET);
+	data[30] = axienet_ior(lp, XAE_AF0_OFFSET);
+	data[31] = axienet_ior(lp, XAE_AF1_OFFSET);
+	/* Support only single DMA queue */
+	data[32] = axienet_dma_in32(lp->dq[0], XAXIDMA_TX_CR_OFFSET);
+	data[33] = axienet_dma_in32(lp->dq[0], XAXIDMA_TX_SR_OFFSET);
+	data[34] = axienet_dma_in32(lp->dq[0], XAXIDMA_TX_CDESC_OFFSET);
+	data[35] = axienet_dma_in32(lp->dq[0], XAXIDMA_TX_TDESC_OFFSET);
+	data[36] = axienet_dma_in32(lp->dq[0], XAXIDMA_RX_CR_OFFSET);
+	data[37] = axienet_dma_in32(lp->dq[0], XAXIDMA_RX_SR_OFFSET);
+	data[38] = axienet_dma_in32(lp->dq[0], XAXIDMA_RX_CDESC_OFFSET);
+	data[39] = axienet_dma_in32(lp->dq[0], XAXIDMA_RX_TDESC_OFFSET);
+}
+
+static void
+axienet_ethtools_get_ringparam(struct net_device *ndev,
+			       struct ethtool_ringparam *ering,
+			       struct kernel_ethtool_ringparam *kernel_ering,
+			       struct netlink_ext_ack *extack)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	ering->rx_max_pending = RX_BD_NUM_MAX;
+	ering->rx_mini_max_pending = 0;
+	ering->rx_jumbo_max_pending = 0;
+	ering->tx_max_pending = TX_BD_NUM_MAX;
+	ering->rx_pending = lp->rx_bd_num;
+	ering->rx_mini_pending = 0;
+	ering->rx_jumbo_pending = 0;
+	ering->tx_pending = lp->tx_bd_num;
+}
+
+static int
+axienet_ethtools_set_ringparam(struct net_device *ndev,
+			       struct ethtool_ringparam *ering,
+			       struct kernel_ethtool_ringparam *kernel_ering,
+			       struct netlink_ext_ack *extack)
+
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (ering->rx_pending > RX_BD_NUM_MAX ||
+	    ering->rx_mini_pending ||
+	    ering->rx_jumbo_pending ||
+	    ering->rx_pending > TX_BD_NUM_MAX)
+		return -EINVAL;
+
+	if (netif_running(ndev))
+		return -EBUSY;
+
+	lp->rx_bd_num = ering->rx_pending;
+	lp->tx_bd_num = ering->tx_pending;
+	return 0;
+}
+
+/**
+ * axienet_ethtools_get_pauseparam - Get the pause parameter setting for
+ *				     Tx and Rx paths.
+ * @ndev:	Pointer to net_device structure
+ * @epauseparm:	Pointer to ethtool_pauseparam structure.
+ *
+ * This implements ethtool command for getting axi ethernet pause frame
+ * setting. Issue "ethtool -a ethX" to execute this function.
+ */
+static void
+axienet_ethtools_get_pauseparam(struct net_device *ndev,
+				struct ethtool_pauseparam *epauseparm)
+{
+	u32 regval;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	epauseparm->autoneg  = 0;
+	regval = axienet_ior(lp, XAE_FCC_OFFSET);
+	epauseparm->tx_pause = regval & XAE_FCC_FCTX_MASK;
+	epauseparm->rx_pause = regval & XAE_FCC_FCRX_MASK;
+}
+
+/**
+ * axienet_ethtools_set_pauseparam - Set device pause parameter(flow control)
+ *				     settings.
+ * @ndev:	Pointer to net_device structure
+ * @epauseparm:	Pointer to ethtool_pauseparam structure
+ *
+ * This implements ethtool command for enabling flow control on Rx and Tx
+ * paths. Issue "ethtool -A ethX tx on|off" under linux prompt to execute this
+ * function.
+ *
+ * Return: 0 on success, -EFAULT if device is running
+ */
+static int
+axienet_ethtools_set_pauseparam(struct net_device *ndev,
+				struct ethtool_pauseparam *epauseparm)
+{
+	u32 regval = 0;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (netif_running(ndev)) {
+		netdev_err(ndev,
+			   "Please stop netif before applying configuration\n");
+		return -EFAULT;
+	}
+
+	regval = axienet_ior(lp, XAE_FCC_OFFSET);
+	if (epauseparm->tx_pause)
+		regval |= XAE_FCC_FCTX_MASK;
+	else
+		regval &= ~XAE_FCC_FCTX_MASK;
+	if (epauseparm->rx_pause)
+		regval |= XAE_FCC_FCRX_MASK;
+	else
+		regval &= ~XAE_FCC_FCRX_MASK;
+	axienet_iow(lp, XAE_FCC_OFFSET, regval);
+
+	return 0;
+}
+
+/**
+ * axienet_ethtools_get_coalesce - Get DMA interrupt coalescing count.
+ * @ndev:	Pointer to net_device structure
+ * @ecoalesce:	Pointer to ethtool_coalesce structure
+ * @kernel_coal: ethtool CQE mode setting structure
+ * @extack:	extack for reporting error messages
+ *
+ * This implements ethtool command for getting the DMA interrupt coalescing
+ * count on Tx and Rx paths. Issue "ethtool -c ethX" under linux prompt to
+ * execute this function.
+ *
+ * Return: 0 always
+ */
+int axienet_ethtools_get_coalesce(struct net_device *ndev,
+				  struct ethtool_coalesce *ecoalesce,
+				  struct kernel_ethtool_coalesce *kernel_coal,
+				  struct netlink_ext_ack *extack)
+{
+	u32 regval = 0;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+	int i;
+
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		if (!q)
+			return 0;
+
+		regval = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		ecoalesce->rx_max_coalesced_frames +=
+						(regval & XAXIDMA_COALESCE_MASK)
+						     >> XAXIDMA_COALESCE_SHIFT;
+	}
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		if (!q)
+			return 0;
+		regval = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+		ecoalesce->tx_max_coalesced_frames +=
+						(regval & XAXIDMA_COALESCE_MASK)
+						     >> XAXIDMA_COALESCE_SHIFT;
+	}
+	return 0;
+}
+
+/**
+ * axienet_ethtools_set_coalesce - Set DMA interrupt coalescing count.
+ * @ndev:	Pointer to net_device structure
+ * @ecoalesce:	Pointer to ethtool_coalesce structure
+ * @kernel_coal: ethtool CQE mode setting structure
+ * @extack:	extack for reporting error messages
+ *
+ * This implements ethtool command for setting the DMA interrupt coalescing
+ * count on Tx and Rx paths. Issue "ethtool -C ethX rx-frames 5" under linux
+ * prompt to execute this function.
+ *
+ * Return: 0, on success, Non-zero error value on failure.
+ */
+int axienet_ethtools_set_coalesce(struct net_device *ndev,
+				  struct ethtool_coalesce *ecoalesce,
+				  struct kernel_ethtool_coalesce *kernel_coal,
+				  struct netlink_ext_ack *extack)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (netif_running(ndev)) {
+		netdev_err(ndev,
+			   "Please stop netif before applying configuration\n");
+		return -EFAULT;
+	}
+
+	if (ecoalesce->rx_coalesce_usecs ||
+	    ecoalesce->rx_coalesce_usecs_irq ||
+	    ecoalesce->rx_max_coalesced_frames_irq ||
+	    ecoalesce->tx_coalesce_usecs ||
+	    ecoalesce->tx_coalesce_usecs_irq ||
+	    ecoalesce->tx_max_coalesced_frames_irq ||
+	    ecoalesce->stats_block_coalesce_usecs ||
+	    ecoalesce->use_adaptive_rx_coalesce ||
+	    ecoalesce->use_adaptive_tx_coalesce ||
+	    ecoalesce->pkt_rate_low ||
+	    ecoalesce->rx_coalesce_usecs_low ||
+	    ecoalesce->rx_max_coalesced_frames_low ||
+	    ecoalesce->tx_coalesce_usecs_low ||
+	    ecoalesce->tx_max_coalesced_frames_low ||
+	    ecoalesce->pkt_rate_high ||
+	    ecoalesce->rx_coalesce_usecs_high ||
+	    ecoalesce->rx_max_coalesced_frames_high ||
+	    ecoalesce->tx_coalesce_usecs_high ||
+	    ecoalesce->tx_max_coalesced_frames_high ||
+	    ecoalesce->rate_sample_interval)
+		return -EOPNOTSUPP;
+	if (ecoalesce->rx_max_coalesced_frames)
+		lp->coalesce_count_rx = ecoalesce->rx_max_coalesced_frames;
+	if (ecoalesce->tx_max_coalesced_frames)
+		lp->coalesce_count_tx = ecoalesce->tx_max_coalesced_frames;
+
+	return 0;
+}
+
+#if defined(CONFIG_XILINX_TSN_PTP)
+/**
+ * axienet_ethtools_get_ts_info - Get h/w timestamping capabilities.
+ * @ndev:	Pointer to net_device structure
+ * @info:	Pointer to ethtool_ts_info structure
+ *
+ * Return: 0, on success, Non-zero error value on failure.
+ */
+static int axienet_ethtools_get_ts_info(struct net_device *ndev,
+					struct ethtool_ts_info *info)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	info->so_timestamping = SOF_TIMESTAMPING_TX_HARDWARE |
+				SOF_TIMESTAMPING_RX_HARDWARE |
+				SOF_TIMESTAMPING_RAW_HARDWARE;
+	info->tx_types = (1 << HWTSTAMP_TX_OFF) | (1 << HWTSTAMP_TX_ON) |
+			(1 << HWTSTAMP_TX_ONESTEP_SYNC) |
+			(1 << HWTSTAMP_TX_ONESTEP_P2P);
+	info->rx_filters = (1 << HWTSTAMP_FILTER_NONE) |
+			   (1 << HWTSTAMP_FILTER_ALL);
+	info->phc_index = lp->phc_index;
+
+#ifdef CONFIG_XILINX_TSN_PTP
+	info->phc_index = axienet_phc_index;
+#endif
+	return 0;
+}
+#endif
+
+/**
+ * axienet_ethtools_sset_count - Get number of strings that
+ *				 get_strings will write.
+ * @ndev:	Pointer to net_device structure
+ * @sset:	Get the set strings
+ *
+ * Return: number of strings, on success, Non-zero error value on
+ *	   failure.
+ */
+static int axienet_ethtools_sset_count(struct net_device *ndev, int sset)
+{
+	switch (sset) {
+	case ETH_SS_STATS:
+		return axienet_sset_count_tsn(ndev, sset);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+/**
+ * axienet_ethtools_get_stats - Get the extended statistics
+ *				about the device.
+ * @ndev:	Pointer to net_device structure
+ * @stats:	Pointer to ethtool_stats structure
+ * @data:	To store the statistics values
+ *
+ * Return: None.
+ */
+static void axienet_ethtools_get_stats(struct net_device *ndev,
+				       struct ethtool_stats *stats,
+				       u64 *data)
+{
+	unsigned int i = 0;
+
+	data[i++] = ndev->stats.tx_packets;
+	data[i++] = ndev->stats.rx_packets;
+	data[i++] = ndev->stats.tx_bytes;
+	data[i++] = ndev->stats.rx_bytes;
+	data[i++] = ndev->stats.tx_errors;
+	data[i++] = ndev->stats.rx_missed_errors + ndev->stats.rx_frame_errors;
+
+	axienet_get_stats_tsn(ndev, stats, data);
+}
+
+/**
+ * axienet_ethtools_strings - Set of strings that describe
+ *			 the requested objects.
+ * @ndev:	Pointer to net_device structure
+ * @sset:	Get the set strings
+ * @data:	Data of Transmit and Receive statistics
+ *
+ * Return: None.
+ */
+static void axienet_ethtools_strings(struct net_device *ndev, u32 sset, u8 *data)
+{
+	int i;
+
+	for (i = 0; i < AXIENET_ETHTOOLS_SSTATS_LEN; i++) {
+		if (sset == ETH_SS_STATS)
+			memcpy(data + i * ETH_GSTRING_LEN,
+			       axienet_get_ethtools_strings_stats[i].name,
+			       ETH_GSTRING_LEN);
+	}
+	axienet_strings_tsn(ndev, sset, data);
+}
+
+static const struct ethtool_ops axienet_ethtool_ops = {
+	.supported_coalesce_params = ETHTOOL_COALESCE_MAX_FRAMES,
+	.get_drvinfo    = axienet_ethtools_get_drvinfo,
+	.get_regs_len   = axienet_ethtools_get_regs_len,
+	.get_regs       = axienet_ethtools_get_regs,
+	.get_link       = ethtool_op_get_link,
+	.get_ringparam	= axienet_ethtools_get_ringparam,
+	.set_ringparam  = axienet_ethtools_set_ringparam,
+	.get_pauseparam = axienet_ethtools_get_pauseparam,
+	.set_pauseparam = axienet_ethtools_set_pauseparam,
+	.get_coalesce   = axienet_ethtools_get_coalesce,
+	.set_coalesce   = axienet_ethtools_set_coalesce,
+	.get_sset_count	= axienet_ethtools_sset_count,
+	.get_ethtool_stats = axienet_ethtools_get_stats,
+	.get_strings = axienet_ethtools_strings,
+#if defined(CONFIG_XILINX_TSN_PTP)
+	.get_ts_info    = axienet_ethtools_get_ts_info,
+#endif
+	.get_link_ksettings = phy_ethtool_get_link_ksettings,
+	.set_link_ksettings = phy_ethtool_set_link_ksettings,
+};
+
+static int axienet_clk_init(struct platform_device *pdev,
+			    struct clk **axi_aclk, struct clk **axis_clk,
+			    struct clk **ref_clk, struct clk **tmpclk)
+{
+	int err;
+
+	*tmpclk = NULL;
+
+	/* The "ethernet_clk" is deprecated and will be removed sometime in
+	 * the future. For proper clock usage check axiethernet binding
+	 * documentation.
+	 */
+	*axi_aclk = devm_clk_get(&pdev->dev, "ethernet_clk");
+	if (IS_ERR(*axi_aclk)) {
+		if (PTR_ERR(*axi_aclk) != -ENOENT) {
+			err = PTR_ERR(*axi_aclk);
+			return err;
+		}
+
+		*axi_aclk = devm_clk_get(&pdev->dev, "s_axi_lite_clk");
+		if (IS_ERR(*axi_aclk)) {
+			if (PTR_ERR(*axi_aclk) != -ENOENT) {
+				err = PTR_ERR(*axi_aclk);
+				return err;
+			}
+			*axi_aclk = NULL;
+		}
+
+	} else {
+		dev_warn(&pdev->dev, "ethernet_clk is deprecated and will be removed sometime in the future\n");
+	}
+
+	*axis_clk = devm_clk_get(&pdev->dev, "axis_clk");
+	if (IS_ERR(*axis_clk)) {
+		if (PTR_ERR(*axis_clk) != -ENOENT) {
+			err = PTR_ERR(*axis_clk);
+			return err;
+		}
+		*axis_clk = NULL;
+	}
+
+	*ref_clk = devm_clk_get(&pdev->dev, "ref_clk");
+	if (IS_ERR(*ref_clk)) {
+		if (PTR_ERR(*ref_clk) != -ENOENT) {
+			err = PTR_ERR(*ref_clk);
+			return err;
+		}
+		*ref_clk = NULL;
+	}
+
+	err = clk_prepare_enable(*axi_aclk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable axi_aclk/ethernet_clk (%d)\n", err);
+		return err;
+	}
+
+	err = clk_prepare_enable(*axis_clk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable axis_clk (%d)\n", err);
+		goto err_disable_axi_aclk;
+	}
+
+	err = clk_prepare_enable(*ref_clk);
+	if (err) {
+		dev_err(&pdev->dev, "failed to enable ref_clk (%d)\n", err);
+		goto err_disable_axis_clk;
+	}
+
+	return 0;
+
+err_disable_axis_clk:
+	clk_disable_unprepare(*axis_clk);
+err_disable_axi_aclk:
+	clk_disable_unprepare(*axi_aclk);
+
+	return err;
+}
+
+static void axienet_clk_disable(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	clk_disable_unprepare(lp->dma_sg_clk);
+	clk_disable_unprepare(lp->dma_tx_clk);
+	clk_disable_unprepare(lp->dma_rx_clk);
+	clk_disable_unprepare(lp->eth_sclk);
+	clk_disable_unprepare(lp->eth_refclk);
+	clk_disable_unprepare(lp->eth_dclk);
+	clk_disable_unprepare(lp->aclk);
+}
+
+static const struct axienet_config axienet_1g_config_tsn = {
+	.mactype = XAXIENET_1G,
+	.setoptions = axienet_setoptions_tsn,
+	.clk_init = axienet_clk_init,
+	.tx_ptplen = XAE_TX_PTP_LEN,
+};
+
+/* Match table for of_platform binding */
+static const struct of_device_id axienet_of_match[] = {
+	{ .compatible = "xlnx,tsn-ethernet-1.00.a", .data = &axienet_1g_config_tsn},
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, axienet_of_match);
+
+/**
+ * axienet_probe - Axi Ethernet probe function.
+ * @pdev:	Pointer to platform device structure.
+ *
+ * Return: 0, on success
+ *	    Non-zero error value on failure.
+ *
+ * This is the probe routine for Axi Ethernet driver. This is called before
+ * any other driver routines are invoked. It allocates and sets up the Ethernet
+ * device. Parses through device tree and populates fields of
+ * axienet_local. It registers the Ethernet device.
+ */
+static int axienet_probe(struct platform_device *pdev)
+{
+	int (*axienet_clk_init)(struct platform_device *pdev,
+				struct clk **axi_aclk, struct clk **axis_clk,
+				struct clk **ref_clk, struct clk **tmpclk) =
+					axienet_clk_init;
+	int ret = 0;
+	struct axienet_local *lp;
+	struct net_device *ndev;
+	u8 mac_addr[ETH_ALEN];
+	struct resource *ethres;
+	u32 value;
+	u16 num_queues = XAE_MAX_QUEUES;
+
+	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-queues",
+				   &num_queues);
+	if (num_queues < XAE_TSN_MIN_QUEUES)
+		num_queues = XAE_TSN_MIN_QUEUES;
+	else if (num_queues > XAE_MAX_QUEUES)
+		num_queues = XAE_MAX_QUEUES;
+
+	ndev = alloc_etherdev_mq(sizeof(*lp), num_queues);
+	if (!ndev)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, ndev);
+
+	SET_NETDEV_DEV(ndev, &pdev->dev);
+	ndev->flags &= ~IFF_MULTICAST;  /* clear multicast */
+	ndev->features = NETIF_F_SG;
+	ndev->netdev_ops = &axienet_netdev_ops;
+	ndev->ethtool_ops = &axienet_ethtool_ops;
+
+	/* MTU range: 64 - 9000 */
+	ndev->min_mtu = 64;
+	ndev->max_mtu = XAE_JUMBO_MTU;
+
+	lp = netdev_priv(ndev);
+	lp->ndev = ndev;
+	lp->dev = &pdev->dev;
+	lp->options = XAE_OPTION_DEFAULTS;
+	lp->num_tx_queues = num_queues;
+	lp->num_rx_queues = num_queues;
+	lp->rx_bd_num = RX_BD_NUM_DEFAULT;
+	lp->tx_bd_num = TX_BD_NUM_DEFAULT;
+
+	lp->axi_clk = devm_clk_get_optional(&pdev->dev, "s_axi_lite_clk");
+	if (!lp->axi_clk) {
+		/* For backward compatibility, if named AXI clock is not present,
+		 * treat the first clock specified as the AXI clock.
+		 */
+		lp->axi_clk = devm_clk_get_optional(&pdev->dev, NULL);
+	}
+	if (IS_ERR(lp->axi_clk)) {
+		ret = PTR_ERR(lp->axi_clk);
+		goto free_netdev;
+	}
+	ret = clk_prepare_enable(lp->axi_clk);
+	if (ret) {
+		dev_err(&pdev->dev, "Unable to enable AXI clock: %d\n", ret);
+		goto free_netdev;
+	}
+
+	lp->misc_clks[0].id = "axis_clk";
+	lp->misc_clks[1].id = "ref_clk";
+	lp->misc_clks[2].id = "mgt_clk";
+
+	ret = devm_clk_bulk_get_optional(&pdev->dev, XAE_NUM_MISC_CLOCKS, lp->misc_clks);
+	if (ret)
+		goto cleanup_clk;
+
+	ret = clk_bulk_prepare_enable(XAE_NUM_MISC_CLOCKS, lp->misc_clks);
+	if (ret)
+		goto cleanup_clk;
+	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-tc",
+				   &lp->num_tc);
+	if (ret || (lp->num_tc != 2 && lp->num_tc != 3))
+		lp->num_tc = XAE_MAX_TSN_TC;
+
+	/* Map device registers */
+	lp->regs = devm_platform_get_and_ioremap_resource(pdev, 0, &ethres);
+	if (IS_ERR(lp->regs)) {
+		ret = PTR_ERR(lp->regs);
+		goto cleanup_clk;
+	}
+	lp->regs_start = ethres->start;
+
+	/* Setup checksum offload, but default to off if not specified */
+	lp->features = 0;
+
+	if (pdev->dev.of_node) {
+		const struct of_device_id *match;
+
+		match = of_match_node(axienet_of_match, pdev->dev.of_node);
+		if (match && match->data) {
+			lp->axienet_config = match->data;
+			axienet_clk_init = lp->axienet_config->clk_init;
+		}
+	}
+
+	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,txcsum", &value);
+	if (!ret) {
+		dev_info(&pdev->dev, "TX_CSUM %d\n", value);
+
+		switch (value) {
+		case 1:
+			lp->csum_offload_on_tx_path =
+				XAE_FEATURE_PARTIAL_TX_CSUM;
+			lp->features |= XAE_FEATURE_PARTIAL_TX_CSUM;
+			/* Can checksum TCP/UDP over IPv4. */
+			ndev->features |= NETIF_F_IP_CSUM | NETIF_F_SG;
+			break;
+		case 2:
+			lp->csum_offload_on_tx_path =
+				XAE_FEATURE_FULL_TX_CSUM;
+			lp->features |= XAE_FEATURE_FULL_TX_CSUM;
+			/* Can checksum TCP/UDP over IPv4. */
+			ndev->features |= NETIF_F_IP_CSUM | NETIF_F_SG;
+			break;
+		default:
+			lp->csum_offload_on_tx_path = XAE_NO_CSUM_OFFLOAD;
+		}
+	}
+	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,rxcsum", &value);
+	if (!ret) {
+		dev_info(&pdev->dev, "RX_CSUM %d\n", value);
+
+		switch (value) {
+		case 1:
+			lp->csum_offload_on_rx_path =
+				XAE_FEATURE_PARTIAL_RX_CSUM;
+			lp->features |= XAE_FEATURE_PARTIAL_RX_CSUM;
+			break;
+		case 2:
+			lp->csum_offload_on_rx_path =
+				XAE_FEATURE_FULL_RX_CSUM;
+			lp->features |= XAE_FEATURE_FULL_RX_CSUM;
+			break;
+		default:
+			lp->csum_offload_on_rx_path = XAE_NO_CSUM_OFFLOAD;
+		}
+	}
+	/* For supporting jumbo frames, the Axi Ethernet hardware must have
+	 * a larger Rx/Tx Memory. Typically, the size must be large so that
+	 * we can enable jumbo option and start supporting jumbo frames.
+	 * Here we check for memory allocated for Rx/Tx in the hardware from
+	 * the device-tree and accordingly set flags.
+	 */
+	of_property_read_u32(pdev->dev.of_node, "xlnx,rxmem", &lp->rxmem);
+
+	/* The phy_mode is optional but when it is not specified it should not
+	 *  be a value that alters the driver behavior so set it to an invalid
+	 *  value as the default.
+	 */
+	lp->phy_mode = PHY_INTERFACE_MODE_NA;
+	ret = of_property_read_u32(pdev->dev.of_node, "xlnx,phy-type", &lp->phy_mode);
+	if (!ret)
+		netdev_warn(ndev, "xlnx,phy-type is deprecated, Please upgrade your device tree to use phy-mode");
+
+	/* Set default USXGMII rate */
+	lp->usxgmii_rate = SPEED_1000;
+	of_property_read_u32(pdev->dev.of_node, "xlnx,usxgmii-rate",
+			     &lp->usxgmii_rate);
+
+	/* Set default MRMAC rate */
+	lp->mrmac_rate = SPEED_10000;
+	of_property_read_u32(pdev->dev.of_node, "xlnx,mrmac-rate",
+			     &lp->mrmac_rate);
+
+	lp->eth_hasnobuf = of_property_read_bool(pdev->dev.of_node,
+						 "xlnx,eth-hasnobuf");
+	lp->eth_hasptp = of_property_read_bool(pdev->dev.of_node,
+					       "xlnx,eth-hasptp");
+
+	if (lp->axienet_config->mactype == XAXIENET_1G && !lp->eth_hasnobuf)
+		lp->eth_irq = platform_get_irq(pdev, 0);
+
+	ret = axienet_tsn_probe(pdev, lp, ndev);
+
+	ret = axienet_clk_init(pdev, &lp->aclk, &lp->eth_sclk,
+			       &lp->eth_refclk, &lp->eth_dclk);
+	if (ret) {
+		if (ret != -EPROBE_DEFER)
+			dev_err(&pdev->dev, "Ethernet clock init failed %d\n", ret);
+		goto err_disable_clk;
+	}
+
+	lp->eth_irq = platform_get_irq(pdev, 0);
+	/* Check for Ethernet core IRQ (optional) */
+	if (lp->eth_irq <= 0)
+		dev_info(&pdev->dev, "Ethernet core IRQ not defined\n");
+
+	/* Retrieve the MAC address */
+	ret = of_get_mac_address(pdev->dev.of_node, mac_addr);
+	if (!ret) {
+		axienet_set_mac_address_tsn(ndev, mac_addr);
+	} else {
+		dev_warn(&pdev->dev, "could not find MAC address property: %d\n",
+			 ret);
+		axienet_set_mac_address_tsn(ndev, NULL);
+	}
+
+	lp->coalesce_count_rx = XAXIDMA_DFT_RX_THRESHOLD;
+	lp->coalesce_count_tx = XAXIDMA_DFT_TX_THRESHOLD;
+
+	ret = of_get_phy_mode(pdev->dev.of_node, &lp->phy_mode);
+	if (ret < 0)
+		dev_warn(&pdev->dev, "couldn't find phy i/f\n");
+	if (lp->phy_mode == PHY_INTERFACE_MODE_1000BASEX)
+		lp->phy_flags = XAE_PHY_TYPE_1000BASE_X;
+
+	lp->phy_node = of_parse_phandle(pdev->dev.of_node, "phy-handle", 0);
+	if (lp->phy_node) {
+		ret = axienet_mdio_setup_tsn(lp);
+		if (ret)
+			dev_warn(&pdev->dev,
+				 "error registering MDIO bus: %d\n", ret);
+	}
+
+	/* Create sysfs file entries for the device */
+	ret = axeinet_mcdma_create_sysfs_tsn(&lp->dev->kobj);
+	if (ret < 0) {
+		dev_err(lp->dev, "unable to create sysfs entries\n");
+		return ret;
+	}
+
+	ret = register_netdev(lp->ndev);
+	if (ret) {
+		dev_err(lp->dev, "register_netdev() error (%i)\n", ret);
+		axienet_mdio_teardown_tsn(lp);
+		goto cleanup_clk;
+	}
+
+	return 0;
+
+cleanup_clk:
+	clk_bulk_disable_unprepare(XAE_NUM_MISC_CLOCKS, lp->misc_clks);
+	clk_disable_unprepare(lp->axi_clk);
+
+err_disable_clk:
+	axienet_clk_disable(pdev);
+
+free_netdev:
+	free_netdev(ndev);
+
+	return ret;
+}
+
+static int axienet_remove(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+	struct axienet_local *lp = netdev_priv(ndev);
+
+#ifdef CONFIG_XILINX_TSN_PTP
+	if (lp->timer_priv)
+		axienet_ptp_timer_remove(lp->timer_priv);
+#ifdef CONFIG_XILINX_TSN_QBV
+		axienet_qbv_remove(ndev);
+#endif
+#endif
+	unregister_netdev(ndev);
+	axienet_clk_disable(pdev);
+
+	if (lp->mii_bus)
+		axienet_mdio_teardown_tsn(lp);
+
+	clk_bulk_disable_unprepare(XAE_NUM_MISC_CLOCKS, lp->misc_clks);
+	clk_disable_unprepare(lp->axi_clk);
+
+	axeinet_mcdma_remove_sysfs_tsn(&lp->dev->kobj);
+	of_node_put(lp->phy_node);
+	lp->phy_node = NULL;
+
+	free_netdev(ndev);
+
+	return 0;
+}
+
+static void axienet_shutdown(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+
+	rtnl_lock();
+	netif_device_detach(ndev);
+
+	if (netif_running(ndev))
+		dev_close(ndev);
+
+	rtnl_unlock();
+}
+
+static struct platform_driver axienet_driver_tsn = {
+	.probe = axienet_probe,
+	.remove = axienet_remove,
+	.shutdown = axienet_shutdown,
+	.driver = {
+		 .name = "xilinx_axienet_tsn",
+		 .of_match_table = axienet_of_match,
+	},
+};
+
+module_platform_driver(axienet_driver_tsn);
+
+MODULE_DESCRIPTION("Xilinx Axi Ethernet driver");
+MODULE_AUTHOR("Xilinx");
+MODULE_LICENSE("GPL");
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_axienet_mcdma_tsn.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_axienet_mcdma_tsn.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,1117 @@
+// SPDX-License-Identifier: GPL-2.0
+
+/* Xilinx AXI Ethernet (MCDMA programming)
+ *
+ * Copyright (c) 2008 Nissin Systems Co., Ltd.,  Yoshio Kashiwagi
+ * Copyright (c) 2005-2008 DLA Systems,  David H. Lynch Jr. <dhlii@dlasys.net>
+ * Copyright (c) 2008-2009 Secret Lab Technologies Ltd.
+ * Copyright (c) 2010 - 2011 Michal Simek <monstr@monstr.eu>
+ * Copyright (c) 2010 - 2011 PetaLogix
+ * Copyright (c) 2010 - 2012 Xilinx, Inc.
+ * Copyright (C) 2018 Xilinx, Inc. All rights reserved.
+ *
+ * This file contains helper functions for AXI MCDMA TX and RX programming.
+ */
+
+#include <linux/module.h>
+#include <linux/of_mdio.h>
+#include <linux/of_platform.h>
+#include <linux/of_irq.h>
+#include <linux/of_address.h>
+#include <linux/of_net.h>
+
+#include "xilinx_axienet_tsn.h"
+
+struct axienet_stat {
+	const char *name;
+};
+
+/* TODO
+ * The channel numbers for managemnet frames in 5 channel mcdma on EP+Switch
+ * system. These are not exposed via hdf/dtsi, so need to hardcode here
+ */
+#define TSN_MAX_RX_Q_EX_EPSWITCH 7
+#define TSN_MIN_RX_Q_EX_EPSWITCH 5
+#define TSN_MGMT_CHAN 3
+#define TSN_MAX_EX_EP_BE_CHAN 5
+#define TSN_MAX_EX_EP_ST_CHAN 6
+#define TSN_MAX_EX_EP_RES_CHAN 7
+#define TSN_MIN_EX_EP_BE_CHAN 4
+#define TSN_MIN_EX_EP_ST_CHAN 5
+
+static struct axienet_stat axienet_get_tx_strings_stats[] = {
+	{ "txq0_packets" },
+	{ "txq0_bytes"   },
+	{ "txq1_packets" },
+	{ "txq1_bytes"   },
+	{ "txq2_packets" },
+	{ "txq2_bytes"   },
+	{ "txq3_packets" },
+	{ "txq3_bytes"   },
+	{ "txq4_packets" },
+	{ "txq4_bytes"   },
+	{ "txq5_packets" },
+	{ "txq5_bytes"   },
+	{ "txq6_packets" },
+	{ "txq6_bytes"   },
+	{ "txq7_packets" },
+	{ "txq7_bytes"   },
+	{ "txq8_packets" },
+	{ "txq8_bytes"   },
+	{ "txq9_packets" },
+	{ "txq9_bytes"   },
+	{ "txq10_packets" },
+	{ "txq10_bytes"   },
+	{ "txq11_packets" },
+	{ "txq11_bytes"   },
+	{ "txq12_packets" },
+	{ "txq12_bytes"   },
+	{ "txq13_packets" },
+	{ "txq13_bytes"   },
+	{ "txq14_packets" },
+	{ "txq14_bytes"   },
+	{ "txq15_packets" },
+	{ "txq15_bytes"   },
+};
+
+static struct axienet_stat axienet_get_rx_strings_stats[] = {
+	{ "rxq0_packets" },
+	{ "rxq0_bytes"   },
+	{ "rxq1_packets" },
+	{ "rxq1_bytes"   },
+	{ "rxq2_packets" },
+	{ "rxq2_bytes"   },
+	{ "rxq3_packets" },
+	{ "rxq3_bytes"   },
+	{ "rxq4_packets" },
+	{ "rxq4_bytes"   },
+	{ "rxq5_packets" },
+	{ "rxq5_bytes"   },
+	{ "rxq6_packets" },
+	{ "rxq6_bytes"   },
+	{ "rxq7_packets" },
+	{ "rxq7_bytes"   },
+	{ "rxq8_packets" },
+	{ "rxq8_bytes"   },
+	{ "rxq9_packets" },
+	{ "rxq9_bytes"   },
+	{ "rxq10_packets" },
+	{ "rxq10_bytes"   },
+	{ "rxq11_packets" },
+	{ "rxq11_bytes"   },
+	{ "rxq12_packets" },
+	{ "rxq12_bytes"   },
+	{ "rxq13_packets" },
+	{ "rxq13_bytes"   },
+	{ "rxq14_packets" },
+	{ "rxq14_bytes"   },
+	{ "rxq15_packets" },
+	{ "rxq15_bytes"   },
+};
+
+/**
+ * axienet_mcdma_tx_bd_free_tsn - Release MCDMA Tx buffer descriptor rings
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * This function is used to release the descriptors allocated in
+ * axienet_mcdma_tx_q_init_tsn.
+ */
+void __maybe_unused axienet_mcdma_tx_bd_free_tsn(struct net_device *ndev,
+						 struct axienet_dma_q *q)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (q->txq_bd_v) {
+		dma_free_coherent(ndev->dev.parent,
+				  sizeof(*q->txq_bd_v) * lp->tx_bd_num,
+				  q->txq_bd_v,
+				  q->tx_bd_p);
+	}
+	if (q->tx_bufs) {
+		dma_free_coherent(ndev->dev.parent,
+				  XAE_MAX_PKT_LEN * lp->tx_bd_num,
+				  q->tx_bufs,
+				  q->tx_bufs_dma);
+	}
+}
+
+/**
+ * axienet_mcdma_rx_bd_free_tsn - Release MCDMA Rx buffer descriptor rings
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * This function is used to release the descriptors allocated in
+ * axienet_mcdma_rx_q_init_tsn.
+ */
+void __maybe_unused axienet_mcdma_rx_bd_free_tsn(struct net_device *ndev,
+						 struct axienet_dma_q *q)
+{
+	int i;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	if (!q->rxq_bd_v)
+		return;
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		if (q->rxq_bd_v[i].phys)
+			dma_unmap_single(ndev->dev.parent, q->rxq_bd_v[i].phys,
+					 lp->max_frm_size, DMA_FROM_DEVICE);
+		dev_kfree_skb((struct sk_buff *)
+			      (q->rxq_bd_v[i].sw_id_offset));
+	}
+
+	dma_free_coherent(ndev->dev.parent,
+			  sizeof(*q->rxq_bd_v) * lp->rx_bd_num,
+			  q->rxq_bd_v,
+			  q->rx_bd_p);
+	q->rxq_bd_v = NULL;
+}
+
+/**
+ * axienet_mcdma_tx_q_init_tsn - Setup buffer descriptor rings for individual Axi
+ * MCDMA-Tx
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * Return: 0, on success -ENOMEM, on failure
+ *
+ * This function is helper function to axienet_dma_bd_init
+ */
+int __maybe_unused axienet_mcdma_tx_q_init_tsn(struct net_device *ndev,
+					       struct axienet_dma_q *q)
+{
+	u32 cr, chan_en;
+	int i;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	q->tx_bd_ci = 0;
+	q->tx_bd_tail = 0;
+
+	q->txq_bd_v = dma_alloc_coherent(ndev->dev.parent,
+					 sizeof(*q->txq_bd_v) * lp->tx_bd_num,
+					 &q->tx_bd_p, GFP_KERNEL);
+	if (!q->txq_bd_v)
+		goto out;
+
+	if (!q->eth_hasdre) {
+		q->tx_bufs = dma_alloc_coherent(ndev->dev.parent,
+						XAE_MAX_PKT_LEN * lp->tx_bd_num,
+						&q->tx_bufs_dma,
+						GFP_KERNEL);
+		if (!q->tx_bufs)
+			goto out;
+
+		for (i = 0; i < lp->tx_bd_num; i++)
+			q->tx_buf[i] = &q->tx_bufs[i * XAE_MAX_PKT_LEN];
+	}
+
+	for (i = 0; i < lp->tx_bd_num; i++) {
+		q->txq_bd_v[i].next = q->tx_bd_p +
+				      sizeof(*q->txq_bd_v) *
+				      ((i + 1) % lp->tx_bd_num);
+	}
+
+	/* Start updating the Tx channel control register */
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	/* Update the interrupt coalesce count */
+	cr = (((cr & ~XMCDMA_COALESCE_MASK)) |
+	      ((lp->coalesce_count_tx) << XMCDMA_COALESCE_SHIFT));
+	/* Update the delay timer count */
+	cr = (((cr & ~XMCDMA_DELAY_MASK)) |
+	      (XAXIDMA_DFT_TX_WAITBOUND << XMCDMA_DELAY_SHIFT));
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XMCDMA_IRQ_ALL_MASK;
+	/* Write to the Tx channel control register */
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+
+	/* Write to the RS (Run-stop) bit in the Tx channel control register.
+	 * Tx channel is now ready to run. But only after we write to the
+	 * tail pointer register that the Tx channel will start transmitting.
+	 */
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id),
+			  q->tx_bd_p);
+	cr = axienet_dma_in32(q, XMCDMA_CR_OFFSET);
+	axienet_dma_out32(q, XMCDMA_CR_OFFSET,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id),
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	chan_en = axienet_dma_in32(q, XMCDMA_CHEN_OFFSET);
+	chan_en |= (1 << (q->chan_id - 1));
+	axienet_dma_out32(q, XMCDMA_CHEN_OFFSET, chan_en);
+
+	return 0;
+out:
+	for_each_tx_dma_queue(lp, i) {
+		axienet_mcdma_tx_bd_free_tsn(ndev, lp->dq[i]);
+	}
+	return -ENOMEM;
+}
+
+/**
+ * axienet_mcdma_rx_q_init_tsn - Setup buffer descriptor rings for individual Axi
+ * MCDMA-Rx
+ * @ndev:	Pointer to the net_device structure
+ * @q:		Pointer to DMA queue structure
+ *
+ * Return: 0, on success -ENOMEM, on failure
+ *
+ * This function is helper function to axienet_dma_bd_init
+ */
+int __maybe_unused axienet_mcdma_rx_q_init_tsn(struct net_device *ndev,
+					       struct axienet_dma_q *q)
+{
+	u32 cr, chan_en;
+	int i;
+	struct sk_buff *skb;
+	struct axienet_local *lp = netdev_priv(ndev);
+	dma_addr_t mapping;
+
+	q->rx_bd_ci = 0;
+	q->rx_offset = XMCDMA_CHAN_RX_OFFSET;
+
+	q->rxq_bd_v = dma_alloc_coherent(ndev->dev.parent,
+					 sizeof(*q->rxq_bd_v) * lp->rx_bd_num,
+					 &q->rx_bd_p, GFP_KERNEL);
+	if (!q->rxq_bd_v)
+		goto out;
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		q->rxq_bd_v[i].next = q->rx_bd_p +
+				      sizeof(*q->rxq_bd_v) *
+				      ((i + 1) % lp->rx_bd_num);
+
+		skb = netdev_alloc_skb(ndev, lp->max_frm_size);
+		if (!skb)
+			goto out;
+
+		/* Ensure that the skb is completely updated
+		 * prio to mapping the DMA
+		 */
+		wmb();
+
+		q->rxq_bd_v[i].sw_id_offset = (phys_addr_t)skb;
+		mapping = dma_map_single(ndev->dev.parent,
+					 skb->data,
+					 lp->max_frm_size,
+					 DMA_FROM_DEVICE);
+		if (unlikely(dma_mapping_error(ndev->dev.parent, mapping))) {
+			dev_err(&ndev->dev, "mcdma map error\n");
+			goto out;
+		}
+
+		q->rxq_bd_v[i].phys = mapping;
+		q->rxq_bd_v[i].cntrl = lp->max_frm_size;
+	}
+
+	/* check if this is a mgmt channel */
+	if (lp->num_rx_queues == TSN_MAX_RX_Q_EX_EPSWITCH &&
+	    (q->chan_id == TSN_MAX_EX_EP_BE_CHAN ||
+	     q->chan_id == TSN_MAX_EX_EP_ST_CHAN ||
+	     q->chan_id == TSN_MAX_EX_EP_RES_CHAN)) {
+		q->flags = MCDMA_EP_EX_CHAN;
+	} else if ((lp->num_rx_queues == TSN_MIN_RX_Q_EX_EPSWITCH) &&
+		   ((q->chan_id == TSN_MIN_EX_EP_BE_CHAN) ||
+		    (q->chan_id == TSN_MIN_EX_EP_ST_CHAN))) {
+		q->flags = MCDMA_EP_EX_CHAN;
+	} else if (!(lp->abl_reg & TSN_BRIDGEEP_EPONLY) && q->chan_id == TSN_MGMT_CHAN) {
+		q->flags = MCDMA_MGMT_CHAN;
+	}
+
+	/* Start updating the Rx channel control register */
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+			      q->rx_offset);
+	/* Update the interrupt coalesce count */
+	cr = ((cr & ~XMCDMA_COALESCE_MASK) |
+	      ((lp->coalesce_count_rx) << XMCDMA_COALESCE_SHIFT));
+	/* Update the delay timer count */
+	cr = ((cr & ~XMCDMA_DELAY_MASK) |
+	      (XAXIDMA_DFT_RX_WAITBOUND << XMCDMA_DELAY_SHIFT));
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XMCDMA_IRQ_ALL_MASK;
+	/* Write to the Rx channel control register */
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+			  q->rx_offset, cr);
+
+	/* Populate the tail pointer and bring the Rx Axi DMA engine out of
+	 * halted state. This will make the Rx side ready for reception.
+	 */
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id) +
+			    q->rx_offset, q->rx_bd_p);
+	cr = axienet_dma_in32(q, XMCDMA_CR_OFFSET +  q->rx_offset);
+	axienet_dma_out32(q, XMCDMA_CR_OFFSET +  q->rx_offset,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				q->rx_offset);
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) + q->rx_offset,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id) +
+			    q->rx_offset, q->rx_bd_p + (sizeof(*q->rxq_bd_v) *
+			    (lp->rx_bd_num - 1)));
+	chan_en = axienet_dma_in32(q, XMCDMA_CHEN_OFFSET + q->rx_offset);
+	chan_en |= (1 << (q->chan_id - 1));
+	axienet_dma_out32(q, XMCDMA_CHEN_OFFSET + q->rx_offset, chan_en);
+
+	return 0;
+
+out:
+	for_each_rx_dma_queue(lp, i) {
+		axienet_mcdma_rx_bd_free_tsn(ndev, lp->dq[i]);
+	}
+	return -ENOMEM;
+}
+
+static inline int get_mcdma_tx_q(struct axienet_local *lp, u32 chan_id)
+{
+	int i;
+
+	for_each_tx_dma_queue(lp, i) {
+		if (chan_id == lp->chan_num[i])
+			return lp->qnum[i];
+	}
+
+	return -ENODEV;
+}
+
+static inline int get_mcdma_rx_q(struct axienet_local *lp, u32 chan_id)
+{
+	int i;
+
+	for_each_rx_dma_queue(lp, i) {
+		if (chan_id == lp->chan_num[i])
+			return lp->qnum[i];
+	}
+
+	return -ENODEV;
+}
+
+static inline int map_dma_q_txirq(int irq, struct axienet_local *lp)
+{
+	int i, chan_sermask;
+	u16 chan_id = 1;
+	struct axienet_dma_q *q = lp->dq[0];
+
+	chan_sermask = axienet_dma_in32(q, XMCDMA_TXINT_SER_OFFSET);
+
+	for (i = 1, chan_id = 1; i != 0 && i <= chan_sermask;
+	     i <<= 1, chan_id++) {
+		if (chan_sermask & i)
+			return chan_id;
+	}
+
+	return -ENODEV;
+}
+
+irqreturn_t __maybe_unused axienet_mcdma_tx_irq_tsn(int irq, void *_ndev)
+{
+	u32 cr;
+	unsigned int status;
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+	int i, j = map_dma_q_txirq(irq, lp);
+	struct axienet_dma_q *q;
+
+	if (j < 0)
+		return IRQ_NONE;
+
+	i = get_mcdma_tx_q(lp, j);
+	q = lp->dq[i];
+
+	status = axienet_dma_in32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id));
+	if (status & (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK)) {
+		axienet_dma_out32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id), status);
+		axienet_start_xmit_done_tsn(lp->ndev, q);
+		goto out;
+	}
+	if (!(status & XMCDMA_IRQ_ALL_MASK))
+		return IRQ_NONE;
+	if (status & XMCDMA_IRQ_ERR_MASK) {
+		dev_err(&ndev->dev, "DMA Tx error 0x%x\n", status);
+		dev_err(&ndev->dev, "Current BD is at: %pa\n",
+			&q->txq_bd_v[q->tx_bd_ci].phys);
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XMCDMA_IRQ_ALL_MASK);
+		/* Finally write to the Tx channel control register */
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      q->rx_offset);
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XMCDMA_IRQ_ALL_MASK);
+		/* write to the Rx channel control register */
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				  q->rx_offset, cr);
+
+		tasklet_schedule(&lp->dma_err_tasklet[i]);
+		axienet_dma_out32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset, status);
+	}
+out:
+	return IRQ_HANDLED;
+}
+
+static inline int map_dma_q_rxirq(int irq, struct axienet_local *lp)
+{
+	int i, chan_sermask;
+	u16 chan_id = 1;
+	struct axienet_dma_q *q = lp->dq[0];
+
+	chan_sermask = axienet_dma_in32(q, XMCDMA_RXINT_SER_OFFSET +
+					q->rx_offset);
+
+	for (i = 1, chan_id = 1; i != 0 && i <= chan_sermask;
+		i <<= 1, chan_id++) {
+		if (chan_sermask & i)
+			return chan_id;
+	}
+
+	return -ENODEV;
+}
+
+irqreturn_t __maybe_unused axienet_mcdma_rx_irq_tsn(int irq, void *_ndev)
+{
+	u32 cr;
+	unsigned int status;
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+	int i, j = map_dma_q_rxirq(irq, lp);
+	struct axienet_dma_q *q;
+
+	if (j < 0)
+		return IRQ_NONE;
+
+	i = get_mcdma_rx_q(lp, j);
+	q = lp->dq[i];
+
+	status = axienet_dma_in32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset);
+	if (status & (XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK)) {
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      q->rx_offset);
+		cr &= ~(XMCDMA_IRQ_IOC_MASK | XMCDMA_IRQ_DELAY_MASK);
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				  q->rx_offset, cr);
+		napi_schedule(&lp->napi[i]);
+	}
+
+	if (!(status & XMCDMA_IRQ_ALL_MASK))
+		return IRQ_NONE;
+
+	if (status & XMCDMA_IRQ_ERR_MASK) {
+		dev_err(&ndev->dev, "DMA Rx error 0x%x\n", status);
+		dev_err(&ndev->dev, "Current BD is at: %pa\n",
+			&q->rxq_bd_v[q->rx_bd_ci].phys);
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XMCDMA_IRQ_ALL_MASK);
+		/* Finally write to the Tx channel control register */
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+
+		cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				      q->rx_offset);
+		/* Disable coalesce, delay timer and error interrupts */
+		cr &= (~XMCDMA_IRQ_ALL_MASK);
+		/* write to the Rx channel control register */
+		axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				  q->rx_offset, cr);
+
+		tasklet_schedule(&lp->dma_err_tasklet[i]);
+		axienet_dma_out32(q, XMCDMA_CHAN_SR_OFFSET(q->chan_id) +
+				  q->rx_offset, status);
+	}
+
+	return IRQ_HANDLED;
+}
+
+void axienet_strings_tsn(struct net_device *ndev, u32 sset, u8 *data)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+	int i = AXIENET_ETHTOOLS_SSTATS_LEN, j, k = 0, l = 0;
+	static const char tx_packets[] = "tx_packets";
+	static const char tx_bytes[] = "tx_bytes";
+	static const char rx_packets[] = "rx_packets";
+	static const char rx_bytes[] = "rx_bytes";
+
+	for (j = 0; i < AXIENET_TX_SSTATS_LEN(lp) + AXIENET_ETHTOOLS_SSTATS_LEN;) {
+		if (j >= lp->num_tx_queues)
+			break;
+		q = lp->dq[j];
+		if (!q) {
+			if (sset == ETH_SS_STATS) {
+				memcpy(data + l++ * ETH_GSTRING_LEN, tx_packets,
+				       sizeof(tx_packets));
+				memcpy(data + l++ * ETH_GSTRING_LEN, tx_bytes, sizeof(tx_bytes));
+				memcpy(data + l++ * ETH_GSTRING_LEN, rx_packets,
+				       sizeof(rx_packets));
+				memcpy(data + l++ * ETH_GSTRING_LEN, rx_bytes, sizeof(rx_bytes));
+			}
+			return;
+		}
+		if (i % 2 == 0)
+			k = (q->chan_id - 1) * 2;
+		if (sset == ETH_SS_STATS)
+			memcpy(data + i * ETH_GSTRING_LEN,
+			       axienet_get_tx_strings_stats[k].name,
+			       ETH_GSTRING_LEN);
+		++i;
+		k++;
+		if (i % 2 == 0)
+			++j;
+	}
+	k = 0;
+	for (j = 0; i < AXIENET_TX_SSTATS_LEN(lp) + AXIENET_RX_SSTATS_LEN(lp) +
+			AXIENET_ETHTOOLS_SSTATS_LEN;) {
+		if (j >= lp->num_rx_queues)
+			break;
+		q = lp->dq[j];
+		if (!q) {
+			if (sset == ETH_SS_STATS) {
+				memcpy(data + l++ * ETH_GSTRING_LEN, tx_packets,
+				       sizeof(tx_packets));
+				memcpy(data + l++ * ETH_GSTRING_LEN, tx_bytes, sizeof(tx_bytes));
+				memcpy(data + l++ * ETH_GSTRING_LEN, rx_packets,
+				       sizeof(rx_packets));
+				memcpy(data + l++ * ETH_GSTRING_LEN, rx_bytes, sizeof(rx_bytes));
+			}
+			return;
+		}
+		if (i % 2 == 0)
+			k = (q->chan_id - 1) * 2;
+		if (sset == ETH_SS_STATS)
+			memcpy(data + i * ETH_GSTRING_LEN,
+			       axienet_get_rx_strings_stats[k].name,
+			       ETH_GSTRING_LEN);
+		++i;
+		k++;
+		if (i % 2 == 0)
+			++j;
+	}
+}
+
+int axienet_sset_count_tsn(struct net_device *ndev, int sset)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int i;
+
+	switch (sset) {
+	case ETH_SS_STATS:
+		for (i = 0; i < AXIENET_TX_SSTATS_LEN(lp); i++) {
+			if (!(lp->dq[i]))
+				return 4;
+		}
+		return (AXIENET_TX_SSTATS_LEN(lp) + AXIENET_RX_SSTATS_LEN(lp) +
+			AXIENET_ETHTOOLS_SSTATS_LEN);
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+void axienet_get_stats_tsn(struct net_device *ndev,
+			   struct ethtool_stats *stats,
+			   u64 *data)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+	int i = AXIENET_ETHTOOLS_SSTATS_LEN, j, k = 0;
+
+	for (j = 0; i < AXIENET_TX_SSTATS_LEN(lp) + AXIENET_ETHTOOLS_SSTATS_LEN;) {
+		if (j >= lp->num_tx_queues)
+			break;
+
+		q = lp->dq[j];
+		if (!q) {
+			data[k++] = ndev->stats.tx_packets;
+			data[k++] = ndev->stats.tx_bytes;
+			data[k++] = ndev->stats.rx_packets;
+			data[k++] = ndev->stats.rx_bytes;
+			return;
+		}
+
+		data[i++] = q->tx_packets;
+		data[i++] = q->tx_bytes;
+		++j;
+	}
+	for (j = 0; i < AXIENET_TX_SSTATS_LEN(lp) + AXIENET_RX_SSTATS_LEN(lp) +
+			AXIENET_ETHTOOLS_SSTATS_LEN;) {
+		if (j >= lp->num_rx_queues)
+			break;
+
+		q = lp->dq[j];
+		if (!q) {
+			data[k++] = ndev->stats.tx_packets;
+			data[k++] = ndev->stats.tx_bytes;
+			data[k++] = ndev->stats.rx_packets;
+			data[k++] = ndev->stats.rx_bytes;
+			return;
+		}
+		data[i++] = q->rx_packets;
+		data[i++] = q->rx_bytes;
+		++j;
+	}
+}
+
+/**
+ * axienet_mcdma_err_handler_tsn - Tasklet handler for Axi MCDMA Error
+ * @data:	Data passed
+ *
+ * Resets the Axi MCDMA and Axi Ethernet devices, and reconfigures the
+ * Tx/Rx BDs.
+ */
+void __maybe_unused axienet_mcdma_err_handler_tsn(unsigned long data)
+{
+	u32 axienet_status;
+	u32 cr, i, chan_en;
+	struct axienet_dma_q *q = (struct axienet_dma_q *)data;
+	struct axienet_local *lp = q->lp;
+	struct net_device *ndev = lp->ndev;
+	struct aximcdma_bd *cur_p;
+
+	lp->axienet_config->setoptions(ndev, lp->options &
+				       ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+	__axienet_device_reset_tsn(q);
+
+	for (i = 0; i < lp->tx_bd_num; i++) {
+		cur_p = &q->txq_bd_v[i];
+		if (cur_p->phys)
+			dma_unmap_single(ndev->dev.parent, cur_p->phys,
+					 (cur_p->cntrl &
+					  XAXIDMA_BD_CTRL_LENGTH_MASK),
+					 DMA_TO_DEVICE);
+		if (cur_p->tx_skb)
+			dev_kfree_skb_irq((struct sk_buff *)cur_p->tx_skb);
+		cur_p->phys = 0;
+		cur_p->cntrl = 0;
+		cur_p->status = 0;
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		cur_p->app2 = 0;
+		cur_p->app3 = 0;
+		cur_p->app4 = 0;
+		cur_p->sw_id_offset = 0;
+		cur_p->tx_skb = 0;
+	}
+
+	for (i = 0; i < lp->rx_bd_num; i++) {
+		cur_p = &q->rxq_bd_v[i];
+		cur_p->status = 0;
+		cur_p->app0 = 0;
+		cur_p->app1 = 0;
+		cur_p->app2 = 0;
+		cur_p->app3 = 0;
+		cur_p->app4 = 0;
+	}
+
+	q->tx_bd_ci = 0;
+	q->tx_bd_tail = 0;
+	q->rx_bd_ci = 0;
+
+	/* Start updating the Rx channel control register */
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+			      q->rx_offset);
+	/* Update the interrupt coalesce count */
+	cr = ((cr & ~XMCDMA_COALESCE_MASK) |
+	      ((lp->coalesce_count_rx) << XMCDMA_COALESCE_SHIFT));
+	/* Update the delay timer count */
+	cr = ((cr & ~XMCDMA_DELAY_MASK) |
+	      (XAXIDMA_DFT_RX_WAITBOUND << XMCDMA_DELAY_SHIFT));
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XMCDMA_IRQ_ALL_MASK;
+	/* Write to the Rx channel control register */
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+			  q->rx_offset, cr);
+
+	/* Start updating the Tx channel control register */
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	/* Update the interrupt coalesce count */
+	cr = (((cr & ~XMCDMA_COALESCE_MASK)) |
+	      ((lp->coalesce_count_tx) << XMCDMA_COALESCE_SHIFT));
+	/* Update the delay timer count */
+	cr = (((cr & ~XMCDMA_DELAY_MASK)) |
+	      (XAXIDMA_DFT_TX_WAITBOUND << XMCDMA_DELAY_SHIFT));
+	/* Enable coalesce, delay timer and error interrupts */
+	cr |= XMCDMA_IRQ_ALL_MASK;
+	/* Write to the Tx channel control register */
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id), cr);
+
+	/* Populate the tail pointer and bring the Rx Axi DMA engine out of
+	 * halted state. This will make the Rx side ready for reception.
+	 */
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id) +
+			    q->rx_offset, q->rx_bd_p);
+	cr = axienet_dma_in32(q, XMCDMA_CR_OFFSET +  q->rx_offset);
+	axienet_dma_out32(q, XMCDMA_CR_OFFSET +  q->rx_offset,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) +
+				q->rx_offset);
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id) + q->rx_offset,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	axienet_dma_bdout(q, XMCDMA_CHAN_TAILDESC_OFFSET(q->chan_id) +
+			    q->rx_offset, q->rx_bd_p + (sizeof(*q->rxq_bd_v) *
+			    (lp->rx_bd_num - 1)));
+	chan_en = axienet_dma_in32(q, XMCDMA_CHEN_OFFSET + q->rx_offset);
+	chan_en |= (1 << (q->chan_id - 1));
+	axienet_dma_out32(q, XMCDMA_CHEN_OFFSET + q->rx_offset, chan_en);
+
+	/* Write to the RS (Run-stop) bit in the Tx channel control register.
+	 * Tx channel is now ready to run. But only after we write to the
+	 * tail pointer register that the Tx channel will start transmitting.
+	 */
+	axienet_dma_bdout(q, XMCDMA_CHAN_CURDESC_OFFSET(q->chan_id),
+			  q->tx_bd_p);
+	cr = axienet_dma_in32(q, XMCDMA_CR_OFFSET);
+	axienet_dma_out32(q, XMCDMA_CR_OFFSET,
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	cr = axienet_dma_in32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id));
+	axienet_dma_out32(q, XMCDMA_CHAN_CR_OFFSET(q->chan_id),
+			  cr | XMCDMA_CR_RUNSTOP_MASK);
+	chan_en = axienet_dma_in32(q, XMCDMA_CHEN_OFFSET);
+	chan_en |= (1 << (q->chan_id - 1));
+	axienet_dma_out32(q, XMCDMA_CHEN_OFFSET, chan_en);
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC) {
+		axienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);
+		axienet_status &= ~XAE_RCW1_RX_MASK;
+		axienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);
+	}
+
+	if (lp->axienet_config->mactype == XAXIENET_1G && !lp->eth_hasnobuf) {
+		axienet_status = axienet_ior(lp, XAE_IP_OFFSET);
+		if (axienet_status & XAE_INT_RXRJECT_MASK)
+			axienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);
+	}
+
+	if (lp->axienet_config->mactype != XAXIENET_10G_25G &&
+	    lp->axienet_config->mactype != XAXIENET_MRMAC)
+		axienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);
+
+	lp->axienet_config->setoptions(ndev, lp->options &
+				       ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+	axienet_set_mac_address_tsn(ndev, NULL);
+	axienet_set_multicast_list_tsn(ndev);
+	lp->axienet_config->setoptions(ndev, lp->options);
+}
+
+int __maybe_unused axienet_mcdma_tx_probe_tsn(struct platform_device *pdev,
+					      struct device_node *np,
+					      struct axienet_local *lp)
+{
+	int i;
+	char dma_name[24];
+
+	u32 num = XAE_TSN_MIN_QUEUES;
+	int ret = 0;
+	/* get number of associated queues */
+	ret = of_property_read_u32(np, "xlnx,num-mm2s-channels", &num);
+	if (ret)
+		num = XAE_TSN_MIN_QUEUES;
+	lp->num_tx_queues = num;
+
+	for_each_tx_dma_queue(lp, i) {
+		struct axienet_dma_q *q;
+
+		q = lp->dq[i];
+
+		q->dma_regs = lp->mcdma_regs;
+		snprintf(dma_name, sizeof(dma_name), "mm2s_ch%d_introut",
+			 q->chan_id);
+		q->tx_irq = of_irq_get_byname(np, dma_name);
+		q->eth_hasdre = of_property_read_bool(np,
+						      "xlnx,include-mm2s-dre");
+		spin_lock_init(&q->tx_lock);
+	}
+	of_node_put(np);
+
+	return 0;
+}
+
+int __maybe_unused axienet_mcdma_rx_probe_tsn(struct platform_device *pdev,
+					      struct device_node *np,
+					      struct net_device *ndev)
+{
+	int i;
+	char dma_name[24];
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	for_each_rx_dma_queue(lp, i) {
+		struct axienet_dma_q *q;
+
+		q = lp->dq[i];
+
+		q->dma_regs = lp->mcdma_regs;
+		snprintf(dma_name, sizeof(dma_name), "s2mm_ch%d_introut",
+			 q->chan_id);
+		q->rx_irq = of_irq_get_byname(np, dma_name);
+		spin_lock_init(&q->rx_lock);
+
+		netif_napi_add(ndev, &lp->napi[i], xaxienet_rx_poll_tsn);
+	}
+
+	return 0;
+}
+
+static ssize_t rxch_obs1_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS1_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 1 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs2_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS2_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 2 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs3_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS3_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 3 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs4_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS4_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 4 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs5_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS5_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 5 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t rxch_obs6_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS6_OFFSET + q->rx_offset);
+
+	return sprintf(buf, "Ingress Channel Observer 6 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs1_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS1_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 1 Contents is 0x%x\n",
+		       reg);
+}
+
+static ssize_t txch_obs2_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS2_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 2 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs3_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS3_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 3 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs4_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS4_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 4 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs5_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS5_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 5 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t txch_obs6_show(struct device *dev,
+			      struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	u32 reg;
+
+	reg = axienet_dma_in32(q, XMCDMA_CHOBS6_OFFSET);
+
+	return sprintf(buf, "Egress Channel Observer 6 Contents is 0x%x\n\r",
+		       reg);
+}
+
+static ssize_t chan_weight_show(struct device *dev,
+				struct device_attribute *attr, char *buf)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	return sprintf(buf, "chan_id is %d and weight is %d\n",
+		       lp->chan_id, lp->weight);
+}
+
+static ssize_t chan_weight_store(struct device *dev,
+				 struct device_attribute *attr,
+				 const char *buf, size_t count)
+{
+	struct net_device *ndev = dev_get_drvdata(dev);
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q = lp->dq[0];
+	int ret;
+	u16 flags, chan_id;
+	u32 val;
+
+	ret = kstrtou16(buf, 16, &flags);
+	if (ret)
+		return ret;
+
+	lp->chan_id = (flags & 0xF0) >> 4;
+	lp->weight = flags & 0x0F;
+
+	if (lp->chan_id < 8)
+		val = axienet_dma_in32(q, XMCDMA_TXWEIGHT0_OFFSET);
+	else
+		val = axienet_dma_in32(q, XMCDMA_TXWEIGHT1_OFFSET);
+
+	if (lp->chan_id > 7)
+		chan_id = lp->chan_id - 8;
+	else
+		chan_id = lp->chan_id;
+
+	val &= ~XMCDMA_TXWEIGHT_CH_MASK(chan_id);
+	val |= lp->weight << XMCDMA_TXWEIGHT_CH_SHIFT(chan_id);
+
+	if (lp->chan_id < 8)
+		axienet_dma_out32(q, XMCDMA_TXWEIGHT0_OFFSET, val);
+	else
+		axienet_dma_out32(q, XMCDMA_TXWEIGHT1_OFFSET, val);
+
+	return count;
+}
+
+static DEVICE_ATTR_RW(chan_weight);
+static DEVICE_ATTR_RO(rxch_obs1);
+static DEVICE_ATTR_RO(rxch_obs2);
+static DEVICE_ATTR_RO(rxch_obs3);
+static DEVICE_ATTR_RO(rxch_obs4);
+static DEVICE_ATTR_RO(rxch_obs5);
+static DEVICE_ATTR_RO(rxch_obs6);
+static DEVICE_ATTR_RO(txch_obs1);
+static DEVICE_ATTR_RO(txch_obs2);
+static DEVICE_ATTR_RO(txch_obs3);
+static DEVICE_ATTR_RO(txch_obs4);
+static DEVICE_ATTR_RO(txch_obs5);
+static DEVICE_ATTR_RO(txch_obs6);
+static const struct attribute *mcdma_attrs[] = {
+	&dev_attr_chan_weight.attr,
+	&dev_attr_rxch_obs1.attr,
+	&dev_attr_rxch_obs2.attr,
+	&dev_attr_rxch_obs3.attr,
+	&dev_attr_rxch_obs4.attr,
+	&dev_attr_rxch_obs5.attr,
+	&dev_attr_rxch_obs6.attr,
+	&dev_attr_txch_obs1.attr,
+	&dev_attr_txch_obs2.attr,
+	&dev_attr_txch_obs3.attr,
+	&dev_attr_txch_obs4.attr,
+	&dev_attr_txch_obs5.attr,
+	&dev_attr_txch_obs6.attr,
+	NULL,
+};
+
+static const struct attribute_group mcdma_attributes = {
+	.attrs = (struct attribute **)mcdma_attrs,
+};
+
+int axeinet_mcdma_create_sysfs_tsn(struct kobject *kobj)
+{
+	return sysfs_create_group(kobj, &mcdma_attributes);
+}
+
+void axeinet_mcdma_remove_sysfs_tsn(struct kobject *kobj)
+{
+	sysfs_remove_group(kobj, &mcdma_attributes);
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_axienet_mdio_tsn.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_axienet_mdio_tsn.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,297 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * MDIO bus driver for the Xilinx Axi Ethernet device
+ *
+ * Copyright (c) 2009 Secret Lab Technologies, Ltd.
+ * Copyright (c) 2010 - 2011 Michal Simek <monstr@monstr.eu>
+ * Copyright (c) 2010 - 2011 PetaLogix
+ * Copyright (c) 2019 SED Systems, a division of Calian Ltd.
+ * Copyright (c) 2010 - 2012 Xilinx, Inc. All rights reserved.
+ */
+
+#include <linux/clk.h>
+#include <linux/of_address.h>
+#include <linux/of_mdio.h>
+#include <linux/jiffies.h>
+#include <linux/iopoll.h>
+
+#include "xilinx_axienet_tsn.h"
+
+#define MAX_MDIO_FREQ		2500000 /* 2.5 MHz */
+#define DEFAULT_HOST_CLOCK	150000000 /* 150 MHz */
+
+/* Wait till MDIO interface is ready to accept a new transaction */
+int axienet_mdio_wait_until_ready_tsn(struct axienet_local *lp)
+{
+	u32 val;
+
+	return readx_poll_timeout(axinet_ior_read_mcr, lp,
+				  val, val & XAE_MDIO_MCR_READY_MASK,
+				  1, 20000);
+}
+
+/* Enable the MDIO MDC. Called prior to a read/write operation */
+static void axienet_mdio_mdc_enable(struct axienet_local *lp)
+{
+	axienet_iow(lp, XAE_MDIO_MC_OFFSET,
+		    ((u32)lp->mii_clk_div | XAE_MDIO_MC_MDIOEN_MASK));
+}
+
+/* Disable the MDIO MDC. Called after a read/write operation */
+static void axienet_mdio_mdc_disable(struct axienet_local *lp)
+{
+	u32 mc_reg;
+
+	mc_reg = axienet_ior(lp, XAE_MDIO_MC_OFFSET);
+	axienet_iow(lp, XAE_MDIO_MC_OFFSET,
+		    (mc_reg & ~XAE_MDIO_MC_MDIOEN_MASK));
+}
+
+/**
+ * axienet_mdio_read - MDIO interface read function
+ * @bus:	Pointer to mii bus structure
+ * @phy_id:	Address of the PHY device
+ * @reg:	PHY register to read
+ *
+ * Return:	The register contents on success, -ETIMEDOUT on a timeout
+ *
+ * Reads the contents of the requested register from the requested PHY
+ * address by first writing the details into MCR register. After a while
+ * the register MRD is read to obtain the PHY register content.
+ */
+static int axienet_mdio_read(struct mii_bus *bus, int phy_id, int reg)
+{
+	u32 rc;
+	int ret;
+	struct axienet_local *lp = bus->priv;
+
+	axienet_mdio_mdc_enable(lp);
+
+	ret = axienet_mdio_wait_until_ready_tsn(lp);
+	if (ret < 0) {
+		axienet_mdio_mdc_disable(lp);
+		return ret;
+	}
+
+	axienet_iow(lp, XAE_MDIO_MCR_OFFSET,
+		    (((phy_id << XAE_MDIO_MCR_PHYAD_SHIFT) &
+		      XAE_MDIO_MCR_PHYAD_MASK) |
+		     ((reg << XAE_MDIO_MCR_REGAD_SHIFT) &
+		      XAE_MDIO_MCR_REGAD_MASK) |
+		     XAE_MDIO_MCR_INITIATE_MASK |
+		     XAE_MDIO_MCR_OP_READ_MASK));
+
+	ret = axienet_mdio_wait_until_ready_tsn(lp);
+	if (ret < 0) {
+		axienet_mdio_mdc_disable(lp);
+		return ret;
+	}
+
+	rc = axienet_ior(lp, XAE_MDIO_MRD_OFFSET) & 0x0000FFFF;
+
+	dev_dbg(lp->dev, "%s (phy_id=%i, reg=%x) == %x\n",
+		__func__, phy_id, reg, rc);
+
+	axienet_mdio_mdc_disable(lp);
+	return rc;
+}
+
+/**
+ * axienet_mdio_write - MDIO interface write function
+ * @bus:	Pointer to mii bus structure
+ * @phy_id:	Address of the PHY device
+ * @reg:	PHY register to write to
+ * @val:	Value to be written into the register
+ *
+ * Return:	0 on success, -ETIMEDOUT on a timeout
+ *
+ * Writes the value to the requested register by first writing the value
+ * into MWD register. The MCR register is then appropriately setup
+ * to finish the write operation.
+ */
+static int axienet_mdio_write(struct mii_bus *bus, int phy_id, int reg,
+			      u16 val)
+{
+	int ret;
+	struct axienet_local *lp = bus->priv;
+
+	dev_dbg(lp->dev, "%s (phy_id=%i, reg=%x, val=%x)\n",
+		__func__, phy_id, reg, val);
+
+	axienet_mdio_mdc_enable(lp);
+
+	ret = axienet_mdio_wait_until_ready_tsn(lp);
+	if (ret < 0) {
+		axienet_mdio_mdc_disable(lp);
+		return ret;
+	}
+
+	axienet_iow(lp, XAE_MDIO_MWD_OFFSET, (u32)val);
+	axienet_iow(lp, XAE_MDIO_MCR_OFFSET,
+		    (((phy_id << XAE_MDIO_MCR_PHYAD_SHIFT) &
+		      XAE_MDIO_MCR_PHYAD_MASK) |
+		     ((reg << XAE_MDIO_MCR_REGAD_SHIFT) &
+		      XAE_MDIO_MCR_REGAD_MASK) |
+		     XAE_MDIO_MCR_INITIATE_MASK |
+		     XAE_MDIO_MCR_OP_WRITE_MASK));
+
+	ret = axienet_mdio_wait_until_ready_tsn(lp);
+	if (ret < 0) {
+		axienet_mdio_mdc_disable(lp);
+		return ret;
+	}
+	axienet_mdio_mdc_disable(lp);
+	return 0;
+}
+
+/**
+ * axienet_mdio_enable_tsn - MDIO hardware setup function
+ * @lp:		Pointer to axienet local data structure.
+ *
+ * Return:	0 on success, -ETIMEDOUT on a timeout.
+ *
+ * Sets up the MDIO interface by initializing the MDIO clock and enabling the
+ * MDIO interface in hardware.
+ */
+int axienet_mdio_enable_tsn(struct axienet_local *lp)
+{
+	u32 host_clock;
+
+	lp->mii_clk_div = 0;
+
+	if (lp->axi_clk) {
+		host_clock = clk_get_rate(lp->axi_clk);
+	} else {
+		struct device_node *np1;
+
+		/* Legacy fallback: detect CPU clock frequency and use as AXI
+		 * bus clock frequency. This only works on certain platforms.
+		 */
+		np1 = of_find_node_by_name(NULL, "cpu");
+		if (!np1) {
+			netdev_warn(lp->ndev, "Could not find CPU device node.\n");
+			host_clock = DEFAULT_HOST_CLOCK;
+		} else {
+			int ret = of_property_read_u32(np1, "clock-frequency",
+						       &host_clock);
+			if (ret) {
+				netdev_warn(lp->ndev, "CPU clock-frequency property not found.\n");
+				host_clock = DEFAULT_HOST_CLOCK;
+			}
+			of_node_put(np1);
+		}
+		netdev_info(lp->ndev, "Setting assumed host clock to %u\n",
+			    host_clock);
+	}
+
+	/* clk_div can be calculated by deriving it from the equation:
+	 * fMDIO = fHOST / ((1 + clk_div) * 2)
+	 *
+	 * Where fMDIO <= 2500000, so we get:
+	 * fHOST / ((1 + clk_div) * 2) <= 2500000
+	 *
+	 * Then we get:
+	 * 1 / ((1 + clk_div) * 2) <= (2500000 / fHOST)
+	 *
+	 * Then we get:
+	 * 1 / (1 + clk_div) <= ((2500000 * 2) / fHOST)
+	 *
+	 * Then we get:
+	 * 1 / (1 + clk_div) <= (5000000 / fHOST)
+	 *
+	 * So:
+	 * (1 + clk_div) >= (fHOST / 5000000)
+	 *
+	 * And finally:
+	 * clk_div >= (fHOST / 5000000) - 1
+	 *
+	 * fHOST can be read from the flattened device tree as property
+	 * "clock-frequency" from the CPU
+	 */
+
+	lp->mii_clk_div = (host_clock / (MAX_MDIO_FREQ * 2)) - 1;
+	/* If there is any remainder from the division of
+	 * fHOST / (MAX_MDIO_FREQ * 2), then we need to add
+	 * 1 to the clock divisor or we will surely be above 2.5 MHz
+	 */
+	if (host_clock % (MAX_MDIO_FREQ * 2))
+		lp->mii_clk_div++;
+
+	netdev_dbg(lp->ndev,
+		   "Setting MDIO clock divisor to %u/%u Hz host clock.\n",
+		   lp->mii_clk_div, host_clock);
+
+	axienet_iow(lp, XAE_MDIO_MC_OFFSET, lp->mii_clk_div | XAE_MDIO_MC_MDIOEN_MASK);
+
+	return axienet_mdio_wait_until_ready_tsn(lp);
+}
+
+/**
+ * axienet_mdio_disable_tsn - MDIO hardware disable function
+ * @lp:		Pointer to axienet local data structure.
+ *
+ * Disable the MDIO interface in hardware.
+ */
+void axienet_mdio_disable_tsn(struct axienet_local *lp)
+{
+	axienet_iow(lp, XAE_MDIO_MC_OFFSET, 0);
+}
+
+/**
+ * axienet_mdio_setup_tsn - MDIO setup function
+ * @lp:		Pointer to axienet local data structure.
+ *
+ * Return:	0 on success, -ETIMEDOUT on a timeout, -ENOMEM when
+ *		mdiobus_alloc (to allocate memory for mii bus structure) fails.
+ *
+ * Sets up the MDIO interface by initializing the MDIO clock.
+ * Register the MDIO interface.
+ */
+int axienet_mdio_setup_tsn(struct axienet_local *lp)
+{
+	struct device_node *mdio_node;
+	struct mii_bus *bus;
+	int ret;
+
+	ret = axienet_mdio_enable_tsn(lp);
+	if (ret < 0)
+		return ret;
+
+	bus = mdiobus_alloc();
+	if (!bus)
+		return -ENOMEM;
+
+	snprintf(bus->id, MII_BUS_ID_SIZE, "axienet-%.8llx",
+		 (unsigned long long)lp->regs_start);
+
+	bus->priv = lp;
+	bus->name = "Xilinx Axi Ethernet MDIO";
+	bus->read = axienet_mdio_read;
+	bus->write = axienet_mdio_write;
+	bus->parent = lp->dev;
+	lp->mii_bus = bus;
+
+	mdio_node = of_get_child_by_name(lp->dev->of_node, "mdio");
+	ret = of_mdiobus_register(bus, mdio_node);
+	of_node_put(mdio_node);
+	if (ret) {
+		mdiobus_free(bus);
+		lp->mii_bus = NULL;
+		return ret;
+	}
+	axienet_mdio_mdc_disable(lp);
+	return 0;
+}
+
+/**
+ * axienet_mdio_teardown_tsn - MDIO remove function
+ * @lp:		Pointer to axienet local data structure.
+ *
+ * Unregisters the MDIO and frees any associate memory for mii bus.
+ */
+void axienet_mdio_teardown_tsn(struct axienet_local *lp)
+{
+	mdiobus_unregister(lp->mii_bus);
+	mdiobus_free(lp->mii_bus);
+	lp->mii_bus = NULL;
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_axienet_tsn.h
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_axienet_tsn.h	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,1333 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Definitions for Xilinx Axi Ethernet device driver.
+ *
+ * Copyright (c) 2009 Secret Lab Technologies, Ltd.
+ * Copyright (c) 2010 - 2012 Xilinx, Inc. All rights reserved.
+ */
+
+#ifndef XILINX_AXIENET_TSN_H
+#define XILINX_AXIENET_TSN_H
+
+#include <linux/netdevice.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/if_vlan.h>
+#include <linux/net_tstamp.h>
+#include <linux/phy.h>
+#include <linux/of_platform.h>
+#include <linux/clk.h>
+
+/* Packet size info */
+#define XAE_HDR_SIZE			14 /* Size of Ethernet header */
+#define XAE_TRL_SIZE			 4 /* Size of Ethernet trailer (FCS) */
+#define XAE_MTU			      1500 /* Max MTU of an Ethernet frame */
+#define XAE_JUMBO_MTU		      9000 /* Max MTU of a jumbo Eth. frame */
+
+#define XAE_MAX_FRAME_SIZE	 (XAE_MTU + XAE_HDR_SIZE + XAE_TRL_SIZE)
+#define XAE_MAX_VLAN_FRAME_SIZE  (XAE_MTU + VLAN_ETH_HLEN + XAE_TRL_SIZE)
+#define XAE_MAX_JUMBO_FRAME_SIZE (XAE_JUMBO_MTU + XAE_HDR_SIZE + XAE_TRL_SIZE)
+
+/* Queue Numbers of BE, RES, ST and PTP */
+
+#define BE_QUEUE_NUMBER  0
+#define RES_QUEUE_NUMBER 1
+#define ST_QUEUE_NUMBER  2
+#define PTP_QUEUE_NUMBER 3
+
+/* DMA address width min and max range */
+#define XAE_DMA_MASK_MIN	32
+#define XAE_DMA_MASK_MAX	64
+
+/* In AXI DMA Tx and Rx queue count is same */
+#define for_each_tx_dma_queue(lp, var) \
+	for ((var) = 0; (var) < (lp)->num_tx_queues; (var)++)
+
+#define for_each_rx_dma_queue(lp, var) \
+	for ((var) = 0; (var) < (lp)->num_rx_queues; (var)++)
+/* Configuration options */
+
+/* Accept all incoming packets. Default: disabled (cleared) */
+#define XAE_OPTION_PROMISC			BIT(0)
+
+/* Jumbo frame support for Tx & Rx. Default: disabled (cleared) */
+#define XAE_OPTION_JUMBO			BIT(1)
+
+/* VLAN Rx & Tx frame support. Default: disabled (cleared) */
+#define XAE_OPTION_VLAN				BIT(2)
+
+/* Enable recognition of flow control frames on Rx. Default: enabled (set) */
+#define XAE_OPTION_FLOW_CONTROL			BIT(4)
+
+/* Strip FCS and PAD from incoming frames. Note: PAD from VLAN frames is not
+ * stripped. Default: disabled (set)
+ */
+#define XAE_OPTION_FCS_STRIP			BIT(5)
+
+/* Generate FCS field and add PAD automatically for outgoing frames.
+ * Default: enabled (set)
+ */
+#define XAE_OPTION_FCS_INSERT			BIT(6)
+
+/* Enable Length/Type error checking for incoming frames. When this option is
+ * set, the MAC will filter frames that have a mismatched type/length field
+ * and if XAE_OPTION_REPORT_RXERR is set, the user is notified when these
+ * types of frames are encountered. When this option is cleared, the MAC will
+ * allow these types of frames to be received. Default: enabled (set)
+ */
+#define XAE_OPTION_LENTYPE_ERR			BIT(7)
+
+/* Enable the transmitter. Default: enabled (set) */
+#define XAE_OPTION_TXEN				BIT(11)
+
+/*  Enable the receiver. Default: enabled (set) */
+#define XAE_OPTION_RXEN				BIT(12)
+
+/*  Default options set when device is initialized or reset */
+#define XAE_OPTION_DEFAULTS				   \
+				(XAE_OPTION_TXEN |	   \
+				 XAE_OPTION_FLOW_CONTROL | \
+				 XAE_OPTION_RXEN)
+
+/* Axi DMA Register definitions */
+
+#define XAXIDMA_TX_CR_OFFSET	0x00000000 /* Channel control */
+#define XAXIDMA_TX_SR_OFFSET	0x00000004 /* Status */
+#define XAXIDMA_TX_CDESC_OFFSET	0x00000008 /* Current descriptor pointer */
+#define XAXIDMA_TX_TDESC_OFFSET	0x00000010 /* Tail descriptor pointer */
+
+#define XAXIDMA_RX_CR_OFFSET	0x00000030 /* Channel control */
+#define XAXIDMA_RX_SR_OFFSET	0x00000034 /* Status */
+#define XAXIDMA_RX_CDESC_OFFSET	0x00000038 /* Current descriptor pointer */
+#define XAXIDMA_RX_TDESC_OFFSET	0x00000040 /* Tail descriptor pointer */
+
+#define XAXIDMA_CR_RUNSTOP_MASK	0x00000001 /* Start/stop DMA channel */
+#define XAXIDMA_CR_RESET_MASK	0x00000004 /* Reset DMA engine */
+
+#define XAXIDMA_SR_HALT_MASK	0x00000001 /* Indicates DMA channel halted */
+
+#define XAXIDMA_BD_NDESC_OFFSET		0x00 /* Next descriptor pointer */
+#define XAXIDMA_BD_BUFA_OFFSET		0x08 /* Buffer address */
+#define XAXIDMA_BD_CTRL_LEN_OFFSET	0x18 /* Control/buffer length */
+#define XAXIDMA_BD_STS_OFFSET		0x1C /* Status */
+#define XAXIDMA_BD_USR0_OFFSET		0x20 /* User IP specific word0 */
+#define XAXIDMA_BD_USR1_OFFSET		0x24 /* User IP specific word1 */
+#define XAXIDMA_BD_USR2_OFFSET		0x28 /* User IP specific word2 */
+#define XAXIDMA_BD_USR3_OFFSET		0x2C /* User IP specific word3 */
+#define XAXIDMA_BD_USR4_OFFSET		0x30 /* User IP specific word4 */
+#define XAXIDMA_BD_ID_OFFSET		0x34 /* Sw ID */
+#define XAXIDMA_BD_HAS_STSCNTRL_OFFSET	0x38 /* Whether has stscntrl strm */
+#define XAXIDMA_BD_HAS_DRE_OFFSET	0x3C /* Whether has DRE */
+
+#define XAXIDMA_BD_HAS_DRE_SHIFT	8 /* Whether has DRE shift */
+#define XAXIDMA_BD_HAS_DRE_MASK		0xF00 /* Whether has DRE mask */
+#define XAXIDMA_BD_WORDLEN_MASK		0xFF /* Whether has DRE mask */
+
+#define XAXIDMA_BD_CTRL_LENGTH_MASK	0x007FFFFF /* Requested len */
+#define XAXIDMA_BD_CTRL_TXSOF_MASK	0x08000000 /* First tx packet */
+#define XAXIDMA_BD_CTRL_TXEOF_MASK	0x04000000 /* Last tx packet */
+#define XAXIDMA_BD_CTRL_ALL_MASK	0x0C000000 /* All control bits */
+
+#define XAXIDMA_DELAY_MASK		0xFF000000 /* Delay timeout counter */
+#define XAXIDMA_COALESCE_MASK		0x00FF0000 /* Coalesce counter */
+
+#define XAXIDMA_DELAY_SHIFT		24
+#define XAXIDMA_COALESCE_SHIFT		16
+
+#define XAXIDMA_IRQ_IOC_MASK		0x00001000 /* Completion intr */
+#define XAXIDMA_IRQ_DELAY_MASK		0x00002000 /* Delay interrupt */
+#define XAXIDMA_IRQ_ERROR_MASK		0x00004000 /* Error interrupt */
+#define XAXIDMA_IRQ_ALL_MASK		0x00007000 /* All interrupts */
+
+/* Default TX/RX Threshold and waitbound values for SGDMA mode */
+#define XAXIDMA_DFT_TX_THRESHOLD	24
+#define XAXIDMA_DFT_TX_WAITBOUND	254
+#define XAXIDMA_DFT_RX_THRESHOLD	1
+#define XAXIDMA_DFT_RX_WAITBOUND	254
+
+#define XAXIDMA_BD_CTRL_TXSOF_MASK	0x08000000 /* First tx packet */
+#define XAXIDMA_BD_CTRL_TXEOF_MASK	0x04000000 /* Last tx packet */
+#define XAXIDMA_BD_CTRL_ALL_MASK	0x0C000000 /* All control bits */
+
+#define XAXIDMA_BD_STS_ACTUAL_LEN_MASK	0x007FFFFF /* Actual len */
+#define XAXIDMA_BD_STS_COMPLETE_MASK	0x80000000 /* Completed */
+#define XAXIDMA_BD_STS_DEC_ERR_MASK	0x40000000 /* Decode error */
+#define XAXIDMA_BD_STS_SLV_ERR_MASK	0x20000000 /* Slave error */
+#define XAXIDMA_BD_STS_INT_ERR_MASK	0x10000000 /* Internal err */
+#define XAXIDMA_BD_STS_ALL_ERR_MASK	0x70000000 /* All errors */
+#define XAXIDMA_BD_STS_RXSOF_MASK	0x08000000 /* First rx pkt */
+#define XAXIDMA_BD_STS_RXEOF_MASK	0x04000000 /* Last rx pkt */
+#define XAXIDMA_BD_STS_ALL_MASK		0xFC000000 /* All status bits */
+
+#define XAXIDMA_BD_MINIMUM_ALIGNMENT	0x40
+
+/* AXI Tx Timestamp Stream FIFO Register Definitions */
+#define XAXIFIFO_TXTS_ISR	0x00000000 /* Interrupt Status Register */
+#define XAXIFIFO_TXTS_TDFV	0x0000000C /* Transmit Data FIFO Vacancy */
+#define XAXIFIFO_TXTS_TXFD	0x00000010 /* Tx Data Write Port */
+#define XAXIFIFO_TXTS_TLR	0x00000014 /* Transmit Length Register */
+#define XAXIFIFO_TXTS_RFO	0x0000001C /* Rx Fifo Occupancy */
+#define XAXIFIFO_TXTS_RDFR	0x00000018 /* Rx Fifo reset */
+#define XAXIFIFO_TXTS_RXFD	0x00000020 /* Rx Data Read Port */
+#define XAXIFIFO_TXTS_RLR	0x00000024 /* Receive Length Register */
+#define XAXIFIFO_TXTS_SRR	0x00000028 /* AXI4-Stream Reset */
+
+#define XAXIFIFO_TXTS_INT_RC_MASK	0x04000000
+#define XAXIFIFO_TXTS_RXFD_MASK		0x7FFFFFFF
+#define XAXIFIFO_TXTS_RESET_MASK	0x000000A5
+#define XAXIFIFO_TXTS_TAG_MASK		0xFFFF0000
+#define XAXIFIFO_TXTS_TAG_SHIFT		16
+#define XAXIFIFO_TXTS_TAG_MAX		0xFFFE
+
+/* Axi Ethernet registers definition */
+#define XAE_RAF_OFFSET		0x00000000 /* Reset and Address filter */
+#define XAE_TPF_OFFSET		0x00000004 /* Tx Pause Frame */
+#define XAE_IFGP_OFFSET		0x00000008 /* Tx Inter-frame gap adjustment*/
+#define XAE_IS_OFFSET		0x0000000C /* Interrupt status */
+#define XAE_IP_OFFSET		0x00000010 /* Interrupt pending */
+#define XAE_IE_OFFSET		0x00000014 /* Interrupt enable */
+#define XAE_TTAG_OFFSET		0x00000018 /* Tx VLAN TAG */
+#define XAE_RTAG_OFFSET		0x0000001C /* Rx VLAN TAG */
+#define XAE_UAWL_OFFSET		0x00000020 /* Unicast address word lower */
+#define XAE_UAWU_OFFSET		0x00000024 /* Unicast address word upper */
+#define XAE_TPID0_OFFSET	0x00000028 /* VLAN TPID0 register */
+#define XAE_TPID1_OFFSET	0x0000002C /* VLAN TPID1 register */
+#define XAE_PPST_OFFSET		0x00000030 /* PCS PMA Soft Temac Status Reg */
+#define XAE_RCW0_OFFSET		0x00000400 /* Rx Configuration Word 0 */
+#define XAE_RCW1_OFFSET		0x00000404 /* Rx Configuration Word 1 */
+#define XAE_TC_OFFSET		0x00000408 /* Tx Configuration */
+#define XAE_FCC_OFFSET		0x0000040C /* Flow Control Configuration */
+#define XAE_ID_OFFSET		0x000004F8 /* Identification register */
+#define XAE_EMMC_OFFSET		0x00000410 /* MAC speed configuration */
+#define XAE_RMFC_OFFSET		0x00000414 /* RX Max Frame Configuration */
+#define XAE_TSN_ABL_OFFSET	0x000004FC /* Ability Register */
+#define XAE_MDIO_MC_OFFSET	0x00000500 /* MDIO Setup */
+#define XAE_MDIO_MCR_OFFSET	0x00000504 /* MDIO Control */
+#define XAE_MDIO_MWD_OFFSET	0x00000508 /* MDIO Write Data */
+#define XAE_MDIO_MRD_OFFSET	0x0000050C /* MDIO Read Data */
+#define XAE_TEMAC_IS_OFFSET	0x00000600 /* TEMAC Interrupt Status */
+#define XAE_TEMAC_IP_OFFSET	0x00000610 /* TEMAC Interrupt Pending Status */
+#define XAE_TEMAC_IE_OFFSET	0x00000620 /* TEMAC Interrupt Enable Status */
+#define XAE_TEMAC_IC_OFFSET	0x00000630 /* TEMAC Interrupt Clear Status */
+#define XAE_UAW0_OFFSET		0x00000700 /* Unicast address word 0 */
+#define XAE_UAW1_OFFSET		0x00000704 /* Unicast address word 1 */
+#define XAE_FMC_OFFSET		0x00000708 /* Frame Filter Control */
+#define XAE_AF0_OFFSET		0x00000710 /* Address Filter 0 */
+#define XAE_AF1_OFFSET		0x00000714 /* Address Filter 1 */
+#define XAE_FF_3_OFFSET		0x0000071C /* Frame Filter 3 */
+#define XAE_FF_5_OFFSET		0x00000724 /* Frame Filter 5 */
+#define XAE_FF_9_OFFSET		0x00000734 /* Frame Filter 9 */
+#define XAE_FF_10_OFFSET	0x00000738 /* Frame Filter 10 */
+#define XAE_AF0_MASK_OFFSET	0x00000750 /* Address Filter Mask 0 */
+#define XAE_AF1_MASK_OFFSET	0x00000754 /* Address Filter Mask 1 */
+#define XAE_FF_5_MASK_OFFSET	0x00000764 /* Frame Filter Mask register 5 */
+#define XAE_FF_9_MASK_OFFSET	0x00000774 /* Frame Filter Mask register 9 */
+#define XAE_FF_10_MASK_OFFSET	0x00000778 /* Frame Filter Mask register 10 */
+
+#define XAE_TX_VLAN_DATA_OFFSET 0x00004000 /* TX VLAN data table address */
+#define XAE_RX_VLAN_DATA_OFFSET 0x00008000 /* RX VLAN data table address */
+#define XAE_MCAST_TABLE_OFFSET	0x00020000 /* Multicast table address */
+
+/* Bit Masks for Axi Ethernet RAF register */
+/* Reject receive multicast destination address */
+#define XAE_RAF_MCSTREJ_MASK		0x00000002
+/* Reject receive broadcast destination address */
+#define XAE_RAF_BCSTREJ_MASK		0x00000004
+#define XAE_RAF_TXVTAGMODE_MASK		0x00000018 /* Tx VLAN TAG mode */
+#define XAE_RAF_RXVTAGMODE_MASK		0x00000060 /* Rx VLAN TAG mode */
+#define XAE_RAF_TXVSTRPMODE_MASK	0x00000180 /* Tx VLAN STRIP mode */
+#define XAE_RAF_RXVSTRPMODE_MASK	0x00000600 /* Rx VLAN STRIP mode */
+#define XAE_RAF_NEWFNCENBL_MASK		0x00000800 /* New function mode */
+/* Extended Multicast Filtering mode */
+#define XAE_RAF_EMULTIFLTRENBL_MASK	0x00001000
+#define XAE_RAF_STATSRST_MASK		0x00002000 /* Stats. Counter Reset */
+#define XAE_RAF_RXBADFRMEN_MASK		0x00004000 /* Recv Bad Frame Enable */
+#define XAE_RAF_TXVTAGMODE_SHIFT	3 /* Tx Tag mode shift bits */
+#define XAE_RAF_RXVTAGMODE_SHIFT	5 /* Rx Tag mode shift bits */
+#define XAE_RAF_TXVSTRPMODE_SHIFT	7 /* Tx strip mode shift bits*/
+#define XAE_RAF_RXVSTRPMODE_SHIFT	9 /* Rx Strip mode shift bits*/
+
+/* Bit Masks for Axi Ethernet TPF and IFGP registers */
+#define XAE_TPF_TPFV_MASK		0x0000FFFF /* Tx pause frame value */
+/* Transmit inter-frame gap adjustment value */
+#define XAE_IFGP0_IFGP_MASK		0x0000007F
+
+/* Bit Masks for Axi Ethernet IS, IE and IP registers, Same masks apply
+ * for all 3 registers.
+ */
+/* Hard register access complete */
+#define XAE_INT_HARDACSCMPLT_MASK	0x00000001
+/* Auto negotiation complete */
+#define XAE_INT_AUTONEG_MASK		0x00000002
+#define XAE_INT_RXCMPIT_MASK		0x00000004 /* Rx complete */
+#define XAE_INT_RXRJECT_MASK		0x00000008 /* Rx frame rejected */
+#define XAE_INT_RXFIFOOVR_MASK		0x00000010 /* Rx fifo overrun */
+#define XAE_INT_TXCMPIT_MASK		0x00000020 /* Tx complete */
+#define XAE_INT_RXDCMLOCK_MASK		0x00000040 /* Rx Dcm Lock */
+#define XAE_INT_MGTRDY_MASK		0x00000080 /* MGT clock Lock */
+#define XAE_INT_PHYRSTCMPLT_MASK	0x00000100 /* Phy Reset complete */
+#define XAE_INT_ALL_MASK		0x0000003F /* All the ints */
+
+/* INT bits that indicate receive errors */
+#define XAE_INT_RECV_ERROR_MASK				\
+	(XAE_INT_RXRJECT_MASK | XAE_INT_RXFIFOOVR_MASK)
+
+/* Bit masks for Axi Ethernet VLAN TPID Word 0 register */
+#define XAE_TPID_0_MASK		0x0000FFFF /* TPID 0 */
+#define XAE_TPID_1_MASK		0xFFFF0000 /* TPID 1 */
+
+/* Bit masks for Axi Ethernet VLAN TPID Word 1 register */
+#define XAE_TPID_2_MASK		0x0000FFFF /* TPID 0 */
+#define XAE_TPID_3_MASK		0xFFFF0000 /* TPID 1 */
+
+/* Bit masks for Axi Ethernet RCW1 register */
+#define XAE_RCW1_INBAND1588_MASK 0x00400000 /* Inband 1588 Enable */
+#define XAE_RCW1_RST_MASK	0x80000000 /* Reset */
+#define XAE_RCW1_JUM_MASK	0x40000000 /* Jumbo frame enable */
+/* In-Band FCS enable (FCS not stripped) */
+#define XAE_RCW1_FCS_MASK	0x20000000
+#define XAE_RCW1_RX_MASK	0x10000000 /* Receiver enable */
+#define XAE_RCW1_VLAN_MASK	0x08000000 /* VLAN frame enable */
+/* Length/type field valid check disable */
+#define XAE_RCW1_LT_DIS_MASK	0x02000000
+/* Control frame Length check disable */
+#define XAE_RCW1_CL_DIS_MASK	0x01000000
+/* Pause frame source address bits [47:32]. Bits [31:0] are
+ * stored in register RCW0
+ */
+#define XAE_RCW1_PAUSEADDR_MASK 0x0000FFFF
+
+/* Bit masks for Axi Ethernet TC register */
+#define XAE_TC_INBAND1588_MASK 0x00400000 /* Inband 1588 Enable */
+#define XAE_TC_RST_MASK		0x80000000 /* Reset */
+#define XAE_TC_JUM_MASK		0x40000000 /* Jumbo frame enable */
+/* In-Band FCS enable (FCS not generated) */
+#define XAE_TC_FCS_MASK		0x20000000
+#define XAE_TC_TX_MASK		0x10000000 /* Transmitter enable */
+#define XAE_TC_VLAN_MASK	0x08000000 /* VLAN frame enable */
+/* Inter-frame gap adjustment enable */
+#define XAE_TC_IFG_MASK		0x02000000
+
+/* Bit masks for Axi Ethernet FCC register */
+#define XAE_FCC_FCRX_MASK	0x20000000 /* Rx flow control enable */
+#define XAE_FCC_FCTX_MASK	0x40000000 /* Tx flow control enable */
+
+/* Bit masks for Axi Ethernet EMMC register */
+#define XAE_EMMC_LINKSPEED_MASK	0xC0000000 /* Link speed */
+#define XAE_EMMC_RGMII_MASK	0x20000000 /* RGMII mode enable */
+#define XAE_EMMC_SGMII_MASK	0x10000000 /* SGMII mode enable */
+#define XAE_EMMC_GPCS_MASK	0x08000000 /* 1000BaseX mode enable */
+#define XAE_EMMC_HOST_MASK	0x04000000 /* Host interface enable */
+#define XAE_EMMC_TX16BIT	0x02000000 /* 16 bit Tx client enable */
+#define XAE_EMMC_RX16BIT	0x01000000 /* 16 bit Rx client enable */
+#define XAE_EMMC_LINKSPD_10	0x00000000 /* Link Speed mask for 10 Mbit */
+#define XAE_EMMC_LINKSPD_100	0x40000000 /* Link Speed mask for 100 Mbit */
+#define XAE_EMMC_LINKSPD_1000	0x80000000 /* Link Speed mask for 1000 Mbit */
+#define XAE_EMMC_LINKSPD_2500	0x80000000 /* Link Speed mask for 2500 Mbit */
+
+/* Bit masks for Axi Ethernet MDIO interface MC register */
+#define XAE_MDIO_MC_MDIOEN_MASK		0x00000040 /* MII management enable */
+#define XAE_MDIO_MC_CLOCK_DIVIDE_MAX	0x3F	   /* Maximum MDIO divisor */
+
+/* Bit masks for Axi Ethernet MDIO interface MCR register */
+#define XAE_MDIO_MCR_PHYAD_MASK		0x1F000000 /* Phy Address Mask */
+#define XAE_MDIO_MCR_PHYAD_SHIFT	24	   /* Phy Address Shift */
+#define XAE_MDIO_MCR_REGAD_MASK		0x001F0000 /* Reg Address Mask */
+#define XAE_MDIO_MCR_REGAD_SHIFT	16	   /* Reg Address Shift */
+#define XAE_MDIO_MCR_OP_MASK		0x0000C000 /* Operation Code Mask */
+#define XAE_MDIO_MCR_OP_SHIFT		13	   /* Operation Code Shift */
+#define XAE_MDIO_MCR_OP_READ_MASK	0x00008000 /* Op Code Read Mask */
+#define XAE_MDIO_MCR_OP_WRITE_MASK	0x00004000 /* Op Code Write Mask */
+#define XAE_MDIO_MCR_INITIATE_MASK	0x00000800 /* Ready Mask */
+#define XAE_MDIO_MCR_READY_MASK		0x00000080 /* Ready Mask */
+
+/* Bit masks for Axi Ethernet UAW1 register */
+/* Station address bits [47:32]; Station address
+ * bits [31:0] are stored in register UAW0
+ */
+#define XAE_UAW1_UNICASTADDR_MASK	0x0000FFFF
+
+/* Bit masks for Axi Ethernet FMC register */
+#define XAE_FMC_PM_MASK			0x80000000 /* Promis. mode enable */
+#define XAE_FMC_IND_MASK		0x00000003 /* Index Mask */
+
+#define XAE_MDIO_DIV_DFT		29 /* Default MDIO clock divisor */
+
+/* Total number of entries in the hardware multicast table. */
+#define XAE_MULTICAST_CAM_TABLE_NUM	4
+
+/* Axi Ethernet Synthesis features */
+#define XAE_FEATURE_PARTIAL_RX_CSUM	BIT(0)
+#define XAE_FEATURE_PARTIAL_TX_CSUM	BIT(1)
+#define XAE_FEATURE_FULL_RX_CSUM	BIT(2)
+#define XAE_FEATURE_FULL_TX_CSUM	BIT(3)
+#define XAE_FEATURE_DMA_64BIT		BIT(4)
+
+#define XAE_NO_CSUM_OFFLOAD		0
+
+#define XAE_FULL_CSUM_STATUS_MASK	0x00000038
+#define XAE_IP_UDP_CSUM_VALIDATED	0x00000003
+#define XAE_IP_TCP_CSUM_VALIDATED	0x00000002
+
+#define DELAY_OF_ONE_MILLISEC		1000
+
+#define XAXIENET_NAPI_WEIGHT		64
+
+/* Definition of 1588 PTP in Axi Ethernet IP */
+#define TX_TS_OP_NOOP           0x0
+#define TX_TS_OP_ONESTEP        0x1
+#define TX_TS_OP_TWOSTEP        0x2
+#define TX_TS_CSUM_UPDATE       0x1
+#define TX_TS_CSUM_UPDATE_MRMAC		0x4
+#define TX_TS_PDELAY_UPDATE_MRMAC	0x8
+#define TX_PTP_CSUM_OFFSET      0x28
+#define TX_PTP_TS_OFFSET        0x4C
+#define TX_PTP_CF_OFFSET        0x32
+
+/* XXV MAC Register Definitions */
+#define XXV_GT_RESET_OFFSET		0x00000000
+#define XXV_TC_OFFSET			0x0000000C
+#define XXV_RCW1_OFFSET			0x00000014
+#define XXV_JUM_OFFSET			0x00000018
+#define XXV_TICKREG_OFFSET		0x00000020
+#define XXV_STATRX_BLKLCK_OFFSET	0x0000040C
+#define XXV_USXGMII_AN_OFFSET		0x000000C8
+#define XXV_USXGMII_AN_STS_OFFSET	0x00000458
+#define XXV_STAT_GTWIZ_OFFSET		0x000004A0
+#define XXV_CONFIG_REVISION		0x00000024
+
+/* XXV MAC Register Mask Definitions */
+#define XXV_GT_RESET_MASK	BIT(0)
+#define XXV_TC_TX_MASK		BIT(0)
+#define XXV_RCW1_RX_MASK	BIT(0)
+#define XXV_RCW1_FCS_MASK	BIT(1)
+#define XXV_TC_FCS_MASK		BIT(1)
+#define XXV_MIN_JUM_MASK	GENMASK(7, 0)
+#define XXV_MAX_JUM_MASK	GENMASK(10, 8)
+#define XXV_RX_BLKLCK_MASK	BIT(0)
+#define XXV_TICKREG_STATEN_MASK BIT(0)
+#define XXV_MAC_MIN_PKT_LEN	64
+#define XXV_GTWIZ_RESET_DONE	(BIT(0) | BIT(1))
+#define XXV_MAJ_MASK		GENMASK(7, 0)
+#define XXV_MIN_MASK		GENMASK(15, 8)
+
+/* USXGMII Register Mask Definitions  */
+#define USXGMII_AN_EN		BIT(5)
+#define USXGMII_AN_RESET	BIT(6)
+#define USXGMII_AN_RESTART	BIT(7)
+#define USXGMII_EN		BIT(16)
+#define USXGMII_RATE_MASK	0x0E000700
+#define USXGMII_RATE_1G		0x04000200
+#define USXGMII_RATE_2G5	0x08000400
+#define USXGMII_RATE_10M	0x0
+#define USXGMII_RATE_100M	0x02000100
+#define USXGMII_RATE_5G		0x0A000500
+#define USXGMII_RATE_10G	0x06000300
+#define USXGMII_FD		BIT(28)
+#define USXGMII_LINK_STS	BIT(31)
+
+/* USXGMII AN STS register mask definitions */
+#define USXGMII_AN_STS_COMP_MASK	BIT(16)
+
+/* MCDMA Register Definitions */
+#define XMCDMA_CR_OFFSET	0x00
+#define XMCDMA_SR_OFFSET	0x04
+#define XMCDMA_CHEN_OFFSET	0x08
+#define XMCDMA_CHSER_OFFSET	0x0C
+#define XMCDMA_ERR_OFFSET	0x10
+#define XMCDMA_PKTDROP_OFFSET	0x14
+#define XMCDMA_TXWEIGHT0_OFFSET 0x18
+#define XMCDMA_TXWEIGHT1_OFFSET 0x1C
+#define XMCDMA_RXINT_SER_OFFSET 0x20
+#define XMCDMA_TXINT_SER_OFFSET 0x28
+
+#define XMCDMA_CHOBS1_OFFSET	0x440
+#define XMCDMA_CHOBS2_OFFSET	0x444
+#define XMCDMA_CHOBS3_OFFSET	0x448
+#define XMCDMA_CHOBS4_OFFSET	0x44C
+#define XMCDMA_CHOBS5_OFFSET	0x450
+#define XMCDMA_CHOBS6_OFFSET	0x454
+
+#define XMCDMA_CHAN_RX_OFFSET  0x500
+
+/* Per Channel Registers */
+#define XMCDMA_CHAN_CR_OFFSET(chan_id)		(0x40 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_SR_OFFSET(chan_id)		(0x44 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_CURDESC_OFFSET(chan_id)	(0x48 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_TAILDESC_OFFSET(chan_id)	(0x50 + ((chan_id) - 1) * 0x40)
+#define XMCDMA_CHAN_PKTDROP_OFFSET(chan_id)	(0x58 + ((chan_id) - 1) * 0x40)
+
+#define XMCDMA_RX_OFFSET	0x500
+
+/* MCDMA Mask registers */
+#define XMCDMA_CR_RUNSTOP_MASK		BIT(0) /* Start/stop DMA channel */
+#define XMCDMA_CR_RESET_MASK		BIT(2) /* Reset DMA engine */
+
+#define XMCDMA_SR_HALTED_MASK		BIT(0)
+#define XMCDMA_SR_IDLE_MASK		BIT(1)
+
+#define XMCDMA_IRQ_ERRON_OTHERQ_MASK	BIT(3)
+#define XMCDMA_IRQ_PKTDROP_MASK		BIT(4)
+#define XMCDMA_IRQ_IOC_MASK		BIT(5)
+#define XMCDMA_IRQ_DELAY_MASK		BIT(6)
+#define XMCDMA_IRQ_ERR_MASK		BIT(7)
+#define XMCDMA_IRQ_ALL_MASK		GENMASK(7, 5)
+#define XMCDMA_PKTDROP_COALESCE_MASK	GENMASK(15, 8)
+#define XMCDMA_COALESCE_MASK		GENMASK(23, 16)
+#define XMCDMA_DELAY_MASK		GENMASK(31, 24)
+
+#define XMCDMA_CHEN_MASK		GENMASK(7, 0)
+#define XMCDMA_CHID_MASK		GENMASK(7, 0)
+
+#define XMCDMA_ERR_INTERNAL_MASK	BIT(0)
+#define XMCDMA_ERR_SLAVE_MASK		BIT(1)
+#define XMCDMA_ERR_DECODE_MASK		BIT(2)
+#define XMCDMA_ERR_SG_INT_MASK		BIT(4)
+#define XMCDMA_ERR_SG_SLV_MASK		BIT(5)
+#define XMCDMA_ERR_SG_DEC_MASK		BIT(6)
+
+#define XMCDMA_PKTDROP_CNT_MASK		GENMASK(31, 0)
+
+#define XMCDMA_BD_CTRL_TXSOF_MASK	0x80000000 /* First tx packet */
+#define XMCDMA_BD_CTRL_TXEOF_MASK	0x40000000 /* Last tx packet */
+#define XMCDMA_BD_CTRL_ALL_MASK		0xC0000000 /* All control bits */
+#define XMCDMA_BD_STS_ALL_MASK		0xF0000000 /* All status bits */
+#define XMCDMA_BD_SD_STS_ALL_MASK	0x00000030 /* TUSER input port bits */
+
+#define XMCDMA_BD_SD_STS_TUSER_EP	0x00000000
+#define XMCDMA_BD_SD_STS_TUSER_MAC_1	0x00000010
+#define XMCDMA_BD_SD_STS_TUSER_MAC_2	0x00000020
+#define XMCDMA_BD_SD_STS_TUSER_EX_EP	0x00000030
+
+#define XMCDMA_COALESCE_SHIFT		16
+#define XMCDMA_DELAY_SHIFT		24
+#define XMCDMA_DFT_TX_THRESHOLD		1
+
+#define XMCDMA_TXWEIGHT_CH_MASK(chan_id)	GENMASK(((chan_id) * 4 + 3), \
+							(chan_id) * 4)
+#define XMCDMA_TXWEIGHT_CH_SHIFT(chan_id)	((chan_id) * 4)
+
+/* PTP Packet length */
+#define XAE_TX_PTP_LEN		16
+#define XXV_TX_PTP_LEN		12
+
+/* Macros used when AXI DMA h/w is configured without DRE */
+#define XAE_TX_BUFFERS		64
+#define XAE_MAX_PKT_LEN		8192
+
+/* MRMAC Register Definitions */
+/* Configuration Registers */
+#define MRMAC_REV_OFFSET		0x00000000
+#define MRMAC_RESET_OFFSET		0x00000004
+#define MRMAC_MODE_OFFSET		0x00000008
+#define MRMAC_CONFIG_TX_OFFSET		0x0000000C
+#define MRMAC_CONFIG_RX_OFFSET		0x00000010
+#define MRMAC_TICK_OFFSET		0x0000002C
+#define MRMAC_CFG1588_OFFSET	0x00000040
+
+/* Status Registers */
+#define MRMAC_TX_STS_OFFSET		0x00000740
+#define MRMAC_RX_STS_OFFSET		0x00000744
+#define MRMAC_TX_RT_STS_OFFSET		0x00000748
+#define MRMAC_RX_RT_STS_OFFSET		0x0000074C
+#define MRMAC_STATRX_BLKLCK_OFFSET	0x00000754
+#define MRMAC_STATRX_VALID_CTRL_OFFSET	0x000007B8
+
+/* Register bit masks */
+#define MRMAC_RX_SERDES_RST_MASK	(BIT(3) | BIT(2) | BIT(1) | BIT(0))
+#define MRMAC_TX_SERDES_RST_MASK	BIT(4)
+#define MRMAC_RX_RST_MASK		BIT(5)
+#define MRMAC_TX_RST_MASK		BIT(6)
+#define MRMAC_RX_AXI_RST_MASK		BIT(8)
+#define MRMAC_TX_AXI_RST_MASK		BIT(9)
+#define MRMAC_STS_ALL_MASK		0xFFFFFFFF
+
+#define MRMAC_RX_EN_MASK		BIT(0)
+#define MRMAC_RX_DEL_FCS_MASK		BIT(1)
+
+#define MRMAC_TX_EN_MASK		BIT(0)
+#define MRMAC_TX_INS_FCS_MASK		BIT(1)
+
+#define MRMAC_RX_BLKLCK_MASK		BIT(0)
+#define MRMAC_RX_STATUS_MASK		BIT(0)
+#define MRMAC_RX_VALID_MASK		BIT(0)
+
+#define MRMAC_CTL_DATA_RATE_MASK	GENMASK(2, 0)
+#define MRMAC_CTL_DATA_RATE_10G		0
+#define MRMAC_CTL_DATA_RATE_25G		1
+#define MRMAC_CTL_DATA_RATE_40G		2
+#define MRMAC_CTL_DATA_RATE_50G		3
+#define MRMAC_CTL_DATA_RATE_100G	4
+
+#define MRMAC_CTL_AXIS_CFG_MASK		GENMASK(11, 9)
+#define MRMAC_CTL_AXIS_CFG_SHIFT	9
+#define MRMAC_CTL_AXIS_CFG_10G_IND	1
+#define MRMAC_CTL_AXIS_CFG_25G_IND	1
+
+#define MRMAC_CTL_SERDES_WIDTH_MASK	GENMASK(6, 4)
+#define MRMAC_CTL_SERDES_WIDTH_SHIFT	4
+#define MRMAC_CTL_SERDES_WIDTH_10G	4
+#define MRMAC_CTL_SERDES_WIDTH_25G	6
+
+#define MRMAC_CTL_RATE_CFG_MASK		(MRMAC_CTL_DATA_RATE_MASK |	\
+					 MRMAC_CTL_AXIS_CFG_MASK |	\
+					 MRMAC_CTL_SERDES_WIDTH_MASK)
+
+#define MRMAC_CTL_PM_TICK_MASK		BIT(30)
+#define MRMAC_TICK_TRIGGER		BIT(0)
+#define MRMAC_ONE_STEP_EN		BIT(0)
+
+/* MRMAC GT wrapper registers */
+#define MRMAC_GT_PLL_OFFSET		0x0
+#define MRMAC_GT_PLL_STS_OFFSET		0x8
+#define MRMAC_GT_RATE_OFFSET		0x0
+#define MRMAC_GT_CTRL_OFFSET		0x8
+
+#define MRMAC_GT_PLL_RST_MASK		0x00030003
+#define MRMAC_GT_PLL_DONE_MASK		0xFF
+#define MRMAC_GT_RST_ALL_MASK		BIT(0)
+#define MRMAC_GT_RST_RX_MASK		BIT(1)
+#define MRMAC_GT_RST_TX_MASK		BIT(2)
+#define MRMAC_GT_10G_MASK		0x00000001
+#define MRMAC_GT_25G_MASK		0x00000002
+
+#define MRMAC_GT_LANE_OFFSET		BIT(16)
+#define MRMAC_MAX_GT_LANES		4
+/**
+ * struct axidma_bd - Axi Dma buffer descriptor layout
+ * @next:         MM2S/S2MM Next Descriptor Pointer
+ * @reserved1:    Reserved and not used for 32-bit
+ * @phys:         MM2S/S2MM Buffer Address
+ * @reserved2:    Reserved and not used for 32-bit
+ * @reserved3:    Reserved and not used
+ * @reserved4:    Reserved and not used
+ * @cntrl:        MM2S/S2MM Control value
+ * @status:       MM2S/S2MM Status value
+ * @app0:         MM2S/S2MM User Application Field 0.
+ * @app1:         MM2S/S2MM User Application Field 1.
+ * @app2:         MM2S/S2MM User Application Field 2.
+ * @app3:         MM2S/S2MM User Application Field 3.
+ * @app4:         MM2S/S2MM User Application Field 4.
+ * @sw_id_offset: MM2S/S2MM Sw ID
+ * @ptp_tx_skb:   If timestamping is enabled used for timestamping skb
+ *		  Otherwise reserved.
+ * @ptp_tx_ts_tag: Tag value of 2 step timestamping if timestamping is enabled
+ *		   Otherwise reserved.
+ * @tx_skb:	  Transmit skb address
+ * @tx_desc_mapping: Tx Descriptor DMA mapping type.
+ */
+struct axidma_bd {
+	phys_addr_t next;	/* Physical address of next buffer descriptor */
+#ifndef CONFIG_PHYS_ADDR_T_64BIT
+	u32 reserved1;
+#endif
+	phys_addr_t phys;
+#ifndef CONFIG_PHYS_ADDR_T_64BIT
+	u32 reserved2;
+#endif
+	u32 reserved3;
+	u32 reserved4;
+	u32 cntrl;
+	u32 status;
+	u32 app0;
+	u32 app1;	/* TX start << 16 | insert */
+	u32 app2;	/* TX csum seed */
+	u32 app3;
+	u32 app4;
+	phys_addr_t sw_id_offset; /* first unused field by h/w */
+	phys_addr_t ptp_tx_skb;
+	u32 ptp_tx_ts_tag;
+	phys_addr_t tx_skb;
+	u32 tx_desc_mapping;
+} __aligned(XAXIDMA_BD_MINIMUM_ALIGNMENT);
+/**
+ * struct aximcdma_bd - Axi MCDMA buffer descriptor layout
+ * @next:         MM2S/S2MM Next Descriptor Pointer
+ * @reserved1:    Reserved and not used for 32-bit
+ * @phys:         MM2S/S2MM Buffer Address
+ * @reserved2:    Reserved and not used for 32-bit
+ * @reserved3:    Reserved and not used
+ * @cntrl:        MM2S/S2MM Control value
+ * @status:       S2MM Status value
+ * @sband_stats:  S2MM Sideband Status value
+ *		  MM2S Status value
+ * @app0:         MM2S/S2MM User Application Field 0.
+ * @app1:         MM2S/S2MM User Application Field 1.
+ * @app2:         MM2S/S2MM User Application Field 2.
+ * @app3:         MM2S/S2MM User Application Field 3.
+ * @app4:         MM2S/S2MM User Application Field 4.
+ * @sw_id_offset: MM2S/S2MM Sw ID
+ * @ptp_tx_skb:   If timestamping is enabled used for timestamping skb
+ *		  Otherwise reserved.
+ * @ptp_tx_ts_tag: Tag value of 2 step timestamping if timestamping is enabled
+ *		   Otherwise reserved.
+ * @tx_skb:	  Transmit skb address
+ * @tx_desc_mapping: Tx Descriptor DMA mapping type.
+ */
+struct aximcdma_bd {
+	phys_addr_t next;	/* Physical address of next buffer descriptor */
+#ifndef CONFIG_PHYS_ADDR_T_64BIT
+	u32 reserved1;
+#endif
+	phys_addr_t phys;
+#ifndef CONFIG_PHYS_ADDR_T_64BIT
+	u32 reserved2;
+#endif
+	u32 reserved3;
+	u32 cntrl;
+	u32 status;
+	u32 sband_stats;
+	u32 app0;
+	u32 app1;	/* TX start << 16 | insert */
+	u32 app2;	/* TX csum seed */
+	u32 app3;
+	u32 app4;
+	phys_addr_t sw_id_offset; /* first unused field by h/w */
+	phys_addr_t ptp_tx_skb;
+	u32 ptp_tx_ts_tag;
+	phys_addr_t tx_skb;
+	u32 tx_desc_mapping;
+} __aligned(XAXIDMA_BD_MINIMUM_ALIGNMENT);
+
+#define XAE_NUM_MISC_CLOCKS 3
+#define DESC_DMA_MAP_SINGLE 0
+#define DESC_DMA_MAP_PAGE 1
+
+#define XAE_MAX_QUEUES		7
+
+/* TSN queues range is 2 to 5. For eg: for num_tc = 2 minimum queues = 2;
+ * for num_tc = 3 with sideband signalling maximum queues = 5
+ */
+#define XAE_MAX_TSN_TC		3
+#define XAE_TSN_MIN_QUEUES	4
+
+#define TSN_BRIDGEEP_EPONLY	BIT(29)
+
+#ifdef CONFIG_AXIENET_HAS_TADMA
+#define TADMA_MAX_NO_STREAM	128
+struct axitadma_bd {
+	u32 phys;
+	phys_addr_t tx_skb;
+	u32 tx_desc_mapping;
+	u32 num_frag;
+	u32 len;
+};
+#endif
+
+enum axienet_tsn_ioctl {
+	SIOCCHIOCTL = SIOCDEVPRIVATE,
+	SIOC_GET_SCHED,
+	SIOC_PREEMPTION_CFG,
+	SIOC_PREEMPTION_CTRL,
+	SIOC_PREEMPTION_STS,
+	SIOC_PREEMPTION_COUNTER,
+	SIOC_QBU_USER_OVERRIDE,
+	SIOC_QBU_STS,
+	SIOC_TADMA_STR_ADD,
+	SIOC_TADMA_PROG_ALL,
+	SIOC_TADMA_STR_FLUSH,
+	SIOC_PREEMPTION_RECEIVE,
+	SIOC_TADMA_OFF,
+};
+
+/**
+ * struct axienet_local - axienet private per device data
+ * @ndev:	Pointer for net_device to which it will be attached.
+ * @dev:	Pointer to device structure
+ * @phy_node:	Pointer to device node structure
+ * @axi_clk:	AXI4-Lite bus clock
+ * @misc_clks:	Misc ethernet clocks (AXI4-Stream, Ref, MGT clocks)
+ * @mii_bus:	Pointer to MII bus structure
+ * @mii_clk_div: MII bus clock divider value
+ * @regs_start: Resource start for axienet device addresses
+ * @regs:	Base address for the axienet_local device address space
+ * @mcdma_regs:	Base address for the aximcdma device address space
+ * @napi:	Napi Structure array for all dma queues
+ * @num_tx_queues: Total number of Tx DMA queues
+ * @num_rx_queues: Total number of Rx DMA queues
+ * @dq:		DMA queues data
+ * @phy_mode:	Phy type to identify between MII/GMII/RGMII/SGMII/1000 Base-X
+ * @num_tc:	Total number of TSN Traffic classes
+ * @abl_reg:	TSN abilities register
+ * @st_pcp:     pcp values mapped to scheduled traffic.
+ * @res_pcp:    pcp values mapped to reserved traffic.
+ * @master:	Master endpoint
+ * @slaves:	Front panel ports
+ * @ex_ep:	extended end point
+ * @packet_switch: packet switching parameter
+ * @switch_prt: Switch port number
+ * @timer_priv: PTP timer private data pointer
+ * @ptp_tx_irq: PTP tx irq
+ * @ptp_rx_irq: PTP rx irq
+ * @rtc_irq:	PTP RTC irq
+ * @qbv_irq:	QBV shed irq
+ * @ptp_ts_type: ptp time stamp type - 1 or 2 step mode
+ * @ptp_rx_hw_pointer: ptp rx hw pointer
+ * @ptp_rx_sw_pointer: ptp rx sw pointer
+ * @ptp_txq:	PTP tx queue header
+ * @tx_tstamp_work: PTP timestamping work queue
+ * @qbv_regs:	pointer to qbv registers base address
+ * @tadma_regs: pointer to tadma registers base address
+ * @tadma_irq: TADMA IRQ number
+ * @t_cb: pointer to tadma_cb
+ * @active_sfm: current active stream fetch memory
+ * @num_tadma_buffers: number of TADMA buffers per stream
+ * @num_streams: maximum number of streams TADMA can fetch
+ * @num_entries: maximum number of entries in TADMA streams config
+ * @tx_bd: tadma transmit buffer descriptor
+ * @tx_bd_head: transmit BD head indices
+ * @tx_bd_tail: transmit BD tail indices
+ * @tx_bd_rd: TADMA read pointer offset
+ * @tadma_tx_lock: TADMA tx lock
+ * @ptp_tx_lock: PTP tx lock
+ * @dma_err_tasklet: Tasklet structure to process Axi DMA errors
+ * @eth_irq:	Axi Ethernet IRQ number
+ * @options:	AxiEthernet option word
+ * @last_link:	Phy link state in which the PHY was negotiated earlier
+ * @features:	Stores the extended features supported by the axienet hw
+ * @tx_bd_num:	Number of TX buffer descriptors.
+ * @rx_bd_num:	Number of RX buffer descriptors.
+ * @max_frm_size: Stores the maximum size of the frame that can be that
+ *		  Txed/Rxed in the existing hardware. If jumbo option is
+ *		  supported, the maximum frame size would be 9k. Else it is
+ *		  1522 bytes (assuming support for basic VLAN)
+ * @rxmem:	Stores rx memory size for jumbo frame handling.
+ * @csum_offload_on_tx_path:	Stores the checksum selection on TX side.
+ * @csum_offload_on_rx_path:	Stores the checksum selection on RX side.
+ * @coalesce_count_rx:	Store the irq coalesce on RX side.
+ * @coalesce_count_tx:	Store the irq coalesce on TX side.
+ * @phy_flags:	Phy interface flags.
+ * @eth_hasnobuf: Ethernet is configured in Non buf mode.
+ * @eth_hasptp: Ethernet is configured for ptp.
+ * @axienet_config: Ethernet config structure
+ * @tx_ts_regs:	  Base address for the axififo device address space.
+ * @rx_ts_regs:	  Base address for the rx axififo device address space.
+ * @tstamp_config: Hardware timestamp config structure.
+ * @current_rx_filter : Current rx filter.
+ * @tx_ptpheader: Stores the tx ptp header.
+ * @aclk: AXI4-Lite clock for ethernet and dma.
+ * @eth_sclk: AXI4-Stream interface clock.
+ * @eth_refclk: Stable clock used by signal delay primitives and transceivers.
+ * @eth_dclk: Dynamic Reconfiguration Port(DRP) clock.
+ * @dma_sg_clk: DMA Scatter Gather Clock.
+ * @dma_rx_clk: DMA S2MM Primary Clock.
+ * @dma_tx_clk: DMA MM2S Primary Clock.
+ * @qnum:     Axi Ethernet queue number to be operate on.
+ * @chan_num: MCDMA Channel number to be operate on.
+ * @chan_id:  MCMDA Channel id used in conjunction with weight parameter.
+ * @weight:   MCDMA Channel weight value to be configured for.
+ * @dma_mask: Specify the width of the DMA address space.
+ * @usxgmii_rate: USXGMII PHY speed.
+ * @mrmac_rate: MRMAC speed.
+ * @gt_pll: Common GT PLL mask control register space.
+ * @gt_ctrl: GT speed and reset control register space.
+ * @phc_index: Index to corresponding PTP clock used.
+ * @gt_lane: MRMAC GT lane index used.
+ * @ptp_os_cf: CF TS of PTP PDelay req for one step usage.
+ * @xxv_ip_version: XXV IP version
+ */
+struct axienet_local {
+	struct net_device *ndev;
+	struct device *dev;
+
+	struct device_node *phy_node;
+
+	struct clk *axi_clk;
+	struct clk_bulk_data misc_clks[XAE_NUM_MISC_CLOCKS];
+
+	struct mii_bus *mii_bus;
+	u8 mii_clk_div;
+
+	resource_size_t regs_start;
+	void __iomem *regs;
+	void __iomem *mcdma_regs;
+
+	struct tasklet_struct dma_err_tasklet[XAE_MAX_QUEUES];
+	struct napi_struct napi[XAE_MAX_QUEUES];	/* NAPI Structure */
+
+	u16    num_tx_queues;	/* Number of TX DMA queues */
+	u16    num_rx_queues;	/* Number of RX DMA queues */
+	struct axienet_dma_q *dq[XAE_MAX_QUEUES];	/* DMA queue data*/
+
+	phy_interface_t phy_mode;
+
+	u16   num_tc;
+	u32   abl_reg;
+	u8    st_pcp;
+	u8    res_pcp;
+	struct net_device *master; /* master endpoint */
+	struct net_device *slaves[2]; /* two front panel ports */
+	struct net_device *ex_ep; /* extended endpoint*/
+	u8	packet_switch;
+	u8      switch_prt;	/* port on the switch */
+#ifdef CONFIG_XILINX_TSN_PTP
+	void *timer_priv;
+	int ptp_tx_irq;
+	int ptp_rx_irq;
+	int rtc_irq;
+	int qbv_irq;
+	int ptp_ts_type;
+	u8  ptp_rx_hw_pointer;
+	u8  ptp_rx_sw_pointer;
+	struct sk_buff_head ptp_txq;
+	struct work_struct tx_tstamp_work;
+#endif
+#ifdef CONFIG_XILINX_TSN_QBV
+	void __iomem *qbv_regs;
+#endif
+#ifdef CONFIG_AXIENET_HAS_TADMA
+	void __iomem *tadma_regs;
+	int tadma_irq;
+	void *t_cb;
+	int active_sfm;
+	int num_tadma_buffers;
+	int num_streams;
+	int num_entries;
+	struct axitadma_bd **tx_bd;
+	u32 tx_bd_head[TADMA_MAX_NO_STREAM];
+	u32 tx_bd_tail[TADMA_MAX_NO_STREAM];
+	u32 tx_bd_rd[TADMA_MAX_NO_STREAM];
+	spinlock_t tadma_tx_lock;               /* TSN TADMA tx lock*/
+#endif
+	spinlock_t ptp_tx_lock;		/* PTP tx lock*/
+	int eth_irq;
+
+	u32 options;			/* Current options word */
+	u32 last_link;
+	u32 features;
+
+	u16 tx_bd_num;
+	u32 rx_bd_num;
+
+	u32 max_frm_size;
+	u32 rxmem;
+
+	int csum_offload_on_tx_path;
+	int csum_offload_on_rx_path;
+
+	u32 coalesce_count_rx;
+	u32 coalesce_count_tx;
+	u32 phy_flags;
+	bool eth_hasnobuf;
+	bool eth_hasptp;
+	const struct axienet_config *axienet_config;
+
+#if defined(CONFIG_XILINX_TSN_PTP)
+	void __iomem *tx_ts_regs;
+	void __iomem *rx_ts_regs;
+	struct hwtstamp_config tstamp_config;
+	int current_rx_filter;
+	u8 *tx_ptpheader;
+#endif
+	struct clk *aclk;
+	struct clk *eth_sclk;
+	struct clk *eth_refclk;
+	struct clk *eth_dclk;
+	struct clk *dma_sg_clk;
+	struct clk *dma_rx_clk;
+	struct clk *dma_tx_clk;
+
+	/* MCDMA Fields */
+	int qnum[XAE_MAX_QUEUES];
+	int chan_num[XAE_MAX_QUEUES];
+	/* WRR Fields */
+	u16 chan_id;
+	u16 weight;
+
+	u8 dma_mask;
+	u32 usxgmii_rate;
+
+	u32 mrmac_rate;		/* MRMAC speed */
+	void __iomem *gt_pll;	/* Common GT PLL mask control register space */
+	void __iomem *gt_ctrl;	/* GT speed and reset control register space */
+	u32 phc_index;		/* Index to corresponding PTP clock used  */
+	u32 gt_lane;		/* MRMAC GT lane index used */
+	u64 ptp_os_cf;		/* CF TS of PTP PDelay req for one step usage */
+	u32 xxv_ip_version;
+};
+
+/**
+ * struct axienet_dma_q - axienet private per dma queue data
+ * @lp:		Parent pointer
+ * @dma_regs:	Base address for the axidma device address space
+ * @tx_irq:	Axidma TX IRQ number
+ * @rx_irq:	Axidma RX IRQ number
+ * @tx_lock:	Spin lock for tx path
+ * @rx_lock:	Spin lock for tx path
+ * @tx_bd_v:	Virtual address of the TX buffer descriptor ring
+ * @tx_bd_p:	Physical address(start address) of the TX buffer descr. ring
+ * @rx_bd_v:	Virtual address of the RX buffer descriptor ring
+ * @rx_bd_p:	Physical address(start address) of the RX buffer descr. ring
+ * @tx_buf:	Virtual address of the Tx buffer pool used by the driver when
+ *		DMA h/w is configured without DRE.
+ * @tx_bufs:	Virutal address of the Tx buffer address.
+ * @tx_bufs_dma: Physical address of the Tx buffer address used by the driver
+ *		 when DMA h/w is configured without DRE.
+ * @eth_hasdre: Tells whether DMA h/w is configured with dre or not.
+ * @tx_bd_ci:	Stores the index of the Tx buffer descriptor in the ring being
+ *		accessed currently. Used while alloc. BDs before a TX starts
+ * @tx_bd_tail:	Stores the index of the Tx buffer descriptor in the ring being
+ *		accessed currently. Used while processing BDs after the TX
+ *		completed.
+ * @rx_bd_ci:	Stores the index of the Rx buffer descriptor in the ring being
+ *		accessed currently.
+ * @flags:      MCDMA management channel flags
+ * @chan_id:    MCDMA channel to operate on.
+ * @rx_offset:	MCDMA S2MM channel starting offset.
+ * @txq_bd_v:	Virtual address of the MCDMA TX buffer descriptor ring
+ * @rxq_bd_v:	Virtual address of the MCDMA RX buffer descriptor ring
+ * @tx_packets: Number of transmit packets processed by the dma queue.
+ * @tx_bytes:   Number of transmit bytes processed by the dma queue.
+ * @rx_packets: Number of receive packets processed by the dma queue.
+ * @rx_bytes:	Number of receive bytes processed by the dma queue.
+ */
+struct axienet_dma_q {
+	struct axienet_local	*lp; /* parent */
+	void __iomem *dma_regs;
+
+	int tx_irq;
+	int rx_irq;
+
+	spinlock_t tx_lock;		/* tx lock */
+	spinlock_t rx_lock;		/* rx lock */
+
+	/* Buffer descriptors */
+	struct axidma_bd *tx_bd_v;
+	struct axidma_bd *rx_bd_v;
+	dma_addr_t rx_bd_p;
+	dma_addr_t tx_bd_p;
+
+	unsigned char *tx_buf[XAE_TX_BUFFERS];
+	unsigned char *tx_bufs;
+	dma_addr_t tx_bufs_dma;
+	bool eth_hasdre;
+
+	u32 tx_bd_ci;
+	u32 rx_bd_ci;
+	u32 tx_bd_tail;
+
+	/* MCDMA fields */
+#ifdef CONFIG_XILINX_TSN
+#define MCDMA_MGMT_CHAN		BIT(0)
+#define MCDMA_EP_EX_CHAN	BIT(1)
+	u32 flags;
+#endif
+	u16 chan_id;
+	u32 rx_offset;
+	struct aximcdma_bd *txq_bd_v;
+	struct aximcdma_bd *rxq_bd_v;
+
+	unsigned long tx_packets;
+	unsigned long tx_bytes;
+	unsigned long rx_packets;
+	unsigned long rx_bytes;
+};
+
+#define AXIENET_ETHTOOLS_SSTATS_LEN 6
+#define AXIENET_TX_SSTATS_LEN(lp) ((lp)->num_tx_queues * 2)
+#define AXIENET_RX_SSTATS_LEN(lp) ((lp)->num_rx_queues * 2)
+
+/**
+ * enum axienet_ip_type - AXIENET IP/MAC type.
+ *
+ * @XAXIENET_1G:	 IP is 1G MAC
+ * @XAXIENET_2_5G:	 IP type is 2.5G MAC.
+ * @XAXIENET_LEGACY_10G: IP type is legacy 10G MAC.
+ * @XAXIENET_10G_25G:	 IP type is 10G/25G MAC(XXV MAC).
+ * @XAXIENET_MRMAC:	 IP type is hardened Multi Rate MAC (MRMAC).
+ *
+ */
+enum axienet_ip_type {
+	XAXIENET_1G = 0,
+	XAXIENET_2_5G,
+	XAXIENET_LEGACY_10G,
+	XAXIENET_10G_25G,
+	XAXIENET_MRMAC,
+};
+
+struct axienet_config {
+	enum axienet_ip_type mactype;
+	void (*setoptions)(struct net_device *ndev, u32 options);
+	int (*clk_init)(struct platform_device *pdev, struct clk **axi_aclk,
+			struct clk **axis_clk, struct clk **ref_clk,
+			struct clk **dclk);
+	u32 tx_ptplen;
+	u8 ts_header_len;
+};
+
+/**
+ * struct axienet_option - Used to set axi ethernet hardware options
+ * @opt:	Option to be set.
+ * @reg:	Register offset to be written for setting the option
+ * @m_or:	Mask to be ORed for setting the option in the register
+ */
+struct axienet_option {
+	u32 opt;
+	u32 reg;
+	u32 m_or;
+};
+
+struct xxvenet_option {
+	u32 opt;
+	u32 reg;
+	u32 m_or;
+};
+
+extern void __iomem *mrmac_gt_pll;
+extern void __iomem *mrmac_gt_ctrl;
+extern int mrmac_pll_reg;
+extern int mrmac_pll_rst;
+
+/**
+ * axienet_ior - Memory mapped Axi Ethernet register read
+ * @lp:         Pointer to axienet local structure
+ * @offset:     Address offset from the base address of Axi Ethernet core
+ *
+ * Return: The contents of the Axi Ethernet register
+ *
+ * This function returns the contents of the corresponding register.
+ */
+static inline u32 axienet_ior(struct axienet_local *lp, off_t offset)
+{
+	return ioread32(lp->regs + offset);
+}
+
+static inline u32 axinet_ior_read_mcr(struct axienet_local *lp)
+{
+	return axienet_ior(lp, XAE_MDIO_MCR_OFFSET);
+}
+
+/**
+ * axienet_iow - Memory mapped Axi Ethernet register write
+ * @lp:         Pointer to axienet local structure
+ * @offset:     Address offset from the base address of Axi Ethernet core
+ * @value:      Value to be written into the Axi Ethernet register
+ *
+ * This function writes the desired value into the corresponding Axi Ethernet
+ * register.
+ */
+static inline void axienet_iow(struct axienet_local *lp, off_t offset,
+			       u32 value)
+{
+	iowrite32(value, lp->regs + offset);
+}
+
+/**
+ * axienet_get_mrmac_blocklock - Write to Clear MRMAC RX block lock status register
+ * and read the latest status
+ * @lp:         Pointer to axienet local structure
+ *
+ * Return: The contents of the Contents of MRMAC RX block lock status register
+ */
+
+static inline u32 axienet_get_mrmac_blocklock(struct axienet_local *lp)
+{
+	axienet_iow(lp, MRMAC_STATRX_BLKLCK_OFFSET, MRMAC_STS_ALL_MASK);
+	return axienet_ior(lp, MRMAC_STATRX_BLKLCK_OFFSET);
+}
+
+/**
+ * axienet_get_mrmac_rx_status - Write to Clear MRMAC RX status register
+ * and read the latest status
+ * @lp:		Pointer to axienet local structure
+ *
+ * Return: The contents of the Contents of MRMAC RX status register
+ */
+
+static inline u32 axienet_get_mrmac_rx_status(struct axienet_local *lp)
+{
+	axienet_iow(lp, MRMAC_RX_STS_OFFSET, MRMAC_STS_ALL_MASK);
+	return axienet_ior(lp, MRMAC_RX_STS_OFFSET);
+}
+
+/**
+ * axienet_dma_in32 - Memory mapped Axi DMA register read
+ * @q:		Pointer to DMA queue structure
+ * @reg:	Address offset from the base address of the Axi DMA core
+ *
+ * Return: The contents of the Axi DMA register
+ *
+ * This function returns the contents of the corresponding Axi DMA register.
+ */
+static inline u32 axienet_dma_in32(struct axienet_dma_q *q, off_t reg)
+{
+	return ioread32(q->dma_regs + reg);
+}
+
+/**
+ * axienet_dma_out32 - Memory mapped Axi DMA register write.
+ * @q:		Pointer to DMA queue structure
+ * @reg:	Address offset from the base address of the Axi DMA core
+ * @value:	Value to be written into the Axi DMA register
+ *
+ * This function writes the desired value into the corresponding Axi DMA
+ * register.
+ */
+static inline void axienet_dma_out32(struct axienet_dma_q *q,
+				     off_t reg, u32 value)
+{
+	iowrite32(value, q->dma_regs + reg);
+}
+
+/**
+ * axienet_dma_bdout - Memory mapped Axi DMA register Buffer Descriptor write.
+ * @q:		Pointer to DMA queue structure
+ * @reg:	Address offset from the base address of the Axi DMA core
+ * @value:	Value to be written into the Axi DMA register
+ *
+ * This function writes the desired value into the corresponding Axi DMA
+ * register.
+ */
+static inline void axienet_dma_bdout(struct axienet_dma_q *q,
+				     off_t reg, dma_addr_t value)
+{
+#if defined(CONFIG_PHYS_ADDR_T_64BIT)
+	writeq(value, (q->dma_regs + reg));
+#else
+	writel(value, (q->dma_regs + reg));
+#endif
+}
+
+#ifdef CONFIG_XILINX_TSN_QBV
+/**
+ * axienet_qbv_ior - Memory mapped TSN QBV register read
+ * @lp:         Pointer to axienet local structure
+ * @offset:     Address offset from the base address of Axi Ethernet core
+ *
+ * Return: The contents of the Axi Ethernet register
+ *
+ * This function returns the contents of the corresponding register.
+ */
+static inline u32 axienet_qbv_ior(struct axienet_local *lp, off_t offset)
+{
+	return ioread32(lp->qbv_regs + offset);
+}
+
+/**
+ * axienet_qbv_iow - Memory mapped TSN QBV register write
+ * @lp:         Pointer to axienet local structure
+ * @offset:     Address offset from the base address of Axi Ethernet core
+ * @value:      Value to be written into the Axi Ethernet register
+ *
+ * This function writes the desired value into the corresponding Axi Ethernet
+ * register.
+ */
+static inline void axienet_qbv_iow(struct axienet_local *lp, off_t offset,
+				   u32 value)
+{
+	iowrite32(value, (lp->qbv_regs + offset));
+}
+#endif
+
+/* Function prototypes visible in xilinx_axienet_mdio.c for other files */
+int axienet_mdio_enable_tsn(struct axienet_local *lp);
+void axienet_mdio_disable_tsn(struct axienet_local *lp);
+int axienet_mdio_setup_tsn(struct axienet_local *lp);
+void axienet_mdio_teardown_tsn(struct axienet_local *lp);
+void axienet_adjust_link_tsn(struct net_device *ndev);
+int axienet_tsn_open(struct net_device *ndev);
+int axienet_tsn_stop(struct net_device *ndev);
+int axienet_tsn_probe(struct platform_device *pdev,
+		      struct axienet_local *lp,
+		      struct net_device *ndev);
+int tsn_mcdma_probe(struct platform_device *pdev, struct axienet_local *lp,
+		    struct net_device *ndev);
+int axienet_tsn_xmit(struct sk_buff *skb, struct net_device *ndev);
+u16 axienet_tsn_select_queue(struct net_device *ndev, struct sk_buff *skb,
+			     struct net_device *sb_dev);
+u16 axienet_tsn_pcp_to_queue(struct net_device *ndev, struct sk_buff *skb);
+int axienet_get_pcp_mask(struct axienet_local *lp, u16 num_tc);
+int tsn_data_path_open(struct net_device *ndev);
+int tsn_data_path_close(struct net_device *ndev);
+#ifdef CONFIG_XILINX_TSN_PTP
+void *axienet_ptp_timer_probe(void __iomem *base, struct platform_device *pdev);
+void axienet_tx_tstamp(struct work_struct *work);
+int axienet_ptp_timer_remove(void *priv);
+#endif
+#ifdef CONFIG_XILINX_TSN_QBV
+int axienet_qbv_init(struct net_device *ndev);
+void axienet_qbv_remove(struct net_device *ndev);
+int axienet_set_schedule(struct net_device *ndev, void __user *useraddr);
+int axienet_get_schedule(struct net_device *ndev, void __user *useraddr);
+#endif
+
+#ifdef CONFIG_XILINX_TSN_QBR
+int axienet_preemption(struct net_device *ndev, void __user *useraddr);
+int axienet_preemption_ctrl(struct net_device *ndev, void __user *useraddr);
+int axienet_preemption_sts(struct net_device *ndev, void __user *useraddr);
+int axienet_preemption_receive(struct net_device *ndev);
+int axienet_preemption_cnt(struct net_device *ndev, void __user *useraddr);
+#ifdef CONFIG_XILINX_TSN_QBV
+int axienet_qbu_user_override(struct net_device *ndev, void __user *useraddr);
+int axienet_qbu_sts(struct net_device *ndev, void __user *useraddr);
+#endif
+#endif
+
+int axienet_mdio_wait_until_ready_tsn(struct axienet_local *lp);
+void __maybe_unused axienet_bd_free(struct net_device *ndev,
+				    struct axienet_dma_q *q);
+
+#ifdef CONFIG_AXIENET_HAS_TADMA
+int axienet_tadma_add_stream(struct net_device *ndev, void __user *useraddr);
+int axienet_tadma_flush_stream(struct net_device *ndev, void __user *useraddr);
+int axienet_tadma_off(struct net_device *ndev, void __user *useraddr);
+int axienet_tadma_program(struct net_device *ndev, void __user *useraddr);
+int axienet_tadma_probe(struct platform_device *pdev, struct net_device *ndev);
+int axienet_tadma_xmit(struct sk_buff *skb, struct net_device *ndev, u16 queue_type);
+int axienet_tadma_open(struct net_device *ndev);
+int axienet_tadma_stop(struct net_device *ndev);
+#endif
+
+int __maybe_unused axienet_dma_q_init(struct net_device *ndev,
+				      struct axienet_dma_q *q);
+void axienet_dma_err_handler(unsigned long data);
+irqreturn_t __maybe_unused axienet_tx_irq(int irq, void *_ndev);
+irqreturn_t __maybe_unused axienet_rx_irq(int irq, void *_ndev);
+void axienet_start_xmit_done_tsn(struct net_device *ndev, struct axienet_dma_q *q);
+void axienet_dma_bd_release_tsn(struct net_device *ndev);
+void __axienet_device_reset_tsn(struct axienet_dma_q *q);
+void axienet_set_mac_address_tsn(struct net_device *ndev, const void *address);
+void axienet_set_multicast_list_tsn(struct net_device *ndev);
+int xaxienet_rx_poll_tsn(struct napi_struct *napi, int quota);
+void axienet_setoptions_tsn(struct net_device *ndev, u32 options);
+int axienet_queue_xmit_tsn(struct sk_buff *skb, struct net_device *ndev,
+			   u16 map);
+
+int __maybe_unused axienet_mcdma_rx_q_init_tsn(struct net_device *ndev,
+					       struct axienet_dma_q *q);
+int __maybe_unused axienet_mcdma_tx_q_init_tsn(struct net_device *ndev,
+					       struct axienet_dma_q *q);
+void __maybe_unused axienet_mcdma_tx_bd_free_tsn(struct net_device *ndev,
+						 struct axienet_dma_q *q);
+void __maybe_unused axienet_mcdma_rx_bd_free_tsn(struct net_device *ndev,
+						 struct axienet_dma_q *q);
+irqreturn_t __maybe_unused axienet_mcdma_tx_irq_tsn(int irq, void *_ndev);
+irqreturn_t __maybe_unused axienet_mcdma_rx_irq_tsn(int irq, void *_ndev);
+void __maybe_unused axienet_mcdma_err_handler_tsn(unsigned long data);
+void axienet_strings_tsn(struct net_device *ndev, u32 sset, u8 *data);
+int axienet_sset_count_tsn(struct net_device *ndev, int sset);
+void axienet_get_stats_tsn(struct net_device *ndev,
+			   struct ethtool_stats *stats,
+			   u64 *data);
+int axeinet_mcdma_create_sysfs_tsn(struct kobject *kobj);
+void axeinet_mcdma_remove_sysfs_tsn(struct kobject *kobj);
+int __maybe_unused axienet_mcdma_tx_probe_tsn(struct platform_device *pdev,
+					      struct device_node *np,
+					      struct axienet_local *lp);
+int __maybe_unused axienet_mcdma_rx_probe_tsn(struct platform_device *pdev,
+					      struct device_node *np,
+					      struct net_device *ndev);
+int axienet_ethtools_get_coalesce(struct net_device *ndev,
+				  struct ethtool_coalesce *ecoalesce,
+				  struct kernel_ethtool_coalesce *kernel_coal,
+				  struct netlink_ext_ack *extack);
+int axienet_ethtools_set_coalesce(struct net_device *ndev,
+				  struct ethtool_coalesce *ecoalesce,
+				  struct kernel_ethtool_coalesce *kernel_coal,
+				  struct netlink_ext_ack *extack);
+#if defined(CONFIG_XILINX_TSN_SWITCH)
+int tsn_switch_get_port_parent_id(struct net_device *dev,
+				  struct netdev_phys_item_id *ppid);
+#endif
+
+#endif /* XILINX_AXI_ENET_TSN_H */
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_cb.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_cb.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,199 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN QCI Controller module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+#include "xilinx_tsn_switch.h"
+
+#define IN_PORTID_MASK				0x3
+#define IN_PORTID_SHIFT				24
+#define MAX_SEQID_MASK				0x0000FFFF
+#define REMAINING_TICKS_MASK			0x0000FFFF
+
+#define SEQ_REC_HIST_LEN_MASK			0x000000FF
+#define SEQ_REC_HIST_LEN_SHIFT			16
+#define SPLIT_STREAM_INPORTID_SHIFT		12
+#define SPLIT_STREAM_INPORTID_MASK		0x3
+#define SPLIT_STREAM_VLANID_MASK		0x00000FFF
+
+#define GATE_ID_SHIFT				24
+#define MEMBER_ID_SHIFT				8
+#define SEQ_RESET_SHIFT				7
+#define REC_TIMEOUT_SHIFT			6
+#define GATE_STATE_SHIFT			5
+#define FRER_VALID_SHIFT			4
+#define WR_OP_TYPE_SHIFT			2
+#define OP_TYPE_SHIFT				1
+#define WR_OP_TYPE_MASK				0x3
+#define FRER_EN_CONTROL_MASK			0x1
+
+/**
+ * frer_control - Configure thr control for frer
+ * @data:	Value to be programmed
+ */
+void frer_control(struct frer_ctrl data)
+{
+	u32 mask = 0;
+
+	mask = data.gate_id << GATE_ID_SHIFT;
+	mask |= data.memb_id << MEMBER_ID_SHIFT;
+	mask |= data.seq_reset << SEQ_RESET_SHIFT;
+	mask |= data.gate_state << GATE_STATE_SHIFT;
+	mask |= data.rcvry_tmout << REC_TIMEOUT_SHIFT;
+	mask |= data.frer_valid << FRER_VALID_SHIFT;
+	mask |= (data.wr_op_type & WR_OP_TYPE_MASK) << WR_OP_TYPE_SHIFT;
+	mask |= data.op_type << OP_TYPE_SHIFT;
+	mask |= FRER_EN_CONTROL_MASK;
+
+	axienet_iow(&lp, FRER_CONTROL_OFFSET, mask);
+
+	/* wait for write to complete */
+	while ((axienet_ior(&lp, FRER_CONTROL_OFFSET) & FRER_EN_CONTROL_MASK))
+		;
+}
+
+/**
+ * get_ingress_filter_config -  Get Ingress Filter Configuration
+ * @data:	Value returned
+ */
+void get_ingress_filter_config(struct in_fltr *data)
+{
+	u32 reg_val = 0;
+
+	reg_val = axienet_ior(&lp, INGRESS_FILTER_OFFSET);
+
+	data->max_seq_id = reg_val & MAX_SEQID_MASK;
+	data->in_port_id = (reg_val >> IN_PORTID_SHIFT) & IN_PORTID_MASK;
+}
+
+/**
+ * config_ingress_filter -  Configure Ingress Filter Configuration
+ * @data:	Value to be programmed
+ */
+void config_ingress_filter(struct cb data)
+{
+	u32 conf_r1;
+
+	conf_r1 = axienet_ior(&lp, INGRESS_FILTER_OFFSET);
+	conf_r1 &= ~(IN_PORTID_MASK << IN_PORTID_SHIFT);
+	conf_r1 |= ((data.in_fltr_data.in_port_id & IN_PORTID_MASK) <<
+			IN_PORTID_SHIFT);
+	axienet_iow(&lp, INGRESS_FILTER_OFFSET, conf_r1);
+
+	conf_r1 = axienet_ior(&lp, FRER_CONFIG_REG1);
+	conf_r1 &= ~(SEQ_REC_HIST_LEN_MASK << SEQ_REC_HIST_LEN_SHIFT);
+	conf_r1 |= (data.frer_memb_config_data.seq_rec_hist_len &
+			SEQ_REC_HIST_LEN_MASK)
+			<< SEQ_REC_HIST_LEN_SHIFT;
+	axienet_iow(&lp, FRER_CONFIG_REG1, conf_r1);
+	conf_r1 = axienet_ior(&lp, FRER_CONFIG_REG2);
+	conf_r1 &= ~(REMAINING_TICKS_MASK);
+	conf_r1 |= data.frer_memb_config_data.rem_ticks;
+	axienet_iow(&lp, FRER_CONFIG_REG2, conf_r1);
+}
+
+/**
+ * get_member_reg -  Read frer member Configuration registers value
+ * @data:	Value returned
+ */
+void get_member_reg(struct frer_memb_config *data)
+{
+	u32 conf_r1 = 0;
+
+	conf_r1 = axienet_ior(&lp, FRER_CONFIG_REG1);
+	data->rem_ticks = axienet_ior(&lp, FRER_CONFIG_REG2);
+
+	data->seq_rec_hist_len = (conf_r1 >> SEQ_REC_HIST_LEN_SHIFT)
+						& SEQ_REC_HIST_LEN_MASK;
+	data->split_strm_egport_id = (conf_r1 >> SPLIT_STREAM_INPORTID_SHIFT)
+						& SPLIT_STREAM_INPORTID_MASK;
+	data->split_strm_vlan_id = conf_r1 & SPLIT_STREAM_VLANID_MASK;
+}
+
+/**
+ * program_member_reg -  configure frer member Configuration registers
+ * @data:	Value to be programmed
+ */
+void program_member_reg(struct cb data)
+{
+	u32 conf_r1 = 0;
+
+	conf_r1 = axienet_ior(&lp, FRER_CONFIG_REG1);
+	conf_r1 &= ~(SPLIT_STREAM_INPORTID_MASK << SPLIT_STREAM_INPORTID_SHIFT);
+	conf_r1 &= ~(SPLIT_STREAM_VLANID_MASK);
+	conf_r1 |= ((data.frer_memb_config_data.split_strm_egport_id
+					& SPLIT_STREAM_INPORTID_MASK)
+					<< SPLIT_STREAM_INPORTID_SHIFT);
+	conf_r1 |= (data.frer_memb_config_data.split_strm_vlan_id &
+					SPLIT_STREAM_VLANID_MASK);
+
+	axienet_iow(&lp, FRER_CONFIG_REG1, conf_r1);
+	axienet_iow(&lp, FRER_CONFIG_REG2,
+		    data.frer_memb_config_data.rem_ticks);
+
+	conf_r1 = axienet_ior(&lp, INGRESS_FILTER_OFFSET);
+	conf_r1 &= ~MAX_SEQID_MASK;
+	conf_r1 |= (data.in_fltr_data.max_seq_id & MAX_SEQID_MASK);
+	axienet_iow(&lp, INGRESS_FILTER_OFFSET, conf_r1);
+}
+
+/**
+ * get_frer_static_counter -  get frer static counters value
+ * @data:	return value, containing counter value
+ */
+void get_frer_static_counter(struct frer_static_counter *data)
+{
+	int offset = (data->num) * 8;
+
+	data->frer_fr_count.lsb = axienet_ior(&lp, TOTAL_FRER_FRAMES_OFFSET +
+									offset);
+	data->frer_fr_count.msb = axienet_ior(&lp, TOTAL_FRER_FRAMES_OFFSET +
+								offset + 0x4);
+
+	data->disc_frames_in_portid.lsb = axienet_ior(&lp,
+						      FRER_DISCARD_INGS_FLTR_OFFSET + offset);
+	data->disc_frames_in_portid.msb = axienet_ior(&lp,
+						      FRER_DISCARD_INGS_FLTR_OFFSET + offset + 0x4);
+
+	data->pass_frames_ind_recv.lsb = axienet_ior(&lp,
+						     FRER_PASS_FRAMES_INDV_OFFSET + offset);
+	data->pass_frames_ind_recv.msb = axienet_ior(&lp,
+						     FRER_PASS_FRAMES_INDV_OFFSET + offset + 0x4);
+
+	data->disc_frames_ind_recv.lsb = axienet_ior(&lp,
+						     FRER_DISCARD_FRAMES_INDV_OFFSET + offset);
+	data->disc_frames_ind_recv.msb =
+	axienet_ior(&lp, FRER_DISCARD_FRAMES_INDV_OFFSET + offset + 0x4);
+
+	data->pass_frames_seq_recv.lsb = axienet_ior(&lp,
+						     FRER_PASS_FRAMES_SEQ_OFFSET + offset);
+	data->pass_frames_seq_recv.msb = axienet_ior(&lp,
+						     FRER_PASS_FRAMES_SEQ_OFFSET + offset + 0x4);
+
+	data->disc_frames_seq_recv.lsb = axienet_ior(&lp,
+						     FRER_DISCARD_FRAMES_SEQ_OFFSET + offset);
+	data->disc_frames_seq_recv.msb = axienet_ior(&lp,
+						     FRER_DISCARD_FRAMES_SEQ_OFFSET + offset + 0x4);
+
+	data->rogue_frames_seq_recv.lsb = axienet_ior(&lp,
+						      FRER_ROGUE_FRAMES_SEQ_OFFSET + offset);
+	data->rogue_frames_seq_recv.msb = axienet_ior(&lp,
+						      FRER_ROGUE_FRAMES_SEQ_OFFSET + offset + 0x4);
+
+	data->seq_recv_rst.lsb = axienet_ior(&lp,
+					     SEQ_RECV_RESETS_OFFSET + offset);
+	data->seq_recv_rst.msb = axienet_ior(&lp,
+					     SEQ_RECV_RESETS_OFFSET + offset + 0x4);
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_ep.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_ep.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,638 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN End point driver.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/etherdevice.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/of_net.h>
+#include <linux/skbuff.h>
+
+#include "xilinx_axienet_tsn.h"
+#include "xilinx_tsn_switch.h"
+
+#define TX_BD_NUM_DEFAULT	64
+#define RX_BD_NUM_DEFAULT	1024
+
+static u8 st_pcp[8] = {4};
+static uint st_count = 1;
+module_param_array(st_pcp, byte, &st_count, 0644);
+MODULE_PARM_DESC(st_pcp, "Array of pcp values mapped to ST class at the compile time");
+
+static u8 res_pcp[8] = {2, 3};
+static uint res_count = 2;
+module_param_array(res_pcp, byte, &res_count, 0644);
+MODULE_PARM_DESC(res_pcp, "Array of pcp values mapped to RES class at the compile time");
+
+extern int axienet_phc_index;
+
+int tsn_data_path_open(struct net_device *ndev)
+{
+	int ret, i = 0;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+
+	static char irq_name[XAE_MAX_QUEUES + XAE_TSN_MIN_QUEUES][24];
+	u8 irq_cnt = 0;
+
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		/*MCDMA TX RESET*/
+		__axienet_device_reset_tsn(q);
+	}
+
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
+
+		ret = axienet_mcdma_rx_q_init_tsn(ndev, q);
+		/* Enable interrupts for Axi MCDMA Rx
+		 */
+		sprintf(irq_name[irq_cnt], "%s_mcdma_rx_%d", ndev->name, i + 1);
+		ret = request_irq(q->rx_irq, axienet_mcdma_rx_irq_tsn,
+				  IRQF_SHARED, irq_name[irq_cnt], ndev);
+		if (ret)
+			goto err_dma_rx_irq;
+
+		tasklet_init(&lp->dma_err_tasklet[i],
+			     axienet_mcdma_err_handler_tsn,
+			     (unsigned long)lp->dq[i]);
+		napi_enable(&lp->napi[i]);
+		irq_cnt++;
+	}
+
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+
+		ret = axienet_mcdma_tx_q_init_tsn(ndev, q);
+		/* Enable interrupts for Axi MCDMA Tx */
+		sprintf(irq_name[irq_cnt], "%s_mcdma_tx_%d", ndev->name, i + 1);
+		ret = request_irq(q->tx_irq, axienet_mcdma_tx_irq_tsn,
+				  IRQF_SHARED, irq_name[irq_cnt], ndev);
+		if (ret)
+			goto err_dma_tx_irq;
+		irq_cnt++;
+	}
+#ifdef CONFIG_AXIENET_HAS_TADMA
+	ret = axienet_tadma_open(ndev);
+	if (ret)
+		goto err_tadma;
+#endif
+
+	netif_tx_start_all_queues(ndev);
+	return 0;
+
+#ifdef CONFIG_AXIENET_HAS_TADMA
+err_tadma:
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		free_irq(q->tx_irq, ndev);
+	}
+#endif
+err_dma_tx_irq:
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		free_irq(q->rx_irq, ndev);
+	}
+err_dma_rx_irq:
+	return ret;
+}
+
+/**
+ * tsn_ep_open - TSN EP driver open routine.
+ * @ndev:       Pointer to net_device structure
+ *
+ * Return: 0, on success.
+ *          non-zero error value on failure
+ *
+ * This is the driver open routine. It also allocates interrupt service
+ * routines, enables the interrupt lines and ISR handling. Axi Ethernet
+ * core is reset through Axi DMA core. Buffer descriptors are initialized.
+ */
+static int tsn_ep_open(struct net_device *ndev)
+{
+	return tsn_data_path_open(ndev);
+}
+
+int tsn_data_path_close(struct net_device *ndev)
+{
+	u32 cr;
+	u32 i;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct axienet_dma_q *q;
+
+	for_each_tx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		cr = axienet_dma_in32(q, XAXIDMA_TX_CR_OFFSET);
+		axienet_dma_out32(q, XAXIDMA_TX_CR_OFFSET,
+				  cr & (~XAXIDMA_CR_RUNSTOP_MASK));
+		if (netif_running(ndev))
+			netif_stop_queue(ndev);
+		free_irq(q->tx_irq, ndev);
+	}
+	for_each_rx_dma_queue(lp, i) {
+		q = lp->dq[i];
+		cr = axienet_dma_in32(q, XAXIDMA_RX_CR_OFFSET);
+		axienet_dma_out32(q, XAXIDMA_RX_CR_OFFSET,
+				  cr & (~XAXIDMA_CR_RUNSTOP_MASK));
+		if (netif_running(ndev))
+			netif_stop_queue(ndev);
+		napi_disable(&lp->napi[i]);
+		tasklet_kill(&lp->dma_err_tasklet[i]);
+
+		free_irq(q->rx_irq, ndev);
+	}
+#ifdef CONFIG_AXIENET_HAS_TADMA
+	axienet_tadma_stop(ndev);
+#endif
+	axienet_dma_bd_release_tsn(ndev);
+
+	return 0;
+}
+
+/**
+ * tsn_ep_stop - TSN EP driver stop routine.
+ * @ndev:	Pointer to net_device structure
+ *
+ * Return: 0, on success.
+ *
+ * This is the driver stop routine. It also removes the interrupt handlers
+ * and disables the interrupts. The Axi DMA Tx/Rx BDs are released.
+ */
+static int tsn_ep_stop(struct net_device *ndev)
+{
+	return tsn_data_path_close(ndev);
+}
+
+/**
+ * tsn_ep_ioctl - TSN endpoint ioctl interface.
+ * @dev: Pointer to the net_device structure
+ * @rq: Socket ioctl interface request structure
+ * @data: User data
+ * @cmd: Ioctl case
+ *
+ * Return: 0 on success, Non-zero error value on failure.
+ *
+ * This is the ioctl interface for TSN end point. Currently this
+ * supports only gate programming.
+ */
+static int tsn_ep_ioctl(struct net_device *dev, struct ifreq *rq, void __user *data, int cmd)
+{
+	switch (cmd) {
+#ifdef CONFIG_XILINX_TSN_QBV
+	case SIOCCHIOCTL:
+		return axienet_set_schedule(dev, data);
+	case SIOC_GET_SCHED:
+		return axienet_get_schedule(dev, data);
+#endif
+#ifdef CONFIG_AXIENET_HAS_TADMA
+	case SIOC_TADMA_OFF:
+		return axienet_tadma_off(dev, data);
+	case SIOC_TADMA_STR_ADD:
+		return axienet_tadma_add_stream(dev, data);
+	case SIOC_TADMA_PROG_ALL:
+		return axienet_tadma_program(dev, data);
+	case SIOC_TADMA_STR_FLUSH:
+		return axienet_tadma_flush_stream(dev, data);
+#endif
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+u16 axienet_tsn_pcp_to_queue(struct net_device *ndev, struct sk_buff *skb)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct ethhdr *hdr = (struct ethhdr *)skb->data;
+	u16 ether_type = ntohs(hdr->h_proto);
+	u16 vlan_tci;
+	u8 pcp = 0;
+	int i;
+
+	if (unlikely(ether_type == ETH_P_8021Q)) {
+		struct vlan_ethhdr *vhdr = (struct vlan_ethhdr *)skb->data;
+
+		/* ether_type = ntohs(vhdr->h_vlan_encapsulated_proto); */
+
+		vlan_tci = ntohs(vhdr->h_vlan_TCI);
+
+		pcp = (vlan_tci & VLAN_PRIO_MASK) >> VLAN_PRIO_SHIFT;
+#ifdef CONFIG_AXIENET_HAS_TADMA
+		for (i = 0; i < st_count; i++) {
+			if (st_pcp[i] == pcp)
+				return ST_QUEUE_NUMBER;
+		}
+#endif
+		if (lp->num_tc == 3 && lp->num_tx_queues > 1) {
+			for (i = 0; i < res_count; i++) {
+				if (res_pcp[i] == pcp)
+					return RES_QUEUE_NUMBER;
+			}
+		}
+	}
+	return BE_QUEUE_NUMBER;
+}
+
+static u16 axienet_tsn_ep_select_queue(struct net_device *ndev, struct sk_buff *skb,
+				       struct net_device *sb_dev)
+{
+	return axienet_tsn_pcp_to_queue(ndev, skb);
+}
+
+/**
+ * tsn_ep_xmit - TSN endpoint xmit routine.
+ * @skb: Packet data
+ * @ndev: Pointer to the net_device structure
+ *
+ * Return: Always returns NETDEV_TX_OK.
+ *
+ * This is dummy xmit function for endpoint as all the data path is assumed to
+ * be connected by TEMAC1 as per linux view
+ */
+static int tsn_ep_xmit(struct sk_buff *skb, struct net_device *ndev)
+{
+	u16 map = skb_get_queue_mapping(skb);
+
+#ifdef CONFIG_AXIENET_HAS_TADMA
+	if (map == ST_QUEUE_NUMBER) /* ST Traffic */
+		return axienet_tadma_xmit(skb, ndev, map);
+#endif
+
+	return axienet_queue_xmit_tsn(skb, ndev, map);
+}
+
+static void tsn_ep_set_mac_address(struct net_device *ndev, const void *address)
+{
+	if (address)
+		eth_hw_addr_set(ndev, address);
+	if (!is_valid_ether_addr(ndev->dev_addr))
+		eth_hw_addr_random(ndev);
+}
+
+/**
+ * netdev_set_mac_address - Write the MAC address (from outside the driver)
+ * @ndev:	Pointer to the net_device structure
+ * @p:		6 byte Address to be written as MAC address
+ *
+ * Return: 0 for all conditions. Presently, there is no failure case.
+ *
+ * This function is called to initialize the MAC address of the Axi Ethernet
+ * core. This is the function that goes into net_device_ops structure entry
+ * ndo_set_mac_address.
+ */
+static int netdev_set_mac_address(struct net_device *ndev, void *p)
+{
+	struct sockaddr *addr = p;
+
+	tsn_ep_set_mac_address(ndev, addr->sa_data);
+	return 0;
+}
+
+#if defined(CONFIG_XILINX_TSN_PTP)
+/**
+ * tsn_ethtools_get_ts_info - Get h/w timestamping capabilities.
+ * @ndev:       Pointer to net_device structure
+ * @info:       Pointer to ethtool_ts_info structure
+ *
+ * Return: 0
+ */
+static int tsn_ethtools_get_ts_info(struct net_device *ndev,
+				    struct ethtool_ts_info *info)
+{
+	info->phc_index = axienet_phc_index;
+	return 0;
+}
+#endif
+
+static const struct ethtool_ops ep_ethtool_ops = {
+	.supported_coalesce_params = ETHTOOL_COALESCE_MAX_FRAMES,
+	.get_sset_count	 = axienet_sset_count_tsn,
+	.get_ethtool_stats = axienet_get_stats_tsn,
+	.get_coalesce   = axienet_ethtools_get_coalesce,
+	.set_coalesce   = axienet_ethtools_set_coalesce,
+	.get_strings = axienet_strings_tsn,
+#if defined(CONFIG_XILINX_TSN_PTP)
+	.get_ts_info    = tsn_ethtools_get_ts_info,
+#endif
+};
+
+static const struct net_device_ops ep_netdev_ops = {
+	.ndo_open = tsn_ep_open,
+	.ndo_stop = tsn_ep_stop,
+	.ndo_siocdevprivate = tsn_ep_ioctl,
+	.ndo_start_xmit = tsn_ep_xmit,
+	.ndo_set_mac_address = netdev_set_mac_address,
+	.ndo_select_queue = axienet_tsn_ep_select_queue,
+#if defined(CONFIG_XILINX_TSN_SWITCH)
+	.ndo_get_port_parent_id = tsn_switch_get_port_parent_id,
+#endif
+};
+
+static const struct of_device_id tsn_ep_of_match[] = {
+	{ .compatible = "xlnx,tsn-ep"},
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, tsn_ep_of_match);
+
+/* separate function is needed to probe tsn mcdma
+ * as there is asymmetry between rx channels and tx channels
+ * having unique probe for both tsn and axienet with mcdma is not possible
+ */
+int __maybe_unused tsn_mcdma_probe(struct platform_device *pdev, struct axienet_local *lp,
+				   struct net_device *ndev)
+{
+	int i, ret = 0;
+	struct axienet_dma_q *q;
+	struct device_node *np;
+	struct resource dmares;
+	const char *str;
+	u32 num;
+
+	ret = of_property_count_strings(pdev->dev.of_node, "xlnx,channel-ids");
+	if (ret < 0)
+		return -EINVAL;
+
+	np = of_parse_phandle(pdev->dev.of_node, "axistream-connected-rx",
+			      0);
+	/* get number of associated queues */
+	ret = of_property_read_u32(np, "xlnx,num-s2mm-channels", &num);
+	if (ret < 0)
+		return -EINVAL;
+
+	lp->num_rx_queues = num;
+	pr_info("%s: num_rx_queues: %d\n", __func__, lp->num_rx_queues);
+
+	for_each_rx_dma_queue(lp, i) {
+		q = kzalloc(sizeof(*q), GFP_KERNEL);
+
+		/* parent */
+		q->lp = lp;
+		lp->dq[i] = q;
+		ret = of_property_read_string_index(pdev->dev.of_node,
+						    "xlnx,channel-ids", i,
+						    &str);
+		ret = kstrtou16(str, 16, &q->chan_id);
+		lp->qnum[i] = i;
+		lp->chan_num[i] = q->chan_id;
+	}
+
+	if (IS_ERR(np)) {
+		dev_err(&pdev->dev, "could not find DMA node\n");
+		return ret;
+	}
+
+	ret = of_address_to_resource(np, 0, &dmares);
+	if (ret) {
+		dev_err(&pdev->dev, "unable to get DMA resource\n");
+		return ret;
+	}
+
+	lp->mcdma_regs = devm_ioremap_resource(&pdev->dev, &dmares);
+	if (IS_ERR(lp->mcdma_regs)) {
+		dev_err(&pdev->dev, "iormeap failed for the dma\n");
+		ret = PTR_ERR(lp->mcdma_regs);
+		return ret;
+	}
+
+	axienet_mcdma_rx_probe_tsn(pdev, np, ndev);
+	axienet_mcdma_tx_probe_tsn(pdev, np, lp);
+
+	return 0;
+}
+
+static const struct axienet_config tsn_endpoint_cfg = {
+	.mactype = XAXIENET_1G,
+	.setoptions = NULL,
+	.tx_ptplen = XAE_TX_PTP_LEN,
+};
+
+/**
+ *axienet_get_pcp_mask - gets the compile time pcp values that
+ *are mapped to ST and RES traffic from uEnv.txt and assigns them
+ *to st_pcp and res_pcp fields of axienet_local structure
+ *@lp:		axienet local structure
+ *@num_tc:	number of traffic classes
+ *
+ * Return: Always returns 0
+ */
+int axienet_get_pcp_mask(struct axienet_local *lp, u16 num_tc)
+{
+	u8 i;
+	u8 invalid_pcp = 0;
+
+	lp->st_pcp = 0;
+	lp->res_pcp = 0;
+	if (st_count == 0 || st_count > 8) {
+		lp->st_pcp = 1 << 4;
+	} else {
+		for (i = 0; i < st_count; i++) {
+			if (st_pcp[i] >= 8) {
+				invalid_pcp = 1;
+				break;
+			}
+			lp->st_pcp = lp->st_pcp | 1 << st_pcp[i];
+		}
+		if (invalid_pcp) {
+			pr_warn("pcp value cannot be greater than or equal to 8\n");
+			lp->st_pcp = 1 << 4;
+			invalid_pcp = 0;
+		}
+	}
+	if (num_tc == 3) {
+		if (res_count == 0 || res_count > 8) {
+			lp->res_pcp = 1 << 2 | 1 << 3;
+		} else {
+			for (i = 0; i < res_count; i++) {
+				if (res_pcp[i] >= 8) {
+					invalid_pcp = 1;
+					break;
+				}
+				lp->res_pcp = lp->res_pcp | 1 << res_pcp[i];
+			}
+			if (invalid_pcp) {
+				pr_warn("pcp value cannot be greater than or equal to 8\n");
+				lp->res_pcp = 1 << 2 | 1 << 3;
+			}
+		}
+	}
+	return 0;
+}
+
+/**
+ * tsn_ep_probe - TSN ep pointer probe function.
+ * @pdev:	Pointer to platform device structure.
+ *
+ * Return: 0, on success
+ *	    Non-zero error value on failure.
+ *
+ * This is the probe routine for TSN endpoint driver.
+ */
+static int tsn_ep_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+	struct axienet_local *lp;
+	struct net_device *ndev;
+	struct resource *ethres;
+	u16 num_queues = XAE_MAX_QUEUES;
+	u16 num_tc = 0;
+	struct device_node *np;
+	u8 mac_addr[ETH_ALEN];
+	char irq_name[32];
+
+	ndev = alloc_netdev_mq(sizeof(*lp), "ep",
+			       NET_NAME_UNKNOWN, ether_setup, num_queues);
+	if (!ndev)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, ndev);
+
+	SET_NETDEV_DEV(ndev, &pdev->dev);
+	ndev->flags &= ~IFF_MULTICAST;  /* clear multicast */
+	ndev->features = NETIF_F_SG;
+	ndev->netdev_ops = &ep_netdev_ops;
+	ndev->ethtool_ops = &ep_ethtool_ops;
+
+	/* MTU range: 64 - 9000 */
+	ndev->min_mtu = 64;
+	ndev->max_mtu = XAE_JUMBO_MTU;
+
+	lp = netdev_priv(ndev);
+	lp->ndev = ndev;
+	lp->dev = &pdev->dev;
+	lp->options = XAE_OPTION_DEFAULTS;
+	lp->tx_bd_num = TX_BD_NUM_DEFAULT;
+	lp->rx_bd_num = RX_BD_NUM_DEFAULT;
+	lp->switch_prt = PORT_EP;
+
+	/* TODO
+	 * there are two temacs or two slaves to ep
+	 * get this infor from design?
+	 */
+	lp->slaves[0] = NULL;
+	lp->slaves[1] = NULL;
+	lp->ex_ep = NULL;
+	lp->packet_switch = 0;
+
+	lp->axienet_config = &tsn_endpoint_cfg;
+
+	lp->max_frm_size = XAE_MAX_VLAN_FRAME_SIZE;
+
+	/* check if ep has dma connected
+	 * in a ep_only system dma(mcdma/tadma) is connected to temac1
+	 */
+	np = of_parse_phandle(pdev->dev.of_node, "axistream-connected-rx", 0);
+	if (!np) {
+		/* dont expose ep dev in ep_only system
+		 * all functionality handled by temac1/eth1
+		 */
+		free_netdev(ndev);
+		of_node_put(np);
+		return 0;
+	}
+
+	/* Setup checksum offload, but default to off if not specified */
+	lp->features = 0;
+
+	lp->eth_hasnobuf = of_property_read_bool(pdev->dev.of_node,
+						 "xlnx,eth-hasnobuf");
+
+	/* Retrieve the MAC address */
+	ret = of_get_mac_address(pdev->dev.of_node, mac_addr);
+	if (ret) {
+		dev_err(&pdev->dev, "could not find MAC address\n");
+		goto free_netdev;
+	}
+	tsn_ep_set_mac_address(ndev, mac_addr);
+	ret = tsn_mcdma_probe(pdev, lp, ndev);
+	if (ret) {
+		dev_err(&pdev->dev, "Getting MCDMA resource failed\n");
+		goto free_netdev;
+	}
+
+#ifdef CONFIG_AXIENET_HAS_TADMA
+	ret = axienet_tadma_probe(pdev, ndev);
+	if (ret) {
+		dev_err(&pdev->dev, "Getting TADMA resource failed\n");
+		goto free_netdev;
+	}
+#endif
+	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-tc", &num_tc);
+	if (ret || (num_tc != 2 && num_tc != 3))
+		lp->num_tc = XAE_MAX_TSN_TC;
+	else
+		lp->num_tc = num_tc;
+	axienet_get_pcp_mask(lp, lp->num_tc);
+	/* Map device registers */
+	ethres = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	lp->regs = devm_ioremap_resource(&pdev->dev, ethres);
+	if (IS_ERR(lp->regs)) {
+		ret = PTR_ERR(lp->regs);
+		goto free_netdev;
+	}
+#ifdef CONFIG_XILINX_TSN_QBV
+	lp->qbv_regs = lp->regs;
+#endif
+
+	sprintf(irq_name, "tsn_ep_scheduler_irq");
+	lp->qbv_irq = platform_get_irq_byname(pdev, irq_name);
+#ifdef CONFIG_XILINX_TSN_QBV
+	axienet_qbv_init(ndev);
+#endif
+
+	ret = register_netdev(lp->ndev);
+	if (ret) {
+		dev_err(lp->dev, "register_netdev() error (%i)\n", ret);
+		goto free_netdev;
+	}
+	return ret;
+
+free_netdev:
+	free_netdev(ndev);
+
+	return ret;
+}
+
+static int tsn_ep_remove(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+
+#ifdef CONFIG_XILINX_TSN_QBV
+	axienet_qbv_remove(ndev);
+#endif
+	unregister_netdev(ndev);
+
+	free_netdev(ndev);
+
+	return 0;
+}
+
+static struct platform_driver tsn_ep_driver = {
+	.probe = tsn_ep_probe,
+	.remove = tsn_ep_remove,
+	.driver = {
+		 .name = "tsn_ep_axienet",
+		 .of_match_table = tsn_ep_of_match,
+	},
+};
+
+module_platform_driver(tsn_ep_driver);
+
+MODULE_DESCRIPTION("Xilinx Axi Ethernet driver");
+MODULE_AUTHOR("Xilinx");
+MODULE_LICENSE("GPL v2");
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_ep_ex.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_ep_ex.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,159 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN End point driver.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/etherdevice.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/of_net.h>
+#include <linux/skbuff.h>
+
+#include "xilinx_axienet_tsn.h"
+#include "xilinx_tsn_switch.h"
+
+static const struct of_device_id tsn_ex_ep_of_match[] = {
+	{ .compatible = "xlnx,tsn-ex-ep"},
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, tsn_ex_ep_of_match);
+
+static int tsn_ex_ep_open(struct net_device *ndev)
+{
+	return 0;
+}
+
+static int tsn_ex_ep_stop(struct net_device *ndev)
+{
+	return 0;
+}
+
+static int tsn_ex_ep_xmit(struct sk_buff *skb, struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct net_device *master = lp->master;
+
+	skb->dev = master;
+	dev_queue_xmit(skb);
+	return NETDEV_TX_OK;
+}
+
+static void tsn_ex_ep_set_mac_address(struct net_device *ndev, const void *address)
+{
+	if (address)
+		eth_hw_addr_set(ndev, address);
+	if (!is_valid_ether_addr(ndev->dev_addr))
+		eth_hw_addr_random(ndev);
+}
+
+static int netdev_set_ex_ep_mac_address(struct net_device *ndev, void *p)
+{
+	struct sockaddr *addr = p;
+
+	tsn_ex_ep_set_mac_address(ndev, addr->sa_data);
+	return 0;
+}
+
+static const struct net_device_ops ex_ep_netdev_ops = {
+	.ndo_open = tsn_ex_ep_open,
+	.ndo_stop = tsn_ex_ep_stop,
+	.ndo_start_xmit = tsn_ex_ep_xmit,
+	.ndo_set_mac_address = netdev_set_ex_ep_mac_address,
+};
+
+static int tsn_ex_ep_probe(struct platform_device *pdev)
+{
+	struct axienet_local *lp;
+	struct net_device *ndev;
+	struct device_node *ep_node;
+	struct axienet_local *ep_lp;
+	const void *mac_addr;
+	int ret = 0;
+	const void *packet_switch;
+
+	ndev = alloc_netdev(sizeof(*lp), "exep",
+			    NET_NAME_UNKNOWN, ether_setup);
+	if (!ndev)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, ndev);
+
+	SET_NETDEV_DEV(ndev, &pdev->dev);
+	ndev->flags &= ~IFF_MULTICAST;  /* clear multicast */
+	ndev->features = NETIF_F_SG;
+	ndev->netdev_ops = &ex_ep_netdev_ops;
+
+	/* MTU range: 64 - 9000 */
+	ndev->min_mtu = 64;
+	ndev->max_mtu = XAE_JUMBO_MTU;
+
+	lp = netdev_priv(ndev);
+	lp->ndev = ndev;
+	lp->dev = &pdev->dev;
+	lp->options = XAE_OPTION_DEFAULTS;
+	/* Retrieve the MAC address */
+	ret = of_get_mac_address(pdev->dev.of_node, (u8 *)mac_addr);
+	if (ret) {
+		dev_err(&pdev->dev, "could not find MAC address\n");
+		goto free_netdev;
+	}
+	tsn_ex_ep_set_mac_address(ndev, mac_addr);
+	packet_switch = of_get_property(pdev->dev.of_node, "packet-switch", NULL);
+	ep_node = of_parse_phandle(pdev->dev.of_node, "tsn,endpoint", 0);
+
+	lp->master = of_find_net_device_by_node(ep_node);
+	ret = register_netdev(lp->ndev);
+	if (ret) {
+		dev_err(lp->dev, "register_netdev() error (%i)\n", ret);
+		goto free_netdev;
+	}
+	ep_lp = netdev_priv(lp->master);
+	ep_lp->ex_ep = ndev;
+	if (packet_switch)
+		ep_lp->packet_switch = 1;
+	return ret;
+free_netdev:
+	free_netdev(ndev);
+
+	return ret;
+}
+
+static int tsn_ex_ep_remove(struct platform_device *pdev)
+{
+	struct net_device *ndev = platform_get_drvdata(pdev);
+
+	unregister_netdev(ndev);
+	free_netdev(ndev);
+	return 0;
+}
+
+static struct platform_driver tsn_ex_ep_driver = {
+	.probe = tsn_ex_ep_probe,
+	.remove = tsn_ex_ep_remove,
+	.driver = {
+		 .name = "tsn_ex_ep_axienet",
+		 .of_match_table = tsn_ex_ep_of_match,
+	},
+};
+
+module_platform_driver(tsn_ex_ep_driver);
+
+MODULE_DESCRIPTION("Xilinx Axi Ethernet driver");
+MODULE_AUTHOR("Xilinx");
+MODULE_LICENSE("GPL v2");
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_ip.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_ip.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,403 @@
+// SPDX-License-Identifier: GPL-2.0
+
+/* Xilinx FPGA Xilinx TSN IP driver.
+ *
+ * Copyright (C) 2018 Xilinx, Inc. All rights reserved.
+ *
+ * Author: Priyadarshini Babu <priyadar@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/module.h>
+#include <linux/of_platform.h>
+#include <linux/netdevice.h>
+#include <linux/skbuff.h>
+#include <linux/phy.h>
+#include <linux/udp.h>
+#include <linux/mii.h>
+#include <linux/of_mdio.h>
+#include <linux/of_net.h>
+#include <linux/of_address.h>
+#include <linux/xilinx_phy.h>
+
+#include "xilinx_axienet_tsn.h"
+#include "xilinx_tsn_switch.h"
+
+#ifdef CONFIG_XILINX_TSN_PTP
+#include "xilinx_tsn_ptp.h"
+#include "xilinx_tsn_timer.h"
+#endif
+
+#define TSN_TX_BE_QUEUE  0
+#define TSN_TX_RES_QUEUE 1
+#define TSN_TX_ST_QUEUE  2
+
+#define XAE_TEMAC1 0
+#define XAE_TEMAC2 1
+static const struct of_device_id tsn_ip_of_match[] = {
+	{ .compatible = "xlnx,tsn-endpoint-ethernet-mac-1.0"},
+	{ .compatible = "xlnx,tsn-endpoint-ethernet-mac-2.0"},
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, tsn_ip_of_match);
+
+/**
+ * tsn_ip_probe - TSN ip pointer probe function.
+ * @pdev:	Pointer to platform device structure.
+ *
+ * Return: 0, on success
+ *	    Non-zero error value on failure.
+ *
+ * This is the probe routine for TSN driver.
+ */
+static int tsn_ip_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+
+	pr_info("TSN endpoint ethernet mac Probe\n");
+
+	ret = of_platform_populate(pdev->dev.of_node, NULL, NULL, &pdev->dev);
+	if (ret)
+		pr_err("TSN endpoint probe error (%i)\n", ret);
+
+	return ret;
+}
+
+static int tsn_ip_remove(struct platform_device *pdev)
+{
+	return 0;
+}
+
+u16 axienet_tsn_select_queue(struct net_device *ndev, struct sk_buff *skb,
+			     struct net_device *sb_dev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct ethhdr *hdr = (struct ethhdr *)skb->data;
+#ifdef CONFIG_XILINX_TSN_PTP
+	const struct udphdr *udp;
+
+	udp = udp_hdr(skb);
+	if (hdr->h_proto == htons(ETH_P_1588) ||
+	    (lp->current_rx_filter == HWTSTAMP_FILTER_PTP_V2_L4_EVENT &&
+	     hdr->h_proto == htons(ETH_P_IP) && udp->dest == htons(0x013f))) {
+		return PTP_QUEUE_NUMBER;
+	}
+#endif
+	if (lp->abl_reg & TSN_BRIDGEEP_EPONLY)
+		return axienet_tsn_pcp_to_queue(ndev, skb);
+
+	return BE_QUEUE_NUMBER;
+}
+
+/**
+ * axienet_tsn_xmit - Starts the TSN transmission.
+ * @skb:	sk_buff pointer that contains data to be Txed.
+ * @ndev:	Pointer to net_device structure.
+ *
+ * Return: NETDEV_TX_OK, on success
+ *	    Non-zero error value on failure.
+ *
+ * This function is invoked from upper layers to initiate transmission. The
+ * function uses the next available free BDs and populates their fields to
+ * start the transmission. Use axienet_ptp_xmit() for PTP 1588 packets and
+ * use master EP xmit for other packets transmission.
+ */
+int axienet_tsn_xmit(struct sk_buff *skb, struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct net_device *master = lp->master;
+	u16 map = skb_get_queue_mapping(skb);
+
+#ifdef CONFIG_XILINX_TSN_PTP
+	/* check if skb is a PTP frame ? */
+	if (map == PTP_QUEUE_NUMBER)
+		return axienet_ptp_xmit(skb, ndev);
+#endif
+	if (lp->abl_reg & TSN_BRIDGEEP_EPONLY) {
+#ifdef CONFIG_AXIENET_HAS_TADMA
+		if (map == ST_QUEUE_NUMBER) /* ST Traffic */
+			return axienet_tadma_xmit(skb, ndev, map);
+#endif
+		return axienet_queue_xmit_tsn(skb, ndev, map);
+	}
+	/* use EP to xmit non-PTP frames */
+	skb->dev = master;
+	dev_queue_xmit(skb);
+
+	return NETDEV_TX_OK;
+}
+
+/**
+ * axienet_tsn_probe - TSN mac probe function.
+ * @pdev:	Pointer to platform device structure.
+ * @lp:		Pointer to axienet local structure
+ * @ndev:	Pointer to net_device structure.
+ *
+ * Return: 0, on success
+ *	    Non-zero error value on failure.
+ *
+ * This is the probe for TSN mac nodes.
+ */
+int axienet_tsn_probe(struct platform_device *pdev,
+		      struct axienet_local *lp,
+		      struct net_device *ndev)
+{
+	int ret = 0;
+	char irq_name[32];
+	bool slave = false;
+	u8     temac_no;
+	u32 qbv_addr, qbv_size;
+	struct device_node *ep_node;
+	struct axienet_local *ep_lp;
+
+	slave = of_property_read_bool(pdev->dev.of_node,
+				      "xlnx,tsn-slave");
+	if (slave) {
+		temac_no = XAE_TEMAC2;
+		lp->switch_prt = PORT_MAC2;
+	} else {
+		temac_no = XAE_TEMAC1;
+		lp->switch_prt = PORT_MAC1;
+	}
+	lp->current_rx_filter = HWTSTAMP_FILTER_PTP_V2_L2_EVENT;
+	sprintf(irq_name, "interrupt_ptp_rx_%d", temac_no + 1);
+	lp->ptp_rx_irq = platform_get_irq_byname(pdev, irq_name);
+
+	pr_info("ptp RX irq: %d %s\n", lp->ptp_rx_irq, irq_name);
+	sprintf(irq_name, "interrupt_ptp_tx_%d", temac_no + 1);
+	lp->ptp_tx_irq = platform_get_irq_byname(pdev, irq_name);
+	pr_info("ptp TX irq: %d %s\n", lp->ptp_tx_irq, irq_name);
+
+	sprintf(irq_name, "tsn_switch_scheduler_irq_%d", temac_no + 1);
+	lp->qbv_irq = platform_get_irq_byname(pdev, irq_name);
+
+	/*Ignoring if the qbv_irq is not exist*/
+	if (lp->qbv_irq > 0)
+		pr_info("qbv_irq: %d %s\n", lp->qbv_irq, irq_name);
+
+	spin_lock_init(&lp->ptp_tx_lock);
+
+	if (temac_no == XAE_TEMAC1)
+		lp->timer_priv = axienet_ptp_timer_probe((lp->regs + XAE_RTC_OFFSET), pdev);
+
+	/* enable VLAN */
+	lp->options |= XAE_OPTION_VLAN;
+	axienet_setoptions_tsn(lp->ndev, lp->options);
+
+	/* get the ep device */
+	ep_node = of_parse_phandle(pdev->dev.of_node, "tsn,endpoint", 0);
+
+	if (ep_node)
+		lp->master = of_find_net_device_by_node(ep_node);
+
+	lp->abl_reg = axienet_ior(lp, XAE_TSN_ABL_OFFSET);
+
+	/* in ep only case tie the data path to eth1 */
+	if (lp->abl_reg & TSN_BRIDGEEP_EPONLY && temac_no == XAE_TEMAC1) {
+		axienet_get_pcp_mask(lp, lp->num_tc);
+		ret = tsn_mcdma_probe(pdev, lp, ndev);
+		if (ret) {
+			dev_err(&pdev->dev, "Getting MCDMA resource failed\n");
+			goto err_1;
+		}
+#ifdef CONFIG_AXIENET_HAS_TADMA
+		ret = axienet_tadma_probe(pdev, ndev);
+		if (ret) {
+			dev_err(&pdev->dev, "Getting TADMA resource failed\n");
+			goto err_1;
+		}
+#endif
+	}
+
+#ifdef CONFIG_XILINX_TSN_QBV
+	lp->qbv_regs = NULL;
+	if (!(lp->abl_reg & TSN_BRIDGEEP_EPONLY)) {
+		of_property_read_u32(pdev->dev.of_node, "xlnx,qbv-addr",
+				     &qbv_addr);
+		of_property_read_u32(pdev->dev.of_node, "xlnx,qbv-size",
+				     &qbv_size);
+	} else {
+		struct resource res;
+
+		/* get qbv info from ep_node */
+		if (of_address_to_resource(ep_node, 0, &res) < 0)
+			dev_err(&pdev->dev, "error reading reg property\n");
+		qbv_addr = res.start;
+		qbv_size = res.end - res.start;
+	}
+	lp->qbv_regs = devm_ioremap(&pdev->dev, qbv_addr, qbv_size);
+	if (IS_ERR(lp->qbv_regs)) {
+		dev_err(&pdev->dev, "ioremap failed for the qbv\n");
+		ret = PTR_ERR(lp->qbv_regs);
+		return ret;
+	}
+	ret = axienet_qbv_init(ndev);
+#endif
+	if (!(lp->abl_reg & TSN_BRIDGEEP_EPONLY)) {
+		/* EP+Switch */
+		/* store the slaves to master(ep) */
+		ep_lp = netdev_priv(lp->master);
+		ep_lp->slaves[temac_no] = ndev;
+	}
+
+	return 0;
+err_1:
+	return -EINVAL;
+}
+
+/**
+ * axienet_device_reset - Reset and initialize the Axi Ethernet hardware.
+ * @ndev:	Pointer to the net_device structure
+ *
+ * This function is called to reset and initialize the Axi Ethernet core. This
+ * is typically called during initialization. It does a reset of the Axi DMA
+ * Rx/Tx channels and initializes the Axi DMA BDs. Since Axi DMA reset lines
+ * areconnected to Axi Ethernet reset lines, this in turn resets the Axi
+ * Ethernet core. No separate hardware reset is done for the Axi Ethernet
+ * core.
+ */
+static void axienet_device_reset(struct net_device *ndev)
+{
+	u32 axienet_status;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	lp->max_frm_size = XAE_MAX_VLAN_FRAME_SIZE;
+
+	lp->options |= XAE_OPTION_VLAN;
+	lp->options &= (~XAE_OPTION_JUMBO);
+
+	if (ndev->mtu > XAE_MTU && ndev->mtu <= XAE_JUMBO_MTU) {
+		lp->max_frm_size = ndev->mtu + VLAN_ETH_HLEN +
+					XAE_TRL_SIZE;
+		if (lp->max_frm_size <= lp->rxmem)
+			lp->options |= XAE_OPTION_JUMBO;
+	}
+
+	axienet_status = axienet_ior(lp, XAE_RCW1_OFFSET);
+	axienet_status &= ~XAE_RCW1_RX_MASK;
+	axienet_iow(lp, XAE_RCW1_OFFSET, axienet_status);
+
+	if (lp->axienet_config->mactype == XAXIENET_1G &&
+	    !lp->eth_hasnobuf) {
+		axienet_status = axienet_ior(lp, XAE_IP_OFFSET);
+		if (axienet_status & XAE_INT_RXRJECT_MASK)
+			axienet_iow(lp, XAE_IS_OFFSET, XAE_INT_RXRJECT_MASK);
+
+		/* Enable Receive errors */
+		axienet_iow(lp, XAE_IE_OFFSET, XAE_INT_RECV_ERROR_MASK);
+	}
+
+	axienet_iow(lp, XAE_FCC_OFFSET, XAE_FCC_FCRX_MASK);
+	lp->axienet_config->setoptions(ndev, lp->options &
+				       ~(XAE_OPTION_TXEN | XAE_OPTION_RXEN));
+
+	axienet_set_mac_address_tsn(ndev, NULL);
+	axienet_set_multicast_list_tsn(ndev);
+	lp->axienet_config->setoptions(ndev, lp->options);
+
+	netif_trans_update(ndev);
+}
+
+/**
+ * axienet_tsn_open - TSN driver open routine.
+ * @ndev:	Pointer to net_device structure
+ *
+ * Return: 0, on success.
+ *	    non-zero error value on failure
+ *
+ * This is the driver open routine. It calls phy_start to start the PHY device.
+ * It also allocates interrupt service routines, enables the interrupt lines
+ * and ISR handling. Axi Ethernet core is reset through Axi DMA core.
+ */
+int axienet_tsn_open(struct net_device *ndev)
+{
+	int ret;
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct phy_device *phydev = NULL;
+
+	axienet_device_reset(ndev);
+
+	if (lp->phy_node) {
+		phydev = of_phy_connect(lp->ndev, lp->phy_node,
+					axienet_adjust_link_tsn,
+					lp->phy_flags,
+					lp->phy_mode);
+		if (!phydev)
+			dev_err(lp->dev, "of_phy_connect() failed\n");
+		else
+			phy_start(phydev);
+	}
+
+	INIT_WORK(&lp->tx_tstamp_work, axienet_tx_tstamp);
+	skb_queue_head_init(&lp->ptp_txq);
+
+	lp->ptp_rx_hw_pointer = 0;
+	lp->ptp_rx_sw_pointer = 0xff;
+
+	axienet_iow(lp, PTP_RX_CONTROL_OFFSET, PTP_RX_PACKET_CLEAR);
+
+	ret = request_irq(lp->ptp_rx_irq, axienet_ptp_rx_irq,
+			  0, "ptp_rx", ndev);
+	if (ret)
+		goto err_ptp_rx_irq;
+
+	ret = request_irq(lp->ptp_tx_irq, axienet_ptp_tx_irq,
+			  0, "ptp_tx", ndev);
+	if (ret)
+		goto err_ptp_tx_irq;
+
+	if (lp->abl_reg & TSN_BRIDGEEP_EPONLY)
+		tsn_data_path_open(ndev);
+
+	netif_tx_start_all_queues(ndev);
+
+	return 0;
+
+err_ptp_tx_irq:
+	free_irq(lp->ptp_rx_irq, ndev);
+err_ptp_rx_irq:
+	return ret;
+}
+
+int axienet_tsn_stop(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	free_irq(lp->ptp_tx_irq, ndev);
+	free_irq(lp->ptp_rx_irq, ndev);
+
+	if (lp->axienet_config->mactype == XAXIENET_1G && !lp->eth_hasnobuf)
+		free_irq(lp->eth_irq, ndev);
+
+	if (ndev->phydev)
+		phy_disconnect(ndev->phydev);
+
+	if (lp->abl_reg & TSN_BRIDGEEP_EPONLY)
+		tsn_data_path_close(ndev);
+
+	return 0;
+}
+
+static struct platform_driver tsn_ip_driver = {
+	.probe = tsn_ip_probe,
+	.remove = tsn_ip_remove,
+	.driver = {
+		 .name = "tsn_ip_axienet",
+		 .of_match_table = tsn_ip_of_match,
+	},
+};
+
+module_platform_driver(tsn_ip_driver);
+
+MODULE_DESCRIPTION("Xilinx Axi Ethernet driver");
+MODULE_AUTHOR("Xilinx");
+MODULE_LICENSE("GPL v2");
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_preemption.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_preemption.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,258 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN QBU/QBR - Frame Preemption module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Priyadarshini Babu <priyadar@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include "xilinx_axienet_tsn.h"
+#include "xilinx_tsn_preemption.h"
+
+/**
+ * axienet_preemption -  Configure Frame Preemption
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: Value to be programmed
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_preemption(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u8 preemp;
+
+	if (copy_from_user(&preemp, useraddr, sizeof(preemp)))
+		return -EFAULT;
+
+	axienet_iow(lp, PREEMPTION_ENABLE_REG, preemp & PREEMPTION_ENABLE);
+	return 0;
+}
+
+/**
+ * axienet_preemption_ctrl -  Configure Frame Preemption Control register
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: Value to be programmed
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_preemption_ctrl(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct preempt_ctrl_sts data;
+	u32 value;
+
+	if (copy_from_user(&data, useraddr, sizeof(struct preempt_ctrl_sts)))
+		return -EFAULT;
+	value = axienet_ior(lp, PREEMPTION_CTRL_STS_REG);
+
+	value &= ~(VERIFY_TIMER_VALUE_MASK << VERIFY_TIMER_VALUE_SHIFT);
+	value |= (data.verify_timer_value << VERIFY_TIMER_VALUE_SHIFT);
+	value &= ~(ADDITIONAL_FRAG_SIZE_MASK << ADDITIONAL_FRAG_SIZE_SHIFT);
+	value |= (data.additional_frag_size << ADDITIONAL_FRAG_SIZE_SHIFT);
+	value &= ~(DISABLE_PREEMPTION_VERIFY);
+	value |= (data.disable_preemp_verify);
+
+	axienet_iow(lp, PREEMPTION_CTRL_STS_REG, value);
+	return 0;
+}
+
+/**
+ * axienet_preemption_sts -  Get Frame Preemption Status
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: return value, containing Frame Preemption status
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_preemption_sts(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct preempt_status status;
+	u32 value;
+
+	value = axienet_ior(lp, PREEMPTION_ENABLE_REG);
+	status.preemp_en = value & PREEMPTION_ENABLE;
+	value = axienet_ior(lp, XAE_TSN_ABL_OFFSET);
+	status.preemp_sup = (value & PREEMPTION_SUPPORT) ? 1 : 0;
+
+	value = axienet_ior(lp, PREEMPTION_CTRL_STS_REG);
+	status.ctrl.tx_preemp_sts = (value & TX_PREEMPTION_STS) ? 1 : 0;
+	status.ctrl.mac_tx_verify_sts = (value >> MAC_MERGE_TX_VERIFY_STS_SHIFT)
+					& MAC_MERGE_TX_VERIFY_STS_MASK;
+	status.ctrl.verify_timer_value = (value >> VERIFY_TIMER_VALUE_SHIFT) &
+					VERIFY_TIMER_VALUE_MASK;
+	status.ctrl.additional_frag_size = (value >> ADDITIONAL_FRAG_SIZE_SHIFT)
+						& ADDITIONAL_FRAG_SIZE_MASK;
+	status.ctrl.disable_preemp_verify = value & DISABLE_PREEMPTION_VERIFY;
+
+	if (copy_to_user(useraddr, &status, sizeof(struct preempt_status)))
+		return -EFAULT;
+	return 0;
+}
+
+int axienet_preemption_receive(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 value;
+	u8 preemption_support;
+
+	value = axienet_ior(lp, XAE_TSN_ABL_OFFSET);
+	preemption_support = (value & PREEMPTION_SUPPORT) ? 1 : 0;
+
+	if (!preemption_support)
+		return -EOPNOTSUPP;
+
+	value = axienet_ior(lp, PREEMPTION_ENABLE_REG);
+	value = value | PREEMPTION_ENABLE;
+	axienet_iow(lp, PREEMPTION_ENABLE_REG, value);
+	return 0;
+}
+
+/**
+ * statistic_cnts -  Read statistics counter registers
+ * @ndev: Pointer to the net_device structure
+ * @ptr: Buffer addr to fill the counter values
+ * @count: read #count number of registers
+ * @addr_off: Register address to be read
+ */
+static void statistic_cnts(struct net_device *ndev, void *ptr,
+			   unsigned int count, unsigned int addr_off)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int *buf = (int *)ptr;
+	int i = 0;
+
+	for (i = 0; i < count; i++) {
+		buf[i] = axienet_ior(lp, addr_off);
+		addr_off += 4;
+	}
+}
+
+/**
+ * axienet_preemption_cnt -  Get Frame Preemption Statistics counter
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: return value, containing counters value
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_preemption_cnt(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct emac_pmac_stats stats;
+
+	statistic_cnts(ndev, &stats.emac,
+		       sizeof(struct statistics_counters) / 4,
+		       RX_BYTES_EMAC_REG);
+
+	stats.preemp_en = axienet_ior(lp, PREEMPTION_ENABLE_REG);
+	if (stats.preemp_en) {
+		statistic_cnts(ndev, &stats.pmac.sts,
+			       sizeof(struct statistics_counters) / 4,
+			       RX_BYTES_PMAC_REG);
+		statistic_cnts(ndev, &stats.pmac.merge,
+			       sizeof(struct mac_merge_counters) / 4,
+			       TX_HOLD_REG);
+	}
+
+	if (copy_to_user(useraddr, &stats, sizeof(struct emac_pmac_stats)))
+		return -EFAULT;
+	return 0;
+}
+
+/**
+ * axienet_qbu_user_override -  Configure QBU user override register
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: Value to be programmed
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_qbu_user_override(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct qbu_prog data;
+	u32 value;
+
+	if (copy_from_user(&data, useraddr, sizeof(struct qbu_prog)))
+		return -EFAULT;
+
+	value = axienet_ior(lp, QBU_USER_OVERRIDE_REG);
+
+	if (data.set & QBU_WINDOW) {
+		if (data.user.hold_rel_window) {
+			value |= USER_HOLD_REL_ENABLE_VALUE;
+			value |= HOLD_REL_WINDOW_OVERRIDE;
+		} else {
+			value &= ~(USER_HOLD_REL_ENABLE_VALUE);
+			value &= ~(HOLD_REL_WINDOW_OVERRIDE);
+		}
+	}
+	if (data.set & QBU_GUARD_BAND) {
+		if (data.user.guard_band)
+			value |= GUARD_BAND_OVERRUN_CNT_INC_OVERRIDE;
+		else
+			value &= ~(GUARD_BAND_OVERRUN_CNT_INC_OVERRIDE);
+	}
+	if (data.set & QBU_HOLD_TIME) {
+		if (data.user.hold_time_override) {
+			value |= HOLD_TIME_OVERRIDE;
+			value &= ~(USER_HOLD_TIME_MASK << USER_HOLD_TIME_SHIFT);
+			value |= data.user.user_hold_time <<
+					USER_HOLD_TIME_SHIFT;
+		} else {
+			value &= ~(HOLD_TIME_OVERRIDE);
+			value &= ~(USER_HOLD_TIME_MASK << USER_HOLD_TIME_SHIFT);
+		}
+	}
+	if (data.set & QBU_REL_TIME) {
+		if (data.user.rel_time_override) {
+			value |= REL_TIME_OVERRIDE;
+			value &= ~(USER_REL_TIME_MASK << USER_REL_TIME_SHIFT);
+			value |= data.user.user_rel_time << USER_REL_TIME_SHIFT;
+		} else {
+			value &= ~(REL_TIME_OVERRIDE);
+			value &= ~(USER_REL_TIME_MASK << USER_REL_TIME_SHIFT);
+		}
+	}
+
+	axienet_iow(lp, QBU_USER_OVERRIDE_REG, value);
+	return 0;
+}
+
+/**
+ * axienet_qbu_sts -  Get QBU Core status
+ * @ndev: Pointer to the net_device structure
+ * @useraddr: return value, containing QBU core status value
+ * Return: 0 on success, Non-zero error value on failure
+ */
+int axienet_qbu_sts(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct qbu_all_status status;
+	u32 value = 0;
+
+	value = axienet_ior(lp, QBU_USER_OVERRIDE_REG);
+	status.prog.hold_rel_window = (value & USER_HOLD_REL_ENABLE_VALUE)
+					? 1 : 0;
+	status.prog.guard_band = (value & GUARD_BAND_OVERRUN_CNT_INC_OVERRIDE)
+					? 1 : 0;
+	status.prog.user_hold_time = (value >> USER_HOLD_TIME_SHIFT) &
+					USER_HOLD_TIME_MASK;
+	status.prog.user_rel_time = (value >> USER_REL_TIME_SHIFT) &
+					USER_REL_TIME_MASK;
+
+	value = axienet_ior(lp, QBU_CORE_STS_REG);
+	status.core.hold_time = (value >> HOLD_TIME_STS_SHIFT) &
+					HOLD_TIME_STS_MASK;
+	status.core.rel_time = (value >> REL_TIME_STS_SHIFT) &
+					REL_TIME_STS_MASK;
+	status.core.hold_rel_en = (value & HOLD_REL_ENABLE_STS) ? 1 : 0;
+	status.core.pmac_hold_req = value & PMAC_HOLD_REQ_STS;
+
+	if (copy_to_user(useraddr, &status, sizeof(struct qbu_all_status)))
+		return -EFAULT;
+	return 0;
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_preemption.h
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_preemption.h	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,171 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Xilinx TSN QBU/QBR - Frame Preemption header
+ *
+ * Copyright (C) 2017 Xilinx, Inc.
+ *
+ * Author: Priyadarshini Babu <priyadar@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef XILINX_TSN_PREEMPTION_H
+#define XILINX_TSN_PREEMPTION_H
+
+#define PREEMPTION_ENABLE_REG			0x00000440
+#define PREEMPTION_CTRL_STS_REG			0x00000444
+#define QBU_USER_OVERRIDE_REG			0x00000448
+#define QBU_CORE_STS_REG			0x0000044c
+#define TX_HOLD_REG				0x00000910
+#define RX_BYTES_EMAC_REG			0x00000200
+#define RX_BYTES_PMAC_REG			0x00000800
+
+#define PREEMPTION_ENABLE			BIT(0)
+#define PREEMPTION_SUPPORT			BIT(15)
+
+#define TX_PREEMPTION_STS			BIT(31)
+#define MAC_MERGE_TX_VERIFY_STS_MASK		0x7
+#define MAC_MERGE_TX_VERIFY_STS_SHIFT		24
+#define VERIFY_TIMER_VALUE_MASK			0x7F
+#define VERIFY_TIMER_VALUE_SHIFT		8
+#define ADDITIONAL_FRAG_SIZE_MASK		0x3
+#define ADDITIONAL_FRAG_SIZE_SHIFT		4
+#define DISABLE_PREEMPTION_VERIFY		BIT(0)
+
+#define USER_HOLD_REL_ENABLE_VALUE		BIT(31)
+#define USER_HOLD_TIME_MASK			0x1FF
+#define USER_HOLD_TIME_SHIFT			16
+#define USER_REL_TIME_MASK			0x3F
+#define USER_REL_TIME_SHIFT			8
+#define GUARD_BAND_OVERRUN_CNT_INC_OVERRIDE	BIT(3)
+#define HOLD_REL_WINDOW_OVERRIDE		BIT(2)
+#define HOLD_TIME_OVERRIDE			BIT(1)
+#define REL_TIME_OVERRIDE			BIT(0)
+
+#define HOLD_REL_ENABLE_STS			BIT(31)
+#define HOLD_TIME_STS_MASK			0x1FF
+#define HOLD_TIME_STS_SHIFT			16
+#define REL_TIME_STS_MASK			0x3F
+#define REL_TIME_STS_SHIFT			8
+#define PMAC_HOLD_REQ_STS			BIT(0)
+
+struct preempt_ctrl_sts {
+	u8 tx_preemp_sts;
+	u8 mac_tx_verify_sts;
+	u8 verify_timer_value;
+	u8 additional_frag_size;
+	u8 disable_preemp_verify;
+};
+
+struct qbu_prog_override {
+	u8 enable_value:1;
+	u16 user_hold_time:9;
+	u8 user_rel_time:6;
+	u8 guard_band:1;
+	u8 hold_rel_window:1;
+	u8 hold_time_override:1;
+	u8 rel_time_override:1;
+} __packed;
+
+struct qbu_prog {
+	struct qbu_prog_override user;
+	u8 set;
+};
+
+#define QBU_WINDOW BIT(0)
+#define QBU_GUARD_BAND BIT(1)
+#define QBU_HOLD_TIME BIT(2)
+#define QBU_REL_TIME BIT(3)
+
+struct qbu_core_status {
+	u16 hold_time;
+	u8 rel_time;
+	u8 hold_rel_en:1;
+	u8 pmac_hold_req:1;
+} __packed;
+
+struct qbu_all_status {
+	struct qbu_prog_override prog;
+	struct qbu_core_status core;
+};
+
+struct cnt_64 {
+	unsigned int msb;
+	unsigned int lsb;
+};
+
+union static_cntr {
+	u64 cnt;
+	struct cnt_64 word;
+};
+
+struct mac_merge_counters {
+	union static_cntr tx_hold_cnt;
+	union static_cntr tx_frag_cnt;
+	union static_cntr rx_assembly_ok_cnt;
+	union static_cntr rx_assembly_err_cnt;
+	union static_cntr rx_smd_err_cnt;
+	union static_cntr rx_frag_cnt;
+};
+
+struct statistics_counters {
+	union static_cntr rx_bytes_cnt;
+	union static_cntr tx_bytes_cnt;
+	union static_cntr undersize_frames_cnt;
+	union static_cntr frag_frames_cnt;
+	union static_cntr rx_64_bytes_frames_cnt;
+	union static_cntr rx_65_127_bytes_frames_cnt;
+	union static_cntr rx_128_255_bytes_frames_cnt;
+	union static_cntr rx_256_511_bytes_frames_cnt;
+	union static_cntr rx_512_1023_bytes_frames_cnt;
+	union static_cntr rx_1024_max_frames_cnt;
+	union static_cntr rx_oversize_frames_cnt;
+	union static_cntr tx_64_bytes_frames_cnt;
+	union static_cntr tx_65_127_bytes_frames_cnt;
+	union static_cntr tx_128_255_bytes_frames_cnt;
+	union static_cntr tx_256_511_bytes_frames_cnt;
+	union static_cntr tx_512_1023_bytes_frames_cnt;
+	union static_cntr tx_1024_max_frames_cnt;
+	union static_cntr tx_oversize_frames_cnt;
+	union static_cntr rx_good_frames_cnt;
+	union static_cntr rx_fcs_err_cnt;
+	union static_cntr rx_good_broadcast_frames_cnt;
+	union static_cntr rx_good_multicast_frames_cnt;
+	union static_cntr rx_good_control_frames_cnt;
+	union static_cntr rx_out_of_range_err_cnt;
+	union static_cntr rx_good_vlan_frames_cnt;
+	union static_cntr rx_good_pause_frames_cnt;
+	union static_cntr rx_bad_opcode_frames_cnt;
+	union static_cntr tx_good_frames_cnt;
+	union static_cntr tx_good_broadcast_frames_cnt;
+	union static_cntr tx_good_multicast_frames_cnt;
+	union static_cntr tx_underrun_err_cnt;
+	union static_cntr tx_good_control_frames_cnt;
+	union static_cntr tx_good_vlan_frames_cnt;
+	union static_cntr tx_good_pause_frames_cnt;
+};
+
+struct pmac_counters {
+	struct statistics_counters sts;
+	struct mac_merge_counters merge;
+};
+
+struct emac_pmac_stats {
+	u8 preemp_en;
+	struct statistics_counters emac;
+	struct pmac_counters pmac;
+};
+
+struct preempt_status {
+	u8 preemp_en;
+	u8 preemp_sup;
+	struct preempt_ctrl_sts ctrl;
+};
+#endif /* XILINX_TSN_PREEMPTION_H */
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_ptp.h
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_ptp.h	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,89 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Xilinx TSN PTP header
+ *
+ * Copyright (C) 2017 Xilinx, Inc.
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef _TSN_PTP_H_
+#define _TSN_PTP_H_
+
+#define PTP_HW_TSTAMP_SIZE  8   /* 64 bit timestamp */
+#define PTP_RX_HWBUF_SIZE   256
+#define PTP_RX_FRAME_SIZE   252
+#define PTP_HW_TSTAMP_OFFSET (PTP_RX_HWBUF_SIZE - PTP_HW_TSTAMP_SIZE)
+
+#define PTP_MSG_TYPE_MASK				BIT(3)
+#define PTP_TYPE_SYNC                                   0x0
+#define PTP_TYPE_FOLLOW_UP                              0x8
+#define PTP_TYPE_PDELAYREQ                              0x2
+#define PTP_TYPE_PDELAYRESP                             0x3
+#define PTP_TYPE_PDELAYRESP_FOLLOW_UP                   0xA
+#define PTP_TYPE_ANNOUNCE                               0xB
+#define PTP_TYPE_SIGNALING                              0xC
+
+#define PTP_TX_CONTROL_OFFSET		0x00012000 /**< Tx PTP Control Reg */
+#define PTP_RX_CONTROL_OFFSET		0x00012004 /**< Rx PTP Control Reg */
+#define RX_FILTER_CONTROL		0x00012008 /**< Rx Filter Ctrl Reg */
+
+#define PTP_RX_BASE_OFFSET		0x00010000
+#define PTP_RX_CONTROL_OFFSET		0x00012004 /**< Rx PTP Control Reg */
+#define PTP_RX_PACKET_FIELD_MASK	0x00000F00
+#define PTP_RX_PACKET_CLEAR		0x00000001
+
+#define PTP_TX_BUFFER_OFFSET(index)	   (0x00011000 + (index) * 0x100)
+
+#define PTP_TX_CMD_FIELD_LEN			8
+#define PTP_TX_CMD_1STEP_SHIFT			BIT(16)
+#define PTP_TX_BUFFER_CMD2_FIELD		0x4
+
+#define PTP_TX_SYNC_OFFSET                 0x00011000
+#define PTP_TX_FOLLOW_UP_OFFSET            0x00011100
+#define PTP_TX_PDELAYREQ_OFFSET            0x00011200
+#define PTP_TX_PDELAYRESP_OFFSET           0x00011300
+#define PTP_TX_PDELAYRESP_FOLLOW_UP_OFFSET 0x00011400
+#define PTP_TX_ANNOUNCE_OFFSET             0x00011500
+#define PTP_TX_SIGNALING_OFFSET		   0x00011600
+#define PTP_TX_GENERIC_OFFSET		   0x00011700
+#define PTP_TX_SEND_SYNC_FRAME_MASK                     0x00000001
+#define PTP_TX_SEND_FOLLOWUP_FRAME_MASK                 0x00000002
+#define PTP_TX_SEND_PDELAYREQ_FRAME_MASK                0x00000004
+#define PTP_TX_SEND_PDELAYRESP_FRAME_MASK               0x00000008
+#define PTP_TX_SEND_PDELAYRESPFOLLOWUP_FRAME_MASK       0x00000010
+#define PTP_TX_SEND_ANNOUNCE_FRAME_MASK                 0x00000020
+#define PTP_TX_SEND_FRAME6_BIT_MASK                     0x00000040
+#define PTP_TX_SEND_FRAME7_BIT_MASK                     0x00000080
+#define PTP_TX_FRAME_WAITING_MASK			0x0000ff00
+#define PTP_TX_FRAME_WAITING_SHIFT			8
+#define PTP_TX_WAIT_SYNC_FRAME_MASK                     0x00000100
+#define PTP_TX_WAIT_FOLLOWUP_FRAME_MASK                 0x00000200
+#define PTP_TX_WAIT_PDELAYREQ_FRAME_MASK                0x00000400
+#define PTP_TX_WAIT_PDELAYRESP_FRAME_MASK               0x00000800
+#define PTP_TX_WAIT_PDELAYRESPFOLLOWUP_FRAME_MASK       0x00001000
+#define PTP_TX_WAIT_ANNOUNCE_FRAME_MASK                 0x00002000
+#define PTP_TX_WAIT_FRAME6_BIT_MASK                     0x00004000
+#define PTP_TX_WAIT_FRAME7_BIT_MASK                     0x00008000
+#define PTP_TX_WAIT_ALL_FRAMES_MASK                     0x0000FF00
+#define PTP_TX_PACKET_FIELD_MASK                        0x00070000
+#define PTP_TX_PACKET_FIELD_SHIFT                       16
+/* 1-step Correction Field offset 802.1 ASrev */
+#define PTP_CRCT_FIELD_OFFSET				22
+/* 1-step Time Of Day offset 1588-2008 */
+#define PTP_TOD_FIELD_OFFSET				48
+
+int axienet_ptp_xmit(struct sk_buff *skb, struct net_device *ndev);
+irqreturn_t axienet_ptp_rx_irq(int irq, void *_ndev);
+irqreturn_t axienet_ptp_tx_irq(int irq, void *_ndev);
+
+#endif
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_ptp_clock.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_ptp_clock.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,325 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN PTP protocol clock Controller module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/device.h>
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/interrupt.h>
+#include <linux/slab.h>
+#include <linux/kernel.h>
+#include <linux/ptp_clock_kernel.h>
+#include <linux/platform_device.h>
+#include <linux/of_irq.h>
+#include "xilinx_tsn_timer.h"
+
+struct xlnx_ptp_timer {
+	struct                 device *dev;
+	void __iomem          *baseaddr;
+	struct ptp_clock      *ptp_clock;
+	struct ptp_clock_info  ptp_clock_info;
+	spinlock_t             reg_lock; /* ptp timer lock */
+	int                    irq;
+	int                    pps_enable;
+	int                    countpulse;
+};
+
+static void xlnx_tod_read(struct xlnx_ptp_timer *timer, struct timespec64 *ts)
+{
+	u32 sec, nsec;
+
+	nsec = in_be32(timer->baseaddr + XTIMER1588_CURRENT_RTC_NS);
+	sec = in_be32(timer->baseaddr + XTIMER1588_CURRENT_RTC_SEC_L);
+
+	ts->tv_sec = sec;
+	ts->tv_nsec = nsec;
+}
+
+static void xlnx_rtc_offset_write(struct xlnx_ptp_timer *timer,
+				  const struct timespec64 *ts)
+{
+	pr_debug("%s: sec: %lld nsec: %ld\n", __func__, ts->tv_sec, ts->tv_nsec);
+
+	out_be32((timer->baseaddr + XTIMER1588_RTC_OFFSET_SEC_H), 0);
+	out_be32((timer->baseaddr + XTIMER1588_RTC_OFFSET_SEC_L),
+		 (ts->tv_sec));
+	out_be32((timer->baseaddr + XTIMER1588_RTC_OFFSET_NS), ts->tv_nsec);
+}
+
+static void xlnx_rtc_offset_read(struct xlnx_ptp_timer *timer,
+				 struct timespec64 *ts)
+{
+	ts->tv_sec = in_be32(timer->baseaddr + XTIMER1588_RTC_OFFSET_SEC_L);
+	ts->tv_nsec = in_be32(timer->baseaddr + XTIMER1588_RTC_OFFSET_NS);
+}
+
+/* PTP clock operations
+ */
+static int xlnx_ptp_adjfreq(struct ptp_clock_info *ptp, s32 ppb)
+{
+	struct xlnx_ptp_timer *timer = container_of(ptp, struct xlnx_ptp_timer,
+						    ptp_clock_info);
+
+	int neg_adj = 0;
+	u64 freq;
+	u32 diff, incval;
+
+	/* This number should be replaced by a call to get the frequency
+	 * from the device-tree. Currently assumes 125MHz
+	 */
+	incval = 0x800000;
+	/* for 156.25 MHZ Ref clk the value is  incval = 0x800000; */
+
+	if (ppb < 0) {
+		neg_adj = 1;
+		ppb = -ppb;
+	}
+
+	freq = incval;
+	freq *= ppb;
+	diff = div_u64(freq, 1000000000ULL);
+
+	pr_debug("%s: adj: %d ppb: %d\n", __func__, diff, ppb);
+
+	incval = neg_adj ? (incval - diff) : (incval + diff);
+	out_be32((timer->baseaddr + XTIMER1588_RTC_INCREMENT), incval);
+	return 0;
+}
+
+static int xlnx_ptp_adjtime(struct ptp_clock_info *ptp, s64 delta)
+{
+	unsigned long flags;
+	struct xlnx_ptp_timer *timer = container_of(ptp, struct xlnx_ptp_timer,
+						    ptp_clock_info);
+	struct timespec64 now, then = ns_to_timespec64(delta);
+
+	spin_lock_irqsave(&timer->reg_lock, flags);
+
+	xlnx_rtc_offset_read(timer, &now);
+
+	now = timespec64_add(now, then);
+
+	xlnx_rtc_offset_write(timer, (const struct timespec64 *)&now);
+	spin_unlock_irqrestore(&timer->reg_lock, flags);
+
+	return 0;
+}
+
+static int xlnx_ptp_gettime(struct ptp_clock_info *ptp, struct timespec64 *ts)
+{
+	unsigned long flags;
+	struct xlnx_ptp_timer *timer = container_of(ptp, struct xlnx_ptp_timer,
+						    ptp_clock_info);
+	spin_lock_irqsave(&timer->reg_lock, flags);
+
+	xlnx_tod_read(timer, ts);
+
+	spin_unlock_irqrestore(&timer->reg_lock, flags);
+	return 0;
+}
+
+/**
+ * xlnx_ptp_settime - Set the current time on the hardware clock
+ * @ptp: ptp clock structure
+ * @ts: timespec64 containing the new time for the cycle counter
+ *
+ * Return: 0 in all cases.
+ *
+ * The seconds register is written first, then the nanosecond
+ * The hardware loads the entire new value when a nanosecond register
+ * is written
+ **/
+static int xlnx_ptp_settime(struct ptp_clock_info *ptp,
+			    const struct timespec64 *ts)
+{
+	struct xlnx_ptp_timer *timer = container_of(ptp, struct xlnx_ptp_timer,
+						    ptp_clock_info);
+	struct timespec64 delta, tod;
+	struct timespec64 offset;
+	unsigned long flags;
+
+	spin_lock_irqsave(&timer->reg_lock, flags);
+
+	/* First zero the offset */
+	offset.tv_sec = 0;
+	offset.tv_nsec = 0;
+	xlnx_rtc_offset_write(timer, &offset);
+
+	/* Get the current timer value */
+	xlnx_tod_read(timer, &tod);
+
+	/* Subtract the current reported time from our desired time */
+	delta = timespec64_sub(*ts, tod);
+
+	/* Don't write a negative offset */
+	if (delta.tv_sec <= 0) {
+		delta.tv_sec = 0;
+		if (delta.tv_nsec < 0)
+			delta.tv_nsec = 0;
+	}
+
+	xlnx_rtc_offset_write(timer, &delta);
+	spin_unlock_irqrestore(&timer->reg_lock, flags);
+	return 0;
+}
+
+static int xlnx_ptp_enable(struct ptp_clock_info *ptp,
+			   struct ptp_clock_request *rq, int on)
+{
+	struct xlnx_ptp_timer *timer = container_of(ptp, struct xlnx_ptp_timer,
+						    ptp_clock_info);
+
+	switch (rq->type) {
+	case PTP_CLK_REQ_PPS:
+		timer->pps_enable = 1;
+		return 0;
+	default:
+		break;
+	}
+
+	return -EOPNOTSUPP;
+}
+
+static struct ptp_clock_info xlnx_ptp_clock_info = {
+	.owner    = THIS_MODULE,
+	.name     = "Xilinx Timer",
+	.max_adj  = 999999999,
+	.n_ext_ts	= 0,
+	.pps      = 1,
+	.adjfreq  = xlnx_ptp_adjfreq,
+	.adjtime  = xlnx_ptp_adjtime,
+	.gettime64  = xlnx_ptp_gettime,
+	.settime64 = xlnx_ptp_settime,
+	.enable   = xlnx_ptp_enable,
+};
+
+/* module operations */
+
+/**
+ * xlnx_ptp_timer_isr - Interrupt Service Routine
+ * @irq:               IRQ number
+ * @priv:              pointer to the timer structure
+ *
+ * Returns: IRQ_HANDLED for all cases
+ *
+ * Handles the timer interrupt. The timer interrupt fires 128 times per
+ * secound. When our count reaches 128 emit a PTP_CLOCK_PPS event
+ */
+static irqreturn_t xlnx_ptp_timer_isr(int irq, void *priv)
+{
+	struct xlnx_ptp_timer *timer = (struct xlnx_ptp_timer *)priv;
+	struct ptp_clock_event event;
+
+	event.type = PTP_CLOCK_PPS;
+	++timer->countpulse;
+	if (timer->countpulse >= PULSESIN1PPS) {
+		timer->countpulse = 0;
+		if (timer->ptp_clock && timer->pps_enable)
+			ptp_clock_event(timer->ptp_clock, &event);
+	}
+	out_be32((timer->baseaddr + XTIMER1588_INTERRUPT),
+		 (1 << XTIMER1588_INT_SHIFT));
+
+	return IRQ_HANDLED;
+}
+
+int axienet_ptp_timer_remove(void *priv)
+{
+	struct xlnx_ptp_timer *timer = (struct xlnx_ptp_timer *)priv;
+
+	free_irq(timer->irq, (void *)timer);
+
+	axienet_phc_index = -1;
+	if (timer->ptp_clock) {
+		ptp_clock_unregister(timer->ptp_clock);
+		timer->ptp_clock = NULL;
+	}
+	kfree(timer);
+	return 0;
+}
+
+int axienet_get_phc_index(void *priv)
+{
+	struct xlnx_ptp_timer *timer = (struct xlnx_ptp_timer *)priv;
+
+	if (timer->ptp_clock)
+		return ptp_clock_index(timer->ptp_clock);
+	else
+		return -1;
+}
+
+void *axienet_ptp_timer_probe(void __iomem *base, struct platform_device *pdev)
+{
+	struct xlnx_ptp_timer *timer;
+	struct timespec64 ts;
+	int err = 0;
+
+	timer = kzalloc(sizeof(*timer), GFP_KERNEL);
+	if (!timer)
+		return NULL;
+
+	timer->baseaddr = base;
+
+	timer->irq = platform_get_irq_byname(pdev, "interrupt_ptp_timer");
+
+	if (timer->irq < 0) {
+		timer->irq = platform_get_irq_byname(pdev, "rtc_irq");
+		if (timer->irq > 0) {
+			pr_err("ptp timer interrupt name 'rtc_irq' is deprecated\n");
+		} else {
+			pr_err("ptp timer interrupt not found\n");
+			kfree(timer);
+			return NULL;
+		}
+	}
+	spin_lock_init(&timer->reg_lock);
+
+	timer->ptp_clock_info = xlnx_ptp_clock_info;
+
+	timer->ptp_clock = ptp_clock_register(&timer->ptp_clock_info,
+					      &pdev->dev);
+
+	if (IS_ERR(timer->ptp_clock)) {
+		err = PTR_ERR(timer->ptp_clock);
+		pr_debug("Failed to register ptp clock\n");
+		goto out;
+	}
+
+	axienet_phc_index = ptp_clock_index(timer->ptp_clock);
+
+	ts = ktime_to_timespec64(ktime_get_real());
+
+	xlnx_ptp_settime(&timer->ptp_clock_info, &ts);
+
+	/* Enable interrupts */
+	err = request_irq(timer->irq,
+			  xlnx_ptp_timer_isr,
+			  0,
+			  "ptp_rtc",
+			  (void *)timer);
+	if (err)
+		goto err_irq;
+
+	return timer;
+
+err_irq:
+	ptp_clock_unregister(timer->ptp_clock);
+out:
+	timer->ptp_clock = NULL;
+	return NULL;
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_ptp_xmit.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_ptp_xmit.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,370 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN PTP transfer protocol module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include "xilinx_axienet_tsn.h"
+#include "xilinx_tsn_ptp.h"
+#include "xilinx_tsn_timer.h"
+#include <linux/ptp_classify.h>
+
+#define PTP_ONE_SECOND            1000000000    /**< Value in ns */
+
+#define msg_type_string(type) \
+	((type) == PTP_TYPE_SYNC) ? "SYNC" : \
+	((type) == PTP_TYPE_FOLLOW_UP)		  ? "FOLLOW_UP" : \
+	((type) == PTP_TYPE_PDELAYREQ)		  ? "PDELAY_REQ" : \
+	((type) == PTP_TYPE_PDELAYRESP)		  ? "PDELAY_RESP" : \
+	((type) == PTP_TYPE_PDELAYRESP_FOLLOW_UP) ? "PDELAY_RESP_FOLLOW_UP" : \
+	((type) == PTP_TYPE_ANNOUNCE)		  ? "ANNOUNCE" : \
+	"UNKNOWN"
+
+/**
+ * memcpy_fromio_32 - copy ptp buffer from HW
+ * @lp:		Pointer to axienet local structure
+ * @offset:	Offset in the PTP buffer
+ * @data:	Destination buffer
+ * @len:	Len to copy
+ *
+ * This functions copies the data from PTP buffer to destination data buffer
+ */
+static void memcpy_fromio_32(struct axienet_local *lp,
+			     unsigned long offset, u8 *data, size_t len)
+{
+	while (len >= 4) {
+		*(u32 *)data = axienet_ior(lp, offset);
+		len -= 4;
+		offset += 4;
+		data += 4;
+	}
+
+	if (len > 0) {
+		u32 leftover = axienet_ior(lp, offset);
+		u8 *src = (u8 *)&leftover;
+
+		while (len) {
+			*data++ = *src++;
+			len--;
+		}
+	}
+}
+
+/**
+ * memcpy_toio_32 - copy ptp buffer from HW
+ * @lp:		Pointer to axienet local structure
+ * @offset:	Offset in the PTP buffer
+ * @data:	Source data
+ * @len:	Len to copy
+ *
+ * This functions copies the source data to destination ptp buffer
+ */
+static void memcpy_toio_32(struct axienet_local *lp,
+			   unsigned long offset, u8 *data, size_t len)
+{
+	while (len >= 4) {
+		axienet_iow(lp, offset, *(u32 *)data);
+		len -= 4;
+		offset += 4;
+		data += 4;
+	}
+
+	if (len > 0) {
+		u32 leftover = 0;
+		u8 *dest = (u8 *)&leftover;
+
+		while (len) {
+			*dest++ = *data++;
+			len--;
+		}
+		axienet_iow(lp, offset, leftover);
+	}
+}
+
+static int is_sync(struct sk_buff *skb)
+{
+	u8 *msg_type;
+
+	msg_type = (u8 *)skb->data + ETH_HLEN;
+
+	return (*msg_type & 0xf) == PTP_TYPE_SYNC;
+}
+
+/**
+ * axienet_ptp_xmit - xmit skb using PTP HW
+ * @skb:	sk_buff pointer that contains data to be Txed.
+ * @ndev:	Pointer to net_device structure.
+ *
+ * Return: NETDEV_TX_OK, on success
+ *	    NETDEV_TX_BUSY, if any of the descriptors are not free
+ *
+ * This function is called to transmit a PTP skb. The function uses
+ * the free PTP TX buffer entry and sends the frame
+ */
+int axienet_ptp_xmit(struct sk_buff *skb, struct net_device *ndev)
+{
+	u8 msg_type;
+	struct axienet_local *lp = netdev_priv(ndev);
+	unsigned long flags;
+	u8 tx_frame_waiting;
+	u8 free_index;
+	u32 cmd1_field = 0;
+	u32 cmd2_field = 0;
+
+	msg_type  = *(u8 *)(skb->data + ETH_HLEN);
+
+	pr_debug("  -->XMIT: protocol: %x message: %s frame_len: %d\n",
+		 skb->protocol,
+		 msg_type_string(msg_type & 0xf), skb->len);
+
+	tx_frame_waiting =  (axienet_ior(lp, PTP_TX_CONTROL_OFFSET) &
+				PTP_TX_FRAME_WAITING_MASK) >>
+				PTP_TX_FRAME_WAITING_SHIFT;
+
+	/* we reached last frame */
+	if (tx_frame_waiting & (1 << 7)) {
+		if (!netif_queue_stopped(ndev))
+			netif_stop_queue(ndev);
+		pr_debug("tx_frame_waiting: %d\n", tx_frame_waiting);
+		return NETDEV_TX_BUSY;
+	}
+
+	/* go to next available slot */
+	free_index  = fls(tx_frame_waiting);
+
+	/* write the len */
+	if (lp->ptp_ts_type == HWTSTAMP_TX_ONESTEP_SYNC &&
+	    is_sync(skb)) {
+		/* enable 1STEP SYNC */
+		cmd1_field |= PTP_TX_CMD_1STEP_SHIFT;
+		cmd2_field |= PTP_TOD_FIELD_OFFSET;
+	}
+
+	cmd1_field |= skb->len;
+
+	axienet_iow(lp, PTP_TX_BUFFER_OFFSET(free_index), cmd1_field);
+	axienet_iow(lp, PTP_TX_BUFFER_OFFSET(free_index) +
+			PTP_TX_BUFFER_CMD2_FIELD, cmd2_field);
+	memcpy_toio_32(lp,
+		       (PTP_TX_BUFFER_OFFSET(free_index) +
+			PTP_TX_CMD_FIELD_LEN),
+		       skb->data, skb->len);
+
+	/* send the frame */
+	axienet_iow(lp, PTP_TX_CONTROL_OFFSET, (1 << free_index));
+
+	if (lp->ptp_ts_type != HWTSTAMP_TX_ONESTEP_SYNC ||
+	    (!is_sync(skb))) {
+		spin_lock_irqsave(&lp->ptp_tx_lock, flags);
+		skb->cb[0] = free_index;
+		skb_queue_tail(&lp->ptp_txq, skb);
+
+		if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)
+			skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+
+		skb_tx_timestamp(skb);
+		spin_unlock_irqrestore(&lp->ptp_tx_lock, flags);
+	}
+	return NETDEV_TX_OK;
+}
+
+/**
+ * axienet_set_timestamp - timestamp skb with HW timestamp
+ * @lp:		Pointer to axienet local structure
+ * @hwtstamps:  Pointer to skb timestamp structure
+ * @offset:	offset of the timestamp in the PTP buffer
+ *
+ * Return:	None.
+ *
+ */
+static void axienet_set_timestamp(struct axienet_local *lp,
+				  struct skb_shared_hwtstamps *hwtstamps,
+				  unsigned int offset)
+{
+	u32 captured_ns;
+	u32 captured_sec;
+
+	captured_ns = axienet_ior(lp, offset + 4);
+	captured_sec = axienet_ior(lp, offset);
+
+	/* Upper 32 bits contain s, lower 32 bits contain ns. */
+	hwtstamps->hwtstamp = ktime_set(captured_sec,
+					captured_ns);
+}
+
+/**
+ * axienet_ptp_recv - receive ptp buffer in skb from HW
+ * @ndev:	Pointer to net_device structure.
+ *
+ * This function is called from the ptp rx isr. It allocates skb, and
+ * copies the ptp rx buffer data to it and calls netif_rx for further
+ * processing.
+ *
+ */
+static void axienet_ptp_recv(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	unsigned long ptp_frame_base_addr = 0;
+	struct sk_buff *skb;
+	u16 msg_len;
+	u8 msg_type;
+	u32 bytes = 0;
+	u32 packets = 0;
+
+	pr_debug("%s:\n ", __func__);
+
+	while (((lp->ptp_rx_hw_pointer & 0xf) !=
+		 (lp->ptp_rx_sw_pointer & 0xf))) {
+		skb = netdev_alloc_skb(ndev, PTP_RX_FRAME_SIZE);
+
+		lp->ptp_rx_sw_pointer += 1;
+
+		ptp_frame_base_addr = PTP_RX_BASE_OFFSET +
+				   ((lp->ptp_rx_sw_pointer & 0xf) *
+							PTP_RX_HWBUF_SIZE);
+
+		memset(skb->data, 0x0, PTP_RX_FRAME_SIZE);
+
+		memcpy_fromio_32(lp, ptp_frame_base_addr, skb->data,
+				 PTP_RX_FRAME_SIZE);
+
+		msg_type  = *(u8 *)(skb->data + ETH_HLEN) & 0xf;
+		msg_len  = *(u16 *)(skb->data + ETH_HLEN + 2);
+
+		skb_put(skb, ntohs(msg_len) + ETH_HLEN);
+
+		bytes += skb->len;
+		packets++;
+
+		skb->protocol = eth_type_trans(skb, ndev);
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+
+		pr_debug("  -->RECV: protocol: %x message: %s frame_len: %d\n",
+			 skb->protocol, msg_type_string(msg_type & 0xf),
+			 skb->len);
+		/* timestamp only event messages */
+		if (!(msg_type & PTP_MSG_TYPE_MASK)) {
+			axienet_set_timestamp(lp, skb_hwtstamps(skb),
+					      (ptp_frame_base_addr +
+					      PTP_HW_TSTAMP_OFFSET));
+		}
+
+		netif_rx(skb);
+	}
+	ndev->stats.rx_packets += packets;
+	ndev->stats.rx_bytes += bytes;
+}
+
+/**
+ * axienet_ptp_rx_irq - PTP RX ISR handler
+ * @irq:		irq number
+ * @_ndev:	net_device pointer
+ *
+ * Return:	IRQ_HANDLED for all cases.
+ */
+irqreturn_t axienet_ptp_rx_irq(int irq, void *_ndev)
+{
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	pr_debug("%s: received\n ", __func__);
+	lp->ptp_rx_hw_pointer = (axienet_ior(lp, PTP_RX_CONTROL_OFFSET)
+					& PTP_RX_PACKET_FIELD_MASK)  >> 8;
+
+	axienet_ptp_recv(ndev);
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * axienet_tx_tstamp - timestamp skb on trasmit path
+ * @work:	Pointer to work_struct structure
+ *
+ * This adds TX timestamp to skb
+ */
+void axienet_tx_tstamp(struct work_struct *work)
+{
+	struct axienet_local *lp = container_of(work, struct axienet_local,
+			tx_tstamp_work);
+	struct net_device *ndev = lp->ndev;
+	struct skb_shared_hwtstamps hwtstamps;
+	struct sk_buff *skb;
+	unsigned long ts_reg_offset;
+	unsigned long flags;
+	u8 tx_packet;
+	u8 index;
+	u32 bytes = 0;
+	u32 packets = 0;
+
+	memset(&hwtstamps, 0, sizeof(struct skb_shared_hwtstamps));
+
+	spin_lock_irqsave(&lp->ptp_tx_lock, flags);
+
+	tx_packet =  (axienet_ior(lp, PTP_TX_CONTROL_OFFSET) &
+				PTP_TX_PACKET_FIELD_MASK) >>
+				PTP_TX_PACKET_FIELD_SHIFT;
+
+	while ((skb = __skb_dequeue(&lp->ptp_txq)) != NULL) {
+		index = skb->cb[0];
+
+		/* dequeued packet yet to be xmited? */
+		if (index > tx_packet) {
+			/* enqueue it back and break */
+			skb_queue_tail(&lp->ptp_txq, skb);
+			break;
+		}
+		/* time stamp reg offset */
+		ts_reg_offset = PTP_TX_BUFFER_OFFSET(index) +
+					PTP_HW_TSTAMP_OFFSET;
+
+		if (skb_shinfo(skb)->tx_flags & SKBTX_IN_PROGRESS) {
+			axienet_set_timestamp(lp, &hwtstamps, ts_reg_offset);
+			skb_tstamp_tx(skb, &hwtstamps);
+		}
+
+		bytes += skb->len;
+		packets++;
+		dev_kfree_skb_any(skb);
+	}
+	ndev->stats.tx_packets += packets;
+	ndev->stats.tx_bytes += bytes;
+
+	spin_unlock_irqrestore(&lp->ptp_tx_lock, flags);
+}
+
+/**
+ * axienet_ptp_tx_irq - PTP TX irq handler
+ * @irq:		irq number
+ * @_ndev:	net_device pointer
+ *
+ * Return:	IRQ_HANDLED for all cases.
+ *
+ */
+irqreturn_t axienet_ptp_tx_irq(int irq, void *_ndev)
+{
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	pr_debug("%s: got tx interrupt\n", __func__);
+
+	/* read ctrl register to clear the interrupt */
+	axienet_ior(lp, PTP_TX_CONTROL_OFFSET);
+
+	schedule_work(&lp->tx_tstamp_work);
+
+	netif_wake_queue(ndev);
+
+	return IRQ_HANDLED;
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_qci.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_qci.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,154 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN QCI Controller module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include "xilinx_tsn_switch.h"
+
+#define SMC_MODE_SHIFT				28
+#define	SMC_CBR_MASK				0x00FFFFFF
+#define	SMC_EBR_MASK				0x00FFFFFF
+#define IN_PORTID_MASK				0x3
+#define IN_PORT_SHIFT				14
+#define MAX_FR_SIZE_MASK			0x00000FFF
+
+#define GATE_ID_SHIFT				24
+#define METER_ID_SHIFT				8
+#define EN_METER_SHIFT				6
+#define ALLOW_STREM_SHIFT			5
+#define EN_PSFP_SHIFT				4
+#define WR_OP_TYPE_MASK				0x3
+#define WR_OP_TYPE_SHIFT			2
+#define OP_TYPE_SHIFT				1
+#define PSFP_EN_CONTROL_MASK			0x1
+
+/**
+ * psfp_control - Configure thr control for PSFP
+ * @data:	Value to be programmed
+ */
+void psfp_control(struct psfp_config data)
+{
+	u32 mask;
+	u32 timeout = 20000;
+
+	mask = data.gate_id << GATE_ID_SHIFT;
+	mask |= data.meter_id << METER_ID_SHIFT;
+	mask |= data.en_meter << EN_METER_SHIFT;
+	mask |= data.allow_stream << ALLOW_STREM_SHIFT;
+	mask |= data.en_psfp << EN_PSFP_SHIFT;
+	mask |= (data.wr_op_type & WR_OP_TYPE_MASK) << WR_OP_TYPE_SHIFT;
+	mask |= data.op_type << OP_TYPE_SHIFT;
+	mask |= PSFP_EN_CONTROL_MASK;
+
+	axienet_iow(&lp, PSFP_CONTROL_OFFSET, mask);
+
+	/* wait for write to complete */
+	while ((axienet_ior(&lp, PSFP_CONTROL_OFFSET) &
+		PSFP_EN_CONTROL_MASK) && timeout)
+		timeout--;
+
+	if (!timeout)
+		pr_warn("PSFP control write took longer time!!");
+}
+
+/**
+ * get_stream_filter_config -  Get Stream Filter Configuration
+ * @data:	Value returned
+ */
+void get_stream_filter_config(struct stream_filter *data)
+{
+	u32 reg_val;
+
+	reg_val = axienet_ior(&lp, STREAM_FILTER_CONFIG_OFFSET);
+
+	data->max_fr_size = reg_val & MAX_FR_SIZE_MASK;
+	data->in_pid = (reg_val >> IN_PORT_SHIFT) & IN_PORTID_MASK;
+}
+
+/**
+ * config_stream_filter -  Configure Stream Filter Configuration
+ * @data:	Value to be programmed
+ */
+void config_stream_filter(struct stream_filter data)
+{
+	u32 mask;
+
+	mask = ((data.in_pid & IN_PORTID_MASK) << IN_PORT_SHIFT) |
+					(data.max_fr_size & MAX_FR_SIZE_MASK);
+	axienet_iow(&lp, STREAM_FILTER_CONFIG_OFFSET, mask);
+}
+
+/**
+ * get_meter_reg -  Read Stream Meter Configuration registers value
+ * @data:	Value returned
+ */
+void get_meter_reg(struct meter_config *data)
+{
+	u32 conf_r4;
+
+	data->cir = axienet_ior(&lp, STREAM_METER_CIR_OFFSET);
+	data->eir = axienet_ior(&lp, STREAM_METER_EIR_OFFSET);
+	data->cbr = axienet_ior(&lp, STREAM_METER_CBR_OFFSET) & SMC_CBR_MASK;
+	conf_r4 = axienet_ior(&lp, STREAM_METER_EBR_OFFSET);
+
+	data->ebr = conf_r4 & SMC_EBR_MASK;
+	data->mode = (conf_r4 & 0xF0000000) >> SMC_MODE_SHIFT;
+}
+
+/**
+ * program_meter_reg -  configure Stream Meter Configuration registers
+ * @data:	Value to be programmed
+ */
+void program_meter_reg(struct meter_config data)
+{
+	u32 conf_r4;
+
+	axienet_iow(&lp, STREAM_METER_CIR_OFFSET, data.cir);
+	axienet_iow(&lp, STREAM_METER_EIR_OFFSET, data.eir);
+	/* TODO: Check if this barrier is necessary */
+	wmb();
+	axienet_iow(&lp, STREAM_METER_CBR_OFFSET, data.cbr & SMC_CBR_MASK);
+
+	conf_r4 = (data.ebr & SMC_EBR_MASK) | (data.mode << SMC_MODE_SHIFT);
+	axienet_iow(&lp, STREAM_METER_EBR_OFFSET, conf_r4);
+}
+
+/**
+ * get_psfp_static_counter -  get memory static counters value
+ * @data  :	return value, containing counter value
+ */
+void get_psfp_static_counter(struct psfp_static_counter *data)
+{
+	int offset = (data->num) * 8;
+
+	data->psfp_fr_count.lsb = axienet_ior(&lp, TOTAL_PSFP_FRAMES_OFFSET +
+									offset);
+	data->psfp_fr_count.msb = axienet_ior(&lp, TOTAL_PSFP_FRAMES_OFFSET  +
+								offset + 0x4);
+
+	data->err_filter_ins_port.lsb = axienet_ior(&lp,
+						    FLTR_INGS_PORT_ERR_OFFSET + offset);
+	data->err_filter_ins_port.msb = axienet_ior(&lp,
+						    FLTR_INGS_PORT_ERR_OFFSET + offset + 0x4);
+
+	data->err_filtr_sdu.lsb = axienet_ior(&lp, FLTR_STDU_ERR_OFFSET +
+									offset);
+	data->err_filtr_sdu.msb = axienet_ior(&lp, FLTR_STDU_ERR_OFFSET +
+								offset + 0x4);
+
+	data->err_meter.lsb = axienet_ior(&lp, METER_ERR_OFFSET + offset);
+	data->err_meter.msb = axienet_ior(&lp, METER_ERR_OFFSET + offset + 0x4);
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_shaper.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_shaper.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,232 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN QBV sheduler module.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include "xilinx_axienet_tsn.h"
+#include "xilinx_tsn_shaper.h"
+
+static inline int axienet_map_gs_to_hw(struct axienet_local *lp, u32 gs)
+{
+	u8 be_queue = 0;
+	u8 re_queue = 1;
+	u8 st_queue = 2;
+	unsigned int acl_bit_map = 0;
+
+	if (lp->num_tc == 2)
+		st_queue = 1;
+
+	if (gs & GS_BE_OPEN)
+		acl_bit_map |= (1 << be_queue);
+	if (gs & GS_ST_OPEN)
+		acl_bit_map |= (1 << st_queue);
+	if (lp->num_tc == 3 && (gs & GS_RE_OPEN))
+		acl_bit_map |= (1 << re_queue);
+
+	return acl_bit_map;
+}
+
+static int __axienet_set_schedule(struct net_device *ndev, struct qbv_info *qbv)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u16 i;
+	unsigned int acl_bit_map = 0;
+	u32 u_config_change = 0;
+
+	if (qbv->cycle_time == 0) {
+		/* clear the gate enable bit */
+		u_config_change &= ~CC_ADMIN_GATE_ENABLE_BIT;
+		/* open all the gates */
+		u_config_change |= CC_ADMIN_GATE_STATE_SHIFT;
+
+		axienet_qbv_iow(lp, CONFIG_CHANGE, u_config_change);
+
+		return 0;
+	}
+
+	if (axienet_qbv_ior(lp, PORT_STATUS) & 1) {
+		if (qbv->force) {
+			u_config_change &= ~CC_ADMIN_GATE_ENABLE_BIT;
+			axienet_qbv_iow(lp, CONFIG_CHANGE, u_config_change);
+		} else {
+			return -EALREADY;
+		}
+	}
+	/* write admin time */
+	axienet_qbv_iow(lp, ADMIN_CYCLE_TIME_DENOMINATOR,
+			qbv->cycle_time & CYCLE_TIME_DENOMINATOR_MASK);
+
+	axienet_qbv_iow(lp, ADMIN_BASE_TIME_NS, qbv->ptp_time_ns);
+
+	axienet_qbv_iow(lp, ADMIN_BASE_TIME_SEC,
+			qbv->ptp_time_sec & 0xFFFFFFFF);
+	axienet_qbv_iow(lp, ADMIN_BASE_TIME_SECS,
+			(qbv->ptp_time_sec >> 32) & BASE_TIME_SECS_MASK);
+
+	u_config_change = axienet_qbv_ior(lp, CONFIG_CHANGE);
+
+	u_config_change &= ~(CC_ADMIN_CTRL_LIST_LENGTH_MASK <<
+				CC_ADMIN_CTRL_LIST_LENGTH_SHIFT);
+	u_config_change |= (qbv->list_length & CC_ADMIN_CTRL_LIST_LENGTH_MASK)
+					<< CC_ADMIN_CTRL_LIST_LENGTH_SHIFT;
+
+	/* program each list */
+	for (i = 0; i < qbv->list_length; i++) {
+		acl_bit_map = axienet_map_gs_to_hw(lp, qbv->acl_gate_state[i]);
+		axienet_qbv_iow(lp,  ADMIN_CTRL_LIST(i),
+				(acl_bit_map & (ACL_GATE_STATE_MASK)) <<
+				ACL_GATE_STATE_SHIFT);
+
+	    /* set the time for each entry */
+	    axienet_qbv_iow(lp, ADMIN_CTRL_LIST_TIME(i),
+			    qbv->acl_gate_time[i] &
+			    CTRL_LIST_TIME_INTERVAL_MASK);
+	}
+
+	/* clear interrupt status */
+	axienet_qbv_iow(lp, INT_STATUS, 0);
+
+	/* kick in new config change */
+	u_config_change |= CC_ADMIN_CONFIG_CHANGE_BIT;
+
+	/* enable gate */
+	u_config_change |= CC_ADMIN_GATE_ENABLE_BIT;
+
+	/* start */
+	axienet_qbv_iow(lp, CONFIG_CHANGE, u_config_change);
+
+	return 0;
+}
+
+int axienet_set_schedule(struct net_device *ndev, void __user *useraddr)
+{
+	struct qbv_info *config;
+	int ret;
+
+	config = kmalloc(sizeof(*config), GFP_KERNEL);
+	if (!config)
+		return -ENOMEM;
+
+	if (copy_from_user(config, useraddr, sizeof(struct qbv_info))) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	pr_debug("setting new schedule\n");
+
+	ret = __axienet_set_schedule(ndev, config);
+out:
+	kfree(config);
+	return ret;
+}
+
+static int __axienet_get_schedule(struct net_device *ndev, struct qbv_info *qbv)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u16 i = 0;
+	u32 u_value = 0;
+
+	if (!(axienet_qbv_ior(lp, CONFIG_CHANGE) &
+			CC_ADMIN_GATE_ENABLE_BIT)) {
+		qbv->cycle_time = 0;
+		return 0;
+	}
+
+	u_value = axienet_qbv_ior(lp, GATE_STATE);
+	qbv->list_length = (u_value >> CC_ADMIN_CTRL_LIST_LENGTH_SHIFT) &
+				CC_ADMIN_CTRL_LIST_LENGTH_MASK;
+
+	u_value = axienet_qbv_ior(lp, OPER_CYCLE_TIME_DENOMINATOR);
+	qbv->cycle_time = u_value & CYCLE_TIME_DENOMINATOR_MASK;
+
+	u_value = axienet_qbv_ior(lp, OPER_BASE_TIME_NS);
+	qbv->ptp_time_ns = u_value & OPER_BASE_TIME_NS_MASK;
+
+	qbv->ptp_time_sec = axienet_qbv_ior(lp, OPER_BASE_TIME_SEC);
+	u_value = axienet_qbv_ior(lp, OPER_BASE_TIME_SECS);
+	qbv->ptp_time_sec |= (u64)(u_value & BASE_TIME_SECS_MASK) << 32;
+
+	for (i = 0; i < qbv->list_length; i++) {
+		u_value = axienet_qbv_ior(lp, OPER_CTRL_LIST(i));
+		qbv->acl_gate_state[i] = (u_value >> ACL_GATE_STATE_SHIFT) &
+					ACL_GATE_STATE_MASK;
+		/**
+		 * In 2Q system, the actual ST Gate state value is 2,
+		 * for user the ST Gate state value is always 4.
+		 */
+		if (lp->num_tc == 2 && qbv->acl_gate_state[i] == 2)
+			qbv->acl_gate_state[i] = 4;
+
+		u_value = axienet_qbv_ior(lp, OPER_CTRL_LIST_TIME(i));
+		qbv->acl_gate_time[i] = u_value & CTRL_LIST_TIME_INTERVAL_MASK;
+	}
+	return 0;
+}
+
+int axienet_get_schedule(struct net_device *ndev, void __user *useraddr)
+{
+	struct qbv_info *qbv;
+	int ret = 0;
+
+	qbv = kmalloc(sizeof(*qbv), GFP_KERNEL);
+	if (!qbv)
+		return -ENOMEM;
+
+	if (copy_from_user(qbv, useraddr, sizeof(struct qbv_info))) {
+		ret = -EFAULT;
+		goto out;
+	}
+
+	__axienet_get_schedule(ndev, qbv);
+
+	if (copy_to_user(useraddr, qbv, sizeof(struct qbv_info)))
+		ret = -EFAULT;
+out:
+	kfree(qbv);
+	return ret;
+}
+
+static irqreturn_t axienet_qbv_irq(int irq, void *_ndev)
+{
+	struct net_device *ndev = _ndev;
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	/* clear status */
+	axienet_qbv_iow(lp, INT_CLEAR, 0);
+
+	return IRQ_HANDLED;
+}
+
+int axienet_qbv_init(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int rc = 0;
+	static char irq_name[24];
+
+	if (lp->qbv_irq > 0) {
+		sprintf(irq_name, "%s_qbv", ndev->name);
+		rc = request_irq(lp->qbv_irq, axienet_qbv_irq, 0, irq_name,
+				 ndev);
+	}
+	return rc;
+}
+
+void axienet_qbv_remove(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	free_irq(lp->qbv_irq, ndev);
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_shaper.h
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_shaper.h	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,137 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Xilinx TSN QBV scheduler header
+ *
+ * Copyright (C) 2017 Xilinx, Inc.
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef XILINX_TSN_SHAPER_H
+#define XILINX_TSN_SHAPER_H
+
+/* 0x0		CONFIG_CHANGE
+ * 0x8		GATE_STATE
+ * 0x10		ADMIN_CTRL_LIST_LENGTH
+ * 0x18		ADMIN_CYCLE_TIME_DENOMINATOR
+ * 0x20         ADMIN_BASE_TIME_NS
+ * 0x24		ADMIN_BASE_TIME_SEC
+ * 0x28		ADMIN_BASE_TIME_SECS
+ * 0x30		INT_STAT
+ * 0x34		INT_EN
+ * 0x38		INT_CLR
+ * 0x3c		STATUS
+ * 0x40		CONFIG_CHANGE_TIME_NS
+ * 0x44		CONFIG_CHANGE_TIME_SEC
+ * 0x48		CONFIG_CHANGE_TIME_SECS
+ * 0x50		OPER_CTRL_LIST_LENGTH
+ * 0x58		OPER_CYCLE_TIME_DENOMINATOR
+ * 0x60		OPER_BASE_TIME_NS
+ * 0x64		OPER_BASE_TIME_SEC
+ * 0x68		OPER_BASE_TIME_SECS
+ * 0x6c		BE_XMIT_OVRRUN_CNT
+ * 0x74		RES_XMIT_OVRRUN_CNT
+ * 0x7c		ST_XMIT_OVRRUN_CNT
+ */
+
+#define CTRL_LIST_BASE			0x1000
+
+/* control list entries
+ * admin control list 0 : 31
+ * "Time interval between two gate entries" must be greater than
+ * "time required to transmit biggest supported frame" on that queue when
+ * the gate for the queue is going from open to close state.
+ */
+#define ADMIN_CTRL_LIST(n)		(CTRL_LIST_BASE + ((n) * 8))
+#define ACL_GATE_STATE_SHIFT		8
+#define ACL_GATE_STATE_MASK		0x7
+#define ADMIN_CTRL_LIST_TIME(n)		(ADMIN_CTRL_LIST(n) + 4)
+
+#define OPER_CTRL_LIST(n)		(CTRL_LIST_BASE + 0x800 + ((n) * 8))
+#define OPER_CTRL_LIST_TIME(n)		(OPER_CTRL_LIST(n) + 4)
+#define CTRL_LIST_TIME_INTERVAL_MASK	0xFFFFF
+
+#define CONFIG_CHANGE			0x0
+#define CC_ADMIN_GATE_STATE_SHIFT	0x7
+#define CC_ADMIN_GATE_STATE_MASK	(7)
+#define CC_ADMIN_CTRL_LIST_LENGTH_SHIFT	(8)
+#define CC_ADMIN_CTRL_LIST_LENGTH_MASK	(0x1FF)
+/* This request bit is set when all the related Admin* filelds are populated.
+ * This bit is set by S/W and clear by core when core start with new schedule.
+ * Once set it can only be cleared by core or hard/soft reset.
+ */
+#define CC_ADMIN_CONFIG_CHANGE_BIT	BIT(30)
+#define CC_ADMIN_GATE_ENABLE_BIT	BIT(31)
+
+#define GATE_STATE			0x8
+#define GS_OPER_GATE_STATE_SHIFT	(0)
+#define GS_OPER_GATE_STATE_MASK		(0x7)
+#define GS_OPER_CTRL_LIST_LENGTH_SHIFT	(8)
+#define GS_OPER_CTRL_LIST_LENGTH_MASK	(0x3F)
+#define GS_SUP_MAX_LIST_LENGTH_SHIFT	(16)
+#define GS_SUP_MAX_LIST_LENGTH_MASK	(0x3F)
+#define GS_TICK_GRANULARITY_SHIFT	(24)
+#define GS_TICK_GRANULARITY_MASK	(0x3F)
+
+#define ADMIN_CYCLE_TIME_DENOMINATOR	0x18
+#define ADMIN_BASE_TIME_NS		0x20
+#define ADMIN_BASE_TIME_SEC		0x24
+#define ADMIN_BASE_TIME_SECS		0x28
+
+#define INT_STATUS			0x30
+#define INT_ENABLE			0x34
+#define INT_CLEAR			0x38
+#define PORT_STATUS			0x3c
+
+/* Config Change time is valid after Config Pending bit is set. */
+#define CONFIG_CHANGE_TIME_NS		0x40
+#define CONFIG_CHANGE_TIME_SEC		0x44
+#define CONFIG_CHANGE_TIME_SECS		0x48
+
+#define OPER_CONTROL_LIST_LENGTH	0x50
+#define OPER_CYCLE_TIME_DENOMINATOR	0x58
+#define CYCLE_TIME_DENOMINATOR_MASK	(0x3FFFFFFF)
+
+#define OPER_BASE_TIME_NS		0x60
+#define OPER_BASE_TIME_NS_MASK		(0x3FFFFFFF)
+#define OPER_BASE_TIME_SEC		0x64
+#define OPER_BASE_TIME_SECS		0x68
+#define BASE_TIME_SECS_MASK		(0xFFFF)
+
+#define BE_XMIT_OVERRUN_COUNT		0x6c
+#define RES_XMIT_OVERRUN_COUNT		0x74
+#define ST_XMIT_OVERRUN_COUNT		0x7c
+
+/* internally hw deals with queues only,
+ * in 3q system ST acl bitmap would be would 1 << 2
+ * in 2q system ST acl bitmap would be 1 << 1
+ * But this is confusing to users.
+ * so use the following fixed gate state and internally
+ * map them to hw
+ */
+#define GS_BE_OPEN			BIT(0)
+#define GS_RE_OPEN			BIT(1)
+#define GS_ST_OPEN			BIT(2)
+#define QBV_MAX_ENTRIES			256
+
+struct qbv_info {
+	u8 port;
+	u8 force;
+	u32 cycle_time;
+	u64 ptp_time_sec;
+	u32 ptp_time_ns;
+	u32 list_length;
+	u32 acl_gate_state[QBV_MAX_ENTRIES];
+	u32 acl_gate_time[QBV_MAX_ENTRIES];
+};
+
+#endif /* XILINX_TSN_SHAPER_H */
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_switch.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_switch.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,1859 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx FPGA Xilinx TSN Switch Controller driver.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/of_platform.h>
+#include <linux/module.h>
+#include <linux/miscdevice.h>
+#include <linux/of_net.h>
+#include "xilinx_tsn_switch.h"
+
+static struct miscdevice switch_dev;
+static struct device_node *ep_node;
+struct axienet_local lp;
+static struct axienet_local *ep_lp;
+static u8 en_hw_addr_learning;
+static u8 sw_mac_addr[ETH_ALEN];
+
+#define DELAY_OF_FIVE_MILLISEC			(5 * DELAY_OF_ONE_MILLISEC)
+
+#define ADD					1
+#define DELETE					0
+
+#define PMAP_EGRESS_QUEUE_MASK			0x7
+#define PMAP_EGRESS_QUEUE0_SELECT		0x0
+#define PMAP_EGRESS_QUEUE1_SELECT		0x1
+#define PMAP_EGRESS_QUEUE2_SELECT		0x2
+#define SDL_EN_CAM_IPV_SHIFT			28
+#define SDL_CAM_IPV_SHIFT			29
+
+#define SDL_CAM_WR_ENABLE			BIT(0)
+#define SDL_CAM_ADD_ENTRY			0x3
+#define SDL_CAM_DELETE_ENTRY			0x5
+#define SDL_CAM_READ_KEY_ENTRY			0x1
+#define SDL_CAM_READ_ENTRY			GENMASK(2, 0)
+#define SDL_CAM_VLAN_SHIFT			16
+#define SDL_CAM_VLAN_MASK			0xFFF
+#define SDL_CAM_IPV_MASK			0x7
+#define SDL_CAM_PORT_LIST_SHIFT			8
+#define SDL_GATEID_SHIFT			16
+#define SDL_CAM_EP_MGMTQ_EN			BIT(15)
+#define SDL_CAM_FWD_TO_EP			BIT(0)
+#define SDL_CAM_FWD_TO_PORT_1			BIT(1)
+#define SDL_CAM_FWD_TO_PORT_2			BIT(2)
+#define SDL_CAM_EP_ACTION_LIST_SHIFT		0
+#define SDL_CAM_MAC_ACTION_LIST_SHIFT		4
+#define SDL_CAM_DEST_MAC_XLATION		BIT(0)
+#define SDL_CAM_VLAN_ID_XLATION			BIT(1)
+#define SDL_CAM_UNTAG_FRAME			BIT(2)
+#define SDL_CAM_TAG_FRAME			BIT(3)
+
+#define PORT_MAC_ADDR_LSB_MASK			(0xF)
+#define MAC2_PORT_MAC_ADDR_LSB_SHIFT		(20)
+#define PORT_STATUS_MASK                       (0x7)
+#define MAC2_PORT_STATUS_SHIFT                 (17)
+#define MAC2_PORT_STATUS_CHG_BIT               BIT(16)
+#define MAC1_PORT_MAC_ADDR_LSB_SHIFT		(12)
+#define MAC1_PORT_STATUS_SHIFT                 (9)
+#define MAC1_PORT_STATUS_CHG_BIT               BIT(8)
+#define EP_PORT_STATUS_SHIFT                   (1)
+#define EP_PORT_STATUS_CHG_BIT                 BIT(0)
+#define EX_EP_PORT_STATUS_SHIFT			(25)
+#define EX_EP_PORT_STATUS_CHG_BIT		BIT(24)
+
+#define SDL_CAM_LEARNT_ENT_MAC2_SHIFT		(20)
+#define SDL_CAM_LEARNT_ENT_MAC1_SHIFT		(8)
+#define SDL_CAM_LEARNT_ENT_MASK			GENMASK(11, 0)
+#define SDL_CAM_FOUND_BIT			BIT(7)
+#define SDL_CAM_READ_KEY_ADDR_SHIFT		(8)
+
+#define HW_ADDR_AGING_TIME_SHIFT		(8)
+#define HW_ADDR_AGING_TIME_MASK			GENMASK(19, 0)
+#define HW_ADDR_AGING_BIT			BIT(2)
+#define HW_ADDR_LEARN_UNTAG_BIT			BIT(1)
+#define HW_ADDR_LEARN_BIT			BIT(0)
+
+#define PORT_VLAN_ID_SHIFT			(16)
+#define PORT_VLAN_ID_MASK			(0xFFF)
+#define PORT_VLAN_IPV_SHIFT			(13)
+#define PORT_VLAN_EN_IPV_SHIFT			(12)
+#define PORT_VLAN_WRITE				(0x3)
+#define PORT_VLAN_READ				(0x1)
+#define PORT_VLAN_WRITE_READ_EN_BIT		BIT(0)
+#define PORT_VLAN_PORT_LIST_VALID_BIT		BIT(3)
+#define PORT_VLAN_HW_ADDR_LEARN_BIT		BIT(9)
+#define PORT_VLAN_HW_ADDR_AGING_BIT		BIT(11)
+#define PORT_VLAN_HW_ADDR_AGING_TIME_SHIFT	(12)
+
+#define NATIVE_MAC1_PCP_SHIFT			(13)
+#define NATIVE_MAC2_VLAN_SHIFT			(16)
+#define NATIVE_MAC2_PCP_SHIFT			(29)
+
+#define DEFAULT_PVID		1
+#define DEFAULT_FWD_ALL		GENMASK(2, 0)
+
+/* Match table for of_platform binding */
+static const struct of_device_id tsnswitch_of_match[] = {
+	{ .compatible = "xlnx,tsn-switch", },
+	{},
+};
+
+MODULE_DEVICE_TABLE(of, tsnswitch_of_match);
+
+static int switch_open(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+static int switch_release(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+/* set_frame_filter_option Frame Filtering Type Field Options */
+static void set_frame_filter_opt(u16 type1, u16 type2)
+{
+	int type = axienet_ior(&lp, XAS_FRM_FLTR_TYPE_FIELD_OPT_OFFSET);
+
+	if (type1)
+		type = (type & 0x0000FFFF) | (type1 << 16);
+	if (type2)
+		type = (type & 0xFFFF0000) | type2;
+	axienet_iow(&lp, XAS_FRM_FLTR_TYPE_FIELD_OPT_OFFSET, type);
+}
+
+/* MAC Port-1 Management Queueing Options */
+static void set_mac1_mngmntq(u32 config)
+{
+	axienet_iow(&lp, XAS_MAC1_MNG_Q_OPTION_OFFSET, config);
+}
+
+/* MAC Port-2 Management Queueing Options */
+static void set_mac2_mngmntq(u32 config)
+{
+	axienet_iow(&lp, XAS_MNG_Q_CTRL_OFFSET, config);
+}
+
+static int set_pmap_config(struct pmap_data pri_info)
+{
+	u32 pmap = 0;
+	u8 pmap_priority_shift = 0;
+	u8 i = 0;
+	u32 reg, err;
+	u16 num_q = lp.num_tc;
+
+	/* wait for switch init done */
+	err = readl_poll_timeout(lp.regs + XAS_STATUS_OFFSET, reg,
+				 (reg & SDL_CAM_WR_ENABLE), 10,
+				  DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("CAM init timed out\n");
+		return -ETIMEDOUT;
+	}
+
+	if (num_q == 3) {
+		/* map pcp's to queues in accordance with device tree */
+		i = 0;
+		while (i < 8) {
+			if (pri_info.st_pcp_reg & (1 << i)) {
+				pmap_priority_shift = 4 * i;
+				pmap = pmap |
+				PMAP_EGRESS_QUEUE2_SELECT << pmap_priority_shift;
+			} else if (pri_info.res_pcp_reg & (1 << i)) {
+				pmap_priority_shift = 4 * i;
+				pmap = pmap |
+				PMAP_EGRESS_QUEUE1_SELECT << pmap_priority_shift;
+			}
+			i = i + 1;
+		}
+	} else {
+		i = 0;
+		while (i < 8) {
+			if (pri_info.st_pcp_reg & (1 << i)) {
+				pmap_priority_shift = 4 * i;
+				pmap = pmap |
+				PMAP_EGRESS_QUEUE1_SELECT << pmap_priority_shift;
+			}
+			i = i + 1;
+		}
+	}
+
+	axienet_iow(&lp, XAS_PMAP_OFFSET, pmap);
+
+	/* wait for cam init done */
+	err = readl_poll_timeout(lp.regs + XAS_SDL_CAM_STATUS_OFFSET, reg,
+				 (reg & SDL_CAM_WR_ENABLE), 10,
+				  DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("CAM init timed out\n");
+		return -ETIMEDOUT;
+	}
+	return 0;
+}
+
+/**
+ * set_switch_regs -  read the various status of switch
+ * @data:	Pointer which will be writen to switch
+ */
+static void set_switch_regs(struct switch_data *data)
+{
+	int tmp;
+	u8 mac_addr[6];
+
+	axienet_iow(&lp, XAS_CONTROL_OFFSET, data->switch_ctrl);
+	axienet_iow(&lp, XAS_PMAP_OFFSET, data->switch_prt);
+	mac_addr[0] = data->sw_mac_addr[0];
+	mac_addr[1] = data->sw_mac_addr[1];
+	mac_addr[2] = data->sw_mac_addr[2];
+	mac_addr[3] = data->sw_mac_addr[3];
+	mac_addr[4] = data->sw_mac_addr[4];
+	mac_addr[5] = data->sw_mac_addr[5];
+	axienet_iow(&lp, XAS_MAC_LSB_OFFSET,
+		    (mac_addr[0] << 24) | (mac_addr[1] << 16) |
+		    (mac_addr[2] << 8)  | (mac_addr[3]));
+	axienet_iow(&lp, XAS_MAC_MSB_OFFSET, (mac_addr[4] << 8) | mac_addr[5]);
+
+	/* Threshold */
+	tmp = (data->thld_ep_mac[0].t1 << 16) | data->thld_ep_mac[0].t2;
+	axienet_iow(&lp, XAS_EP2MAC_ST_FIFOT_OFFSET, tmp);
+
+	tmp = (data->thld_ep_mac[1].t1 << 16) | data->thld_ep_mac[1].t2;
+	axienet_iow(&lp, XAS_EP2MAC_RE_FIFOT_OFFSET, tmp);
+
+	tmp = (data->thld_ep_mac[2].t1 << 16) | data->thld_ep_mac[2].t2;
+	axienet_iow(&lp, XAS_EP2MAC_BE_FIFOT_OFFSET, tmp);
+
+	tmp = (data->thld_mac_mac[0].t1 << 16) | data->thld_mac_mac[0].t2;
+	axienet_iow(&lp, XAS_MAC2MAC_ST_FIFOT_OFFSET, tmp);
+
+	tmp = (data->thld_mac_mac[1].t1 << 16) | data->thld_mac_mac[1].t2;
+	axienet_iow(&lp, XAS_MAC2MAC_RE_FIFOT_OFFSET, tmp);
+
+	tmp = (data->thld_mac_mac[2].t1 << 16) | data->thld_mac_mac[2].t2;
+	axienet_iow(&lp, XAS_MAC2MAC_BE_FIFOT_OFFSET, tmp);
+
+	/* Port VLAN ID */
+	axienet_iow(&lp, XAS_EP_PORT_VLAN_OFFSET, data->ep_vlan);
+	axienet_iow(&lp, XAS_MAC_PORT_VLAN_OFFSET, data->mac_vlan);
+
+	/* max frame size */
+	axienet_iow(&lp, XAS_ST_MAX_FRAME_SIZE_OFFSET, data->max_frame_sc_que);
+	axienet_iow(&lp, XAS_RE_MAX_FRAME_SIZE_OFFSET, data->max_frame_res_que);
+	axienet_iow(&lp, XAS_BE_MAX_FRAME_SIZE_OFFSET, data->max_frame_be_que);
+}
+
+/**
+ * get_switch_regs -  read the various status of switch
+ * @data:	Pointer which will return the switch status
+ */
+static void get_switch_regs(struct switch_data *data)
+{
+	int tmp;
+
+	data->switch_status = axienet_ior(&lp, XAS_STATUS_OFFSET);
+	data->switch_ctrl = axienet_ior(&lp, XAS_CONTROL_OFFSET);
+	data->switch_prt = axienet_ior(&lp, XAS_PMAP_OFFSET);
+	tmp = axienet_ior(&lp, XAS_MAC_LSB_OFFSET);
+	data->sw_mac_addr[0] = (tmp & 0xFF000000) >> 24;
+	data->sw_mac_addr[1] = (tmp & 0xFF0000) >> 16;
+	data->sw_mac_addr[2] = (tmp & 0xFF00) >> 8;
+	data->sw_mac_addr[3] = (tmp & 0xFF);
+	tmp = axienet_ior(&lp, XAS_MAC_MSB_OFFSET);
+	data->sw_mac_addr[4] = (tmp & 0xFF00) >> 8;
+	data->sw_mac_addr[5] = (tmp & 0xFF);
+
+	/* Threshold */
+	tmp = axienet_ior(&lp, XAS_EP2MAC_ST_FIFOT_OFFSET);
+	data->thld_ep_mac[0].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_ep_mac[0].t2 = tmp & (0xFFFF);
+
+	tmp = axienet_ior(&lp, XAS_EP2MAC_RE_FIFOT_OFFSET);
+	data->thld_ep_mac[1].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_ep_mac[1].t2 = tmp & (0xFFFF);
+
+	tmp = axienet_ior(&lp, XAS_EP2MAC_BE_FIFOT_OFFSET);
+	data->thld_ep_mac[2].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_ep_mac[2].t2 = tmp & (0xFFFF);
+
+	tmp = axienet_ior(&lp, XAS_MAC2MAC_ST_FIFOT_OFFSET);
+	data->thld_mac_mac[0].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_mac_mac[0].t2 = tmp & (0xFFFF);
+
+	tmp = axienet_ior(&lp, XAS_MAC2MAC_RE_FIFOT_OFFSET);
+	data->thld_mac_mac[1].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_mac_mac[1].t2 = tmp & (0xFFFF);
+
+	tmp = axienet_ior(&lp, XAS_MAC2MAC_BE_FIFOT_OFFSET);
+	data->thld_mac_mac[2].t1 = ((tmp >> 16) & 0xFFFF);
+	data->thld_mac_mac[2].t2 = tmp & (0xFFFF);
+
+	/* Port VLAN ID */
+	data->ep_vlan = axienet_ior(&lp, XAS_EP_PORT_VLAN_OFFSET);
+	data->mac_vlan = axienet_ior(&lp, XAS_MAC_PORT_VLAN_OFFSET);
+
+	/* max frame size */
+	data->max_frame_sc_que = (axienet_ior(&lp,
+				XAS_ST_MAX_FRAME_SIZE_OFFSET) & 0xFFFF);
+	data->max_frame_res_que = (axienet_ior(&lp,
+				XAS_RE_MAX_FRAME_SIZE_OFFSET) & 0xFFFF);
+	data->max_frame_be_que = (axienet_ior(&lp,
+				XAS_BE_MAX_FRAME_SIZE_OFFSET) & 0xFFFF);
+
+	/* frame filter type options*/
+	tmp = axienet_ior(&lp, XAS_FRM_FLTR_TYPE_FIELD_OPT_OFFSET);
+	data->typefield.type2 = (tmp & 0xFFFF0000) >> 16;
+	data->typefield.type2 = tmp & 0x0000FFFF;
+
+	/* MAC Port 1 Management Q option*/
+	data->mac1_config = axienet_ior(&lp, XAS_MAC1_MNG_Q_OPTION_OFFSET);
+	/* MAC Port 2 Management Q option*/
+	data->mac2_config = axienet_ior(&lp, XAS_MNG_Q_CTRL_OFFSET);
+
+	/* Port VLAN Membership control*/
+	data->port_vlan_mem_ctrl = axienet_ior(&lp, XAS_VLAN_MEMB_CTRL_REG);
+	/* Port VLAN Membership read data*/
+	data->port_vlan_mem_data = axienet_ior(&lp, XAS_VLAN_MEMB_DATA_REG);
+}
+
+int tsn_switch_get_port_parent_id(struct net_device *dev, struct netdev_phys_item_id *ppid)
+{
+	u8 *switchid;
+
+	switchid = tsn_switch_get_id();
+	ppid->id_len = ETH_ALEN;
+	memcpy(&ppid->id, switchid, ppid->id_len);
+
+	return 0;
+}
+
+/**
+ * get_memory_static_counter -  get memory static counters value
+ * @data:	Value to be programmed
+ */
+static void get_memory_static_counter(struct switch_data *data)
+{
+	data->mem_arr_cnt.cam_lookup.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_CAM_LOOKUP);
+	data->mem_arr_cnt.cam_lookup.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_CAM_LOOKUP + 0x4);
+
+	data->mem_arr_cnt.multicast_fr.lsb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_MULTCAST);
+	data->mem_arr_cnt.multicast_fr.msb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_MULTCAST + 0x4);
+
+	data->mem_arr_cnt.err_mac1.lsb = axienet_ior(&lp,
+						     XAS_MEM_STCNTR_ERR_MAC1);
+	data->mem_arr_cnt.err_mac1.msb = axienet_ior(&lp,
+						     XAS_MEM_STCNTR_ERR_MAC1 + 0x4);
+
+	data->mem_arr_cnt.err_mac2.lsb = axienet_ior(&lp,
+						     XAS_MEM_STCNTR_ERR_MAC2);
+	data->mem_arr_cnt.err_mac2.msb = axienet_ior(&lp,
+						     XAS_MEM_STCNTR_ERR_MAC2 + 0x4);
+
+	data->mem_arr_cnt.sc_mac1_ep.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_SC_MAC1_EP);
+	data->mem_arr_cnt.sc_mac1_ep.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_SC_MAC1_EP + 0x4);
+	data->mem_arr_cnt.res_mac1_ep.lsb = axienet_ior(&lp,
+							XAS_MEM_STCNTR_RES_MAC1_EP);
+	data->mem_arr_cnt.res_mac1_ep.msb = axienet_ior(&lp,
+							XAS_MEM_STCNTR_RES_MAC1_EP + 0x4);
+	data->mem_arr_cnt.be_mac1_ep.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_BE_MAC1_EP);
+	data->mem_arr_cnt.be_mac1_ep.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_BE_MAC1_EP + 0x4);
+	data->mem_arr_cnt.err_sc_mac1_ep.lsb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_SC_MAC1_EP);
+	data->mem_arr_cnt.err_sc_mac1_ep.msb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_SC_MAC1_EP + 0x4);
+	data->mem_arr_cnt.err_res_mac1_ep.lsb = axienet_ior(&lp,
+							    XAS_MEM_STCNTR_ERR_RES_MAC1_EP);
+	data->mem_arr_cnt.err_res_mac1_ep.msb = axienet_ior(&lp,
+							    XAS_MEM_STCNTR_ERR_RES_MAC1_EP + 0x4);
+	data->mem_arr_cnt.err_be_mac1_ep.lsb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_BE_MAC1_EP);
+	data->mem_arr_cnt.err_be_mac1_ep.msb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_BE_MAC1_EP + 0x4);
+
+	data->mem_arr_cnt.sc_mac2_ep.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_SC_MAC2_EP);
+	data->mem_arr_cnt.sc_mac2_ep.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_SC_MAC2_EP + 0x4);
+	data->mem_arr_cnt.res_mac2_ep.lsb = axienet_ior(&lp,
+							XAS_MEM_STCNTR_RES_MAC2_EP);
+	data->mem_arr_cnt.res_mac2_ep.msb = axienet_ior(&lp,
+							XAS_MEM_STCNTR_RES_MAC2_EP + 0x4);
+	data->mem_arr_cnt.be_mac2_ep.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_BE_MAC2_EP);
+	data->mem_arr_cnt.be_mac2_ep.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_BE_MAC2_EP + 0x4);
+	data->mem_arr_cnt.err_sc_mac2_ep.lsb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_SC_MAC2_EP);
+	data->mem_arr_cnt.err_sc_mac2_ep.msb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_SC_MAC2_EP + 0x4);
+	data->mem_arr_cnt.err_res_mac2_ep.lsb = axienet_ior(&lp,
+							    XAS_MEM_STCNTR_ERR_RES_MAC2_EP);
+	data->mem_arr_cnt.err_res_mac2_ep.msb = axienet_ior(&lp,
+							    XAS_MEM_STCNTR_ERR_RES_MAC2_EP + 0x4);
+	data->mem_arr_cnt.err_be_mac2_ep.lsb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_BE_MAC2_EP);
+	data->mem_arr_cnt.err_be_mac2_ep.msb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_BE_MAC2_EP + 0x4);
+
+	data->mem_arr_cnt.sc_ep_mac1.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_SC_EP_MAC1);
+	data->mem_arr_cnt.sc_ep_mac1.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_SC_EP_MAC1 + 0x4);
+	data->mem_arr_cnt.res_ep_mac1.lsb = axienet_ior(&lp,
+							XAS_MEM_STCNTR_RES_EP_MAC1);
+	data->mem_arr_cnt.res_ep_mac1.msb = axienet_ior(&lp,
+							XAS_MEM_STCNTR_RES_EP_MAC1 + 0x4);
+	data->mem_arr_cnt.be_ep_mac1.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_BE_EP_MAC1);
+	data->mem_arr_cnt.be_ep_mac1.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_BE_EP_MAC1 + 0x4);
+	data->mem_arr_cnt.err_sc_ep_mac1.lsb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_SC_EP_MAC1);
+	data->mem_arr_cnt.err_sc_ep_mac1.msb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_SC_EP_MAC1 + 0x4);
+	data->mem_arr_cnt.err_res_ep_mac1.lsb = axienet_ior(&lp,
+							    XAS_MEM_STCNTR_ERR_RES_EP_MAC1);
+	data->mem_arr_cnt.err_res_ep_mac1.msb = axienet_ior(&lp,
+							    XAS_MEM_STCNTR_ERR_RES_EP_MAC1 + 0x4);
+	data->mem_arr_cnt.err_be_ep_mac1.lsb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_BE_EP_MAC1);
+	data->mem_arr_cnt.err_be_ep_mac1.msb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_BE_EP_MAC1 + 0x4);
+
+	data->mem_arr_cnt.sc_mac2_mac1.lsb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_SC_MAC2_MAC1);
+	data->mem_arr_cnt.sc_mac2_mac1.msb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_SC_MAC2_MAC1 + 0x4);
+	data->mem_arr_cnt.res_mac2_mac1.lsb = axienet_ior(&lp,
+							  XAS_MEM_STCNTR_RES_MAC2_MAC1);
+	data->mem_arr_cnt.res_mac2_mac1.msb = axienet_ior(&lp,
+							  XAS_MEM_STCNTR_RES_MAC2_MAC1 + 0x4);
+	data->mem_arr_cnt.be_mac2_mac1.lsb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_BE_MAC2_MAC1);
+	data->mem_arr_cnt.be_mac2_mac1.msb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_BE_MAC2_MAC1 + 0x4);
+	data->mem_arr_cnt.err_sc_mac2_mac1.lsb = axienet_ior(&lp,
+							     XAS_MEM_STCNTR_ERR_SC_MAC2_MAC1);
+	data->mem_arr_cnt.err_sc_mac2_mac1.msb = axienet_ior(&lp,
+							     XAS_MEM_STCNTR_ERR_SC_MAC2_MAC1 + 0x4);
+	data->mem_arr_cnt.err_res_mac2_mac1.lsb = axienet_ior(&lp,
+							      XAS_MEM_STCNTR_ERR_RES_MAC2_MAC1);
+	data->mem_arr_cnt.err_res_mac2_mac1.msb =
+	axienet_ior(&lp, XAS_MEM_STCNTR_ERR_RES_MAC2_MAC1 + 0x4);
+	data->mem_arr_cnt.err_be_mac2_mac1.lsb = axienet_ior(&lp,
+							     XAS_MEM_STCNTR_ERR_BE_MAC2_MAC1);
+	data->mem_arr_cnt.err_be_mac2_mac1.msb = axienet_ior(&lp,
+							     XAS_MEM_STCNTR_ERR_BE_MAC2_MAC1 + 0x4);
+
+	data->mem_arr_cnt.sc_ep_mac2.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_SC_EP_MAC2);
+	data->mem_arr_cnt.sc_ep_mac2.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_SC_EP_MAC2 + 0x4);
+	data->mem_arr_cnt.res_ep_mac2.lsb = axienet_ior(&lp,
+							XAS_MEM_STCNTR_RES_EP_MAC2);
+	data->mem_arr_cnt.res_ep_mac2.msb = axienet_ior(&lp,
+							XAS_MEM_STCNTR_RES_EP_MAC2 + 0x4);
+	data->mem_arr_cnt.be_ep_mac2.lsb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_BE_EP_MAC2);
+	data->mem_arr_cnt.be_ep_mac2.msb = axienet_ior(&lp,
+						       XAS_MEM_STCNTR_BE_EP_MAC2 + 0x4);
+	data->mem_arr_cnt.err_sc_ep_mac2.lsb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_SC_EP_MAC2);
+	data->mem_arr_cnt.err_sc_ep_mac2.msb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_SC_EP_MAC2 + 0x4);
+	data->mem_arr_cnt.err_res_ep_mac2.lsb = axienet_ior(&lp,
+							    XAS_MEM_STCNTR_ERR_RES_EP_MAC2);
+	data->mem_arr_cnt.err_res_ep_mac2.msb = axienet_ior(&lp,
+							    XAS_MEM_STCNTR_ERR_RES_EP_MAC2 + 0x4);
+	data->mem_arr_cnt.err_be_ep_mac2.lsb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_BE_EP_MAC2);
+	data->mem_arr_cnt.err_be_ep_mac2.msb = axienet_ior(&lp,
+							   XAS_MEM_STCNTR_ERR_BE_EP_MAC2 + 0x4);
+
+	data->mem_arr_cnt.sc_mac1_mac2.lsb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_SC_MAC1_MAC2);
+	data->mem_arr_cnt.sc_mac1_mac2.msb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_SC_MAC1_MAC2 + 0x4);
+	data->mem_arr_cnt.res_mac1_mac2.lsb = axienet_ior(&lp,
+							  XAS_MEM_STCNTR_RES_MAC1_MAC2);
+	data->mem_arr_cnt.res_mac1_mac2.msb = axienet_ior(&lp,
+							  XAS_MEM_STCNTR_RES_MAC1_MAC2 + 0x4);
+	data->mem_arr_cnt.be_mac1_mac2.lsb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_BE_MAC1_MAC2);
+	data->mem_arr_cnt.be_mac1_mac2.msb = axienet_ior(&lp,
+							 XAS_MEM_STCNTR_BE_MAC1_MAC2 + 0x4);
+	data->mem_arr_cnt.err_sc_mac1_mac2.lsb = axienet_ior(&lp,
+							     XAS_MEM_STCNTR_ERR_SC_MAC1_MAC2);
+	data->mem_arr_cnt.err_sc_mac1_mac2.msb = axienet_ior(&lp,
+							     XAS_MEM_STCNTR_ERR_SC_MAC1_MAC2 + 0x4);
+	data->mem_arr_cnt.err_res_mac1_mac2.lsb = axienet_ior(&lp,
+							      XAS_MEM_STCNTR_ERR_RES_MAC1_MAC2);
+	data->mem_arr_cnt.err_res_mac1_mac2.msb =
+	axienet_ior(&lp, XAS_MEM_STCNTR_ERR_RES_MAC1_MAC2 + 0x4);
+	data->mem_arr_cnt.err_be_mac1_mac2.lsb = axienet_ior(&lp,
+							     XAS_MEM_STCNTR_ERR_BE_MAC1_MAC2);
+	data->mem_arr_cnt.err_be_mac1_mac2.msb = axienet_ior(&lp,
+							     XAS_MEM_STCNTR_ERR_BE_MAC1_MAC2 + 0x4);
+}
+
+int tsn_switch_cam_set(struct cam_struct data, u8 add)
+{
+	u32 port_action = 0;
+	u32 tv2 = 0;
+	u32 reg, err;
+	u8 en_ipv = 0;
+
+	err = readl_poll_timeout(lp.regs + XAS_SDL_CAM_STATUS_OFFSET, reg,
+				 (reg & SDL_CAM_WR_ENABLE), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("CAM init timed out\n");
+		return -ETIMEDOUT;
+	}
+	if (add && ((data.fwd_port & PORT_EX_ONLY) || (data.fwd_port & PORT_EX_EP))) {
+		if (!(ep_lp->ex_ep)) {
+			pr_err("Endpoint extension support is not present in this design\n");
+			return -EINVAL;
+		} else if ((data.fwd_port & PORT_EX_ONLY) &&
+			    (data.fwd_port & PORT_EX_EP)) {
+			if (!(ep_lp->packet_switch)) {
+				pr_err("Support for forwarding packets from endpoint to extended endpoint or vice versa is not present in this design\n");
+				return -EINVAL;
+			}
+		}
+	}
+	/* mac and vlan */
+	axienet_iow(&lp, XAS_SDL_CAM_KEY1_OFFSET,
+		    (data.dest_addr[0] << 24) | (data.dest_addr[1] << 16) |
+		    (data.dest_addr[2] << 8)  | (data.dest_addr[3]));
+	axienet_iow(&lp, XAS_SDL_CAM_KEY2_OFFSET,
+		    ((data.dest_addr[4] << 8) | data.dest_addr[5]) |
+		    ((data.vlanid & SDL_CAM_VLAN_MASK) << SDL_CAM_VLAN_SHIFT));
+
+	/* Introduce wmb to preserve KEY2 and TV1 write order fix possible
+	 * HW hang when KEY2 and TV1 registers are accessed sequentially.
+	 * TODO: Check alternatives to barrier.
+	 */
+	wmb();
+	/* TV 1 and TV 2 */
+	axienet_iow(&lp, XAS_SDL_CAM_TV1_OFFSET,
+		    (data.src_addr[0] << 24) | (data.src_addr[1] << 16) |
+		    (data.src_addr[2] << 8)  | (data.src_addr[3]));
+
+	tv2 = ((data.src_addr[4] << 8) | data.src_addr[5]) |
+	       ((data.tv_vlanid & SDL_CAM_VLAN_MASK) << SDL_CAM_VLAN_SHIFT);
+
+	if (data.flags & XAS_CAM_IPV_EN)
+		en_ipv = 1;
+
+	tv2 = tv2 | ((data.ipv & SDL_CAM_IPV_MASK) << SDL_CAM_IPV_SHIFT)
+				| (en_ipv << SDL_EN_CAM_IPV_SHIFT);
+
+	axienet_iow(&lp, XAS_SDL_CAM_TV2_OFFSET, tv2);
+	/* Force complete write to translation value registers
+	 * before writing to port action register
+	 * TODO: Check alternatives to barrier.
+	 */
+	wmb();
+
+	if (data.fwd_port & PORT_EP)
+		port_action = data.ep_port_act << SDL_CAM_EP_ACTION_LIST_SHIFT;
+	if (data.fwd_port & PORT_MAC1 || data.fwd_port & PORT_MAC2)
+		port_action |= data.mac_port_act <<
+				SDL_CAM_MAC_ACTION_LIST_SHIFT;
+
+	if (data.flags & XAS_CAM_EP_MGMTQ_EN)
+		port_action |= SDL_CAM_EP_MGMTQ_EN;
+
+	port_action = port_action | (data.fwd_port << SDL_CAM_PORT_LIST_SHIFT);
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_QCI) || IS_ENABLED(CONFIG_XILINX_TSN_CB)
+	port_action = port_action | (data.gate_id << SDL_GATEID_SHIFT);
+#endif
+
+	/* port action */
+	axienet_iow(&lp, XAS_SDL_CAM_PORT_ACT_OFFSET, port_action);
+	/* Force complete writes to port action register before initiating
+	 * CAM write by setting CAM ADD bit in control register since this value
+	 * needs to be written to the CAM.
+	 * TODO: Check alternatives to barrier.
+	 */
+	wmb();
+
+	if (add)
+		axienet_iow(&lp, XAS_SDL_CAM_CTRL_OFFSET, SDL_CAM_ADD_ENTRY);
+	else
+		axienet_iow(&lp, XAS_SDL_CAM_CTRL_OFFSET, SDL_CAM_DELETE_ENTRY);
+
+	/* wait for write to complete */
+	err = readl_poll_timeout(lp.regs + XAS_SDL_CAM_CTRL_OFFSET, reg,
+				 (!(reg & SDL_CAM_WR_ENABLE)), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("CAM write timed out\n");
+		return -ETIMEDOUT;
+	}
+
+	return 0;
+}
+
+static void port_vlan_mem_ctrl(u32 port_vlan_mem)
+{
+		axienet_iow(&lp, XAS_VLAN_MEMB_CTRL_REG, port_vlan_mem);
+}
+
+static int read_cam_entry(struct cam_struct data, void __user *arg)
+{
+	u32 u_value, reg, err;
+
+	/* wait for cam init done */
+	err = readl_poll_timeout(lp.regs + XAS_SDL_CAM_STATUS_OFFSET, reg,
+				 (reg & SDL_CAM_WR_ENABLE), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("CAM init timed out\n");
+		return -ETIMEDOUT;
+	}
+
+	/* mac and vlan */
+	axienet_iow(&lp, XAS_SDL_CAM_KEY1_OFFSET,
+		    (data.dest_addr[0] << 24) | (data.dest_addr[1] << 16) |
+		    (data.dest_addr[2] << 8)  | (data.dest_addr[3]));
+	axienet_iow(&lp, XAS_SDL_CAM_KEY2_OFFSET,
+		    ((data.dest_addr[4] << 8) | data.dest_addr[5]) |
+		    ((data.vlanid & SDL_CAM_VLAN_MASK) << SDL_CAM_VLAN_SHIFT));
+	/* Finish writing vlan id and mac address before triggering a read
+	 * from the CAM since read depends on these parameters
+	 * TODO: Check alternatives to barrier.
+	 */
+	wmb();
+	axienet_iow(&lp, XAS_SDL_CAM_CTRL_OFFSET, SDL_CAM_READ_ENTRY);
+	err = readl_poll_timeout(lp.regs + XAS_SDL_CAM_CTRL_OFFSET, reg,
+				 (!(reg & SDL_CAM_WR_ENABLE)), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("CAM write timed out\n");
+		return -ETIMEDOUT;
+	}
+	u_value = axienet_ior(&lp, XAS_SDL_CAM_CTRL_OFFSET);
+
+	if (u_value & SDL_CAM_FOUND_BIT) {
+		u_value = axienet_ior(&lp, XAS_SDL_CAM_TV1_OFFSET);
+		data.src_addr[0] = (u_value >> 24) & 0xFF;
+		data.src_addr[1] = (u_value >> 16) & 0xFF;
+		data.src_addr[2] = (u_value >> 8) & 0xFF;
+		data.src_addr[3] = (u_value) & 0xFF;
+		u_value = axienet_ior(&lp, XAS_SDL_CAM_TV2_OFFSET);
+		data.src_addr[4] = (u_value >> 8) & 0xFF;
+		data.src_addr[5] = (u_value) & 0xFF;
+		data.tv_vlanid   = (u_value >> SDL_CAM_VLAN_SHIFT)
+					& SDL_CAM_VLAN_MASK;
+		data.ipv = (u_value >> SDL_CAM_IPV_SHIFT) & SDL_CAM_IPV_MASK;
+		if ((u_value >> SDL_EN_CAM_IPV_SHIFT) & 0x1)
+			data.flags |= XAS_CAM_IPV_EN;
+		u_value = axienet_ior(&lp, XAS_SDL_CAM_PORT_ACT_OFFSET);
+		if (ep_lp->ex_ep)
+			data.fwd_port = (u_value >> SDL_CAM_PORT_LIST_SHIFT) & 0x1F;
+		else
+			data.fwd_port = (u_value >> SDL_CAM_PORT_LIST_SHIFT) & 0x7;
+		data.ep_port_act = (u_value >> SDL_CAM_EP_ACTION_LIST_SHIFT)
+					& 0xF;
+		data.mac_port_act = (u_value >> SDL_CAM_MAC_ACTION_LIST_SHIFT)
+					& 0xF;
+		data.gate_id = (u_value >> SDL_GATEID_SHIFT) & 0xFF;
+		data.flags |= XAS_CAM_VALID;
+	} else {
+		data.flags &= ~XAS_CAM_VALID;
+	}
+	if (copy_to_user(arg, &data, sizeof(struct cam_struct)))
+		return -EFAULT;
+
+	return 0;
+}
+
+static int set_mac_addr_learn(void __user *arg)
+{
+	struct mac_addr_learn mac_learn;
+	u32 u_value;
+
+	if (copy_from_user(&mac_learn, arg, sizeof(struct mac_addr_learn)))
+		return -EFAULT;
+
+	u_value = axienet_ior(&lp, XAS_HW_ADDR_LEARN_CTRL_OFFSET);
+	if (mac_learn.aging_time) {
+		u_value &= ~(HW_ADDR_AGING_TIME_MASK <<
+				HW_ADDR_AGING_TIME_SHIFT);
+		u_value |= (mac_learn.aging_time << HW_ADDR_AGING_TIME_SHIFT);
+	}
+	if (mac_learn.is_age) {
+		if (!mac_learn.aging)
+			u_value |= HW_ADDR_AGING_BIT;
+		else
+			u_value &= ~HW_ADDR_AGING_BIT;
+	}
+	if (mac_learn.is_learn) {
+		if (!mac_learn.learning)
+			u_value |= HW_ADDR_LEARN_BIT;
+		else
+			u_value &= ~HW_ADDR_LEARN_BIT;
+	}
+	if (mac_learn.is_untag) {
+		if (mac_learn.learn_untag)
+			u_value |= HW_ADDR_LEARN_UNTAG_BIT;
+		else
+			u_value &= ~HW_ADDR_LEARN_UNTAG_BIT;
+	}
+	axienet_iow(&lp, XAS_HW_ADDR_LEARN_CTRL_OFFSET, u_value);
+
+	return 0;
+}
+
+static int get_mac_addr_learn(void __user *arg)
+{
+	struct mac_addr_learn mac_learn;
+	u32 u_value;
+
+	u_value = axienet_ior(&lp, XAS_HW_ADDR_LEARN_CTRL_OFFSET);
+	mac_learn.aging_time = (u_value >> HW_ADDR_AGING_TIME_SHIFT) &
+				HW_ADDR_AGING_TIME_MASK;
+	mac_learn.aging = u_value & HW_ADDR_AGING_BIT;
+	mac_learn.learning = u_value & HW_ADDR_LEARN_BIT;
+	mac_learn.learn_untag = u_value & HW_ADDR_LEARN_UNTAG_BIT;
+
+	u_value = axienet_ior(&lp, XAS_PORT_STATE_CTRL_OFFSET);
+
+	if (copy_to_user(arg, &mac_learn, sizeof(struct mac_addr_learn)))
+		return -EFAULT;
+
+	return 0;
+}
+
+static int get_mac_addr_learnt_list(void __user *arg)
+{
+	struct mac_addr_list *mac_list;
+	u32 i = 0;
+	u32 u_value, reg, err;
+	u16 read_key_addr = 0;
+	int ret = 0;
+
+	mac_list = kzalloc(sizeof(*mac_list), GFP_KERNEL);
+	if (!mac_list) {
+		ret = -ENOMEM;
+		goto ret_status;
+	}
+
+	if (copy_from_user(mac_list, arg, sizeof(u8))) {
+		ret = -EFAULT;
+		goto free_mac_list;
+	}
+	/* wait for cam init done */
+	err = readl_poll_timeout(lp.regs + XAS_SDL_CAM_STATUS_OFFSET, reg,
+				 (reg & SDL_CAM_WR_ENABLE), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("CAM init timed out\n");
+		ret = -ETIMEDOUT;
+		goto free_mac_list;
+	}
+	u_value = axienet_ior(&lp, XAS_SDL_CAM_STATUS_OFFSET);
+
+	if (mac_list->port_num == PORT_MAC1) {
+		mac_list->num_list = (u_value >> SDL_CAM_LEARNT_ENT_MAC1_SHIFT)
+					& SDL_CAM_LEARNT_ENT_MASK;
+	} else {
+		mac_list->num_list = (u_value >> SDL_CAM_LEARNT_ENT_MAC2_SHIFT)
+					& SDL_CAM_LEARNT_ENT_MASK;
+		read_key_addr = 0x800;
+	}
+
+	for (i = 0; i < MAX_NUM_MAC_ENTRIES ; i++) {
+		err = readl_poll_timeout(lp.regs + XAS_SDL_CAM_STATUS_OFFSET, reg,
+					 (reg & SDL_CAM_WR_ENABLE), 10,
+					 DELAY_OF_FIVE_MILLISEC);
+		if (err) {
+			pr_err("CAM init timed out\n");
+			ret = -ETIMEDOUT;
+			goto free_mac_list;
+		}
+
+		u_value = ((read_key_addr + i) << SDL_CAM_READ_KEY_ADDR_SHIFT)
+			  | SDL_CAM_READ_KEY_ENTRY;
+		axienet_iow(&lp, XAS_SDL_CAM_CTRL_OFFSET, u_value);
+
+		err = readl_poll_timeout(lp.regs + XAS_SDL_CAM_CTRL_OFFSET, reg,
+					 (!(reg & SDL_CAM_WR_ENABLE)), 10,
+					 DELAY_OF_FIVE_MILLISEC);
+		if (err) {
+			pr_err("CAM write timed out\n");
+			ret = -ETIMEDOUT;
+			goto free_mac_list;
+		}
+		u_value = axienet_ior(&lp, XAS_SDL_CAM_CTRL_OFFSET);
+
+		if (u_value & SDL_CAM_FOUND_BIT) {
+			u_value = axienet_ior(&lp, XAS_SDL_CAM_KEY1_OFFSET);
+			mac_list->list[i].mac_addr[0] = (u_value >> 24) & 0xFF;
+			mac_list->list[i].mac_addr[1] = (u_value >> 16) & 0xFF;
+			mac_list->list[i].mac_addr[2] = (u_value >> 8) & 0xFF;
+			mac_list->list[i].mac_addr[3] = (u_value) & 0xFF;
+			u_value = axienet_ior(&lp, XAS_SDL_CAM_KEY2_OFFSET);
+			mac_list->list[i].mac_addr[4] = (u_value >> 8) & 0xFF;
+			mac_list->list[i].mac_addr[5] = (u_value) & 0xFF;
+			mac_list->list[i].vlan_id     = (u_value >> 16) & 0xFFF;
+		}
+	}
+	if (copy_to_user(arg, mac_list, sizeof(struct mac_addr_list))) {
+		ret = -EFAULT;
+		goto free_mac_list;
+	}
+
+free_mac_list:
+	kfree(mac_list);
+ret_status:
+	return ret;
+}
+
+int tsn_switch_set_stp_state(struct port_status *port)
+{
+	u32 u_value, reg, err;
+	u32 en_port_sts_chg_bit = 1;
+
+	u_value = axienet_ior(&lp, XAS_PORT_STATE_CTRL_OFFSET);
+	switch (port->port_num) {
+	case PORT_EP:
+		if (!(u_value & EP_PORT_STATUS_CHG_BIT)) {
+			u_value &= ~(PORT_STATUS_MASK <<
+					EP_PORT_STATUS_SHIFT);
+			u_value |= (port->port_status << EP_PORT_STATUS_SHIFT);
+			en_port_sts_chg_bit = EP_PORT_STATUS_CHG_BIT;
+		}
+		break;
+	case PORT_MAC1:
+		if (!(u_value & MAC1_PORT_STATUS_CHG_BIT)) {
+			u_value &= ~(PORT_STATUS_MASK <<
+					MAC1_PORT_STATUS_SHIFT);
+			u_value |= (port->port_status <<
+					MAC1_PORT_STATUS_SHIFT);
+			en_port_sts_chg_bit = MAC1_PORT_STATUS_CHG_BIT;
+		}
+		break;
+	case PORT_MAC2:
+		if (!(u_value & MAC2_PORT_STATUS_CHG_BIT)) {
+			u_value &= ~(PORT_STATUS_MASK <<
+					MAC2_PORT_STATUS_SHIFT);
+			u_value |= (port->port_status <<
+					MAC2_PORT_STATUS_SHIFT);
+			en_port_sts_chg_bit = MAC2_PORT_STATUS_CHG_BIT;
+		}
+		break;
+	case PORT_EX_ONLY:
+		if (!(ep_lp->ex_ep))
+			return -EINVAL;
+		if (!(u_value & EX_EP_PORT_STATUS_CHG_BIT)) {
+			u_value &= ~(PORT_STATUS_MASK << EX_EP_PORT_STATUS_SHIFT);
+			u_value |= (port->port_status << EX_EP_PORT_STATUS_SHIFT);
+			en_port_sts_chg_bit = EX_EP_PORT_STATUS_CHG_BIT;
+		}
+		break;
+	}
+
+	u_value |= en_port_sts_chg_bit;
+	axienet_iow(&lp, XAS_PORT_STATE_CTRL_OFFSET, u_value);
+
+	/* wait for write to complete */
+	err = readl_poll_timeout(lp.regs + XAS_PORT_STATE_CTRL_OFFSET, reg,
+				 (!(reg & en_port_sts_chg_bit)), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("CAM write timed out\n");
+		return -ETIMEDOUT;
+	}
+	u_value = axienet_ior(&lp, XAS_PORT_STATE_CTRL_OFFSET);
+
+	return 0;
+}
+
+static int set_port_status(void __user *arg)
+{
+	struct port_status port;
+
+	if (copy_from_user(&port, arg, sizeof(struct port_status)))
+		return -EFAULT;
+
+	return tsn_switch_set_stp_state(&port);
+}
+
+static int get_port_status(void __user *arg)
+{
+	struct port_status port;
+	u32 u_value;
+
+	if (copy_from_user(&port, arg, sizeof(struct port_status)))
+		return -EINVAL;
+
+	u_value = axienet_ior(&lp, XAS_PORT_STATE_CTRL_OFFSET);
+	switch (port.port_num) {
+	case PORT_EP:
+		port.port_status = (u_value >> EP_PORT_STATUS_SHIFT) &
+					PORT_STATUS_MASK;
+		break;
+	case PORT_MAC1:
+		port.port_status = (u_value >> MAC1_PORT_STATUS_SHIFT) &
+					PORT_STATUS_MASK;
+		break;
+	case PORT_MAC2:
+		port.port_status = (u_value >> MAC2_PORT_STATUS_SHIFT) &
+					PORT_STATUS_MASK;
+		break;
+	case PORT_EX_ONLY:
+		if (!(ep_lp->ex_ep))
+			return -EINVAL;
+		port.port_status = (u_value >> EX_EP_PORT_STATUS_SHIFT) & PORT_STATUS_MASK;
+		break;
+	}
+
+	if (copy_to_user(arg, &port, sizeof(struct port_status)))
+		return -EINVAL;
+
+	return 0;
+}
+
+u8 *tsn_switch_get_id(void)
+{
+	return sw_mac_addr;
+}
+
+int tsn_switch_vlan_add(struct port_vlan *port, int add)
+{
+	u32 u_value, u_value1, reg, err;
+	u8 learning = 0;
+
+	u_value1 = axienet_ior(&lp, XAS_VLAN_MEMB_DATA_REG);
+	learning = (u_value1 & PORT_VLAN_HW_ADDR_LEARN_BIT) ? 0 : 1;
+	if (learning) {
+		if (port->en_ipv == 1) {
+			pr_err("When hardware address learning is enabled for a VLAN, TSN streams cannot be mapped to a particular priority queue\n");
+			return -EPERM;
+		}
+	} else {
+		if (port->en_port_status == 1) {
+			pr_err("When hardware address learning is disabled for a VLAN, port status cannot be set per VLAN\n");
+			return -EPERM;
+		}
+	}
+	u_value = ((port->vlan_id & PORT_VLAN_ID_MASK) << PORT_VLAN_ID_SHIFT)
+			| PORT_VLAN_READ;
+	axienet_iow(&lp, XAS_VLAN_MEMB_CTRL_REG, u_value);
+
+	/* wait for port vlan write completion */
+	err = readl_poll_timeout(lp.regs + XAS_VLAN_MEMB_CTRL_REG, reg,
+				 (!(reg & PORT_VLAN_WRITE_READ_EN_BIT)), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("Port vlan write timed out\n");
+		return -ETIMEDOUT;
+	}
+
+	if (add)
+		u_value1 |= PORT_VLAN_PORT_LIST_VALID_BIT | port->port_num;
+	else
+		u_value1 &= ~(port->port_num);
+	axienet_iow(&lp, XAS_VLAN_MEMB_DATA_REG, u_value1);
+
+	u_value = ((port->vlan_id & PORT_VLAN_ID_MASK) << PORT_VLAN_ID_SHIFT)
+			| PORT_VLAN_WRITE;
+	if (port->en_port_status == 1) {
+		u_value |= ((port->port_status & SDL_CAM_IPV_MASK) << PORT_VLAN_IPV_SHIFT)
+				| (port->en_port_status << PORT_VLAN_EN_IPV_SHIFT);
+	}
+	if (port->en_ipv == 1) {
+		u_value |= ((port->ipv & SDL_CAM_IPV_MASK) << PORT_VLAN_IPV_SHIFT)
+				| (port->en_ipv << PORT_VLAN_EN_IPV_SHIFT);
+	}
+	axienet_iow(&lp, XAS_VLAN_MEMB_CTRL_REG, u_value);
+
+	/* wait for port vlan write completion */
+	err = readl_poll_timeout(lp.regs + XAS_VLAN_MEMB_CTRL_REG, reg,
+				 (!(reg & PORT_VLAN_WRITE_READ_EN_BIT)), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("Port vlan write timed out\n");
+		return -ETIMEDOUT;
+	}
+
+	return 0;
+}
+
+static int add_del_port_vlan(void __user *arg, u8 add)
+{
+	struct port_vlan port;
+
+	if (copy_from_user(&port, arg, sizeof(struct port_vlan)))
+		return -EFAULT;
+
+	return tsn_switch_vlan_add(&port, add);
+}
+
+static int set_vlan_mac_addr_learn(void __user *arg)
+{
+	struct port_vlan port;
+	u32 u_value, u_value1, reg, err;
+
+	if (copy_from_user(&port, arg, sizeof(struct port_vlan)))
+		return -EFAULT;
+
+	u_value = ((port.vlan_id & PORT_VLAN_ID_MASK) << PORT_VLAN_ID_SHIFT)
+			| PORT_VLAN_READ;
+	axienet_iow(&lp, XAS_VLAN_MEMB_CTRL_REG, u_value);
+
+	/* wait for port vlan write completion */
+	err = readl_poll_timeout(lp.regs + XAS_VLAN_MEMB_CTRL_REG, reg,
+				 (!(reg & PORT_VLAN_WRITE_READ_EN_BIT)), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("Port vlan write timed out\n");
+		return -ETIMEDOUT;
+	}
+	u_value = axienet_ior(&lp, XAS_VLAN_MEMB_CTRL_REG);
+
+	u_value1 = axienet_ior(&lp, XAS_VLAN_MEMB_DATA_REG);
+
+	if (port.aging_time) {
+		u_value1 &= ~(HW_ADDR_AGING_TIME_MASK <<
+				PORT_VLAN_HW_ADDR_AGING_TIME_SHIFT);
+		u_value1 |= (port.aging_time <<
+				PORT_VLAN_HW_ADDR_AGING_TIME_SHIFT);
+	}
+	if (port.is_age) {
+		if (port.aging)
+			u_value1 |= PORT_VLAN_HW_ADDR_AGING_BIT;
+		else
+			u_value1 &= ~PORT_VLAN_HW_ADDR_AGING_BIT;
+	}
+	if (port.is_learn) {
+		if (port.learning)
+			u_value1 &= ~PORT_VLAN_HW_ADDR_LEARN_BIT;
+		else
+			u_value1 |= PORT_VLAN_HW_ADDR_LEARN_BIT;
+	}
+	axienet_iow(&lp, XAS_VLAN_MEMB_DATA_REG, u_value1);
+
+	u_value |= PORT_VLAN_WRITE;
+	axienet_iow(&lp, XAS_VLAN_MEMB_CTRL_REG, u_value);
+
+	/* wait for port vlan write completion */
+	err = readl_poll_timeout(lp.regs + XAS_VLAN_MEMB_CTRL_REG, reg,
+				 (!(reg & PORT_VLAN_WRITE_READ_EN_BIT)), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("Port vlan write timed out\n");
+		return -ETIMEDOUT;
+	}
+
+	return 0;
+}
+
+static int get_vlan_mac_addr_learn(void __user *arg)
+{
+	struct port_vlan port;
+	u32 u_value, u_value1, reg, err;
+
+	if (copy_from_user(&port, arg, sizeof(struct port_vlan)))
+		return -EFAULT;
+
+	u_value = ((port.vlan_id & PORT_VLAN_ID_MASK) << PORT_VLAN_ID_SHIFT)
+			| PORT_VLAN_READ;
+	axienet_iow(&lp, XAS_VLAN_MEMB_CTRL_REG, u_value);
+
+	/* wait for port vlan write completion */
+	err = readl_poll_timeout(lp.regs + XAS_VLAN_MEMB_CTRL_REG, reg,
+				 (!(reg & PORT_VLAN_WRITE_READ_EN_BIT)), 10,
+				 DELAY_OF_FIVE_MILLISEC);
+	if (err) {
+		pr_err("Port vlan write timed out\n");
+		return -ETIMEDOUT;
+	}
+	u_value1 = axienet_ior(&lp, XAS_VLAN_MEMB_CTRL_REG);
+
+	u_value = axienet_ior(&lp, XAS_VLAN_MEMB_DATA_REG);
+
+	port.aging_time = (u_value >> PORT_VLAN_HW_ADDR_AGING_TIME_SHIFT) &
+				HW_ADDR_AGING_TIME_MASK;
+	port.aging = (u_value & PORT_VLAN_HW_ADDR_AGING_BIT) ? 1 : 0;
+	port.learning = (u_value & PORT_VLAN_HW_ADDR_LEARN_BIT) ? 0 : 1;
+	port.port_num = u_value & PORT_STATUS_MASK;
+
+#if IS_ENABLED(CONFIG_XILINX_TSN_QCI)
+	if (port.learning) {
+		port.port_status = (u_value1 >> PORT_VLAN_IPV_SHIFT) & SDL_CAM_IPV_MASK;
+		port.en_port_status = (u_value1 >> PORT_VLAN_EN_IPV_SHIFT) & 0x1;
+	} else {
+		port.ipv = (u_value1 >> PORT_VLAN_IPV_SHIFT) & SDL_CAM_IPV_MASK;
+		port.en_ipv = (u_value1 >> PORT_VLAN_EN_IPV_SHIFT) & 0x1;
+	}
+#endif
+	if (copy_to_user(arg, &port, sizeof(struct port_vlan)))
+		return -EFAULT;
+
+	return 0;
+}
+
+int tsn_switch_pvid_add(struct native_vlan *port)
+{
+	u32 u_value;
+
+	switch (port->port_num) {
+	case PORT_EP:
+		u_value = axienet_ior(&lp, XAS_EP_PORT_VLAN_OFFSET);
+		u_value &= ~PORT_VLAN_ID_MASK;
+		u_value |= port->vlan_id;
+		if (port->en_ipv) {
+			u_value &= ~(SDL_CAM_IPV_MASK << NATIVE_MAC1_PCP_SHIFT);
+			u_value |= (port->ipv << NATIVE_MAC1_PCP_SHIFT);
+		}
+		axienet_iow(&lp, XAS_EP_PORT_VLAN_OFFSET, u_value);
+		break;
+	case PORT_MAC1:
+		u_value = axienet_ior(&lp, XAS_MAC_PORT_VLAN_OFFSET);
+		u_value &= ~PORT_VLAN_ID_MASK;
+		u_value |= port->vlan_id;
+		if (port->en_ipv) {
+			u_value &= ~(SDL_CAM_IPV_MASK << NATIVE_MAC1_PCP_SHIFT);
+			u_value |= (port->ipv << NATIVE_MAC1_PCP_SHIFT);
+		}
+		axienet_iow(&lp, XAS_MAC_PORT_VLAN_OFFSET, u_value);
+		break;
+	case PORT_MAC2:
+		u_value = axienet_ior(&lp, XAS_MAC_PORT_VLAN_OFFSET);
+		u_value &= ~(PORT_VLAN_ID_MASK << NATIVE_MAC2_VLAN_SHIFT);
+		u_value |= (port->vlan_id << NATIVE_MAC2_VLAN_SHIFT);
+		if (port->en_ipv) {
+			u_value &= ~(SDL_CAM_IPV_MASK << NATIVE_MAC2_PCP_SHIFT);
+			u_value |= (port->ipv << NATIVE_MAC2_PCP_SHIFT);
+		}
+		axienet_iow(&lp, XAS_MAC_PORT_VLAN_OFFSET, u_value);
+		break;
+	}
+
+	return 0;
+}
+
+static int set_native_vlan(void __user *arg)
+{
+	struct native_vlan port;
+
+	if (copy_from_user(&port, arg, sizeof(struct native_vlan)))
+		return -EFAULT;
+
+	return tsn_switch_pvid_add(&port);
+}
+
+int tsn_switch_pvid_get(struct native_vlan *port)
+{
+	u32 u_value;
+
+	switch (port->port_num) {
+	case PORT_EP:
+		u_value = axienet_ior(&lp, XAS_EP_PORT_VLAN_OFFSET);
+		port->vlan_id = u_value & PORT_VLAN_ID_MASK;
+		port->ipv = (u_value >> NATIVE_MAC1_PCP_SHIFT) &
+				SDL_CAM_IPV_MASK;
+		break;
+	case PORT_MAC1:
+		u_value = axienet_ior(&lp, XAS_MAC_PORT_VLAN_OFFSET);
+		port->vlan_id = u_value & PORT_VLAN_ID_MASK;
+		port->ipv = (u_value >> NATIVE_MAC1_PCP_SHIFT) &
+				SDL_CAM_IPV_MASK;
+		break;
+	case PORT_MAC2:
+		u_value = axienet_ior(&lp, XAS_MAC_PORT_VLAN_OFFSET);
+		port->vlan_id = (u_value >> NATIVE_MAC2_VLAN_SHIFT) &
+				PORT_VLAN_ID_MASK;
+		port->ipv = (u_value >> NATIVE_MAC2_PCP_SHIFT) &
+				SDL_CAM_IPV_MASK;
+		break;
+	}
+
+	return 0;
+}
+
+static int get_native_vlan(void __user *arg)
+{
+	struct native_vlan port;
+
+	if (copy_from_user(&port, arg, sizeof(struct native_vlan)))
+		return -EFAULT;
+
+	tsn_switch_pvid_get(&port);
+	if (copy_to_user(arg, &port, sizeof(struct native_vlan)))
+		return -EFAULT;
+
+	return 0;
+}
+
+static long switch_ioctl(struct file *file, unsigned int cmd,
+			 unsigned long arg)
+{
+	long retval = 0;
+	struct switch_data data;
+	struct pmap_data pri_info;
+#if IS_ENABLED(CONFIG_XILINX_TSN_QCI)
+	struct qci qci_data;
+#endif
+#if IS_ENABLED(CONFIG_XILINX_TSN_CB)
+	struct cb cb_data;
+#endif
+	switch (cmd) {
+	case GET_STATUS_SWITCH:
+		/* Switch configurations */
+		get_switch_regs(&data);
+
+		/* Memory static counter*/
+		get_memory_static_counter(&data);
+		if (copy_to_user((char __user *)arg, &data, sizeof(data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+			}
+		break;
+
+	case SET_STATUS_SWITCH:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		set_switch_regs(&data);
+		break;
+
+	case ADD_CAM_ENTRY:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		if (tsn_switch_cam_set(data.cam_data, ADD)) {
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+
+	case DELETE_CAM_ENTRY:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		if (tsn_switch_cam_set(data.cam_data, DELETE)) {
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+
+	case PORT_VLAN_MEM_CTRL:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		port_vlan_mem_ctrl(data.port_vlan_mem_ctrl);
+		break;
+
+	case READ_CAM_ENTRY:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			retval = -EFAULT;
+			goto end;
+		}
+		retval = read_cam_entry(data.cam_data, (void __user *)arg);
+		break;
+
+	case SET_MAC_ADDR_LEARN_CONFIG:
+		if (!en_hw_addr_learning)
+			return -EPERM;
+		retval = set_mac_addr_learn((void __user *)arg);
+		goto end;
+
+	case GET_MAC_ADDR_LEARN_CONFIG:
+		if (!en_hw_addr_learning)
+			return -EPERM;
+		retval = get_mac_addr_learn((void __user *)arg);
+		goto end;
+
+	case GET_MAC_ADDR_LEARNT_LIST:
+		if (!en_hw_addr_learning)
+			return -EPERM;
+		retval = get_mac_addr_learnt_list((void __user *)arg);
+		goto end;
+
+	case SET_PORT_STATUS:
+		if (!en_hw_addr_learning)
+			return -EPERM;
+		retval = set_port_status((void __user *)arg);
+		goto end;
+
+	case GET_PORT_STATUS:
+		if (!en_hw_addr_learning)
+			return -EPERM;
+		retval = get_port_status((void __user *)arg);
+		goto end;
+
+	case ADD_PORT_VLAN:
+		retval = add_del_port_vlan((void __user *)arg, ADD);
+		break;
+
+	case DEL_PORT_VLAN:
+		retval = add_del_port_vlan((void __user *)arg, DELETE);
+		break;
+
+	case SET_VLAN_MAC_ADDR_LEARN_CONFIG:
+		if (!en_hw_addr_learning)
+			return -EPERM;
+		retval = set_vlan_mac_addr_learn((void __user *)arg);
+		break;
+
+	case GET_VLAN_MAC_ADDR_LEARN_CONFIG:
+		if (!en_hw_addr_learning)
+			return -EPERM;
+		retval = get_vlan_mac_addr_learn((void __user *)arg);
+		break;
+
+	case GET_VLAN_MAC_ADDR_LEARN_CONFIG_VLANM:
+		retval = get_vlan_mac_addr_learn((void __user *)arg);
+		break;
+
+	case SET_PORT_NATIVE_VLAN:
+		retval = set_native_vlan((void __user *)arg);
+		break;
+
+	case GET_PORT_NATIVE_VLAN:
+		retval = get_native_vlan((void __user *)arg);
+		break;
+
+	case SET_PMAP_CONFIG:
+		if (copy_from_user(&pri_info, (char __user *)arg, sizeof(pri_info))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		retval = set_pmap_config(pri_info);
+		break;
+
+	case SET_FRAME_TYPE_FIELD:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		set_frame_filter_opt(data.typefield.type1,
+				     data.typefield.type2);
+		break;
+
+	case SET_MAC1_MNGMNT_Q_CONFIG:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		set_mac1_mngmntq(data.mac1_config);
+		break;
+
+	case SET_MAC2_MNGMNT_Q_CONFIG:
+		if (copy_from_user(&data, (char __user *)arg, sizeof(data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		set_mac2_mngmntq(data.mac2_config);
+		break;
+#if IS_ENABLED(CONFIG_XILINX_TSN_QCI)
+	case CONFIG_METER_MEM:
+		if (copy_from_user(&qci_data, (char __user *)arg,
+				   sizeof(qci_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		program_meter_reg(qci_data.meter_config_data);
+		break;
+
+	case CONFIG_GATE_MEM:
+		if (copy_from_user(&qci_data, (char __user *)arg,
+				   sizeof(qci_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		config_stream_filter(qci_data.stream_config_data);
+		break;
+
+	case PSFP_CONTROL:
+		if (copy_from_user(&qci_data, (char __user *)arg,
+				   sizeof(qci_data))) {
+			retval = -EINVAL;
+			pr_err("Copy from user failed\n");
+			goto end;
+		}
+		psfp_control(qci_data.psfp_config_data);
+		break;
+
+	case GET_STATIC_PSFP_COUNTER:
+		if (copy_from_user(&qci_data, (char __user *)arg,
+				   sizeof(qci_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		get_psfp_static_counter(&qci_data.psfp_counter_data);
+		if (copy_to_user((char __user *)arg, &qci_data,
+				 sizeof(qci_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+	case GET_METER_REG:
+		get_meter_reg(&qci_data.meter_config_data);
+		if (copy_to_user((char __user *)arg, &qci_data,
+				 sizeof(qci_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+	case GET_STREAM_FLTR_CONFIG:
+		get_stream_filter_config(&qci_data.stream_config_data);
+		if (copy_to_user((char __user *)arg, &qci_data,
+				 sizeof(qci_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+#endif
+#if IS_ENABLED(CONFIG_XILINX_TSN_CB)
+	case CONFIG_MEMBER_MEM:
+		if (copy_from_user(&cb_data, (char __user *)arg,
+				   sizeof(cb_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		program_member_reg(cb_data);
+		break;
+
+	case CONFIG_INGRESS_FLTR:
+		if (copy_from_user(&cb_data, (char __user *)arg,
+				   sizeof(cb_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		config_ingress_filter(cb_data);
+		break;
+
+	case FRER_CONTROL:
+		if (copy_from_user(&cb_data, (char __user *)arg,
+				   sizeof(cb_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		frer_control(cb_data.frer_ctrl_data);
+		break;
+
+	case GET_STATIC_FRER_COUNTER:
+		if (copy_from_user(&cb_data, (char __user *)arg,
+				   sizeof(cb_data))) {
+			pr_err("Copy from user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		get_frer_static_counter(&cb_data.frer_counter_data);
+		if (copy_to_user((char __user *)arg, &cb_data,
+				 sizeof(cb_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+
+	case GET_MEMBER_REG:
+		get_member_reg(&cb_data.frer_memb_config_data);
+		if (copy_to_user((char __user *)arg, &cb_data,
+				 sizeof(cb_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+
+	case GET_INGRESS_FLTR:
+		get_ingress_filter_config(&cb_data.in_fltr_data);
+		if (copy_to_user((char __user *)arg, &cb_data,
+				 sizeof(cb_data))) {
+			pr_err("Copy to user failed\n");
+			retval = -EINVAL;
+			goto end;
+		}
+		break;
+#endif
+	}
+end:
+	return retval;
+}
+
+static const struct file_operations switch_fops = {
+	.owner		=	THIS_MODULE,
+	.unlocked_ioctl	=	switch_ioctl,
+	.open		=	switch_open,
+	.release	=       switch_release,
+};
+
+static int tsn_switch_init(void)
+{
+	int ret;
+
+	switch_dev.minor = MISC_DYNAMIC_MINOR;
+	switch_dev.name = "switch";
+	switch_dev.fops = &switch_fops;
+	ret = misc_register(&switch_dev);
+	if (ret < 0) {
+		pr_err("Switch driver registration failed!\n");
+		return ret;
+	}
+
+	eth_random_addr((u8 *)&sw_mac_addr);
+
+	pr_debug("Xilinx TSN Switch driver initialized!\n");
+	return 0;
+}
+
+static inline void tsn_switch_set_src_mac_filter(const u8 *mac, int port)
+{
+	u32 val;
+	u32 shift = (port == 1) ? MAC1_PORT_MAC_ADDR_LSB_SHIFT :
+		MAC2_PORT_MAC_ADDR_LSB_SHIFT;
+
+	/* program the Source MAC address to filter */
+	val = axienet_ior(&lp, XAS_PORT_STATE_CTRL_OFFSET);
+
+	val &= ~(PORT_MAC_ADDR_LSB_MASK << shift);
+	val |= ((mac[5] & PORT_MAC_ADDR_LSB_MASK) << shift);
+
+	axienet_iow(&lp, XAS_PORT_STATE_CTRL_OFFSET, val);
+}
+
+/* initialize pre-configured fdbs in the system */
+static int tsn_switch_fdb_init(struct platform_device *pdev)
+{
+	u32 val, port;
+	u8 mac_addr[ETH_ALEN];
+	struct device_node *np;
+	u16 num_ports;
+	int ret;
+	struct cam_struct cam;
+
+	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-ports",
+				   &num_ports);
+	if (ret) {
+		dev_err(&pdev->dev, "could not read xlnx,num-ports\n");
+		return -EINVAL;
+	}
+
+	/* enable source mac based filtering */
+	val = axienet_ior(&lp, XAS_MNG_Q_CTRL_OFFSET);
+
+	/* compares Source MAC address to determine the Network
+	 * Port on which a frame needs to be forwarded for frames received
+	 * with Internal Endpoint interface.
+	 * [i.e. CPU generated  management frames]
+	 */
+	val |= (1 << XAS_MNG_Q_SRC_MAC_FIL_EN_SHIFT);
+
+	axienet_iow(&lp, XAS_MNG_Q_CTRL_OFFSET, val);
+
+	/* port0 == ep, port1 == temac1 port2 == temac2
+	 * temac1, temac2.. temacn are network ports
+	 */
+	/* network port mac addresses must differ in last lsb nibble
+	 * this is a pre-requisite;
+	 * we program the last nibble here per mac basis
+	 */
+	ep_node = of_parse_phandle(pdev->dev.of_node, "ports", 0);
+	for (port = 1; port < num_ports; port++) {
+		np = of_parse_phandle(pdev->dev.of_node, "ports", port);
+
+		ret = of_get_mac_address(np, mac_addr);
+		if (ret) {
+			dev_err(&pdev->dev, "could not find MAC address\n");
+			return -EINVAL;
+		}
+		tsn_switch_set_src_mac_filter(mac_addr, port);
+	}
+
+	/* rest of the mac addr for all ports would be same
+	 * so use the last mac instance of the loop above
+	 * set the 32bit lsb of mac address
+	 */
+	axienet_iow(&lp, XAS_MAC_LSB_OFFSET,
+		    (mac_addr[2] << 24) | (mac_addr[3] << 16) |
+		    (mac_addr[4] << 8)  | (mac_addr[5]));
+
+	/* set rest of 16bit msb and 4bit filter(0xF) */
+	axienet_iow(&lp, XAS_MAC_MSB_OFFSET,
+		    (0xF << XAS_MAC_MSB_FF_MASK_SHIFT) |
+		    (mac_addr[0] << 8) | mac_addr[1]);
+
+	/* now tell the switch which frames to consider as mgmt frames
+	 */
+	/*  DA list
+	 *  01-80-C2-00-00-00 STP 802.1d && LLDP
+	 */
+	memset(&cam, 0, sizeof(struct cam_struct));
+
+	cam.dest_addr[0] = 0x01;
+	cam.dest_addr[1] = 0x80;
+	cam.dest_addr[2] = 0xc2;
+	cam.dest_addr[3] = 0x00;
+	cam.dest_addr[4] = 0x00;
+	cam.dest_addr[5] = 0x00;
+
+	/* send it all, src mac filter will pick the port */
+	cam.fwd_port = DEFAULT_FWD_ALL;
+	cam.flags |= XAS_CAM_EP_MGMTQ_EN;
+	cam.vlanid = DEFAULT_PVID;
+	/* TODO if pvid changes on the port of switch,
+	 * these cam entries have to be updated
+	 */
+
+	if (tsn_switch_cam_set(cam, ADD) < 0)
+		dev_err(&pdev->dev, "could not add default fdb\n");
+
+	/* 01-80-c2-00-00-0e  LLDP */
+	cam.dest_addr[5] = 0x0e;
+	if (tsn_switch_cam_set(cam, ADD) < 0)
+		dev_err(&pdev->dev, "could not add default fdb\n");
+	/* CDP */
+	cam.dest_addr[0] = 0x01;
+	cam.dest_addr[1] = 0x00;
+	cam.dest_addr[2] = 0x0c;
+	cam.dest_addr[3] = 0xcc;
+	cam.dest_addr[4] = 0xcc;
+	cam.dest_addr[5] = 0xcc;
+	if (tsn_switch_cam_set(cam, ADD) < 0)
+		dev_err(&pdev->dev, "could not add default fdb\n");
+
+	/* PTPv2 UDP Announce Messages */
+	cam.dest_addr[0] = 0x01;
+	cam.dest_addr[1] = 0x00;
+	cam.dest_addr[2] = 0x5e;
+	cam.dest_addr[3] = 0x00;
+	cam.dest_addr[4] = 0x01;
+	cam.dest_addr[5] = 0x81;
+	if (tsn_switch_cam_set(cam, ADD) < 0)
+		dev_err(&pdev->dev, "could not add default fdb\n");
+	/* PTPv2 UDP P2P Mechanism Messages */
+	cam.dest_addr[4] = 0x00;
+	cam.dest_addr[5] = 0x6b;
+	if (tsn_switch_cam_set(cam, ADD) < 0)
+		dev_err(&pdev->dev, "could not add default fdb\n");
+
+	/* on RX path enable sideband management on frames forwarded to ep
+	 * this only applicable if IP param EN_INBAND_MGMT_TAG is 0
+	 */
+	val = axienet_ior(&lp, XAS_MNG_Q_CTRL_OFFSET);
+	val |= (1 << XAS_MNG_Q_SIDEBAND_EN_SHIFT);
+	axienet_iow(&lp, XAS_MNG_Q_CTRL_OFFSET, val);
+
+	return 0;
+}
+
+static int tsnswitch_probe(struct platform_device *pdev)
+{
+	struct resource *swt;
+	int ret;
+	u16 num_tc;
+	int data;
+	struct net_device *ndev;
+	int value;
+	u8 inband_mgmt_tag;
+	struct pmap_data pri_info;
+
+	pr_info("TSN Switch probe\n");
+	/* Map device registers */
+	swt = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	lp.regs = devm_ioremap_resource(&pdev->dev, swt);
+	if (IS_ERR(lp.regs))
+		return PTR_ERR(lp.regs);
+
+	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-tc",
+				   &num_tc);
+	if (ret || (num_tc != 2 && num_tc != 3))
+		num_tc = XAE_MAX_TSN_TC;
+	lp.num_tc = num_tc;
+
+	axienet_get_pcp_mask(&lp, num_tc);
+
+	en_hw_addr_learning = of_property_read_bool(pdev->dev.of_node,
+						    "xlnx,has-hwaddr-learning");
+
+	pr_info("TSN Switch Initializing ....\n");
+	pr_info("TSN Switch hw_addr_learning :%d\n", en_hw_addr_learning);
+
+	ret = tsn_switch_init();
+	if (ret)
+		return ret;
+	pr_info("TSN CAM Initializing ....\n");
+	pri_info.st_pcp_reg = lp.st_pcp;
+	pri_info.res_pcp_reg = lp.res_pcp;
+	ret = set_pmap_config(pri_info);
+	if (ret)
+		return ret;
+	inband_mgmt_tag = of_property_read_bool(pdev->dev.of_node,
+						"xlnx,has-inband-mgmt-tag");
+	/* only support switchdev in sideband management */
+	if (!inband_mgmt_tag) {
+		ret = tsn_switch_fdb_init(pdev);
+		xlnx_switchdev_init();
+	} else {
+		pr_info("TSN IP with inband mgmt: Linux SWITCHDEV turned off\n");
+	}
+
+	/* writing into endpoint extension control register for channel mapping as follows:
+	 *
+	 *	3 traffic classes & EP + switch + Extended EP
+	 *	      +---------+
+	 *	  1   |         |
+	 *    BE-------         |
+	 *	  2   |         |   1
+	 *     ST------         |------BE
+	 *	  3   |         |   2
+	 *   mgmt------         |------RES
+	 *	  4   |         |
+	 *   RES-------         |
+	 *	  5   |         |
+	 * EX-BE-------         |
+	 *	  6   |         |
+	 * EX-ST-------         |
+	 *	  7   |         |
+	 * EX-RES------         |
+	 *	      +---------+
+	 *
+	 *	2 traffic classes & EP + switch + Extended EP
+	 *	      +---------+
+	 *	  1   |         |
+	 *    BE-------         |
+	 *	  2   |         |   1
+	 *    ST-------         |------BE
+	 *	  3   |         |
+	 *   mgmt------         |
+	 *	  4   |         |
+	 * EX-BE-------         |
+	 *	  5   |         |
+	 * EX-ST-------         |
+	 *	      |         |
+	 *	      +---------+
+	 *	3 traffic classes & EP + switch
+	 *	      +---------+
+	 *	  1   |         |
+	 *    BE-------         |
+	 *	  2   |         |   1
+	 *     ST------         |------BE
+	 *	  3   |         |   2
+	 *   mgmt------         |------RES
+	 *	  4   |         |
+	 *   RES-------         |
+	 *	      |         |
+	 *	      +---------+
+	 *
+	 *	2 traffic classes & EP + switch
+	 *	      +---------+
+	 *	  1   |         |
+	 *    BE-------         |
+	 *	  2   |         |   1
+	 *     ST------         |------BE
+	 *	  3   |         |
+	 *   mgmt------         |
+	 *	      |         |
+	 *	      +---------+
+	 */
+	data = axienet_ior(&lp, XAE_EP_EXT_CTRL_OFFSET);
+	pr_info("Data in Endpoint Extension Control Register is %x\n", data);
+	ndev = of_find_net_device_by_node(ep_node);
+	ep_lp = netdev_priv(ndev);
+	if (ep_lp->ex_ep) {
+		if (num_tc == 3) {
+			data = (data & XAE_EX_EP_EXT_CTRL_MASK) |
+					XAE_EX_EP_EXT_CTRL_DATA_TC_3;
+			axienet_iow(&lp, XAE_EP_EXT_CTRL_OFFSET, data);
+		}
+		if (num_tc == 2) {
+			data = (data & XAE_EX_EP_EXT_CTRL_MASK) |
+					XAE_EX_EP_EXT_CTRL_DATA_TC_2;
+			axienet_iow(&lp, XAE_EP_EXT_CTRL_OFFSET, data);
+		}
+		/* Enabling endpoint packet switching extension for broadcast and
+		 * multicast packets received from endpoint
+		 */
+		value = axienet_ior(&lp, XAE_MGMT_QUEUING_OPTIONS_OFFSET);
+		value |= XAE_EX_EP_BROADCAST_PKT_SWITCH;
+		value |= XAE_EX_EP_MULTICAST_PKT_SWITCH;
+		axienet_iow(&lp, XAE_MGMT_QUEUING_OPTIONS_OFFSET, value);
+	} else {
+		if (num_tc == 3) {
+			data = (data & XAE_EP_EXT_CTRL_MASK) |
+					XAE_EP_EXT_CTRL_DATA_TC_3;
+			axienet_iow(&lp, XAE_EP_EXT_CTRL_OFFSET, data);
+		}
+		if (num_tc == 2) {
+			data = (data & XAE_EP_EXT_CTRL_MASK) |
+					XAE_EP_EXT_CTRL_DATA_TC_2;
+			axienet_iow(&lp, XAE_EP_EXT_CTRL_OFFSET, data);
+		}
+	}
+
+	return ret;
+}
+
+static int tsnswitch_remove(struct platform_device *pdev)
+{
+	misc_deregister(&switch_dev);
+	xlnx_switchdev_remove();
+	return 0;
+}
+
+static struct platform_driver tsnswitch_driver = {
+	.probe = tsnswitch_probe,
+	.remove = tsnswitch_remove,
+	.driver = {
+		 .name = "xilinx_tsnswitch",
+		 .of_match_table = tsnswitch_of_match,
+	},
+};
+
+module_platform_driver(tsnswitch_driver);
+
+MODULE_DESCRIPTION("Xilinx TSN Switch driver");
+MODULE_AUTHOR("Xilinx");
+MODULE_LICENSE("GPL v2");
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_switch.h
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_switch.h	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,479 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Xilinx TSN core switch header
+ *
+ * Copyright (C) 2017 Xilinx, Inc.
+ *
+ * Author: Saurabh Sengar <saurabhs@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef XILINX_TSN_SWITCH_H
+#define XILINX_TSN_SWITCH_H
+
+#include "xilinx_axienet_tsn.h"
+
+/* ioctls */
+#define GET_STATUS_SWITCH			0x16
+#define SET_STATUS_SWITCH			0x17
+#define ADD_CAM_ENTRY				0x18
+#define DELETE_CAM_ENTRY			0x19
+#define PORT_VLAN_MEM_CTRL			0x20
+#define SET_FRAME_TYPE_FIELD			0x21
+#define SET_MAC1_MNGMNT_Q_CONFIG		0x22
+#define SET_MAC2_MNGMNT_Q_CONFIG		0x23
+#define CONFIG_METER_MEM			0x24
+#define CONFIG_GATE_MEM				0x25
+#define PSFP_CONTROL				0x26
+#define GET_STATIC_PSFP_COUNTER			0x27
+#define GET_METER_REG				0x28
+#define GET_STREAM_FLTR_CONFIG			0x29
+#define CONFIG_MEMBER_MEM			0x2A
+#define CONFIG_INGRESS_FLTR			0x2B
+#define FRER_CONTROL				0x2C
+#define GET_STATIC_FRER_COUNTER			0x2D
+#define GET_MEMBER_REG				0x2E
+#define GET_INGRESS_FLTR			0x2F
+#define SET_PORT_STATUS				0x33
+#define GET_PORT_STATUS				0x34
+#define SET_MAC_ADDR_LEARN_CONFIG		0x30
+#define GET_MAC_ADDR_LEARN_CONFIG		0x31
+#define GET_MAC_ADDR_LEARNT_LIST		0x32
+#define ADD_PORT_VLAN				0x35
+#define DEL_PORT_VLAN				0x36
+#define SET_VLAN_MAC_ADDR_LEARN_CONFIG		0x37
+#define GET_VLAN_MAC_ADDR_LEARN_CONFIG		0x38
+#define READ_CAM_ENTRY				0x39
+#define GET_VLAN_MAC_ADDR_LEARN_CONFIG_VLANM	0x3C
+#define SET_PORT_NATIVE_VLAN			0x3A
+#define GET_PORT_NATIVE_VLAN			0x3B
+#define SET_PMAP_CONFIG				0x3D
+
+/* Xilinx Axi Switch Offsets*/
+#define XAS_STATUS_OFFSET			0x00000
+#define XAS_CONTROL_OFFSET			0x00004
+#define XAS_PMAP_OFFSET				0x00008
+#define XAS_MAC_LSB_OFFSET			0x0000C
+#define XAS_MAC_MSB_OFFSET			0x00010
+#define XAS_MAC_MSB_FF_MASK_SHIFT		(16)
+
+#define XAS_EP2MAC_ST_FIFOT_OFFSET		0x00020
+#define XAS_EP2MAC_RE_FIFOT_OFFSET		0x00024
+#define XAS_EP2MAC_BE_FIFOT_OFFSET		0x00028
+#define XAS_MAC2MAC_ST_FIFOT_OFFSET		0x00030
+#define XAS_MAC2MAC_RE_FIFOT_OFFSET		0x00034
+#define XAS_MAC2MAC_BE_FIFOT_OFFSET		0x00038
+#define XAS_EP_PORT_VLAN_OFFSET			0x00040
+#define XAS_MAC_PORT_VLAN_OFFSET		0x00044
+#define XAS_HW_ADDR_LEARN_CTRL_OFFSET		0x00048
+#define XAS_PORT_STATE_CTRL_OFFSET		0x0004c
+#define XAS_FRM_FLTR_TYPE_FIELD_OPT_OFFSET	0x00050
+
+#define XAS_MNG_Q_CTRL_OFFSET			0x00054
+#define XAS_MNG_Q_SRC_MAC_FIL_EN_SHIFT		(4)
+#define XAS_MNG_Q_SIDEBAND_EN_SHIFT		(3)
+
+#define XAS_MAC1_MNG_Q_OPTION_OFFSET		0x00058
+#define XAS_ST_MAX_FRAME_SIZE_OFFSET		0x00060
+#define XAS_RE_MAX_FRAME_SIZE_OFFSET		0x00064
+#define XAS_BE_MAX_FRAME_SIZE_OFFSET		0x00068
+
+/* Memory static counters */
+#define XAS_MEM_STCNTR_CAM_LOOKUP		0x00400
+#define XAS_MEM_STCNTR_MULTCAST			0x00408
+#define XAS_MEM_STCNTR_ERR_MAC1			0x00410
+#define XAS_MEM_STCNTR_ERR_MAC2			0x00418
+#define XAS_MEM_STCNTR_SC_MAC1_EP		0x00420
+#define XAS_MEM_STCNTR_RES_MAC1_EP		0x00428
+#define XAS_MEM_STCNTR_BE_MAC1_EP		0x00430
+#define XAS_MEM_STCNTR_ERR_SC_MAC1_EP		0x00438
+#define XAS_MEM_STCNTR_ERR_RES_MAC1_EP		0x00440
+#define XAS_MEM_STCNTR_ERR_BE_MAC1_EP		0x00448
+#define XAS_MEM_STCNTR_SC_MAC2_EP		0x00458
+#define XAS_MEM_STCNTR_RES_MAC2_EP		0x00460
+#define XAS_MEM_STCNTR_BE_MAC2_EP		0x00468
+#define XAS_MEM_STCNTR_ERR_SC_MAC2_EP		0x00470
+#define XAS_MEM_STCNTR_ERR_RES_MAC2_EP		0x00478
+#define XAS_MEM_STCNTR_ERR_BE_MAC2_EP		0x00480
+#define XAS_MEM_STCNTR_SC_EP_MAC1		0x00490
+#define XAS_MEM_STCNTR_RES_EP_MAC1		0x00498
+#define XAS_MEM_STCNTR_BE_EP_MAC1		0x004A0
+#define XAS_MEM_STCNTR_ERR_SC_EP_MAC1		0x004A8
+#define XAS_MEM_STCNTR_ERR_RES_EP_MAC1		0x004B0
+#define XAS_MEM_STCNTR_ERR_BE_EP_MAC1		0x004B8
+#define XAS_MEM_STCNTR_SC_MAC2_MAC1		0x004C0
+#define XAS_MEM_STCNTR_RES_MAC2_MAC1		0x004C8
+#define XAS_MEM_STCNTR_BE_MAC2_MAC1		0x004D0
+#define XAS_MEM_STCNTR_ERR_SC_MAC2_MAC1		0x004D8
+#define XAS_MEM_STCNTR_ERR_RES_MAC2_MAC1	0x004E0
+#define XAS_MEM_STCNTR_ERR_BE_MAC2_MAC1		0x004E8
+#define XAS_MEM_STCNTR_SC_EP_MAC2		0x004F0
+#define XAS_MEM_STCNTR_RES_EP_MAC2		0x004F8
+#define XAS_MEM_STCNTR_BE_EP_MAC2		0x00500
+#define XAS_MEM_STCNTR_ERR_SC_EP_MAC2		0x00508
+#define XAS_MEM_STCNTR_ERR_RES_EP_MAC2		0x00510
+#define XAS_MEM_STCNTR_ERR_BE_EP_MAC2		0x00518
+#define XAS_MEM_STCNTR_SC_MAC1_MAC2		0x00520
+#define XAS_MEM_STCNTR_RES_MAC1_MAC2		0x00528
+#define XAS_MEM_STCNTR_BE_MAC1_MAC2		0x00530
+#define XAS_MEM_STCNTR_ERR_SC_MAC1_MAC2		0x00538
+#define XAS_MEM_STCNTR_ERR_RES_MAC1_MAC2	0x00540
+#define XAS_MEM_STCNTR_ERR_BE_MAC1_MAC2		0x00548
+
+/* Stream Destination Lookup CAM */
+#define XAS_SDL_CAM_CTRL_OFFSET			0x1000
+#define XAS_SDL_CAM_STATUS_OFFSET		0x1004
+#define XAS_SDL_CAM_KEY1_OFFSET			0x1008
+#define XAS_SDL_CAM_KEY2_OFFSET			0x100C
+#define XAS_SDL_CAM_TV1_OFFSET			0x1010
+#define XAS_SDL_CAM_TV2_OFFSET			0x1014
+#define XAS_SDL_CAM_PORT_ACT_OFFSET		0x1018
+
+/* Port VLAN Membership Memory */
+#define XAS_VLAN_MEMB_CTRL_REG			0x1100
+#define XAS_VLAN_MEMB_DATA_REG			0x1104
+
+/* QCI */
+#define PSFP_CONTROL_OFFSET			0x1200
+#define STREAM_FILTER_CONFIG_OFFSET		0x1204
+#define	STREAM_METER_CIR_OFFSET			0x1208
+#define	STREAM_METER_EIR_OFFSET			0x120C
+#define	STREAM_METER_CBR_OFFSET			0x1210
+#define	STREAM_METER_EBR_OFFSET			0x1214
+
+/* PSFP Statistics Counters */
+#define TOTAL_PSFP_FRAMES_OFFSET		0x2000
+#define FLTR_INGS_PORT_ERR_OFFSET		0x2800
+#define FLTR_STDU_ERR_OFFSET			0x3000
+#define METER_ERR_OFFSET			0x3800
+
+/* CB */
+#define FRER_CONTROL_OFFSET			0x1300
+#define INGRESS_FILTER_OFFSET			0x1304
+#define FRER_CONFIG_REG1			0x1308
+#define FRER_CONFIG_REG2			0x130C
+
+/* FRER Statistics Counters */
+#define TOTAL_FRER_FRAMES_OFFSET		0x4000
+#define FRER_DISCARD_INGS_FLTR_OFFSET		0x4800
+#define FRER_PASS_FRAMES_INDV_OFFSET		0x5000
+#define FRER_DISCARD_FRAMES_INDV_OFFSET		0x5800
+#define FRER_PASS_FRAMES_SEQ_OFFSET		0x6000
+#define FRER_DISCARD_FRAMES_SEQ_OFFSET		0x6800
+#define FRER_ROGUE_FRAMES_SEQ_OFFSET		0x7000
+#define SEQ_RECV_RESETS_OFFSET			0x7800
+
+/* endpoint extension control register */
+#define XAE_EP_EXT_CTRL_OFFSET			0x0058
+#define XAE_MGMT_QUEUING_OPTIONS_OFFSET		0x0054
+#define XAE_EX_EP_BROADCAST_PKT_SWITCH		BIT(7)
+#define XAE_EX_EP_MULTICAST_PKT_SWITCH		BIT(6)
+#define XAE_EX_EP_EXT_CTRL_DATA_TC_3		0x00135419
+#define XAE_EX_EP_EXT_CTRL_DATA_TC_2		0x000DC401
+#define XAE_EP_EXT_CTRL_DATA_TC_3		0x00400419
+#define XAE_EP_EXT_CTRL_DATA_TC_2		0x00400409
+#define XAE_EX_EP_EXT_CTRL_MASK			0xFFE00000
+#define XAE_EP_EXT_CTRL_MASK			0xFF1FF000
+
+/* 64 bit counter*/
+struct static_cntr {
+	u32 msb;
+	u32 lsb;
+};
+
+/*********** QCI Structures **************/
+struct psfp_config {
+	u8 gate_id;
+	u8 meter_id;
+	bool en_meter;
+	bool allow_stream;
+	bool en_psfp;
+	u8 wr_op_type;
+	bool op_type;
+};
+
+struct meter_config {
+	u32 cir;
+	u32 eir;
+	u32 cbr;
+	u32 ebr;
+	u8 mode;
+};
+
+struct stream_filter {
+	u8 in_pid; /* ingress port id*/
+	u16 max_fr_size; /* max frame size*/
+};
+
+/* PSFP Static counter*/
+struct psfp_static_counter {
+	struct static_cntr psfp_fr_count;
+	struct static_cntr err_filter_ins_port;
+	struct static_cntr err_filtr_sdu;
+	struct static_cntr err_meter;
+	unsigned char num;
+};
+
+/* QCI Core stuctures */
+struct qci {
+	struct meter_config meter_config_data;
+	struct stream_filter stream_config_data;
+	struct psfp_config psfp_config_data;
+	struct psfp_static_counter psfp_counter_data;
+};
+
+/************* QCI Structures end *************/
+
+/*********** CB Structures **************/
+struct frer_ctrl {
+	u8 gate_id;
+	u8 memb_id;
+	bool seq_reset;
+	bool gate_state;
+	bool rcvry_tmout;
+	bool frer_valid;
+	u8 wr_op_type;
+	bool op_type;
+};
+
+struct in_fltr {
+	u8 in_port_id;
+	u16 max_seq_id;
+};
+
+struct frer_memb_config {
+	u8 seq_rec_hist_len;
+	u8 split_strm_egport_id;
+	u16 split_strm_vlan_id;
+	u32 rem_ticks;
+};
+
+/* FRER Static counter*/
+struct frer_static_counter {
+	struct static_cntr frer_fr_count;
+	struct static_cntr disc_frames_in_portid;
+	struct static_cntr pass_frames_seq_recv;
+	struct static_cntr disc_frames_seq_recv;
+	struct static_cntr rogue_frames_seq_recv;
+	struct static_cntr pass_frames_ind_recv;
+	struct static_cntr disc_frames_ind_recv;
+	struct static_cntr seq_recv_rst;
+	unsigned char num;
+};
+
+/* CB Core stuctures */
+struct cb {
+	struct frer_ctrl frer_ctrl_data;
+	struct in_fltr in_fltr_data;
+	struct frer_memb_config frer_memb_config_data;
+	struct frer_static_counter frer_counter_data;
+};
+
+/************* CB Structures end *************/
+
+/********* Switch Structures Starts ***********/
+struct thershold {
+	u16 t1;
+	u16 t2;
+};
+
+struct pmap_data {
+	int st_pcp_reg;
+	int res_pcp_reg;
+};
+
+/* memory static counters */
+struct mem_static_arr_cntr {
+	struct static_cntr cam_lookup;
+	struct static_cntr multicast_fr;
+	struct static_cntr err_mac1;
+	struct static_cntr err_mac2;
+	struct static_cntr sc_mac1_ep;
+	struct static_cntr res_mac1_ep;
+	struct static_cntr be_mac1_ep;
+	struct static_cntr err_sc_mac1_ep;
+	struct static_cntr err_res_mac1_ep;
+	struct static_cntr err_be_mac1_ep;
+	struct static_cntr sc_mac2_ep;
+	struct static_cntr res_mac2_ep;
+	struct static_cntr be_mac2_ep;
+	struct static_cntr err_sc_mac2_ep;
+	struct static_cntr err_res_mac2_ep;
+	struct static_cntr err_be_mac2_ep;
+	struct static_cntr sc_ep_mac1;
+	struct static_cntr res_ep_mac1;
+	struct static_cntr be_ep_mac1;
+	struct static_cntr err_sc_ep_mac1;
+	struct static_cntr err_res_ep_mac1;
+	struct static_cntr err_be_ep_mac1;
+	struct static_cntr sc_mac2_mac1;
+	struct static_cntr res_mac2_mac1;
+	struct static_cntr be_mac2_mac1;
+	struct static_cntr err_sc_mac2_mac1;
+	struct static_cntr err_res_mac2_mac1;
+	struct static_cntr err_be_mac2_mac1;
+	struct static_cntr sc_ep_mac2;
+	struct static_cntr res_ep_mac2;
+	struct static_cntr be_ep_mac2;
+	struct static_cntr err_sc_ep_mac2;
+	struct static_cntr err_res_ep_mac2;
+	struct static_cntr err_be_ep_mac2;
+	struct static_cntr sc_mac1_mac2;
+	struct static_cntr res_mac1_mac2;
+	struct static_cntr be_mac1_mac2;
+	struct static_cntr err_sc_mac1_mac2;
+	struct static_cntr err_res_mac1_mac2;
+	struct static_cntr err_be_mac1_mac2;
+};
+
+#define XAS_CAM_IPV_EN		BIT(0)
+#define XAS_CAM_EP_MGMTQ_EN	BIT(1)
+#define XAS_CAM_VALID		BIT(2)
+
+/* CAM structure */
+struct cam_struct {
+	u8 src_addr[6];
+	u8 dest_addr[6];
+	u16 vlanid;
+	u16 tv_vlanid;
+	u8 fwd_port;
+	u8 gate_id;
+	u8 ipv;
+	u32 flags;
+	u8 ep_port_act;
+	u8 mac_port_act;
+};
+
+/*Frame Filtering Type Field Option */
+struct ff_type {
+	u16 type1;
+	u16 type2;
+};
+
+/* TODO Fix holes in this structure and corresponding TSN switch_prog app */
+struct mac_addr_learn {
+	bool aging;
+	bool is_age;
+	bool learning;
+	bool is_learn;
+	bool learn_untag;
+	bool is_untag;
+	u32 aging_time;
+};
+
+struct mac_learnt {
+	u8 mac_addr[6];
+	u16 vlan_id;
+};
+
+#define MAX_NUM_MAC_ENTRIES	2048
+struct mac_addr_list {
+	u8 port_num;
+	u16 num_list;
+	struct mac_learnt list[MAX_NUM_MAC_ENTRIES];
+};
+
+struct port_status {
+	u8 port_num;
+	u8 port_status;
+};
+
+struct port_vlan {
+	bool aging;
+	bool is_age;
+	bool learning;
+	bool is_learn;
+	bool is_mgmtq;
+	bool en_ipv;
+	bool en_port_status;
+	u8 mgmt_ext_id;
+	u8 port_num;
+	u8 ipv;
+	u8 port_status;
+	u16 vlan_id;
+	u32 aging_time;
+};
+
+struct native_vlan {
+	bool en_ipv;
+	u8 port_num;
+	u8 ipv;
+	u16 vlan_id;
+};
+
+enum switch_port {
+	PORT_EP = 1,
+	PORT_MAC1 = 2,
+	PORT_MAC2 = 4,
+	PORT_EX_ONLY = 8,
+	PORT_EX_EP = 16,
+};
+
+/* Core switch structure*/
+/* TODO Fix holes in this structure and corresponding TSN switch_prog app */
+struct switch_data {
+	u32 switch_status;
+	u32 switch_ctrl;
+	u32 switch_prt;
+	u8 sw_mac_addr[6];
+	/*0 - schedule, 1 - reserved, 2 - best effort queue*/
+	struct thershold thld_ep_mac[3];
+	struct thershold thld_mac_mac[3];
+	u32 ep_vlan;
+	u32 mac_vlan;
+	u32 max_frame_sc_que;
+	u32 max_frame_res_que;
+	u32 max_frame_be_que;
+	/* Memory counters */
+	struct mem_static_arr_cntr mem_arr_cnt;
+	/* CAM */
+	struct cam_struct cam_data;
+/* Frame Filtering Type Field Option */
+	struct ff_type typefield;
+/* MAC Port-1 Management Queueing Options */
+	int mac1_config;
+/* MAC Port-2 Management Queueing Options */
+	int mac2_config;
+/* Port VLAN Membership Registers */
+	int port_vlan_mem_ctrl;
+	char port_vlan_mem_data;
+};
+
+/********* Switch Structures ends ***********/
+
+extern struct axienet_local lp;
+
+/********* qci function declararions ********/
+void psfp_control(struct psfp_config data);
+void config_stream_filter(struct stream_filter data);
+void program_meter_reg(struct meter_config data);
+void get_psfp_static_counter(struct psfp_static_counter *data);
+void get_meter_reg(struct meter_config *data);
+void get_stream_filter_config(struct stream_filter *data);
+
+/********* cb function declararions ********/
+void frer_control(struct frer_ctrl data);
+void get_ingress_filter_config(struct in_fltr *data);
+void config_ingress_filter(struct cb data);
+void get_member_reg(struct frer_memb_config *data);
+void program_member_reg(struct cb data);
+void get_frer_static_counter(struct frer_static_counter *data);
+int tsn_switch_cam_set(struct cam_struct data, u8 add);
+u8 *tsn_switch_get_id(void);
+int tsn_switch_set_stp_state(struct port_status *port);
+int tsn_switch_vlan_add(struct port_vlan *port, int add);
+int tsn_switch_pvid_get(struct native_vlan *port);
+int tsn_switch_pvid_add(struct native_vlan *port);
+#ifdef CONFIG_XILINX_TSN_SWITCH
+int xlnx_switchdev_init(void);
+void xlnx_switchdev_remove(void);
+#endif
+#endif /* XILINX_TSN_SWITCH_H */
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_switchdev.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_switchdev.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,374 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Xilinx TSN Switch Device support driver
+ *
+ * Copyright (c) 2022 Xilinx, Inc. All rights reserved.
+ */
+#ifdef CONFIG_XILINX_TSN_SWITCH
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/if_bridge.h>
+#include <net/switchdev.h>
+#include "xilinx_axienet_tsn.h"
+#include "xilinx_tsn_switch.h"
+
+#define tsn_to_linux_sw_state(s) \
+	(((s) == BR_STATE_DISABLED)   ? TSN_SW_STATE_DISABLED : \
+	 ((s) == BR_STATE_BLOCKING)   ? TSN_SW_STATE_BLOCKING : \
+	 ((s) == BR_STATE_LISTENING)  ? TSN_SW_STATE_LISTENING : \
+	 ((s) == BR_STATE_LEARNING)   ? TSN_SW_STATE_LEARNING : \
+	 ((s) == BR_STATE_FORWARDING) ? TSN_SW_STATE_FORWARDING : \
+	 TSN_SW_STATE_DISABLED)
+
+#define stp_state_string(s) \
+	(((s) == BR_STATE_DISABLED)   ? "disabled" : \
+	 ((s) == BR_STATE_BLOCKING)   ? "blocking" : \
+	 ((s) == BR_STATE_LISTENING)  ? "listening" : \
+	 ((s) == BR_STATE_LEARNING)   ? "learning" : \
+	 ((s) == BR_STATE_FORWARDING) ? "forwarding" : \
+	 "und_blocked")
+
+#define TSN_SW_STATE_DISABLED		0
+#define TSN_SW_STATE_BLOCKING		1
+#define TSN_SW_STATE_LISTENING		2
+#define TSN_SW_STATE_LEARNING		3
+#define TSN_SW_STATE_FORWARDING		4
+#define TSN_SW_STATE_FLUSH		5
+
+static struct workqueue_struct *xlnx_sw_owq;
+static int
+xlnx_switchdev_port_attr_set_event(struct net_device *netdev,
+				   struct switchdev_notifier_port_attr_info *port_attr_info);
+
+static int xlnx_switch_fdb_set(struct axienet_local *lp,
+			       struct switchdev_notifier_fdb_info *fdb_info,
+			       bool adding)
+{
+	struct cam_struct data;
+
+	memset(&data, 0, sizeof(struct cam_struct));
+	data.fwd_port = lp->switch_prt;
+	ether_addr_copy((u8 *)&data.dest_addr, fdb_info->addr);
+	data.vlanid = fdb_info->vid;
+
+	return tsn_switch_cam_set(data, adding);
+}
+
+struct xlnx_switchdev_event_work {
+	struct work_struct work;
+	struct switchdev_notifier_fdb_info fdb_info;
+	struct axienet_local *lp;
+	unsigned long event;
+};
+
+static void xlnx_sw_fdb_offload_notify(struct axienet_local *lp,
+				       struct switchdev_notifier_fdb_info *recv_info)
+{
+	struct switchdev_notifier_fdb_info info;
+
+	info.addr = recv_info->addr;
+	info.vid = recv_info->vid;
+	call_switchdev_notifiers(SWITCHDEV_FDB_OFFLOADED,
+				 lp->ndev, &info.info, NULL);
+}
+
+static int xlnx_sw_port_obj_vlan_add(struct axienet_local *lp,
+				     const struct switchdev_obj_port_vlan *vlan)
+{
+	struct port_vlan pvl;
+	struct native_vlan nvl;
+	bool flag_pvid = vlan->flags & BRIDGE_VLAN_INFO_PVID;
+	u16 vid;
+	int err;
+
+	memset(&nvl, 0, sizeof(struct native_vlan));
+	memset(&pvl, 0, sizeof(struct port_vlan));
+
+	pvl.port_num = lp->switch_prt;
+	nvl.port_num = lp->switch_prt;
+
+	/* TODO deal with vlan->flags for PVID and untagged */
+	vid = vlan->vid;
+	if (flag_pvid) {
+		nvl.vlan_id = vid;
+		err = tsn_switch_pvid_add(&nvl);
+	} else {
+		pvl.vlan_id = vid;
+		err = tsn_switch_vlan_add(&pvl, true);
+	}
+	if (err)
+		return err;
+
+	return 0;
+}
+
+static int xlnx_sw_port_obj_vlan_del(struct axienet_local *lp,
+				     const struct switchdev_obj_port_vlan *vlan)
+{
+	struct port_vlan pvl;
+	struct native_vlan nvl;
+	u16 vid;
+	int err;
+
+	memset(&nvl, 0, sizeof(struct native_vlan));
+	memset(&pvl, 0, sizeof(struct port_vlan));
+
+	pvl.port_num = lp->switch_prt;
+	nvl.port_num = lp->switch_prt;
+
+	tsn_switch_pvid_get(&nvl);
+
+	vid = vlan->vid;
+	if (vid == nvl.vlan_id) {
+		nvl.vlan_id = 1;
+		err = tsn_switch_pvid_add(&nvl);
+	} else {
+		pvl.vlan_id = vid;
+		err = tsn_switch_vlan_add(&pvl, false);
+	}
+	if (err)
+		return err;
+
+	return 0;
+}
+
+static int xlnx_sw_obj_add(struct net_device *ndev,
+			   const struct switchdev_obj *obj)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int err = 0;
+
+	switch (obj->id) {
+	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+		err = xlnx_sw_port_obj_vlan_add(lp,
+						SWITCHDEV_OBJ_PORT_VLAN(obj));
+		break;
+	default:
+		err = -EOPNOTSUPP;
+		break;
+	}
+
+	return err;
+}
+
+static int xlnx_sw_obj_del(struct net_device *ndev,
+			   const struct switchdev_obj *obj)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int err = 0;
+
+	switch (obj->id) {
+	case SWITCHDEV_OBJ_ID_PORT_VLAN:
+		err = xlnx_sw_port_obj_vlan_del(lp,
+						SWITCHDEV_OBJ_PORT_VLAN(obj));
+		break;
+	default:
+		err = -EOPNOTSUPP;
+		break;
+	}
+
+	return err;
+}
+
+static void xlnx_switchdev_event_work(struct work_struct *work)
+{
+	struct xlnx_switchdev_event_work *switchdev_work =
+		container_of(work, struct xlnx_switchdev_event_work, work);
+	struct axienet_local *lp = switchdev_work->lp;
+	struct switchdev_notifier_fdb_info *fdb_info;
+	int err;
+
+	rtnl_lock();
+	switch (switchdev_work->event) {
+	case SWITCHDEV_FDB_ADD_TO_DEVICE:
+		fdb_info = &switchdev_work->fdb_info;
+		err = xlnx_switch_fdb_set(lp, fdb_info, true);
+		if (err) {
+			netdev_dbg(lp->ndev, "fdb add failed err=%d\n", err);
+			break;
+		}
+		xlnx_sw_fdb_offload_notify(lp, fdb_info);
+		break;
+	case SWITCHDEV_FDB_DEL_TO_DEVICE:
+		fdb_info = &switchdev_work->fdb_info;
+		err = xlnx_switch_fdb_set(lp, fdb_info, false);
+		if (err) {
+			netdev_dbg(lp->ndev, "fdb add failed err=%d\n", err);
+			break;
+		}
+		break;
+	}
+	rtnl_unlock();
+
+	kfree(switchdev_work->fdb_info.addr);
+	kfree(switchdev_work);
+	dev_put(lp->ndev);
+}
+
+static int xlnx_switchdev_event(struct notifier_block *unused,
+				unsigned long event, void *ptr)
+{
+	struct net_device *dev = switchdev_notifier_info_to_dev(ptr);
+	struct switchdev_notifier_fdb_info *fdb_info = ptr;
+	struct xlnx_switchdev_event_work *switchdev_work;
+	struct axienet_local *lp;
+
+	lp = netdev_priv(dev);
+	switchdev_work = kzalloc(sizeof(*switchdev_work), GFP_ATOMIC);
+	if (!switchdev_work)
+		return NOTIFY_BAD;
+
+	if (event == SWITCHDEV_PORT_ATTR_SET)
+		return xlnx_switchdev_port_attr_set_event(dev, ptr);
+
+	INIT_WORK(&switchdev_work->work, xlnx_switchdev_event_work);
+	switchdev_work->lp = lp;
+	switchdev_work->event = event;
+
+	switch (event) {
+	case SWITCHDEV_FDB_ADD_TO_DEVICE:
+	case SWITCHDEV_FDB_DEL_TO_DEVICE:
+		memcpy(&switchdev_work->fdb_info, ptr,
+		       sizeof(switchdev_work->fdb_info));
+		switchdev_work->fdb_info.addr = kzalloc(ETH_ALEN, GFP_ATOMIC);
+		ether_addr_copy((u8 *)switchdev_work->fdb_info.addr,
+				fdb_info->addr);
+		/* take a reference on the switch port dev */
+		dev_hold(dev);
+		break;
+	default:
+		kfree(switchdev_work);
+		return NOTIFY_DONE;
+	}
+
+	queue_work(xlnx_sw_owq, &switchdev_work->work);
+
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block xlnx_switchdev_notifier = {
+	.notifier_call = xlnx_switchdev_event,
+};
+
+static int
+xlnx_switchdev_port_obj_event(unsigned long event, struct net_device *netdev,
+			      struct switchdev_notifier_port_obj_info *port_obj_info)
+{
+	int err = -EOPNOTSUPP;
+
+	switch (event) {
+	case SWITCHDEV_PORT_OBJ_ADD:
+		err = xlnx_sw_obj_add(netdev, port_obj_info->obj);
+		break;
+	case SWITCHDEV_PORT_OBJ_DEL:
+		err = xlnx_sw_obj_del(netdev, port_obj_info->obj);
+		break;
+	}
+
+	port_obj_info->handled = true;
+
+	return notifier_from_errno(err);
+}
+
+static int xlnx_switchdev_blocking_event(struct notifier_block *unused, unsigned long event,
+					 void *ptr)
+{
+	struct net_device *dev = switchdev_notifier_info_to_dev(ptr);
+
+	switch (event) {
+	case SWITCHDEV_PORT_OBJ_ADD:
+	case SWITCHDEV_PORT_OBJ_DEL:
+		return xlnx_switchdev_port_obj_event(event, dev, ptr);
+	case SWITCHDEV_PORT_ATTR_SET:
+		return xlnx_switchdev_port_attr_set_event(dev, ptr);
+	}
+
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block xlnx_switchdev_blocking_notifier = {
+	.notifier_call = xlnx_switchdev_blocking_event,
+};
+
+static int xlnx_sw_port_attr_stp_state_set(struct axienet_local *lp, u8 state)
+{
+	struct port_status ps;
+
+	ps.port_num = lp->switch_prt;
+	ps.port_status = tsn_to_linux_sw_state(state);
+
+	return tsn_switch_set_stp_state(&ps);
+}
+
+static int xlnx_sw_port_attr_pre_bridge_flags_set(struct axienet_local *lp,
+						  struct switchdev_brport_flags brport_flags)
+{
+	if (brport_flags.mask & ~(BR_LEARNING | BR_FLOOD))
+		return -EINVAL;
+
+	return 0;
+}
+
+static int xlnx_sw_attr_set(struct net_device *ndev,
+			    const struct switchdev_attr *attr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	int err = 0;
+
+	switch (attr->id) {
+	case SWITCHDEV_ATTR_ID_PORT_STP_STATE:
+		err = xlnx_sw_port_attr_stp_state_set(lp,
+						      attr->u.stp_state);
+		break;
+	case SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS:
+		pr_info("received request to SWITCHDEV_ATTR_ID_PORT_BRIDGE_FLAGS: %lu\n",
+			attr->u.brport_flags.val);
+		break;
+	case SWITCHDEV_ATTR_ID_PORT_PRE_BRIDGE_FLAGS:
+		err = xlnx_sw_port_attr_pre_bridge_flags_set(lp, attr->u.brport_flags);
+		break;
+	case SWITCHDEV_ATTR_ID_BRIDGE_AGEING_TIME:
+		break;
+	case SWITCHDEV_ATTR_ID_BRIDGE_VLAN_FILTERING:
+		break;
+	default:
+		pr_info("%s: unhandled id: %d\n", __func__, attr->id);
+		err = -EOPNOTSUPP;
+		break;
+	}
+
+	return err;
+}
+
+static int
+xlnx_switchdev_port_attr_set_event(struct net_device *netdev,
+				   struct switchdev_notifier_port_attr_info *port_attr_info)
+{
+	int err;
+
+	err = xlnx_sw_attr_set(netdev, port_attr_info->attr);
+
+	port_attr_info->handled = true;
+
+	return notifier_from_errno(err);
+}
+
+int xlnx_switchdev_init(void)
+{
+	xlnx_sw_owq = alloc_ordered_workqueue("%s_ordered", WQ_MEM_RECLAIM,
+					      "xlnx_sw");
+	if (!xlnx_sw_owq)
+		return -ENOMEM;
+	register_switchdev_notifier(&xlnx_switchdev_notifier);
+	register_switchdev_blocking_notifier(&xlnx_switchdev_blocking_notifier);
+
+	return 0;
+}
+
+void xlnx_switchdev_remove(void)
+{
+	destroy_workqueue(xlnx_sw_owq);
+	unregister_switchdev_notifier(&xlnx_switchdev_notifier);
+	unregister_switchdev_blocking_notifier(&xlnx_switchdev_blocking_notifier);
+}
+#endif
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_tadma.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_tadma.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,735 @@
+// SPDX-License-Identifier: GPL-2.0
+
+/* Xilinx FPGA Xilinx TADMA driver.
+ *
+ * Copyright (C) 2018 Xilinx, Inc. All rights reserved.
+ *
+ * Author: Syed Syed <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/list.h>
+#include <linux/hash.h>
+#include "xilinx_axienet_tsn.h"
+#include "xilinx_tsn_tadma.h"
+
+/* max packets that can be sent in a time trigger */
+#define MAX_TRIG_COUNT 4
+
+/* This driver assumes the num_streams configured in HW is always 2^n */
+
+typedef u32 pm_entry_t;
+
+struct tadma_stream {
+	u8 dmac[6];
+	short vid;
+	u32 trigger;
+	u32 count;
+	u8 start;
+};
+
+static u8  tadma_hash_bits;
+struct tadma_stream_entry {
+	u8 macvlan[8];
+	u32 tticks;
+	struct hlist_node hash_link;
+	int sid;
+	int sfm;
+	int count;
+};
+
+static u32 get_sid;
+static u32 get_sfm;
+#define sfm_entry_offset(lp, sid) \
+	((lp->active_sfm == SFM_UPPER) ? \
+	(XTADMA_USFM_OFFSET) +  ((sid) * sizeof(struct sfm_entry)) : \
+	(XTADMA_LSFM_OFFSET) +  ((sid) * sizeof(struct sfm_entry)))
+
+#define STRID_BE 0
+
+static inline u32 tadma_macvlan_hash(const unsigned char *addr)
+{
+	u64 value = get_unaligned((u64 *)addr);
+
+	return hash_64(value, tadma_hash_bits);
+}
+
+static inline bool mac_vlan_equal(const u8 addr1[8],
+				  const u8 addr2[8])
+{
+	const u16 *a = (const u16 *)addr1;
+	const u16 *b = (const u16 *)addr2;
+	bool ret = 1;
+
+	if ((a[0] ^ b[0]) | (a[1] ^ b[1]) |
+		(a[2] ^ b[2]) | (a[3] ^ a[3])) {
+		ret = 0;
+	}
+
+	return ret;
+}
+
+static struct tadma_stream_entry *tadma_hash_lookup_stream(struct hlist_head *head,
+							   const unsigned char *mac_vlan)
+{
+	struct tadma_stream_entry *entry;
+
+	hlist_for_each_entry(entry, head, hash_link) {
+		if (mac_vlan_equal(entry->macvlan, mac_vlan))
+			return entry;
+	}
+
+	return NULL;
+}
+
+static u32 tadma_stream_alm_offset_irq(int sid, u32 tx_bd_rd, struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 alm_offset;
+
+	alm_offset = XTADMA_ALM_OFFSET +
+		sid * sizeof(struct alm_entry) * lp->num_tadma_buffers;
+
+	return alm_offset + (tx_bd_rd * sizeof(struct alm_entry));
+}
+
+static void tadma_xmit_done(struct net_device *ndev, u8 sid, u32 cnt)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 size = 0, packets = 0;
+	unsigned long flags;
+
+	spin_lock_irqsave(&lp->tadma_tx_lock, flags);
+
+	if (lp->tx_bd_head[sid] == lp->tx_bd_tail[sid]) {
+		spin_unlock_irqrestore(&lp->tadma_tx_lock, flags);
+		return;
+	}
+
+	while ((lp->tx_bd_head[sid] != lp->tx_bd_tail[sid]) && cnt) {
+		if (lp->tx_bd[sid][lp->tx_bd_tail[sid]].tx_desc_mapping ==
+		    DESC_DMA_MAP_PAGE) {
+			dma_unmap_page(ndev->dev.parent,
+				       lp->tx_bd[sid][lp->tx_bd_tail[sid]].phys,
+				       lp->tx_bd[sid][lp->tx_bd_tail[sid]].len,
+				       DMA_TO_DEVICE);
+		} else {
+			dma_unmap_single(ndev->dev.parent,
+					 lp->tx_bd[sid][lp->tx_bd_tail[sid]].
+					 phys,
+					 lp->tx_bd[sid][lp->tx_bd_tail[sid]].
+					 len,
+					 DMA_TO_DEVICE);
+		}
+		if (lp->tx_bd[sid][lp->tx_bd_tail[sid]].tx_skb) {
+			dev_kfree_skb_irq((struct sk_buff *)
+					  lp->tx_bd[sid][lp->tx_bd_tail[sid]].
+					  tx_skb);
+		}
+
+		size += lp->tx_bd[sid][lp->tx_bd_tail[sid]].len;
+		packets++;
+		lp->tx_bd_tail[sid]++;
+		lp->tx_bd_tail[sid] %= lp->num_tadma_buffers;
+		cnt--;
+	}
+
+	ndev->stats.tx_packets += packets;
+	ndev->stats.tx_bytes += size;
+	spin_unlock_irqrestore(&lp->tadma_tx_lock, flags);
+}
+
+static irqreturn_t tadma_irq(int irq, void *_ndev)
+{
+	struct axienet_local *lp = netdev_priv(_ndev);
+	struct alm_entry alm;
+	u32 status, sid = 0, alm_offset;
+	int cnt = 0;
+
+	status = tadma_ior(lp, XTADMA_INT_STA_OFFSET);
+
+	/* clear interrupt */
+	tadma_iow(lp, XTADMA_INT_CLR_OFFSET, status);
+
+	if (status & XTADMA_FFI_INT_EN) {
+		for (sid = 0; sid < lp->num_streams; sid++) {
+			cnt = 0;
+			alm_offset = tadma_stream_alm_offset_irq(sid, lp->tx_bd_rd[sid], _ndev);
+			alm.cfg = tadma_ior(lp, alm_offset + 4);
+			while (((alm.cfg & XTADMA_ALM_UFF) == 0) &&
+			       (cnt < lp->num_tadma_buffers) &&
+			       (lp->tx_bd_rd[sid] != lp->tx_bd_head[sid])) {
+				lp->tx_bd_rd[sid]++;
+				lp->tx_bd_rd[sid] = lp->tx_bd_rd[sid] % lp->num_tadma_buffers;
+				alm_offset = tadma_stream_alm_offset_irq(sid,
+									 lp->tx_bd_rd[sid], _ndev);
+				alm.cfg = tadma_ior(lp, alm_offset + 4);
+				cnt++;
+			}
+			if (cnt)
+				tadma_xmit_done(_ndev, sid, cnt);
+		}
+	}
+	netif_tx_wake_all_queues(_ndev);
+
+	return IRQ_HANDLED;
+}
+
+static int tadma_sfm_hash_init(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct tadma_cb *cb;
+	int i;
+
+	cb = kzalloc(sizeof(*cb), GFP_KERNEL);
+	if (!cb)
+		return -ENOMEM;
+
+	cb->stream_hash = kcalloc(lp->num_entries, sizeof(struct hlist_head *),
+				  GFP_KERNEL);
+
+	if (!cb->stream_hash) {
+		kfree(cb);
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < lp->num_entries; i++)
+		INIT_HLIST_HEAD(&cb->stream_hash[i]);
+
+	lp->t_cb = cb;
+
+	return 0;
+}
+
+static void tadma_sfm_program(struct net_device *ndev,
+			      int sid, u32 tticks, u32 count)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct sfm_entry sfm = {0, };
+	u32 offset;
+
+	pr_debug("%s entry: %d, count: %d\n", __func__, sid, count);
+	offset = sfm_entry_offset(lp, sid);
+
+	/* each tick is 8ns */
+	sfm.tticks = tticks / 8;
+
+	/*clear strid and queue type */
+	sfm.cfg &= ~(XTADMA_STR_ID_MASK | XTADMA_STR_QUE_TYPE_MASK);
+
+	sfm.cfg |= (sid << XTADMA_STR_ID_SHIFT) & XTADMA_STR_ID_MASK;
+	sfm.cfg |= (qt_st << XTADMA_STR_QUE_TYPE_SHIFT) &
+		   XTADMA_STR_QUE_TYPE_MASK;
+	if (count != 0)
+		sfm.cfg &= ~XTADMA_STR_CONT_FETCH_EN;
+	else
+		sfm.cfg |= XTADMA_STR_CONT_FETCH_EN;
+	sfm.cfg |= XTADMA_STR_ENTRY_VALID;
+
+	count  = (count > 0) ? (count - 1) : count;
+	/* hw xmits 1 more than what is programmed, so use count */
+	sfm.cfg |= (count << XTADMA_STR_NUM_FRM_SHIFT) &
+			XTADMA_STR_NUM_FRM_MASK;
+	pr_debug("sfm cfg: %x\n", sfm.cfg);
+	tadma_iow(lp, offset, sfm.tticks);
+	tadma_iow(lp, offset + 4, sfm.cfg);
+}
+
+static int tadma_sfm_init(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	lp->active_sfm = SFM_UPPER;
+
+	tadma_sfm_program(ndev, STRID_BE, NSEC_PER_MSEC, 0);
+
+	return 0;
+}
+
+int axienet_tadma_stop(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u8 i = 0;
+
+	for (i = 0; i < lp->num_streams ; i++)
+		kfree(lp->tx_bd[i]);
+
+	kfree(lp->tx_bd);
+
+	free_irq(lp->tadma_irq, ndev);
+
+	return 0;
+}
+
+int axienet_tadma_open(struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 cr;
+	u8 i = 0;
+	int ret;
+	static char irq_name[24];
+
+	if (lp->tadma_irq) {
+		sprintf(irq_name, "%s_tadma_tx", ndev->name);
+		ret = request_irq(lp->tadma_irq, tadma_irq, IRQF_SHARED,
+				  irq_name, ndev);
+		if (ret)
+			return ret;
+	}
+	pr_debug("%s TADMA irq %d\n", __func__, lp->tadma_irq);
+
+	/* enable all interrupts */
+	tadma_iow(lp, XTADMA_INT_EN_OFFSET, XTADMA_FFI_INT_EN |
+		  XTADMA_IE_INT_EN);
+
+	tadma_sfm_init(ndev);
+	tadma_sfm_hash_init(ndev);
+	cr = XTADMA_CFG_DONE;
+	tadma_iow(lp, XTADMA_CR_OFFSET, cr);
+
+	lp->tx_bd = kmalloc_array(lp->num_streams, sizeof(struct axitadma_bd *),
+				  GFP_KERNEL);
+	if (!lp->tx_bd)
+		ret = -ENOMEM;
+
+	for (i = 0; i < lp->num_streams ; i++) {
+		lp->tx_bd_head[i] = 0;
+		lp->tx_bd_tail[i] = 0;
+		lp->tx_bd_rd[i] = 0;
+		lp->tx_bd[i] = kmalloc_array(lp->num_tadma_buffers,
+					     sizeof(*lp->tx_bd[i]), GFP_KERNEL);
+		if (!lp->tx_bd[i])
+			return -ENOMEM;
+	}
+
+	return ret;
+}
+
+/*TODO: Fix TADMA probe error handling path */
+int __maybe_unused axienet_tadma_probe(struct platform_device *pdev,
+				       struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct resource tadma_res;
+	struct device_node *np;
+	u16 num_tc = XAE_MAX_TSN_TC;
+	u8 count = 0;
+	int ret;
+
+	ret = of_property_read_u16(pdev->dev.of_node, "xlnx,num-tc",
+				   &num_tc);
+	np = of_parse_phandle(pdev->dev.of_node, "axistream-connected-tx",
+			      num_tc - 1);
+
+	if (np) {
+		ret = of_address_to_resource(np, 0, &tadma_res);
+		if (ret >= 0)
+			lp->tadma_regs = devm_ioremap_resource(&pdev->dev,
+							       &tadma_res);
+		else
+			return -ENODEV;
+	} else {
+		return -ENODEV;
+	}
+
+	lp->tadma_irq = irq_of_parse_and_map(np, 0);
+
+	ret = of_property_read_u32(np, "xlnx,num-buffers-per-stream",
+				   &lp->num_tadma_buffers);
+	if (ret)
+		lp->num_tadma_buffers = 64;
+
+	ret = of_property_read_u32(np, "xlnx,num-streams", &lp->num_streams);
+	if (ret)
+		lp->num_streams = 8;
+	ret = of_property_read_u32(np, "xlnx,num-fetch-entries",
+				   &lp->num_entries);
+	if (ret)
+		lp->num_entries = 8;
+
+	while (!((lp->num_streams >> count) & 1))
+		count++;
+
+	tadma_hash_bits = count;
+	pr_debug("%s num_stream: %d hash_bits: %d\n", __func__, lp->num_streams,
+		 tadma_hash_bits);
+	pr_info("TADMA probe done\n");
+	spin_lock_init(&lp->tadma_tx_lock);
+	of_node_put(np);
+
+	return 0;
+}
+
+static inline void tadma_pm_inc(int sid, struct axienet_local *lp)
+{
+	pm_entry_t pm;
+	u32 offset;
+	u8 wr;
+
+	offset = XTADMA_PM_OFFSET + (sid * sizeof(pm_entry_t));
+	pm = tadma_ior(lp, offset);
+	wr = (pm & XTADMA_PM_WR_MASK) >> XTADMA_PM_WR_SHIFT;
+	wr = (wr + 1) % lp->num_tadma_buffers;
+	pm &= ~XTADMA_PM_WR_MASK;
+	pm |= (wr << XTADMA_PM_WR_SHIFT);
+
+	tadma_iow(lp, offset, pm);
+}
+
+static int axienet_check_pm_space(int sid, int num_frag,
+				  u32 wr, u32 rd, int total)
+{
+	int avail;
+
+	avail = rd - wr;
+
+	if (avail < 0)
+		avail = total + avail;
+
+	return (avail >= num_frag);
+}
+
+static int tadma_get_strid(struct sk_buff *skb,
+			   struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct tadma_cb *cb = lp->t_cb;
+	struct tadma_stream_entry *entry;
+	struct vlan_ethhdr *vhdr = (struct vlan_ethhdr *)skb->data;
+	int sid = -1; /* BE entry is always 0 */
+	u32 idx;
+	u16 vlan_tci;
+	u8 mac_vlan[8];
+
+	if (!get_sid)
+		return 0;
+
+	memcpy(mac_vlan, vhdr->h_dest, 6);
+
+	vlan_tci = ntohs(vhdr->h_vlan_TCI);
+
+	mac_vlan[6] = (vlan_tci >> 8) & 0x0f;
+	mac_vlan[7] = (vlan_tci & 0xff);
+
+	idx = tadma_macvlan_hash(mac_vlan);
+	entry = tadma_hash_lookup_stream(&cb->stream_hash[idx],
+					 mac_vlan);
+	if (entry)
+		return entry->sid;
+
+	return sid;
+}
+
+static u32 tadma_stream_alm_offset(int sid, u32 wr, struct net_device *ndev)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	u32 alm_offset;
+
+	alm_offset = XTADMA_ALM_OFFSET +
+		sid * sizeof(struct alm_entry) * lp->num_tadma_buffers;
+
+	wr = (wr + lp->num_tadma_buffers - 1) & (lp->num_tadma_buffers - 1);
+
+	return alm_offset + (wr * sizeof(struct alm_entry));
+}
+
+int axienet_tadma_xmit(struct sk_buff *skb, struct net_device *ndev,
+		       u16 queue_type)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct alm_entry alm, alm_fframe = {0};
+	dma_addr_t phys_addr;
+	pm_entry_t pm;
+	u32 num_frag, len, tot_len, alm_offset, alm_offset_fframe, write_p, read_p;
+	unsigned long flags;
+	int sid, ii, tot_sz8;
+	static int chk_ptr;
+
+	/* fetch stream ID */
+	sid = tadma_get_strid(skb, ndev);
+
+	if (sid < 0) {
+		dev_kfree_skb_irq(skb);
+		return NETDEV_TX_OK;
+	}
+	num_frag = skb_shinfo(skb)->nr_frags;
+
+	spin_lock_irqsave(&lp->tadma_tx_lock, flags);
+	pm = tadma_ior(lp, XTADMA_PM_OFFSET + (sid * sizeof(pm_entry_t)));
+
+	read_p  = pm & XTADMA_PM_RD_MASK;
+	write_p = (pm & XTADMA_PM_WR_MASK) >> XTADMA_PM_WR_SHIFT;
+
+	if (!axienet_check_pm_space(sid, num_frag + 1, write_p, read_p,
+				    lp->num_tadma_buffers)) {
+		if (!chk_ptr) {
+			pr_err("%s NO SPACE rd: %x wd: %x\n", __func__, read_p,
+			       write_p);
+			chk_ptr = 1;
+		}
+		spin_unlock_irqrestore(&lp->tadma_tx_lock, flags);
+		return NETDEV_TX_BUSY;
+	}
+	if (((lp->tx_bd_head[sid] + (num_frag + 1)) % lp->num_tadma_buffers) ==
+	    lp->tx_bd_tail[sid]) {
+		if (!__netif_subqueue_stopped(ndev, queue_type))
+			netif_stop_subqueue(ndev, queue_type);
+		spin_unlock_irqrestore(&lp->tadma_tx_lock, flags);
+		return NETDEV_TX_BUSY;
+	}
+
+	/* get current alm offset */
+	alm_offset_fframe = tadma_stream_alm_offset(sid, write_p, ndev);
+
+	pr_debug("%d: num_frag: %d len: %d\n", sid, num_frag,
+		 skb_headlen(skb));
+	pr_debug("w:%d r:%d\n", write_p, read_p);
+
+	tot_len = skb_headlen(skb);
+	len = skb_headlen(skb);
+	phys_addr = dma_map_single(ndev->dev.parent, skb->data,
+				   len, DMA_TO_DEVICE);
+	if (unlikely(dma_mapping_error(ndev->dev.parent, phys_addr))) {
+		dev_err(&ndev->dev, "tadma map error\n");
+		spin_unlock_irqrestore(&lp->tadma_tx_lock, flags);
+		return NETDEV_TX_BUSY;
+	}
+
+	alm_fframe.addr = (u32)phys_addr;
+
+#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT
+	alm_fframe.cfg |= (u32)(phys_addr >> 32);
+#endif
+
+	lp->tx_bd[sid][lp->tx_bd_head[sid]].num_frag = num_frag + 1;
+	if (num_frag == 0) {
+		lp->tx_bd[sid][lp->tx_bd_head[sid]].tx_skb = (phys_addr_t)skb;
+		alm_fframe.cfg |= XTADMA_ALM_SOP | XTADMA_ALM_EOP;
+	} else {
+		lp->tx_bd[sid][lp->tx_bd_head[sid]].tx_skb = 0;
+		alm_fframe.cfg |= XTADMA_ALM_SOP;
+	}
+	alm_fframe.cfg &= ~XTADMA_ALM_FETCH_SZ_MASK;
+	alm_fframe.cfg |= ((len << XTADMA_ALM_FETCH_SZ_SHIFT) &
+			   XTADMA_ALM_FETCH_SZ_MASK);
+	lp->tx_bd[sid][lp->tx_bd_head[sid]].phys = phys_addr;
+	lp->tx_bd[sid][lp->tx_bd_head[sid]].len = len;
+	lp->tx_bd[sid][lp->tx_bd_head[sid]].tx_desc_mapping =
+							DESC_DMA_MAP_SINGLE;
+	lp->tx_bd_head[sid]++;
+	lp->tx_bd_head[sid] %= lp->num_tadma_buffers;
+
+	for (ii = 0; ii < num_frag; ii++) {
+		skb_frag_t *frag;
+
+		frag = &skb_shinfo(skb)->frags[ii];
+		len = skb_frag_size(frag);
+		tot_len += len;
+		phys_addr = skb_frag_dma_map(ndev->dev.parent, frag, 0,
+					     len, DMA_TO_DEVICE);
+		memset(&alm, 0, sizeof(struct alm_entry));
+		alm.addr = (u32)phys_addr;
+#ifdef CONFIG_ARCH_DMA_ADDR_T_64BIT
+		alm.cfg |= (u32)(phys_addr >> 32);
+#endif
+		lp->tx_bd[sid][lp->tx_bd_head[sid]].tx_skb = 0;
+		if (ii == (num_frag - 1)) {
+			alm.cfg |= XTADMA_ALM_EOP;
+			lp->tx_bd[sid][lp->tx_bd_head[sid]].tx_skb =
+							(phys_addr_t)skb;
+		}
+		alm.cfg &= ~XTADMA_ALM_FETCH_SZ_MASK;
+		alm.cfg |= ((len << XTADMA_ALM_FETCH_SZ_SHIFT) &
+				XTADMA_ALM_FETCH_SZ_MASK);
+		alm.cfg |= XTADMA_ALM_UFF;
+		lp->tx_bd[sid][lp->tx_bd_head[sid]].num_frag = 0;
+		lp->tx_bd[sid][lp->tx_bd_head[sid]].phys = phys_addr;
+		lp->tx_bd[sid][lp->tx_bd_head[sid]].len = len;
+		lp->tx_bd[sid][lp->tx_bd_head[sid]].tx_desc_mapping =
+							DESC_DMA_MAP_PAGE;
+		lp->tx_bd_head[sid]++;
+		lp->tx_bd_head[sid] %= lp->num_tadma_buffers;
+
+		/* increment write */
+		write_p = (write_p + 1) & (lp->num_tadma_buffers - 1);
+		/* get current alm offset */
+		alm_offset = tadma_stream_alm_offset(sid, write_p, ndev);
+
+		tadma_iow(lp, alm_offset, alm.addr);
+		tadma_iow(lp, alm_offset + 4, alm.cfg);
+	}
+	tot_sz8 = tot_len / 8 + 1;
+	alm_fframe.cfg &= ~XTADMA_ALM_TOT_PKT_SZ_BY8_MASK;
+	alm_fframe.cfg |= ((tot_sz8 << XTADMA_ALM_TOT_PKT_SZ_BY8_SHIFT) &
+			  XTADMA_ALM_TOT_PKT_SZ_BY8_MASK);
+	alm_fframe.cfg |= XTADMA_ALM_UFF;
+
+	tadma_iow(lp, alm_offset_fframe, alm_fframe.addr);
+	tadma_iow(lp, alm_offset_fframe + 4, alm_fframe.cfg);
+
+	/* increment write */
+	write_p = (write_p + 1) & (lp->num_tadma_buffers - 1);
+
+	pm &= ~XTADMA_PM_WR_MASK;
+	pm |= (write_p << XTADMA_PM_WR_SHIFT);
+
+	tadma_iow(lp, (XTADMA_PM_OFFSET + (sid * sizeof(pm_entry_t))), pm);
+	spin_unlock_irqrestore(&lp->tadma_tx_lock, flags);
+
+	return NETDEV_TX_OK;
+}
+
+int axienet_tadma_program(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct tadma_cb *cb = lp->t_cb;
+	struct tadma_stream_entry *entry;
+	struct hlist_head *bucket;
+	struct hlist_node *tmp;
+	u32 cr, hash = 0;
+	int ret = 0;
+
+	for (hash = 0; hash < lp->num_entries; hash++) {
+		bucket = &cb->stream_hash[hash];
+		hlist_for_each_entry_safe(entry, tmp, bucket, hash_link) {
+			tadma_sfm_program(ndev, entry->sfm,
+					  entry->tticks, entry->count);
+		}
+	}
+	/* flip memory first so access other sfm bank
+	 * cr = tadma_ior(lp, XTADMA_CR_OFFSET);
+	 * cr |= XTADMA_FLIP_FETCH_MEM;
+	 * tadma_iow(lp, XTADMA_CR_OFFSET, cr);
+	 */
+
+	/* re-enable interrupts */
+	tadma_iow(lp, XTADMA_INT_EN_OFFSET, XTADMA_FFI_INT_EN |
+		  XTADMA_IE_INT_EN);
+	/* enable schedule */
+	cr = XTADMA_CFG_DONE | XTADMA_SCHED_ENABLE;
+	tadma_iow(lp, XTADMA_CR_OFFSET, cr);
+
+	return ret;
+}
+
+int axienet_tadma_off(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+
+	tadma_iow(lp, XTADMA_INT_EN_OFFSET, XTADMA_FFI_INT_EN |
+		  XTADMA_IE_INT_EN);
+	tadma_sfm_program(ndev, STRID_BE, NSEC_PER_MSEC, 0);
+	tadma_iow(lp, XTADMA_CR_OFFSET, XTADMA_CFG_DONE);
+	get_sid = 0;
+	return 0;
+}
+
+int axienet_tadma_flush_stream(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct tadma_cb *cb = lp->t_cb;
+	struct tadma_stream_entry *entry;
+	struct hlist_head *bucket;
+	struct hlist_node *tmp;
+	u32 offset;
+	int hash;
+
+	/* set CFG_DONE to 0 */
+	tadma_iow(lp, XTADMA_CR_OFFSET, 0);
+
+	for (hash = 0; hash < lp->num_entries; hash++) {
+		offset = sfm_entry_offset(lp, hash);
+		tadma_iow(lp, offset, 0);
+		tadma_iow(lp, offset + 4, 0);
+
+		bucket = &cb->stream_hash[hash];
+		hlist_for_each_entry_safe(entry, tmp, bucket, hash_link) {
+			hlist_del(&entry->hash_link);
+			kfree(entry);
+		}
+	}
+
+	return 0;
+}
+
+int axienet_tadma_add_stream(struct net_device *ndev, void __user *useraddr)
+{
+	struct axienet_local *lp = netdev_priv(ndev);
+	struct tadma_stream stream;
+	struct tadma_cb *cb = lp->t_cb;
+	struct tadma_stream_entry *entry;
+	u32 idx, sid;
+	u16 vlan_tci;
+	u8 mac_vlan[8];
+	int st_pcp_val;
+
+	if (copy_from_user(&stream, useraddr, sizeof(struct tadma_stream)))
+		return -EFAULT;
+
+	if (stream.count > MAX_TRIG_COUNT)
+		return -EINVAL;
+
+	if (stream.start) {
+		get_sid = 0;
+		get_sfm = 0;
+	}
+
+	memcpy(mac_vlan, stream.dmac, 6);
+
+	for (st_pcp_val = 0; st_pcp_val < 8; st_pcp_val++) {
+		if (lp->st_pcp & (1 << st_pcp_val))
+			break;
+	}
+	vlan_tci = stream.vid & VLAN_VID_MASK;
+	vlan_tci |= (st_pcp_val << VLAN_PRIO_SHIFT) & VLAN_PRIO_MASK;
+	mac_vlan[6] = (vlan_tci >> 8) & 0x0f;
+	mac_vlan[7] = (vlan_tci & 0xff);
+
+	idx = tadma_macvlan_hash(mac_vlan);
+
+	entry = tadma_hash_lookup_stream(&cb->stream_hash[idx], mac_vlan);
+	if (entry && entry->count == stream.count &&
+	    entry->tticks == stream.trigger) {
+		return -EEXIST;	/*same entry*/
+	}
+
+	if (entry)
+		sid = entry->sid;	/*same sid diff entry*/
+	else
+		sid = get_sid++;
+
+	if (sid >= lp->num_streams) {
+		pr_err("More no. of streams %d\n", sid);
+		return -EINVAL;
+	}
+	if (get_sfm >= lp->num_entries) {
+		pr_err("\nMore no. of entries %d\n", get_sfm + 1);
+		return -EINVAL;
+	}
+
+	entry = kzalloc(sizeof(*entry), GFP_KERNEL);
+	if (!entry)
+		return -ENOMEM;
+
+	entry->tticks = stream.trigger;
+	entry->count = stream.count;
+	entry->sid = sid;
+	memcpy(entry->macvlan, mac_vlan, 8);
+	entry->sfm = get_sfm++;
+
+	pr_debug("%s sid: %d\n", __func__, sid);
+	hlist_add_head(&entry->hash_link, &cb->stream_hash[idx]);
+
+	return 0;
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_tadma.h
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_tadma.h	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,132 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Definitions for TSN TADMA implementation
+ */
+#ifndef _XTADMA_TSN_H
+#define _XTADMA_TSN_H
+
+/* upper&lower stream fetch memory offsets */
+#define XTADMA_USFM_OFFSET	0x1000
+#define XTADMA_LSFM_OFFSET	0x2000
+
+/* pointers memory offset */
+#define XTADMA_PM_OFFSET	0x3000
+#define XTADMA_PM_RD_MASK	0xFF
+#define XTADMA_PM_WR_MASK	0xFF0000
+#define XTADMA_PM_WR_SHIFT	16
+
+/* Address length memory offset */
+#define XTADMA_ALM_OFFSET	0x40000
+
+#define XTADMA_CR_OFFSET		0x0
+#define XTADMA_TO_OFFSET		0x4
+#define XTADMA_FF_THRE_OFFSET		0x8
+#define XTADMA_STR_ID_OFFSET		0xC
+#define XTADMA_INT_EN_OFFSET		0x10
+#define XTADMA_INT_STA_OFFSET		0x14
+#define XTADMA_INT_CLR_OFFSET		0x18
+#define XTADMA_EDI_FFI_STAT_OFFSET	0x20
+#define XTADMA_NRDFI_FNDI_STAT_OFFSET	0x24
+#define XTADMA_BEI_STNSI_STAT_OFFSET	0x28
+#define XTADMA_BENSI_RESNSI_STAT_OFFSET	0x2C
+#define XTADMA_SEI_DEI_STAT_OFFSET	0x30
+#define XTADMA_IEI_STAT_OFFSET		0x34
+
+#define XTADMA_HALTED		BIT(5)
+#define XTADMA_SCHED_ENABLE	BIT(4)
+#define XTADMA_FLIP_FETCH_MEM	BIT(3)
+#define XTADMA_SKIP_DEL_ENTRY	BIT(2)
+#define XTADMA_SOFT_RST		BIT(1)
+#define XTADMA_CFG_DONE		BIT(0)
+
+#define XTADMA_OFFSET_TIME_SHIFT	16
+#define XTADMA_OFFSET_TIME_MASK		(0xFFFF)
+
+#define XTADMA_ENT_NUM_SEC_INTR_SHIFT	16
+#define XTADMA_ENT_NUM_SEC_INTR_MASK	(0xFF)
+#define XTADMA_FRAME_THRES_SHIFT	8
+#define XTADMA_FRAME_THRES_MASK		(0xFF)
+
+#define XTADMA_FIX_RES_QUEUE_ID_SHIFT	16
+#define XTADMA_FIX_RES_QUEUE_ID_MASK	(0xFF0000)
+#define XTADMA_FIX_BE_QUEUE_ID_SHIFT	0
+#define XTADMA_FIX_BE_QUEUE_ID_MASK	(0xFF)
+
+#define XTADMA_SEC_COMP_INT_EN		BIT(12)
+#define XTADMA_IE_INT_EN		BIT(11)
+#define XTADMA_SEI_INT_EN		BIT(10)
+#define XTADMA_DEI_INT_EN		BIT(9)
+#define XTADMA_BENSI_INT_EN		BIT(8)
+#define XTADMA_RESNSI_INT_EN		BIT(7)
+#define XTADMA_STNSI_INT_EN		BIT(6)
+#define XTADMA_BEI_INT_EN		BIT(5)
+#define XTADMA_NRDFI_INT_EN		BIT(4)
+#define XTADMA_FNDI_INT_EN		BIT(3)
+#define XTADMA_CDI_INT_EN		BIT(2)
+#define XTADMA_EDI_INT_EN		BIT(1)
+#define XTADMA_FFI_INT_EN		BIT(0)
+#define XTADMA_INT_EN_ALL_MASK		(0x1FFF)
+
+#define XTADMA_STR_FETCH_ENTRY_SIZE	64
+#define XTADMA_STR_TIME_TICKS_SHIFT	0
+#define XTADMA_STR_TIME_TICKS_MASK	(0x7FFFFFF)
+
+#define XTADMA_STR_ID_SHIFT		0
+#define XTADMA_STR_ID_MASK		0xFF
+#define XTADMA_STR_NUM_FRM_SHIFT	16
+#define XTADMA_STR_NUM_FRM_MASK		0x30000
+#define XTADMA_STR_QUE_TYPE_SHIFT	20
+#define XTADMA_STR_QUE_TYPE_MASK	0x300000
+#define XTADMA_STR_CONT_FETCH_EN	BIT(22)
+#define XTADMA_STR_ENTRY_VALID		BIT(31)
+
+#define XTADMA_ALM_ADDR_MSB_SHIFT	0
+#define XTADMA_ALM_ADDR_MSB_MASK	0xFF
+#define XTADMA_ALM_TOT_PKT_SZ_BY8_SHIFT	8
+#define XTADMA_ALM_TOT_PKT_SZ_BY8_MASK	0xFF00
+#define XTADMA_ALM_FETCH_SZ_SHIFT	16
+#define XTADMA_ALM_FETCH_SZ_MASK	0xFFF0000
+#define XTADMA_ALM_UFF			BIT(28)
+#define XTADMA_ALM_SOP			BIT(30)
+#define XTADMA_ALM_EOP			BIT(31)
+
+#define SFM_UPPER 0
+#define SFM_LOWER 1
+
+
+enum qtype {
+	qt_st = 0,
+	qt_res,
+	qt_be,
+	qt_resbe
+};
+
+/* address/length memory entry */
+struct alm_entry {
+	u32 addr;
+	u32 cfg;
+};
+
+/* stream fetch entry */
+struct sfm_entry {
+	u32 tticks;
+	u32 cfg;
+};
+
+struct tadma_cb {
+	struct hlist_head *stream_hash;
+	int streams;
+	u32 be_trigger;
+};
+
+static inline u32 tadma_ior(struct axienet_local *lp, off_t offset)
+{
+	return ioread32(lp->tadma_regs + offset);
+}
+
+static inline void tadma_iow(struct axienet_local *lp, off_t offset,
+			     u32 value)
+{
+	iowrite32(value, (lp->tadma_regs + offset));
+}
+#endif /* _XTADMA_TSN_H */
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_timer.h
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx-tsn/xilinx_tsn_timer.h	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,74 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Xilinx FPGA Xilinx TSN timer module header.
+ *
+ * Copyright (c) 2017 Xilinx Pvt., Ltd
+ *
+ * Author: Syed S <syeds@xilinx.com>
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef _XILINX_TSN_H_
+#define _XILINX_TSN_H_
+
+#include <linux/platform_device.h>
+
+#define XAE_RTC_OFFSET			0x12800
+/* RTC Nanoseconds Field Offset Register */
+#define XTIMER1588_RTC_OFFSET_NS	0x00000
+/* RTC Seconds Field Offset Register - Low */
+#define XTIMER1588_RTC_OFFSET_SEC_L	0x00008
+/* RTC Seconds Field Offset Register - High */
+#define XTIMER1588_RTC_OFFSET_SEC_H	0x0000C
+/* RTC Increment */
+#define XTIMER1588_RTC_INCREMENT	0x00010
+/* Current TOD Nanoseconds - RO */
+#define XTIMER1588_CURRENT_RTC_NS	0x00014
+/* Current TOD Seconds -Low RO  */
+#define XTIMER1588_CURRENT_RTC_SEC_L	0x00018
+/* Current TOD Seconds -High RO */
+#define XTIMER1588_CURRENT_RTC_SEC_H	0x0001C
+#define XTIMER1588_SYNTONIZED_NS	0x0002C
+#define XTIMER1588_SYNTONIZED_SEC_L	0x00030
+#define XTIMER1588_SYNTONIZED_SEC_H	0x00034
+/* Write to Bit 0 to clear the interrupt */
+#define XTIMER1588_INTERRUPT		0x00020
+/* 8kHz Pulse Offset Register */
+#define XTIMER1588_8KPULSE		0x00024
+/* Correction Field - Low */
+#define XTIMER1588_CF_L			0x0002C
+/* Correction Field - Low */
+#define XTIMER1588_CF_H			0x00030
+
+#define XTIMER1588_RTC_MASK  ((1 << 26) - 1)
+#define XTIMER1588_INT_SHIFT 0
+#define NANOSECOND_BITS 20
+#define NANOSECOND_MASK ((1 << NANOSECOND_BITS) - 1)
+#define SECOND_MASK ((1 << (32 - NANOSECOND_BITS)) - 1)
+#define XTIMER1588_RTC_INCREMENT_SHIFT 20
+#define PULSESIN1PPS 128
+
+/* Read/Write access to the registers */
+#ifndef out_be32
+#if defined(CONFIG_ARCH_ZYNQ) || defined(CONFIG_ARCH_ZYNQMP)
+#define in_be32(offset)		__raw_readl(offset)
+#define out_be32(offset, val)	__raw_writel(val, offset)
+#endif
+#endif
+
+/* The tsn ptp module will set this variable */
+extern int axienet_phc_index;
+
+void *axienet_ptp_timer_probe(void __iomem *base,
+			      struct platform_device *pdev);
+int axienet_ptp_timer_remove(void *priv);
+int axienet_get_phc_index(void *priv);
+#endif
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx_hdcp/Kconfig
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx_hdcp/Kconfig	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,8 @@
+config XILINX_HDCP_COMMON
+	bool
+	help
+	  This driver supports HDCP 1x/2x common functionalities for
+	  Xilinx devices
+
+	  This code will be compiled by selection from the interface
+	  drivers (DisplayPort/HDMI) of Xilinx.
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx_hdcp/MAINTAINERS
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx_hdcp/MAINTAINERS	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,4 @@
+XILINX HDCP COMMON DRIVER
+M:	Lakshmi Prasanna Eachuri <lakshmi.prasanna.eachuri@xilinx.com>
+S:	Maintained
+F:	drivers/staging/xilinx_hdcp
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx_hdcp/Makefile
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx_hdcp/Makefile	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,4 @@
+obj-$(CONFIG_XILINX_HDCP_COMMON) += xlnx_hdcp_rng.o \
+         xlnx_timer.o \
+         xlnx_hdcp_bigdigits.o \
+         xlnx_hdcp2x_cipher.o \
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx_hdcp/xlnx_hdcp2x_cipher.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx_hdcp/xlnx_hdcp2x_cipher.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,89 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx HDCP2X Cipher driver
+ *
+ * Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Author: Lakshmi Prasanna Eachuri <lakshmi.prasanna.eachuri@amd.com>
+ *
+ * This driver initializes the Cipher engine to implement AES-128
+ * standard for encrypting and decrypting the audiovisual content.
+ * The Cipher is required to be programmed with the Lc128, random number Riv,
+ * and session key Ks before encryption is enabled.
+ */
+
+#include <linux/io.h>
+#include <linux/xlnx/xlnx_hdcp2x_cipher.h>
+
+#define swap_bytes(i, buf, ptr, len) \
+do {\
+	typeof(i) (y) = (i); \
+	typeof(len) (x) = (len); \
+	for (y = 0; y < (x); y++) {\
+		buf[(((x) - 1) - y)] = ptr[y]; \
+	} \
+} while (0)
+
+void  xlnx_hdcp2x_tx_cipher_update_encryption(struct xlnx_hdcp2x_cipher_hw *cipher_cfg,
+					      u8 enable)
+{
+	if (enable)
+		xlnx_hdcp2x_cipher_write(cipher_cfg->cipher_coreaddress,
+					 XHDCP2X_CIPHER_REG_CTRL_SET_OFFSET,
+					 XHDCP2X_CIPHER_REG_CTRL_ENCRYPT_MASK);
+	else
+		xlnx_hdcp2x_cipher_write(cipher_cfg->cipher_coreaddress,
+					 XHDCP2X_CIPHER_REG_CTRL_CLR_OFFSET,
+					 XHDCP2X_CIPHER_REG_CTRL_ENCRYPT_MASK);
+}
+
+void xlnx_hdcp2x_cipher_init(struct xlnx_hdcp2x_cipher_hw *cipher_cfg)
+{
+	xlnx_hdcp2x_cipher_enable(cipher_cfg->cipher_coreaddress);
+	xlnx_hdcp2x_cipher_set_txmode(cipher_cfg->cipher_coreaddress);
+	xlnx_hdcp2x_tx_cipher_update_encryption(cipher_cfg, 0);
+	xlnx_hdcp2x_cipher_disable(cipher_cfg->cipher_coreaddress);
+}
+
+int xlnx_hdcp2x_cipher_cfg_init(struct xlnx_hdcp2x_cipher_hw *cipher_cfg)
+{
+	u32 reg_read;
+
+	reg_read = xlnx_hdcp2x_cipher_read(cipher_cfg->cipher_coreaddress,
+					   XHDCP2X_CIPHER_VER_ID_OFFSET);
+	reg_read = FIELD_GET(XHDCP2X_CIPHER_MASK_16, reg_read);
+	if (reg_read != XHDCP2X_CIPHER_VER_ID)
+		return (-EINVAL);
+
+	return reg_read;
+}
+
+void xlnx_hdcp2x_cipher_set_keys(struct xlnx_hdcp2x_cipher_hw *cipher_cfg,
+				 const u8 *cipherkey, u32 offset, u16 len)
+{
+	u8 buf[XHDCP2X_CIPHER_KEY_LENGTH];
+	u32 *bufptr;
+	u8 i = 0;
+
+	swap_bytes(i, buf, cipherkey, len);
+
+	for (i = 0; i < len; i += 4) {
+		bufptr = (u32 *)&buf[i];
+		xlnx_hdcp2x_cipher_write(cipher_cfg->cipher_coreaddress,
+					 offset, *bufptr);
+		/* Increase offset to the next register */
+		offset += 4;
+	}
+}
+
+void xlnx_hdcp2x_cipher_set_lanecount(struct xlnx_hdcp2x_cipher_hw *cipher_cfg,
+				      u8 lanecount)
+{
+	xlnx_hdcp2x_cipher_write(cipher_cfg->cipher_coreaddress,
+				 XHDCP2X_CIPHER_REG_CTRL_CLR_OFFSET,
+				 XHDCP2X_CIPHER_REG_CTRL_LANE_CNT_MASK);
+
+	xlnx_hdcp2x_cipher_write(cipher_cfg->cipher_coreaddress,
+				 XHDCP2X_CIPHER_REG_CTRL_SET_OFFSET,
+				 lanecount << XHDCP2X_CIPHER_REG_CTRL_LANE_CNT_BIT_POS);
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx_hdcp/xlnx_hdcp_bigdigits.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx_hdcp/xlnx_hdcp_bigdigits.c	2023-07-05 08:33:32.292188300 +0900
@@ -0,0 +1,919 @@
+// SPDX-License-Identifier: GPL-2.0
+// Id: bigdigits.c
+
+/***** BEGIN LICENSE BLOCK *****
+ *
+ * This Source Code Form is subject to the terms of the Mozilla Public
+ * License, v. 2.0. If a copy of the MPL was not distributed with this
+ * file, You can obtain one at http://mozilla.org/MPL/2.0/.
+ *
+ * Copyright (c) 2001-15 David Ireland, D.I. Management Services Pty Limited
+ * <http://www.di-mgt.com.au/bigdigits.html>. All rights reserved.
+ *
+ ***** END LICENSE BLOCK *****/
+/*
+ * Last updated:
+ * Date: 2015-10-22 10:23:00 $
+ * Revision: 2.5.0
+ * Author: dai
+ */
+
+/* Core code for BigDigits library "mp" functions */
+
+/* Some part of Code for BigDigits is modified according to Xilinx standards */
+
+#include <linux/slab.h>
+#include <linux/xlnx/xlnx_hdcp_common.h>
+
+#define MAX_DIGIT 0xFFFFFFFFUL
+#define MAX_HALF_DIGIT 0xFFFFUL	/* NB 'L' */
+#define XBITS_PER_DIGIT 32
+#define XMP_HI_BIT_MASK 0x80000000UL
+#define XBITS_PER_HALF_DIGIT (XBITS_PER_DIGIT / 2)
+#define XBYTES_PER_DIGIT (XBITS_PER_DIGIT / 8)
+#define XBIGDIG_LOHALF(x) ((unsigned int)((x) & MAX_HALF_DIGIT))
+#define XBIGDIG_HIHALF(x) ((unsigned int)((x) >> XBITS_PER_HALF_DIGIT & MAX_HALF_DIGIT))
+#define XBIGDIG_TOHALF(x) ((unsigned int)((x) << XBITS_PER_HALF_DIGIT))
+
+static void mp_next_bit_mask(unsigned int *mask, unsigned int *n)
+{
+	if ((*mask) == 1) {
+		*mask = XMP_HI_BIT_MASK; (*n)--;
+	} else {
+		*mask >>= 1;
+	}
+}
+
+static int sp_multiply(unsigned int p[2], unsigned int x, unsigned int y)
+{
+	unsigned int x0, y0, x1, y1;
+	unsigned int t, u, carry;
+
+	/*
+	 *	Split each x,y into two halves
+	 *	x = x0 + B*x1
+	 *	y = y0 + B*y1
+	 *	where B = 2^16, half the digit size
+	 *	Product is
+	 *	xy = x0y0 + B(x0y1 + x1y0) + B^2(x1y1)
+	 */
+	x0 = XBIGDIG_LOHALF(x);
+	x1 = XBIGDIG_HIHALF(x);
+	y0 = XBIGDIG_LOHALF(y);
+	y1 = XBIGDIG_HIHALF(y);
+
+	/* Calc low part - no carry */
+	p[0] = x0 * y0;
+
+	/* Calc middle part */
+	t = x0 * y1;
+	u = x1 * y0;
+	t += u;
+	if (t < u)
+		carry = 1;
+	else
+		carry = 0;
+
+	/*
+	 *	This carry will go to high half of p[1]
+	 *	+ high half of t into low half of p[1]
+	 */
+	carry = XBIGDIG_TOHALF(carry) + XBIGDIG_HIHALF(t);
+
+	/* Add low half of t to high half of p[0] */
+	t = XBIGDIG_TOHALF(t);
+	p[0] += t;
+	if (p[0] < t)
+		carry++;
+
+	p[1] = x1 * y1;
+	p[1] += carry;
+
+	return 0;
+}
+
+#define B (MAX_HALF_DIGIT + 1)
+
+static void sp_mult_sub(unsigned int uu[2], unsigned int qhat,
+			unsigned int v1, unsigned int v0)
+{
+	/*
+	 *  Compute uu = uu - q(v1v0)
+	 *  where uu = u3u2u1u0, u3 = 0
+	 *  and u_n, v_n are all half-digits
+	 *  even though v1, v2 are passed as full digits.
+	 */
+	unsigned int p0, p1, t;
+
+	p0 = qhat * v0;
+	p1 = qhat * v1;
+	t = p0 + XBIGDIG_TOHALF(XBIGDIG_LOHALF(p1));
+	uu[0] -= t;
+	if (uu[0] > MAX_DIGIT - t)
+		uu[1]--;	/* Borrow */
+	uu[1] -= XBIGDIG_HIHALF(p1);
+}
+
+static unsigned int sp_divide(unsigned int *q, unsigned int *r,
+			      const unsigned int u[2], unsigned int v)
+{
+	unsigned int qhat, rhat, t, v0, v1, u0, u1, u2, u3;
+	unsigned int uu[2], q2;
+
+	/* Check for normalisation */
+	if (!(v & XMP_HI_BIT_MASK)) {	/* Stop if assert is working, else return error */
+		/* assert(v & XMP_HI_BIT_MASK); */
+		*q = *r = 0;
+		return MAX_DIGIT;
+	}
+
+	/* Split up into half-digits */
+	v0 = XBIGDIG_LOHALF(v);
+	v1 = XBIGDIG_HIHALF(v);
+	u0 = XBIGDIG_LOHALF(u[0]);
+	u1 = XBIGDIG_HIHALF(u[0]);
+	u2 = XBIGDIG_LOHALF(u[1]);
+	u3 = XBIGDIG_HIHALF(u[1]);
+
+	qhat = (u3 < v1 ? 0 : 1);
+	if (qhat > 0) {	/* qhat is one, so no need to mult */
+		rhat = u3 - v1;
+		/* t = r.b + u2 */
+		t = XBIGDIG_TOHALF(rhat) | u2;
+		if (v0 > t)
+			qhat--;
+	}
+
+	uu[1] = 0;		/* (u4) */
+	uu[0] = u[1];	/* (u3u2) */
+	if (qhat > 0) {
+		/* (u4u3u2) -= qhat(v1v0) where u4 = 0 */
+		sp_mult_sub(uu, qhat, v1, v0);
+		if (XBIGDIG_HIHALF(uu[1]) != 0)	{	/* Add back */
+			qhat--;
+			uu[0] += v;
+			uu[1] = 0;
+		}
+	}
+	q2 = qhat;
+
+	/* ROUND 2. Set j = 1 and calculate q1 */
+
+	/*
+	 * Estimate qhat = (u3u2) / v1
+	 * then set (u3u2u1) -= qhat(v1v0)
+	 */
+	t = uu[0];
+	qhat = t / v1;
+	rhat = t - qhat * v1;
+	/* Test on v0 */
+	t = XBIGDIG_TOHALF(rhat) | u1;
+	if (qhat == B || (qhat * v0 > t)) {
+		qhat--;
+		rhat += v1;
+		t = XBIGDIG_TOHALF(rhat) | u1;
+		if (rhat < B && (qhat * v0 > t))
+			qhat--;
+	}
+
+	/*
+	 * Multiply and subtract
+	 * (u3u2u1)' = (u3u2u1) - qhat(v1v0)
+	 */
+	uu[1] = XBIGDIG_HIHALF(uu[0]);	/* (0u3) */
+	uu[0] = XBIGDIG_TOHALF(XBIGDIG_LOHALF(uu[0])) | u1;	/* (u2u1) */
+	sp_mult_sub(uu, qhat, v1, v0);
+	if (XBIGDIG_HIHALF(uu[1]) != 0) {	/* Add back */
+		qhat--;
+		uu[0] += v;
+		uu[1] = 0;
+	}
+
+	/* q1 = qhat */
+	*q = XBIGDIG_TOHALF(qhat);
+
+	/* ROUND 3. Set j = 0 and calculate q0 */
+	/*
+	 *	Estimate qhat = (u2u1) / v1
+	 *	then set (u2u1u0) -= qhat(v1v0)
+	 */
+	t = uu[0];
+	qhat = t / v1;
+	rhat = t - qhat * v1;
+	/* Test on v0 */
+	t = XBIGDIG_TOHALF(rhat) | u0;
+	if (qhat == B || (qhat * v0 > t)) {
+		qhat--;
+		rhat += v1;
+		t = XBIGDIG_TOHALF(rhat) | u0;
+		if (rhat < B && (qhat * v0 > t))
+			qhat--;
+	}
+
+	/*
+	 *	Multiply and subtract
+	 *	(u2u1u0)" = (u2u1u0)' - qhat(v1v0)
+	 */
+	uu[1] = XBIGDIG_HIHALF(uu[0]);	/* (0u2) */
+	uu[0] = XBIGDIG_TOHALF(XBIGDIG_LOHALF(uu[0])) | u0;	/* (u1u0) */
+	sp_mult_sub(uu, qhat, v1, v0);
+	if (XBIGDIG_HIHALF(uu[1]) != 0) {	/* Add back */
+		qhat--;
+		uu[0] += v;
+		uu[1] = 0;
+	}
+
+	/* q0 = qhat */
+	*q |= XBIGDIG_LOHALF(qhat);
+
+	/* Remainder is in (u1u0) i.e. uu[0] */
+	*r = uu[0];
+	return q2;
+}
+
+static unsigned int mp_add(unsigned int w[], const unsigned int u[],
+			   const unsigned int v[], size_t ndigits)
+{
+	/*
+	 *	Calculates w = u + v
+	 *	where w, u, v are multiprecision integers of ndigits each
+	 *	Returns carry if overflow. Carry = 0 or 1.
+	 *	Ref: Knuth Vol 2 Ch 4.3.1 p 266 Algorithm A.
+	 */
+
+	unsigned int k;
+	size_t j;
+
+	k = 0;
+
+	for (j = 0; j < ndigits; j++) {
+		w[j] = u[j] + k;
+		if (w[j] < k)
+			k = 1;
+		else
+			k = 0;
+
+		w[j] += v[j];
+		if (w[j] < v[j])
+			k++;
+	}
+
+	return k;
+}
+
+static int mp_multiply(unsigned int w[], const unsigned int u[],
+		       const unsigned int v[], size_t ndigits)
+{
+	/*
+	 *	Computes product w = u * v
+	 *	where u, v are multiprecision integers of ndigits each
+	 *	and w is a multiprecision integer of 2*ndigits
+	 *	Ref: Knuth Vol 2 Ch 4.3.1 p 268 Algorithm M.
+	 */
+
+	unsigned int k, t[2];
+	size_t i, j, m, n;
+
+	m = ndigits;
+	n = ndigits;
+
+	/* Step M1. Initialise */
+	for (i = 0; i < 2 * m; i++)
+		w[i] = 0;
+
+	for (j = 0; j < n; j++) {
+		/* Step M2. Zero multiplier? */
+		if (v[j] == 0) {
+			w[j + m] = 0;
+		} else {
+			/* Step M3. Initialise i */
+			k = 0;
+			for (i = 0; i < m; i++) {
+				/* Step M4. Multiply and add */
+				/* t = u_i * v_j + w_(i+j) + k */
+				sp_multiply(t, u[i], v[j]);
+
+				t[0] += k;
+				if (t[0] < k)
+					t[1]++;
+				t[0] += w[i + j];
+				if (t[0] < w[i + j])
+					t[1]++;
+
+				w[i + j] = t[0];
+				k = t[1];
+			}
+			/* Step M5. Loop on i, set w_(j+m) = k */
+			w[j + m] = k;
+		}
+	}	/* Step M6. Loop on j */
+
+	return 0;
+}
+
+static unsigned int mp_mult_sub(unsigned int wn, unsigned int w[],
+				const unsigned int v[],
+				unsigned int q, size_t n)
+{
+	/*
+	 *	Compute w = w - qv
+	 *	where w = (WnW[n-1]...W[0])
+	 *	return modified Wn.
+	 */
+	unsigned int k, t[2];
+	size_t i;
+
+	if (q == 0)	/* No change */
+		return wn;
+
+	k = 0;
+
+	for (i = 0; i < n; i++) {
+		sp_multiply(t, q, v[i]);
+		w[i] -= k;
+		if (w[i] > MAX_DIGIT - k)
+			k = 1;
+		else
+			k = 0;
+		w[i] -= t[0];
+		if (w[i] > MAX_DIGIT - t[0])
+			k++;
+		k += t[1];
+	}
+
+	/* Cope with Wn not stored in array w[0..n-1] */
+	wn -= k;
+
+	return wn;
+}
+
+static int qhat_too_big(unsigned int qhat, unsigned int rhat,
+			unsigned int vn2, unsigned int ujn2)
+{
+	/*
+	 *	Returns true if Qhat is too big
+	 *	i.e. if (Qhat * Vn-2) > (b.Rhat + Uj+n-2)
+	 */
+	unsigned int t[2];
+
+	sp_multiply(t, qhat, vn2);
+	if (t[1] < rhat)
+		return 0;
+	else if (t[1] > rhat)
+		return 1;
+	else if (t[0] > ujn2)
+		return 1;
+
+	return 0;
+}
+
+static int mp_compare(const unsigned int a[], const unsigned int b[],
+		      size_t ndigits)
+{
+	/* All these vars are either 0 or 1 */
+	unsigned int gt = 0;
+	unsigned int lt = 0;
+	unsigned int mask = 1;	/* Set to zero once first inequality found */
+	unsigned int c;
+
+	while (ndigits--) {
+		gt |= (a[ndigits] > b[ndigits]) & mask;
+		lt |= (a[ndigits] < b[ndigits]) & mask;
+		c = (gt | lt);
+		mask &= (c - 1);	/* Unchanged if c==0 or mask==0, else mask=0 */
+	}
+
+	return (int)gt - (int)lt;	/* EQ=0 GT=+1 LT=-1 */
+}
+
+static size_t mp_sizeof(const unsigned int a[], size_t ndigits)
+{
+	while (ndigits--) {
+		if (a[ndigits] != 0)
+			return (++ndigits);
+	}
+	return 0;
+}
+
+static void mp_set_equal(unsigned int a[], const unsigned int b[], size_t ndigits)
+{
+	/* Sets a = b */
+	size_t i;
+
+	for (i = 0; i < ndigits; i++)
+		a[i] = b[i];
+}
+
+static unsigned int mp_set_zero(unsigned int a[], size_t ndigits)
+{
+	/* Sets a = 0 */
+
+	/* Prevent optimiser ignoring this */
+	unsigned int optdummy;
+	unsigned int *p = a;
+
+	while (ndigits--)
+		a[ndigits] = 0;
+
+	optdummy = *p;
+	return optdummy;
+}
+
+static void mp_set_digit(unsigned int a[], unsigned int d, size_t ndigits)
+{
+	/* Sets a = d where d is a single digit */
+	size_t i;
+
+	for (i = 1; i < ndigits; i++)
+		a[i] = 0;
+	a[0] = d;
+}
+
+static unsigned int mp_shift_left(unsigned int a[], const unsigned int *b,
+				  size_t shift, size_t ndigits)
+{
+	/* Computes a = b << shift */
+	/* [v2.1] Modified to cope with shift > BITS_PERDIGIT */
+	size_t i, y, nw, bits;
+	unsigned int mask, carry, nextcarry;
+	u8 shift_bit = 0;
+
+	/* Do we shift whole digits? */
+	if (shift >= XBITS_PER_DIGIT) {
+		nw = shift / XBITS_PER_DIGIT;
+		i = ndigits;
+		while (i--) {
+			if (i >= nw)
+				a[i] = b[i - nw];
+			else
+				a[i] = 0;
+		}
+		/* Call again to shift bits inside digits */
+		bits = shift % XBITS_PER_DIGIT;
+		carry = b[ndigits - nw] << bits;
+		if (bits)
+			carry |= mp_shift_left(a, a, bits, ndigits);
+		return carry;
+	}
+	bits = shift;
+	/* Construct mask = high bits set */
+	mask = ~(~(unsigned int)shift_bit >> bits);
+
+	y = XBITS_PER_DIGIT - bits;
+	carry = 0;
+	for (i = 0; i < ndigits; i++) {
+		nextcarry = (b[i] & mask) >> y;
+		a[i] = b[i] << bits | carry;
+		carry = nextcarry;
+	}
+
+	return carry;
+}
+
+static unsigned int mp_short_div(unsigned int q[], const unsigned int u[], unsigned int v,
+				 size_t ndigits)
+{
+	size_t j;
+	unsigned int t[2], r;
+	size_t shift;
+	unsigned int bitmask, overflow, *uu;
+
+	if (ndigits == 0)
+		return 0;
+	if (v == 0)
+		return 0;
+
+	bitmask = XMP_HI_BIT_MASK;
+	for (shift = 0; shift < XBITS_PER_DIGIT; shift++) {
+		if (v & bitmask)
+			break;
+		bitmask >>= 1;
+	}
+
+	v <<= shift;
+	overflow = mp_shift_left(q, u, shift, ndigits);
+	uu = q;
+
+	/* Step S1 - modified for extra digit. */
+	r = overflow;	/* New digit Un */
+	j = ndigits;
+	while (j--) {
+		/* Step S2. */
+		t[1] = r;
+		t[0] = uu[j];
+		overflow = sp_divide(&q[j], &r, t, v);
+	}
+	r >>= shift;
+
+	return r;
+}
+
+static unsigned int mp_shift_right(unsigned int a[], const unsigned int b[], size_t shift,
+				   size_t ndigits)
+{
+	/* Computes a = b >> shift */
+	/* [v2.1] Modified to cope with shift > BITS_PERDIGIT */
+	size_t i, y, nw, bits;
+	unsigned int mask, carry, nextcarry;
+	u8 shift_bit = 0;
+
+	/* Do we shift whole digits? */
+	if (shift >= XBITS_PER_DIGIT) {
+		nw = shift / XBITS_PER_DIGIT;
+		for (i = 0; i < ndigits; i++) {
+			if ((i + nw) < ndigits)
+				a[i] = b[i + nw];
+			else
+				a[i] = 0;
+		}
+		/* Call again to shift bits inside digits */
+		bits = shift % XBITS_PER_DIGIT;
+		carry = b[nw - 1] >> bits;
+		if (bits)
+			carry |= mp_shift_right(a, a, bits, ndigits);
+		return carry;
+	}
+	bits = shift;
+	/* Construct mask to set low bits */
+	mask = ~(~(unsigned int)shift_bit << bits);
+
+	y = XBITS_PER_DIGIT - bits;
+	carry = 0;
+	i = ndigits;
+	while (i--) {
+		nextcarry = (b[i] & mask) << y;
+		a[i] = b[i] >> bits | carry;
+		carry = nextcarry;
+	}
+
+	return carry;
+}
+
+static int mp_divide(unsigned int q[], unsigned int r[], const unsigned int u[],
+		     size_t udigits, unsigned int v[], size_t vdigits)
+{
+	size_t shift;
+	int n, m, j;
+	unsigned int bitmask, overflow;
+	unsigned int qhat, rhat, t[2];
+	unsigned int *uu, *ww;
+	int qhat_ok, cmp;
+
+	/* Clear q and r */
+	mp_set_zero(q, udigits);
+	mp_set_zero(r, udigits);
+
+	/* Work out exact sizes of u and v */
+	n = (int)mp_sizeof(v, vdigits);
+	m = (int)mp_sizeof(u, udigits);
+	m -= n;
+
+	/* Catch special cases */
+	if (n == 0)
+		return -1;	/* Error: divide by zero */
+
+	if (n == 1) {	/* Use short division instead */
+		r[0] = mp_short_div(q, u, v[0], udigits);
+		return 0;
+	}
+
+	if (m < 0) {	/* v > u, so just set q = 0 and r = u */
+		mp_set_equal(r, u, udigits);
+		return 0;
+	}
+
+	if (m == 0) {	/* u and v are the same length */
+		cmp = mp_compare(u, v, (size_t)n);
+		if (cmp < 0) {	/* v > u, as above */
+			mp_set_equal(r, u, udigits);
+			return 0;
+		} else if (cmp == 0) {	/* v == u, so set q = 1 and r = 0 */
+			mp_set_digit(q, 1, udigits);
+			return 0;
+		}
+	}
+
+	bitmask = XMP_HI_BIT_MASK;
+	for (shift = 0; shift < XBITS_PER_DIGIT; shift++) {
+		if (v[n - 1] & bitmask)
+			break;
+		bitmask >>= 1;
+	}
+
+	/* Normalise v in situ - NB only shift non-zero digits */
+	overflow = mp_shift_left(v, v, shift, n);
+
+	/* Copy normalised dividend u*d into r */
+	overflow = mp_shift_left(r, u, shift, n + m);
+	uu = r;	/* Use ptr to keep notation constant */
+
+	t[0] = overflow;	/* Extra digit Um+n */
+
+	/* Step D2. Initialise j. Set j = m */
+	for (j = m; j >= 0; j--) {
+		/*
+		 * Step D3. Set Qhat = [(b.Uj+n + Uj+n-1)/Vn-1]
+		 * and Rhat = remainder
+		 */
+		qhat_ok = 0;
+		t[1] = t[0];	/* This is Uj+n */
+		t[0] = uu[j + n - 1];
+		overflow = sp_divide(&qhat, &rhat, t, v[n - 1]);
+
+		/* Test Qhat */
+		if (overflow) {	/* Qhat == b so set Qhat = b - 1 */
+			qhat = MAX_DIGIT;
+			rhat = uu[j + n - 1];
+			rhat += v[n - 1];
+			if (rhat < v[n - 1])	/* Rhat >= b, so no re-test */
+				qhat_ok = 1;
+		}
+		/* [VERSION 2: Added extra test "qhat && "] */
+		if (qhat && !qhat_ok && qhat_too_big(qhat, rhat,
+						     v[n - 2], uu[j + n - 2])) {
+			/*
+			 * If Qhat.Vn-2 > b.Rhat + Uj+n-2
+			 * decrease Qhat by one, increase Rhat by Vn-1
+			 */
+			qhat--;
+			rhat += v[n - 1];
+			/* Repeat this test if Rhat < b */
+			if (!(rhat < v[n - 1]))
+				if (qhat_too_big(qhat, rhat, v[n - 2], uu[j + n - 2]))
+					qhat--;
+		}
+
+		/* Step D4. Multiply and subtract */
+		ww = &uu[j];
+		overflow = mp_mult_sub(t[1], ww, v, qhat, (size_t)n);
+
+		/* Step D5. Test remainder. Set Qj = Qhat */
+		q[j] = qhat;
+		if (overflow) {	/* Step D6. Add back if D4 was negative */
+			q[j]--;
+			overflow = mp_add(ww, ww, v, (size_t)n);
+		}
+
+		t[0] = uu[j + n - 1];	/* Uj+n on next round */
+
+	}	/* Step D7. Loop on j */
+
+	/* Clear high digits in uu */
+	for (j = n; j < m + n; j++)
+		uu[j] = 0;
+
+	/* Step D8. Unnormalise. */
+
+	mp_shift_right(r, r, shift, n);
+	mp_shift_right(v, v, shift, n);
+
+	return 0;
+}
+
+static int mp_square(unsigned int w[], const unsigned int x[], size_t ndigits)
+{
+	unsigned int k, p[2], u[2], cbit, carry;
+	size_t i, j, t, i2, cpos;
+
+	t = ndigits;
+	/* 1. For i from 0 to (2t-1) do: w_i = 0 */
+	i2 = t << 1;
+	for (i = 0; i < i2; i++)
+		w[i] = 0;
+
+	carry = 0;
+	cpos = i2 - 1;
+	/* 2. For i from 0 to (t-1) do: */
+	for (i = 0; i < t; i++) {
+	/*
+	 * 2.1 (uv) = w_2i + x_i * x_i, w_2i = v, c = u
+	 *  Careful, w_2i may be double-prec
+	 */
+		i2 = i << 1; /* 2*i */
+		sp_multiply(p, x[i], x[i]);
+		p[0] += w[i2];
+		if (p[0] < w[i2])
+			p[1]++;
+		k = 0;	/* p[1] < b, so no overflow here */
+		if (i2 == cpos && carry) {
+			p[1] += carry;
+			if (p[1] < carry)
+				k++;
+			carry = 0;
+		}
+		w[i2] = p[0];
+		u[0] = p[1];
+		u[1] = k;
+
+		/*
+		 * 2.2 for j from (i+1) to (t-1) do:
+		 * (uv) = w_{i+j} + 2x_j * x_i + c,
+		 * w_{i+j} = v, c = u,
+		 * u is double-prec
+		 * w_{i+j} is dbl if [i+j] == cpos
+		 */
+		k = 0;
+		for (j = i + 1; j < t; j++) {
+			/* p = x_j * x_i */
+			sp_multiply(p, x[j], x[i]);
+			/* p = 2p <=> p <<= 1 */
+			cbit = (p[0] & XMP_HI_BIT_MASK) != 0;
+			k =  (p[1] & XMP_HI_BIT_MASK) != 0;
+			p[0] <<= 1;
+			p[1] <<= 1;
+			p[1] |= cbit;
+			/* p = p + c */
+			p[0] += u[0];
+			if (p[0] < u[0]) {
+				p[1]++;
+				if (p[1] == 0)
+					k++;
+			}
+			p[1] += u[1];
+			if (p[1] < u[1])
+				k++;
+			/* p = p + w_{i+j} */
+			p[0] += w[i + j];
+			if (p[0] < w[i + j]) {
+				p[1]++;
+				if (p[1] == 0)
+					k++;
+			}
+			if ((i + j) == cpos && carry) {/* catch overflow from last round */
+				p[1] += carry;
+				if (p[1] < carry)
+					k++;
+				carry = 0;
+			}
+			/* w_{i+j} = v, c = u */
+			w[i + j] = p[0];
+			u[0] = p[1];
+			u[1] = k;
+		}
+		/* 2.3 w_{i+t} = u */
+		w[i + t] = u[0];
+		/* remember overflow in w_{i+t} */
+		carry = u[1];
+		cpos = i + t;
+	}
+
+	return 0;
+}
+
+size_t mp_conv_from_octets(unsigned int a[], size_t ndigits, const unsigned char *c,
+			   size_t nbytes)
+{
+	/*
+	 *	Converts nbytes octets into big digit a of max size ndigits
+	 *	Returns actual number of digits set (may be larger than mp_sizeof)
+	 */
+
+	size_t i;
+	int j, k = 0;
+	unsigned int t;
+
+	mp_set_zero(a, ndigits);
+	/* Read in octets, least significant first */
+	/* i counts into big_d, j along c, and k is # bits to shift */
+	for (i = 0, j = (int)nbytes - 1; i < ndigits && j >= 0; i++) {
+		t = 0;
+		for (k = 0; (j >= 0) && (k < XBITS_PER_DIGIT) ; j--, k = k + 8)
+			t |= ((unsigned int)c[j]) << k;
+		a[i] = t;
+	}
+	return i;
+}
+
+static size_t mp_bit_length(const unsigned int d[], size_t ndigits)
+{
+	/* Returns no of significant bits in d */
+	size_t n, i, bits;
+	unsigned int mask;
+
+	if (!d || ndigits == 0)
+		return 0;
+
+	n = mp_sizeof(d, ndigits);
+	if (n == 0)
+		return 0;
+
+	for (i = 0, mask = XMP_HI_BIT_MASK; mask > 0; mask >>= 1, i++) {
+		if (d[n - 1] & mask)
+			break;
+	}
+
+	bits = n * XBITS_PER_DIGIT - i;
+
+	return bits;
+}
+
+size_t mp_conv_to_octets(const unsigned int a[], size_t ndigits, unsigned char *c,
+			 size_t nbytes)
+{
+	/*
+	 *	Convert big digit a into string of octets, in big-endian order,
+	 *	padding on the left to nbytes or truncating if necessary.
+	 *	Return number of octets required excluding leading zero bytes.
+	 */
+
+	int j, k, len;
+	unsigned int t;
+	size_t i, noctets, nbits;
+
+	nbits = mp_bit_length(a, ndigits);
+	noctets = (nbits + 7) / 8;
+
+	len = (int)nbytes;
+
+	for (i = 0, j = len - 1; i < ndigits && j >= 0; i++) {
+		t = a[i];
+		for (k = 0; j >= 0 && k < XBITS_PER_DIGIT; j--, k += 8)
+			c[j] = (unsigned char)(t >> k);
+	}
+
+	for ( ; j >= 0; j--)
+		c[j] = 0;
+
+	return (size_t)noctets;
+}
+
+static void mp_mod_square_temp(unsigned int y[], unsigned int m[], size_t ndigits,
+			       unsigned int t1[], unsigned int t2[])
+{
+	mp_square(t1, y, ndigits);
+	mp_divide(t2, y, t1, ndigits * 2, m, ndigits);
+}
+
+static void mp_mod_mult_temp(unsigned int y[], const unsigned int x[], unsigned int m[],
+			     size_t ndigits, unsigned int t1[], unsigned int t2[])
+{
+	mp_multiply(t1, x, y, ndigits);
+	mp_divide(t2, y, t1, ndigits * 2, m, ndigits);
+}
+
+static int mp_mod_exp_1(unsigned int yout[], const unsigned int x[],
+			const unsigned int e[], unsigned int m[], size_t ndigits)
+{
+	/* Computes y = x^e mod m */
+	/* Classic binary left-to-right method */
+	unsigned int mask;
+	size_t n;
+	size_t nn = ndigits * 2;
+	u32 *t1, *t2, *y;
+
+	/*
+	 * [v2.2] removed const restriction on m[] to avoid using an extra alloc'd
+	 * var (m is changed in-situ during the divide operation then restored)
+	 */
+
+	t1 = kzalloc(nn * 3 * sizeof(u32), GFP_KERNEL);
+	t2 = &t1[nn * 1];
+	y  = &t1[nn * 2];
+
+	n = mp_sizeof(e, ndigits);
+	/* Catch e==0 => x^0=1 */
+	if (n == 0) {
+		mp_set_digit(yout, 1, ndigits);
+		goto done;
+	}
+	/* Find second-most significant bit in e */
+	for (mask = XMP_HI_BIT_MASK; mask > 0; mask >>= 1) {
+		if (e[n - 1] & mask)
+			break;
+	}
+	mp_next_bit_mask(&mask, (unsigned int *)&n);
+
+	/* Set y = x */
+	mp_set_equal(y, x, ndigits);
+
+	/* For bit j = k-2 downto 0 */
+	while (n) {
+		/* Square y = y * y mod n */
+		mp_mod_square_temp(y, m, ndigits, t1, t2);
+		if (e[n - 1] & mask) {
+			/*	if e(j) == 1 then multiply
+			 *	y = y * x mod
+			 */
+			mp_mod_mult_temp(y, x, m, ndigits, t1, t2);
+		}
+
+		/* Move to next bit */
+		mp_next_bit_mask(&mask, (unsigned int *)&n);
+	}
+
+	/* Return y */
+	mp_set_equal(yout, y, ndigits);
+
+done:
+	kfree(t1);
+
+	return 0;
+}
+
+int mp_mod_exp(unsigned int y[], const unsigned int x[], const unsigned int n[],
+	       unsigned int d[], size_t ndigits)
+{
+	/* Computes y = x^n mod d */
+
+	return mp_mod_exp_1(y, x, n, d, ndigits);
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx_hdcp/xlnx_hdcp_rng.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx_hdcp/xlnx_hdcp_rng.c	2023-07-05 08:33:32.307812400 +0900
@@ -0,0 +1,71 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx HDCP2X Random Number Generator driver
+ *
+ * Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Author: Lakshmi Prasanna Eachuri <lakshmi.prasanna.eachuri@amd.com>
+ *
+ * This driver initializes Random Number Generator-RNG, which is used to
+ * produce random numbers during the HDCP authentication and Key exchange.
+ */
+
+#include <linux/io.h>
+#include <linux/xlnx/xlnx_hdcp_rng.h>
+
+static int xlnx_hdcp2x_rng_read(void __iomem *rng_coreaddress, int reg_offset)
+{
+	return readl(rng_coreaddress + reg_offset);
+}
+
+static void xlnx_hdcp2x_rng_write(void __iomem *rng_coreaddress, int reg_offset,
+				  u32 data)
+{
+	writel(data, rng_coreaddress + reg_offset);
+}
+
+int xlnx_hdcp2x_rng_cfg_init(struct xlnx_hdcp2x_rng_hw *rng_cfg)
+{
+	u32 reg_read;
+
+	reg_read = xlnx_hdcp2x_rng_read(rng_cfg->rng_coreaddress,
+					XHDCP2X_RNG_VER_ID_OFFSET);
+	reg_read = FIELD_GET(XHDCP2X_RNG_MASK_16, reg_read);
+	if (reg_read != XHDCP2X_RNG_VER_ID)
+		return (-EINVAL);
+
+	return 0;
+}
+
+void xlnx_hdcp2x_rng_get_random_number(struct xlnx_hdcp2x_rng_hw *rng_cfg,
+				       u8 *writeptr, u16 length, u16 randomlength)
+{
+	u32 i, j;
+	u32 offset = 0;
+	u32 random_word;
+	u8 *read_ptr = (u8 *)&random_word;
+
+	/*
+	 * randomlength is the requested length of the random number in bytes.
+	 * The length must be a multiple of 4
+	 */
+	for (i = 0; i < randomlength; i += 4) {
+		random_word = xlnx_hdcp2x_rng_read(rng_cfg->rng_coreaddress,
+						   XHDCP2X_RNG_REG_RN_1_OFFSET + offset);
+		for (j = 0; j < 4; j++)
+			writeptr[i + j] = read_ptr[j];
+		offset = (offset + 4) % 16;
+	}
+}
+
+void xlnx_hdcp2x_rng_enable(struct xlnx_hdcp2x_rng_hw *rng_cfg)
+{
+	xlnx_hdcp2x_rng_write(rng_cfg->rng_coreaddress, XHDCP2X_RNG_REG_CTRL_SET_OFFSET,
+			      XHDCP2X_RNG_REG_CTRL_RUN_MASK);
+}
+
+void xlnx_hdcp2x_rng_disable(struct xlnx_hdcp2x_rng_hw *rng_cfg)
+{
+	xlnx_hdcp2x_rng_write(rng_cfg->rng_coreaddress, XHDCP2X_RNG_REG_CTRL_CLR_OFFSET,
+			      XHDCP2X_RNG_REG_CTRL_RUN_MASK);
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xilinx_hdcp/xlnx_timer.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xilinx_hdcp/xlnx_timer.c	2023-07-05 08:33:32.307812400 +0900
@@ -0,0 +1,248 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AXI Timer driver
+ *
+ * Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Author: Lakshmi Prasanna Eachuri <lakshmi.prasanna.eachuri@amd.com>
+ *
+ * This driver initializes Xilinx timer/counter component.
+ */
+
+#include <linux/io.h>
+#include <linux/xlnx/xlnx_timer.h>
+
+/*
+ * The following data type maps an option to a register mask such that getting
+ * and setting the options may be table driven.
+ */
+struct mapping {
+	u32 option;
+	u32 mask;
+};
+
+/*
+ * Create the table which contains options which are to be processed to get/set
+ * the options. These options are table driven to allow easy maintenance and
+ * expansion of the options.
+ */
+static struct mapping options_table[] = {
+	{XTC_CASCADE_MODE_OPTION, XTC_CSR_CASC_MASK},
+	{XTC_ENABLE_ALL_OPTION, XTC_CSR_ENABLE_ALL_MASK},
+	{XTC_DOWN_COUNT_OPTION, XTC_CSR_DOWN_COUNT_MASK},
+	{XTC_CAPTURE_MODE_OPTION, XTC_CSR_CAPTURE_MODE_MASK |
+	 XTC_CSR_EXT_CAPTURE_MASK},
+	{XTC_INT_MODE_OPTION, XTC_CSR_ENABLE_INT_MASK},
+	{XTC_AUTO_RELOAD_OPTION, XTC_CSR_AUTO_RELOAD_MASK},
+	{XTC_EXT_COMPARE_OPTION, XTC_CSR_EXT_GENERATE_MASK}
+};
+
+#define XTC_NUM_OPTIONS   ARRAY_SIZE(options_table)
+
+static const u8 xtmrctr_offset[] = { 0, XTC_TIMER_COUNTER_OFFSET };
+
+static void xlnx_hdcp_tmrcntr_write_reg(void __iomem *coreaddress,
+					u32 tmrctr_number, u32 offset, u32 value)
+{
+	writel(value, coreaddress + xtmrctr_offset[tmrctr_number] + offset);
+}
+
+static u32 xlnx_hdcp_tmrcntr_read_reg(void __iomem *coreaddress, u32 tmrctr_number,
+				      u32 offset)
+{
+	return readl(coreaddress + xtmrctr_offset[tmrctr_number] + offset);
+}
+
+void xlnx_hdcp_tmrcntr_set_handler(struct xlnx_hdcp_timer_config *xtimercntr,
+				   xlnx_timer_cntr_handler funcptr,
+				   void *callbackref)
+{
+	xtimercntr->handler = funcptr;
+	xtimercntr->callbackref = callbackref;
+}
+
+void xlnx_hdcp_tmrcntr_cfg_init(struct xlnx_hdcp_timer_config *xtimercntr)
+{
+	xtimercntr->callbackref = xtimercntr;
+}
+
+static int xlnx_hdcp_timer_init(struct xlnx_hdcp_timer_config *xtimercntr)
+{
+	struct xlnx_hdcp_timer_hw *hw_config = &xtimercntr->hw_config;
+	u32 tmr_cntr_started[XTC_DEVICE_TIMER_COUNT];
+	int status = -EINVAL;
+	u8 tmr_index;
+
+	tmr_cntr_started[0] = xtimercntr->is_tmrcntr0_started;
+	tmr_cntr_started[1] = xtimercntr->is_tmrcntr1_started;
+
+	for (tmr_index = 0; tmr_index < XTC_DEVICE_TIMER_COUNT; tmr_index++) {
+		if (tmr_cntr_started[tmr_index] == XTC_COMPONENT_IS_STARTED)
+			continue;
+		xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_index,
+					    XTC_TLR_OFFSET, 0);
+		xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_index,
+					    XTC_TCSR_OFFSET,
+					    XTC_CSR_INT_OCCURED_MASK | XTC_CSR_LOAD_MASK);
+		xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_index,
+					    XTC_TCSR_OFFSET, 0);
+		status = 0;
+	}
+
+	return status;
+}
+
+int xlnx_hdcp_tmrcntr_init(struct xlnx_hdcp_timer_config *xtimercntr)
+{
+	if (xtimercntr->is_tmrcntr0_started == XTC_COMPONENT_IS_STARTED &&
+	    xtimercntr->is_tmrcntr1_started == XTC_COMPONENT_IS_STARTED)
+		return 0;
+
+	xlnx_hdcp_tmrcntr_cfg_init(xtimercntr);
+
+	return xlnx_hdcp_timer_init(xtimercntr);
+}
+
+void xlnx_hdcp_tmrcntr_start(struct xlnx_hdcp_timer_config *xtimercntr,
+			     u8 tmr_cntr_number)
+{
+	struct xlnx_hdcp_timer_hw *hw_config = &xtimercntr->hw_config;
+	u32 cntrl_statusreg;
+
+	cntrl_statusreg = xlnx_hdcp_tmrcntr_read_reg(hw_config->coreaddress,
+						     tmr_cntr_number,
+						     XTC_TCSR_OFFSET);
+	xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_cntr_number,
+				    XTC_TCSR_OFFSET,
+				    XTC_CSR_LOAD_MASK);
+
+	if (tmr_cntr_number == XTC_TIMER_0)
+		xtimercntr->is_tmrcntr0_started = XTC_COMPONENT_IS_STARTED;
+	else
+		xtimercntr->is_tmrcntr1_started = XTC_COMPONENT_IS_STARTED;
+	xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_cntr_number,
+				    XTC_TCSR_OFFSET,
+				    cntrl_statusreg | XTC_CSR_ENABLE_TMR_MASK);
+	cntrl_statusreg = xlnx_hdcp_tmrcntr_read_reg(hw_config->coreaddress,
+						     tmr_cntr_number,
+						     XTC_TCSR_OFFSET);
+}
+
+void xlnx_hdcp_tmrcntr_stop(struct xlnx_hdcp_timer_config *xtimercntr,
+			    u8 tmr_cntr_number)
+{
+	struct xlnx_hdcp_timer_hw *hw_config = &xtimercntr->hw_config;
+	u32 cntrl_statusreg;
+
+	cntrl_statusreg = xlnx_hdcp_tmrcntr_read_reg(hw_config->coreaddress,
+						     tmr_cntr_number, XTC_TCSR_OFFSET);
+
+	cntrl_statusreg &= (u32)~(XTC_CSR_ENABLE_TMR_MASK);
+
+	xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_cntr_number,
+				    XTC_TCSR_OFFSET, cntrl_statusreg);
+
+	if (tmr_cntr_number == XTC_TIMER_0)
+		xtimercntr->is_tmrcntr0_started = 0;
+	else
+		xtimercntr->is_tmrcntr1_started = 0;
+}
+
+u32 xlnx_hdcp_tmrcntr_get_value(struct xlnx_hdcp_timer_config *xtimercntr,
+				u8 tmr_cntr_number)
+{
+	struct xlnx_hdcp_timer_hw *hw_config = &xtimercntr->hw_config;
+
+	return xlnx_hdcp_tmrcntr_read_reg(hw_config->coreaddress,
+					  tmr_cntr_number,
+					  XTC_TCR_OFFSET);
+}
+
+void xlnx_hdcp_tmrcntr_set_reset_value(struct xlnx_hdcp_timer_config *xtimercntr,
+				       u8 tmr_cntr_number, u32 reset_value)
+{
+	struct xlnx_hdcp_timer_hw *hw_config = &xtimercntr->hw_config;
+
+	xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_cntr_number,
+				    XTC_TLR_OFFSET, reset_value);
+}
+
+void xlnx_hdcp_tmrcntr_reset(struct xlnx_hdcp_timer_config *xtimercntr,
+			     u8 tmr_cntr_number)
+{
+	struct xlnx_hdcp_timer_hw *hw_config = &xtimercntr->hw_config;
+	u32 counter_cntrl_reg;
+
+	counter_cntrl_reg = xlnx_hdcp_tmrcntr_read_reg(hw_config->coreaddress,
+						       tmr_cntr_number,
+						       XTC_TCSR_OFFSET);
+	xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_cntr_number,
+				    XTC_TCSR_OFFSET,
+				    counter_cntrl_reg | XTC_CSR_LOAD_MASK);
+
+	xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_cntr_number,
+				    XTC_TCSR_OFFSET, counter_cntrl_reg);
+}
+
+void xlnx_hdcp_tmrcntr_set_options(struct xlnx_hdcp_timer_config *xtimercntr,
+				   u8 tmr_cntr_number, u32 options)
+{
+	struct xlnx_hdcp_timer_hw *hw_config = &xtimercntr->hw_config;
+	u32 counter_cntrl_reg = 0;
+	u32 index;
+
+	for (index = 0; index < XTC_NUM_OPTIONS; index++) {
+		if (options & options_table[index].option)
+			counter_cntrl_reg |= options_table[index].mask;
+		else
+			counter_cntrl_reg &= ~options_table[index].mask;
+	}
+	xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress, tmr_cntr_number,
+				    XTC_TCSR_OFFSET, counter_cntrl_reg);
+}
+
+/**
+ * xlnx_hdcp_tmrcntr_interrupt_handler - HDCP timercntr interrupt handler
+ * @xtimercntr: timercounter configuration structure
+ */
+void xlnx_hdcp_tmrcntr_interrupt_handler(struct xlnx_hdcp_timer_config *xtimercntr)
+{
+	struct xlnx_hdcp_timer_hw *hw_config = &xtimercntr->hw_config;
+	u32 control_status_reg;
+	u8 tmr_cntr_number;
+
+	for (tmr_cntr_number = 0;
+		tmr_cntr_number < XTC_DEVICE_TIMER_COUNT; tmr_cntr_number++) {
+		control_status_reg = xlnx_hdcp_tmrcntr_read_reg(hw_config->coreaddress,
+								tmr_cntr_number,
+								XTC_TCSR_OFFSET);
+		if (control_status_reg & XTC_CSR_ENABLE_INT_MASK) {
+			if (control_status_reg & XTC_CSR_INT_OCCURED_MASK) {
+				xtimercntr->handler(xtimercntr->callbackref, tmr_cntr_number);
+				control_status_reg =
+					xlnx_hdcp_tmrcntr_read_reg(hw_config->coreaddress,
+								   tmr_cntr_number,
+								   XTC_TCSR_OFFSET);
+				if (((control_status_reg & XTC_CSR_AUTO_RELOAD_MASK) == 0) &&
+				    ((control_status_reg & XTC_CSR_CAPTURE_MODE_MASK)
+				    == 0)) {
+					control_status_reg &=
+						(u32)~XTC_CSR_ENABLE_TMR_MASK;
+					xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress,
+								    tmr_cntr_number,
+								    XTC_TCSR_OFFSET,
+								    control_status_reg |
+								    XTC_CSR_LOAD_MASK);
+					xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress,
+								    tmr_cntr_number,
+								    XTC_TCSR_OFFSET,
+								    control_status_reg);
+				}
+				xlnx_hdcp_tmrcntr_write_reg(hw_config->coreaddress,
+							    tmr_cntr_number, XTC_TCSR_OFFSET,
+							    control_status_reg |
+							    XTC_CSR_INT_OCCURED_MASK);
+			}
+		}
+	}
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xlnx_hdcp1x/Kconfig
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xlnx_hdcp1x/Kconfig	2023-07-05 08:33:32.307812400 +0900
@@ -0,0 +1,8 @@
+config XLNX_HDCP1X_CIPHER
+	bool
+	help
+	  This code is developed for hdcp1x cipher functionality of
+	  xilinx hdcp1x IP.
+
+	  This code will be compiled by selection from the interface
+	  drivers (DP/HDMI) of xilinx.
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xlnx_hdcp1x/MAINTAINERS
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xlnx_hdcp1x/MAINTAINERS	2023-07-05 08:33:32.307812400 +0900
@@ -0,0 +1,4 @@
+XILINX HDCP1X CIPHER DRIVER
+M:	Jagadeesh Banisetti <jagadeesh.banisetti@amd.com>
+S:	Maintained
+F:	drivers/staging/xlnx_hdcp1x
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xlnx_hdcp1x/Makefile
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xlnx_hdcp1x/Makefile	2023-07-05 08:33:32.307812400 +0900
@@ -0,0 +1 @@
+obj-y += xilinx-hdcp1x-cipher.o
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xlnx_hdcp1x/xilinx-hdcp1x-cipher.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xlnx_hdcp1x/xilinx-hdcp1x-cipher.c	2023-07-05 08:33:32.307812400 +0900
@@ -0,0 +1,717 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx HDCP1X Cipher driver
+ *
+ * Copyright (C) 2022 Xilinx, Inc.
+ *
+ * Author: Jagadeesh Banisetti <jagadeesh.banisetti@xilinx.com>
+ */
+
+#include <linux/bitfield.h>
+#include <linux/xilinx-hdcp1x-cipher.h>
+
+/* HDCP Cipher register offsets */
+#define XHDCP1X_CIPHER_REG_VERSION		0x00
+#define XHDCP1X_CIPHER_REG_TYPE			0x04
+#define XHDCP1X_CIPHER_REG_SCRATCH		0x08
+#define XHDCP1X_CIPHER_REG_CONTROL		0x0C
+#define XHDCP1X_CIPHER_REG_STATUS		0x10
+#define XHDCP1X_CIPHER_REG_INTERRUPT_MASK	0x14
+#define XHDCP1X_CIPHER_REG_INTERRUPT_STATUS	0x18
+#define XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_H	0x20
+#define XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_L	0x24
+#define XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL	0x2C
+#define XHDCP1X_CIPHER_REG_KEYMGMT_STATUS	0x30
+#define XHDCP1X_CIPHER_REG_KSV_LOCAL_H		0x38
+#define XHDCP1X_CIPHER_REG_KSV_LOCAL_L		0x3C
+#define XHDCP1X_CIPHER_REG_KSV_REMOTE_H		0x40
+#define XHDCP1X_CIPHER_REG_KSV_REMOTE_L		0x44
+#define XHDCP1X_CIPHER_REG_KM_H			0x48
+#define XHDCP1X_CIPHER_REG_KM_L			0x4C
+#define XHDCP1X_CIPHER_REG_CIPHER_CONTROL	0x50
+#define XHDCP1X_CIPHER_REG_CIPHER_STATUS	0x54
+#define XHDCP1X_CIPHER_REG_CIPHER_BX		0x58
+#define XHDCP1X_CIPHER_REG_CIPHER_BY		0x5C
+#define XHDCP1X_CIPHER_REG_CIPHER_BZ		0x60
+#define XHDCP1X_CIPHER_REG_CIPHER_KX		0x64
+#define XHDCP1X_CIPHER_REG_CIPHER_KY		0x68
+#define XHDCP1X_CIPHER_REG_CIPHER_KZ		0x6C
+#define XHDCP1X_CIPHER_REG_CIPHER_MI_H		0x70
+#define XHDCP1X_CIPHER_REG_CIPHER_MI_L		0x74
+#define XHDCP1X_CIPHER_REG_CIPHER_RI		0x78
+#define XHDCP1X_CIPHER_REG_CIPHER_RO		0x7C
+#define XHDCP1X_CIPHER_REG_CIPHER_MO_H		0x80
+#define XHDCP1X_CIPHER_REG_CIPHER_MO_L		0x84
+#define XHDCP1X_CIPHER_REG_BLANK_VALUE		0xBC
+#define XHDCP1X_CIPHER_REG_BLANK_SEL		0xC0
+
+/* HDCP Cipher register bit mask definitions */
+#define XHDCP1X_CIPHER_BITMASK_TYPE_PROTOCOL			GENMASK(1, 0)
+#define XHDCP1X_CIPHER_BITMASK_TYPE_DIRECTION			BIT(2)
+#define XHDCP1X_CIPHER_BITMASK_CONTROL_ENABLE			BIT(0)
+#define XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE			BIT(1)
+#define XHDCP1X_CIPHER_BITMASK_CONTROL_NUM_LANES		GENMASK(6, 4)
+#define XHDCP1X_CIPHER_BITMASK_CONTROL_RESET			BIT(31)
+#define XHDCP1X_CIPHER_BITMASK_INTERRUPT_LINK_FAIL		BIT(0)
+#define XHDCP1X_CIPHER_BITMASK_INTERRUPT_RI_UPDATE		BIT(1)
+#define XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_LOCAL_KSV	BIT(0)
+#define XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_BEGIN_KM		BIT(1)
+#define XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_ABORT_KM		BIT(2)
+#define XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_SET_SELECT	GENMASK(18, 16)
+#define XHDCP1X_CIPHER_BITMASK_KEYMGMT_STATUS_KSV_READY		BIT(0)
+#define XHDCP1X_CIPHER_BITMASK_KEYMGMT_STATUS_KM_READY		BIT(1)
+#define XHDCP1X_CIPHER_BITMASK_CIPHER_CONTROL_XOR_ENABLE	BIT(0)
+#define XHDCP1X_CIPHER_BITMASK_CIPHER_CONTROL_REQUEST		GENMASK(10, 8)
+#define XHDCP1X_CIPHER_BITMASK_CIPHER_STATUS_XOR_IN_PROG	BIT(0)
+#define XHDCP1X_CIPHER_BITMASK_CIPHER_STATUS_REQUEST_IN_PROG	GENMASK(10, 8)
+#define XHDCP1X_CIPHER_BITMASK_BLANK_VALUE			GENMASK(31, 0)
+#define XHDCP1X_CIPHER_BITMASK_BLANK_SEL			BIT(0)
+
+/* HDCP Cipher register bit value definitions */
+#define XHDCP1X_CIPHER_VALUE_TYPE_PROTOCOL_DP			0
+#define XHDCP1X_CIPHER_VALUE_TYPE_PROTOCOL_HDMI			1
+#define XHDCP1X_CIPHER_VALUE_TYPE_DIRECTION_MASK		BIT(2)
+#define XHDCP1X_CIPHER_VALUE_TYPE_DIRECTION_RX			0
+#define XHDCP1X_CIPHER_VALUE_TYPE_DIRECTION_TX			1
+#define XHDCP1X_CIPHER_VALUE_CIPHER_CONTROL_REQUEST_BLOCK	BIT(8)
+#define XHDCP1X_CIPHER_VALUE_CIPHER_CONTROL_REQUEST_REKEY	BIT(9)
+#define XHDCP1X_CIPHER_VALUE_CIPHER_CONTROL_REQUEST_RNG		BIT(10)
+
+#define XHDCP1X_CIPHER_SIZE_LOCAL_KSV				5
+#define XHDCP1X_CIPHER_KSV_RETRIES				1024
+#define XHDCP1X_CIPHER_SHIFT_NUM_LANES				4
+#define XHDCP1X_CIPHER_MAX_LANES				4
+#define XHDCP1X_CIPHER_INTR_ALL					GENMASK(31, 0)
+#define XHDCP1X_CIPHER_KEYSELECT_MAX_VALUE			8
+#define XHDCP1X_CIPHER_SHIFT_KEYMGMT_CONTROL_SET_SELECT		16
+#define XHDCP1X_CIPHER_NUM_LANES_1				1
+#define XHDCP1X_CIPHER_NUM_LANES_2				2
+#define XHDCP1X_CIPHER_NUM_LANES_4				4
+#define XHDCP1X_CIPHER_BITMASK_CIPHER_BX			GENMASK(27, 0)
+#define XHDCP1X_CIPHER_BITMASK_CIPHER_BY			GENMASK(27, 0)
+#define XHDCP1X_CIPHER_BITMASK_CIPHER_BZ_REPEATER		BIT(8)
+#define XHDCP1X_CIPHER_BITMASK_CIPHER_BZ			GENMASK(7, 0)
+#define XHDCP1X_CIPHER_SHIFT_CIPHER_B				28
+
+enum xhdcp1x_cipher_request_type {
+	XHDCP1X_CIPHER_REQUEST_BLOCK = 0,
+	XHDCP1X_CIPHER_REQUEST_REKEY = 1,
+	XHDCP1X_CIPHER_REQUEST_RNG = 2,
+	XHDCP1X_CIPHER_REQUEST_MAX = 3,
+};
+
+/**
+ * struct xhdcp1x_cipher - hdcp1x cipher driver structure
+ * @interface_base: iommu base of interface driver
+ * @dev: Platform data
+ * @is_tx: flag for tx, 1 for tx and 0 for rx
+ * @is_hdmi: flag for HDMI, 1 for HDMI and 0 for DP
+ * @num_lanes: number of active lanes in interface driver, possible lanes are 1, 2 and 4
+ */
+struct xhdcp1x_cipher {
+	void __iomem *interface_base;
+	struct device *dev;
+	u8 is_tx;
+	u8 is_hdmi;
+	u8 num_lanes;
+};
+
+/********************** Static function definations ***************************/
+static void xhdcp1x_cipher_write(struct xhdcp1x_cipher *cipher,
+				 int offset, u32 val)
+{
+	writel(val, cipher->interface_base + offset);
+}
+
+static u32 xhdcp1x_cipher_read(struct xhdcp1x_cipher *cipher, int offset)
+{
+	return readl(cipher->interface_base + offset);
+}
+
+static void xhdcp1x_cipher_set_mask(struct xhdcp1x_cipher *cipher, int offset,
+				    u32 set_mask)
+{
+	u32 value;
+
+	value = xhdcp1x_cipher_read(cipher, offset);
+	value |= set_mask;
+	xhdcp1x_cipher_write(cipher, offset, value);
+}
+
+static void xhdcp1x_cipher_clr_mask(struct xhdcp1x_cipher *cipher, int offset,
+				    u32 clr_mask)
+{
+	u32 value;
+
+	value = xhdcp1x_cipher_read(cipher, offset);
+	value &= ~clr_mask;
+	xhdcp1x_cipher_write(cipher, offset, value);
+}
+
+static int xhdcp1x_cipher_is_enabled(struct xhdcp1x_cipher *cipher)
+{
+	return (xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CONTROL) &
+			XHDCP1X_CIPHER_BITMASK_CONTROL_ENABLE);
+}
+
+static u8 xhdcp1x_cipher_is_localksv_ready(struct xhdcp1x_cipher *cipher)
+{
+	return (xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_KEYMGMT_STATUS) &
+				    XHDCP1X_CIPHER_BITMASK_KEYMGMT_STATUS_KSV_READY);
+}
+
+static u8 xhdcp1x_cipher_is_km_ready(struct xhdcp1x_cipher *cipher)
+{
+	return (xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_KEYMGMT_STATUS) &
+				    XHDCP1X_CIPHER_BITMASK_KEYMGMT_STATUS_KM_READY);
+}
+
+static u64 xhdcp1x_cipher_get_localksv(struct xhdcp1x_cipher *cipher)
+{
+	u64 ksv;
+	u32 val;
+	u32 guard = XHDCP1X_CIPHER_KSV_RETRIES;
+
+	if (!xhdcp1x_cipher_is_enabled(cipher))
+		return 0;
+
+	/* Check if the local ksv is not available */
+	val = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_KEYMGMT_STATUS);
+	val &= XHDCP1X_CIPHER_BITMASK_KEYMGMT_STATUS_KSV_READY;
+
+	if (val)
+		return 0;
+
+	/* Abort any running KM calculation just in case */
+	xhdcp1x_cipher_set_mask(cipher,
+				XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_ABORT_KM);
+	xhdcp1x_cipher_clr_mask(cipher,
+				XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_ABORT_KM);
+
+	/* Load the local ksv */
+	xhdcp1x_cipher_set_mask(cipher,
+				XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_LOCAL_KSV);
+	xhdcp1x_cipher_clr_mask(cipher,
+				XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_LOCAL_KSV);
+
+	while ((!xhdcp1x_cipher_is_localksv_ready(cipher)) && (--guard > 0))
+		;
+
+	if (!guard)
+		return 0;
+
+	ksv = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_KSV_LOCAL_H);
+	ksv = (ksv & 0xFF) << 32;
+	ksv |= xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_KSV_LOCAL_L);
+
+	return ksv;
+}
+
+static void xhdcp1x_cipher_config_lanes(struct xhdcp1x_cipher *cipher)
+{
+	u32 value;
+
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CONTROL);
+	value &= ~XHDCP1X_CIPHER_BITMASK_CONTROL_NUM_LANES;
+	value |= FIELD_PREP(XHDCP1X_CIPHER_BITMASK_CONTROL_NUM_LANES,
+			   cipher->num_lanes);
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CONTROL, value);
+}
+
+static int xhdcp1x_cipher_do_request(void *ref,
+				     enum xhdcp1x_cipher_request_type request)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u32 value;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (request < XHDCP1X_CIPHER_REQUEST_BLOCK ||
+	    request >= XHDCP1X_CIPHER_REQUEST_MAX)
+		return -EINVAL;
+
+	if (!xhdcp1x_cipher_is_enabled(cipher))
+		return -EINVAL;
+
+	/* Determine if there is a request in progress */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_STATUS);
+	value &= XHDCP1X_CIPHER_BITMASK_CIPHER_STATUS_REQUEST_IN_PROG;
+	if (value)
+		return -EBUSY;
+
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	/* Set the appropriate request bit and ensure that KM is always used */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_CONTROL);
+	value &= ~XHDCP1X_CIPHER_BITMASK_CIPHER_CONTROL_REQUEST;
+	value |= (XHDCP1X_CIPHER_VALUE_CIPHER_CONTROL_REQUEST_BLOCK << request);
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CIPHER_CONTROL, value);
+
+	/* Ensure that the request bit(s) get cleared for next time */
+	xhdcp1x_cipher_clr_mask(cipher, XHDCP1X_CIPHER_REG_CIPHER_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CIPHER_CONTROL_REQUEST);
+
+	return 0;
+}
+
+/********************** Public function definitions ***************************/
+/**
+ * xhdcp1x_cipher_init - Create and initialize the cipher driver instance
+ * @dev: Pointer to platform structure
+ * @hdcp1x_base: Pointer to interface iomem base
+ * This function instantiate the cipher driver and initializes it.
+ *
+ * Return: void reference to cipher driver instance on success, error otherwise
+ */
+void *xhdcp1x_cipher_init(struct device *dev, void __iomem *hdcp1x_base)
+{
+	struct xhdcp1x_cipher *cipher;
+	u32 reg;
+
+	if (!dev || !hdcp1x_base)
+		return ERR_PTR(-EINVAL);
+
+	cipher = devm_kzalloc(dev, sizeof(*cipher), GFP_KERNEL);
+	if (!cipher)
+		return ERR_PTR(-ENOMEM);
+
+	cipher->dev = dev;
+	cipher->interface_base = hdcp1x_base;
+	cipher->num_lanes = XHDCP1X_CIPHER_MAX_LANES;
+
+	reg = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_TYPE);
+	cipher->is_tx = reg & XHDCP1X_CIPHER_BITMASK_TYPE_DIRECTION;
+	cipher->is_hdmi = (reg & XHDCP1X_CIPHER_BITMASK_TYPE_PROTOCOL) &
+			   XHDCP1X_CIPHER_VALUE_TYPE_PROTOCOL_HDMI;
+
+	xhdcp1x_cipher_reset(cipher);
+
+	return (void *)cipher;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_init);
+
+/**
+ * xhdcp1x_cipher_reset - Reset cipher
+ * @ref: reference to cipher instance
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_reset(void *ref)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!cipher)
+		return -EINVAL;
+
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_RESET);
+	xhdcp1x_cipher_clr_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_RESET);
+
+	/* Ensure all interrupts are disabled and cleared */
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_INTERRUPT_MASK,
+			     XHDCP1X_CIPHER_INTR_ALL);
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_INTERRUPT_STATUS,
+			     XHDCP1X_CIPHER_INTR_ALL);
+
+	if (!cipher->is_hdmi)
+		xhdcp1x_cipher_config_lanes(cipher);
+
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_reset);
+
+/**
+ * xhdcp1x_cipher_enable - Enable cipher
+ * @ref: reference to cipher instance
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_enable(void *ref)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u32 value;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (xhdcp1x_cipher_is_enabled(cipher))
+		return -EBUSY;
+
+	xhdcp1x_cipher_clr_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	/* Ensure that all encryption is disabled for now */
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_H, 0);
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_L, 0);
+
+	/* Ensure that XOR is disabled on tx and enabled for rx to start */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_CONTROL);
+	if (cipher->is_tx)
+		value &= ~XHDCP1X_CIPHER_BITMASK_CIPHER_CONTROL_XOR_ENABLE;
+	else
+		value |= XHDCP1X_CIPHER_BITMASK_CIPHER_CONTROL_XOR_ENABLE;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CIPHER_CONTROL, value);
+
+	/* Enable it */
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_ENABLE);
+
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_enable);
+
+/**
+ * xhdcp1x_cipher_disable - Disable cipher
+ * @ref: reference to cipher instance
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_disable(void *ref)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!cipher)
+		return -EINVAL;
+
+	/* Ensure all interrupts are disabled */
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_INTERRUPT_MASK,
+			     XHDCP1X_CIPHER_INTR_ALL);
+
+	/* Enable bypass operation */
+	xhdcp1x_cipher_clr_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_ENABLE);
+
+	/* Ensure that all encryption is disabled for now */
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_H, 0x00);
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_ENCRYPT_ENABLE_L, 0x00);
+
+	/* Ensure that XOR is disabled */
+	xhdcp1x_cipher_clr_mask(cipher, XHDCP1X_CIPHER_REG_CIPHER_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CIPHER_CONTROL_XOR_ENABLE);
+
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_disable);
+
+/**
+ * xhdcp1x_cipher_set_num_lanes - Set number of active lanes in cipher
+ * @ref: reference to cipher instance
+ * @num_lanes: Number of active lanes
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_set_num_lanes(void *ref, u8 num_lanes)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (num_lanes != XHDCP1X_CIPHER_NUM_LANES_1 &&
+	    num_lanes != XHDCP1X_CIPHER_NUM_LANES_2 &&
+	    num_lanes != XHDCP1X_CIPHER_NUM_LANES_4)
+		return -EINVAL;
+
+	cipher->num_lanes = num_lanes;
+
+	xhdcp1x_cipher_config_lanes(cipher);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_set_num_lanes);
+
+/**
+ * xhdcp1x_cipher_set_keyselect - Selects the key vector to read from keymgmt block
+ * @ref: reference to cipher instance
+ * @keyselect: key vector number
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_set_keyselect(void *ref, u8 keyselect)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u32 value;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (keyselect > XHDCP1X_CIPHER_KEYSELECT_MAX_VALUE)
+		return -EINVAL;
+
+	/* Update the device */
+	value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL);
+	value &= ~XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_SET_SELECT;
+	value |= (keyselect << XHDCP1X_CIPHER_SHIFT_KEYMGMT_CONTROL_SET_SELECT);
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL, value);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_set_keyselect);
+
+/**
+ * xhdcp1x_cipher_load_bksv - load local ksv from cipher to buf
+ * @ref: reference to cipher instance
+ * @buf: 5 byte buffer to store the local KSV
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_load_bksv(void *ref, u8 *buf)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u64 my_ksv;
+	u32 is_enabled;
+
+	if (!cipher || !buf)
+		return -EINVAL;
+
+	is_enabled = xhdcp1x_cipher_is_enabled(cipher);
+
+	xhdcp1x_cipher_enable(cipher);
+	my_ksv = xhdcp1x_cipher_get_localksv(cipher);
+	if (!is_enabled)
+		xhdcp1x_cipher_disable(cipher);
+	if (!my_ksv)
+		return -EAGAIN;
+	memcpy(buf, &my_ksv, XHDCP1X_CIPHER_SIZE_LOCAL_KSV);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_load_bksv);
+
+/**
+ * xhdcp1x_cipher_set_remoteksv - set remote device KSV into cipher
+ * @ref: reference to cipher instance
+ * @ksv: remote device KSV
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_set_remoteksv(void *ref, u64 ksv)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u32 guard = XHDCP1X_CIPHER_KSV_RETRIES;
+	u32 value;
+
+	if (!cipher || !ksv)
+		return -EINVAL;
+
+	if (!xhdcp1x_cipher_is_enabled(cipher))
+		return -EINVAL;
+
+	/* Read local ksv to put things into a known state */
+	xhdcp1x_cipher_get_localksv(cipher);
+
+	xhdcp1x_cipher_clr_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	value = (u32)ksv;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_KSV_REMOTE_L, value);
+	value = (ksv >> 32) & 0xFF;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_KSV_REMOTE_H, value);
+
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	/* Trigger the calculation of theKM */
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_BEGIN_KM);
+	xhdcp1x_cipher_clr_mask(cipher, XHDCP1X_CIPHER_REG_KEYMGMT_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_KEYMGMT_CONTROL_BEGIN_KM);
+
+	/* Wait until KM is available */
+	while ((!xhdcp1x_cipher_is_km_ready(cipher)) && (--guard > 0))
+		;
+
+	if (!guard)
+		return -EAGAIN;
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_set_remoteksv);
+
+/**
+ * xhdcp1x_cipher_get_ro - Read Ro from cipher
+ * @ref: reference to cipher instance
+ * @ro: reference to ro data
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_get_ro(void *ref, u16 *ro)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!cipher || !ro)
+		return -EINVAL;
+
+	if (!xhdcp1x_cipher_is_enabled(cipher))
+		return -EINVAL;
+
+	*ro = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_RO);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_get_ro);
+
+/**
+ * xhdcp1x_cipher_set_b - Set B value into cipher
+ * @ref: reference to cipher instance
+ * @an: An value
+ * @is_repeater: repeater flag
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_set_b(void *ref, u64 an, bool is_repeater)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u32 value, x, y, z;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (!xhdcp1x_cipher_is_enabled(cipher))
+		return -EINVAL;
+
+	x = (u32)(an & XHDCP1X_CIPHER_BITMASK_CIPHER_BX);
+	an >>= XHDCP1X_CIPHER_SHIFT_CIPHER_B;
+	y = (u32)(an & XHDCP1X_CIPHER_BITMASK_CIPHER_BY);
+	an >>= XHDCP1X_CIPHER_SHIFT_CIPHER_B;
+	z = (u32)an;
+	if (is_repeater)
+		z |= XHDCP1X_CIPHER_BITMASK_CIPHER_BZ_REPEATER;
+	z &= XHDCP1X_CIPHER_BITMASK_CIPHER_BZ;
+
+	xhdcp1x_cipher_clr_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	value = x & XHDCP1X_CIPHER_BITMASK_CIPHER_BX;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CIPHER_BX, value);
+	value = y & XHDCP1X_CIPHER_BITMASK_CIPHER_BY;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CIPHER_BY, value);
+	value = z & XHDCP1X_CIPHER_BITMASK_CIPHER_BZ;
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_CIPHER_BZ, value);
+
+	xhdcp1x_cipher_set_mask(cipher, XHDCP1X_CIPHER_REG_CONTROL,
+				XHDCP1X_CIPHER_BITMASK_CONTROL_UPDATE);
+
+	xhdcp1x_cipher_do_request(cipher, XHDCP1X_CIPHER_REQUEST_BLOCK);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_set_b);
+
+/**
+ * xhdcp1x_cipher_is_request_complete - check requested operation is completed
+ * @ref: reference to cipher instance
+ *
+ * Return: 1 on request completion, 0 otherwise
+ */
+int xhdcp1x_cipher_is_request_complete(void *ref)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!cipher)
+		return -EINVAL;
+
+	return !(xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_CIPHER_STATUS) &
+			XHDCP1X_CIPHER_BITMASK_CIPHER_STATUS_REQUEST_IN_PROG);
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_is_request_complete);
+
+/**
+ * xhdcp1x_cipher_set_link_state_check - Enable/Disable link status check
+ * @ref: reference to cipher instance
+ * @is_enabled: 1 for enable, 0 for disable
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_set_link_state_check(void *ref, bool is_enabled)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (cipher->is_hdmi || cipher->is_tx)
+		return -EINVAL;
+
+	xhdcp1x_cipher_write(cipher, XHDCP1X_CIPHER_REG_INTERRUPT_STATUS,
+			     XHDCP1X_CIPHER_BITMASK_INTERRUPT_LINK_FAIL);
+
+	if (is_enabled)
+		xhdcp1x_cipher_clr_mask(cipher,
+					XHDCP1X_CIPHER_REG_INTERRUPT_MASK,
+					XHDCP1X_CIPHER_BITMASK_INTERRUPT_LINK_FAIL);
+	else
+		xhdcp1x_cipher_set_mask(cipher,
+					XHDCP1X_CIPHER_REG_INTERRUPT_MASK,
+					XHDCP1X_CIPHER_BITMASK_INTERRUPT_LINK_FAIL);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_set_link_state_check);
+
+/**
+ * xhdcp1x_cipher_get_interrupts - Read and clear the interrupts and return same
+ * @ref: reference to cipher instance
+ * @interrupts: reference to interrupts data
+ *
+ * Return: 0 on success, error otherwise
+ */
+int xhdcp1x_cipher_get_interrupts(void *ref, u32 *interrupts)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+
+	if (!cipher || !interrupts)
+		return -EINVAL;
+
+	/* Read and clear the interrupts */
+	*interrupts = xhdcp1x_cipher_read(cipher,
+					  XHDCP1X_CIPHER_REG_INTERRUPT_STATUS);
+	*interrupts &= xhdcp1x_cipher_read(cipher,
+					   XHDCP1X_CIPHER_REG_INTERRUPT_MASK);
+	if (*interrupts)
+		xhdcp1x_cipher_write(cipher,
+				     XHDCP1X_CIPHER_REG_INTERRUPT_STATUS,
+				     *interrupts);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_get_interrupts);
+
+/**
+ * xhdcp1x_cipher_is_linkintegrity_failed - Check if link integrity failed
+ * @ref: reference to cipher instance
+ *
+ * Return: 1 if link integrity failed, 0/error value otherwise
+ */
+int xhdcp1x_cipher_is_linkintegrity_failed(void *ref)
+{
+	struct xhdcp1x_cipher *cipher = (struct xhdcp1x_cipher *)ref;
+	u32 value;
+
+	if (!cipher)
+		return -EINVAL;
+
+	if (xhdcp1x_cipher_is_enabled(cipher)) {
+		value = xhdcp1x_cipher_read(cipher, XHDCP1X_CIPHER_REG_STATUS);
+		if (value & XHDCP1X_CIPHER_BITMASK_INTERRUPT_LINK_FAIL)
+			return 1;
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(xhdcp1x_cipher_is_linkintegrity_failed);
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xlnx_tsmux/Kconfig
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xlnx_tsmux/Kconfig	2023-07-05 08:33:32.307812400 +0900
@@ -0,0 +1,11 @@
+config XLNX_TSMUX
+	tristate "Xilinx MPEG2 Transport Stream Muxer"
+	select DMA_SHARED_BUFFER
+	help
+	  This driver is developed for mpeg2 transport stream muxer,
+	  designed to allow passage of multimedia streams from the source
+	  kernel sub-system, prepares mpeg2 transport stream and forward
+	  to the sink kernel subsystem.
+
+	  To compile this driver as a module, choose M here.
+	  If unsure, choose N.
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xlnx_tsmux/MAINTAINERS
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xlnx_tsmux/MAINTAINERS	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,4 @@
+XILINX MPG2TSMUX DRIVER
+M:	Venkateshwar Rao <venkateshwar.rao.gannavarapu@xilinx.com>
+S:	Maintained
+F:	drivers/staging/xlnx_tsmux
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xlnx_tsmux/Makefile
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xlnx_tsmux/Makefile	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1 @@
+obj-$(CONFIG_XLNX_TSMUX) += xlnx_mpg2tsmux.o
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xlnx_tsmux/dt-binding.txt
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xlnx_tsmux/dt-binding.txt	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,28 @@
+The Xilinx mpegtsmux IP reads the elementary streams from memory and
+writes the MPEG2 TS(transport stream) to memory.
+
+The mpeg2 ts muxer follows the dma descriptor based approach. Each DMA
+descriptor contains information about each of the elementary stream
+buffer properties and buffer address. It reads the descriptors one after
+the other and generates the TS packets with the information in the
+descriptor. The IP writes the generated TS packets at the output buffer
+address.
+
+Required properties:
+
+- compatible: must be "xlnx,tsmux-1.0"
+- interrupts: interrupt number
+- interrupts-parent: phandle for interrupt controller
+- reg: base address and size of the IP core
+- clock-names: must contain "ap_clk"
+- clocks: phandle to AXI Lite
+
+Example:
+	ts2mux: ts2mux@0xa0200000 {
+		compatible = "xlnx,tsmux-1.0";
+		interrupt-parent = <&gic>;
+		interrupts = <0 90 4>;
+		reg = <0x0 0xa0200000 0x0 0x30000>;
+		clock-names = "ap_clk";
+		clocks = <&misc_clk_0>;
+	};
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xlnx_tsmux/xlnx_mpg2tsmux.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xlnx_tsmux/xlnx_mpg2tsmux.c	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,1568 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx TS mux driver
+ *
+ * Copyright (C) 2019 Xilinx, Inc.
+ *
+ * Author: Venkateshwar Rao G <venkateshwar.rao.gannavarapu@xilinx.com>
+ */
+#include <linux/bitops.h>
+#include <linux/cdev.h>
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/dmapool.h>
+#include <linux/dma-buf.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/platform_device.h>
+#include <linux/poll.h>
+#include <uapi/linux/xlnx_mpg2tsmux_interface.h>
+
+#define DRIVER_NAME "mpegtsmux-1.0"
+#define DRIVER_CLASS "mpg2mux_ts_cls"
+#define DRIVER_MAX_DEV (10)
+
+/* Register offsets and bit masks */
+#define XTSMUX_RST_CTRL			0x00
+#define XTSMUX_GLBL_IER			0x04
+#define XTSMUX_IER_STAT			0x08
+#define XTSMUX_ISR_STAT			0x0c
+#define XTSMUX_ERR_STAT			0x10
+#define XTSMUX_LAST_NODE_PROCESSED	0x14
+#define XTSMUX_MUXCONTEXT_ADDR		0x20
+#define XTSMUX_STREAMCONTEXT_ADDR	0x30
+#define XTSMUX_NUM_STREAM_IDTBL		0x48
+#define XTSMUX_NUM_DESC			0x70
+#define XTSMUX_STREAM_IDTBL_ADDR	0x78
+#define XTSMUX_CONTEXT_DATA_SIZE	64
+
+#define XTSMUX_RST_CTRL_START_MASK	BIT(0)
+#define XTSMUX_GLBL_IER_ENABLE_MASK	BIT(0)
+#define XTSMUX_IER_ENABLE_MASK		BIT(0)
+
+/* Number of input/output streams supported */
+#define XTSMUX_MAXIN_STRM		112
+#define XTSMUX_MAXIN_PLSTRM		16
+#define XTSMUX_MAXIN_TLSTRM	(XTSMUX_MAXIN_STRM + XTSMUX_MAXIN_PLSTRM)
+#define XTSMUX_MAXOUT_STRM		112
+#define XTSMUX_MAXOUT_PLSTRM		16
+#define XTSMUX_MAXOUT_TLSTRM	(XTSMUX_MAXOUT_STRM + XTSMUX_MAXOUT_PLSTRM)
+#define XTSMUX_POOL_SIZE		128
+/* Initial version is tested with 256 align only */
+#define XTSMUX_POOL_ALIGN		256
+#define XTSMUX_STRMBL_FREE		0
+#define XTSMUX_STRMBL_BUSY		1
+
+/**
+ * struct stream_context - struct to enqueue a stream context descriptor
+ * @command: stream context type
+ * @is_pcr_stream: flag for pcr(programmable clock recovery) stream
+ * @stream_id: stream identification number
+ * @extended_stream_id: extended stream id
+ * @reserved1: reserved for hardware alignment
+ * @pid: packet id number
+ * @dmabuf_id: 0 for buf allocated by driver, nonzero for external buf
+ * @size_data_in: size in bytes of input buffer
+ * @pts: presentation time stamp
+ * @dts: display time stamp
+ * @in_buf_pointer: physical address of src buf address
+ * @reserved2: reserved for hardware alignment
+ * @insert_pcr: inserting pcr in stream context
+ * @reserved3: reserved for hardware alignment
+ * @pcr_extension: pcr extension number
+ * @pcr_base: pcr base number
+ */
+struct stream_context {
+	enum ts_mux_command command;
+	bool is_pcr_stream;
+	u8 stream_id;
+	u8 extended_stream_id;
+	u8 reserved1;
+	u16 pid;
+	u16 dmabuf_id;
+	u32 size_data_in;
+	u64 pts;
+	u64 dts;
+	u64 in_buf_pointer;
+	u32 reserved2;
+	bool insert_pcr;
+	bool reserved3;
+	u16 pcr_extension;
+	u64 pcr_base;
+};
+
+/**
+ * enum node_status_info - status of stream context
+ * @NOT_FILLED: node not filled
+ * @UPDATED_BY_DRIVER: updated by driver
+ * @READ_BY_IP: read by IP
+ * @USED_BY_IP: used by IP
+ * @NODE_INVALID: invalid node
+ */
+enum node_status_info {
+	NOT_FILLED = 0,
+	UPDATED_BY_DRIVER,
+	READ_BY_IP,
+	USED_BY_IP,
+	NODE_INVALID
+};
+
+/**
+ * enum stream_errors - stream context error type
+ * @NO_ERROR: no error
+ * @PARTIAL_FRAME_WRITTEN: partial frame written
+ * @DESCRIPTOR_NOT_READABLE: descriptor not readable
+ */
+enum stream_errors {
+	NO_ERROR = 0,
+	PARTIAL_FRAME_WRITTEN,
+	DESCRIPTOR_NOT_READABLE
+};
+
+/**
+ * struct strm_node - struct to describe stream node in linked list
+ * @node_number: node number to handle streams
+ * @node_status: status of stream node
+ * @element: stream context info
+ * @error_code: error codes
+ * @reserved1: reserved bits for hardware align
+ * @tail_pointer: physical address of next stream node in linked list
+ * @strm_phy_addr: physical address of stream context
+ * @node: struct of linked list head
+ * @reserved2: reserved for hardware align
+ */
+struct stream_context_node {
+	u32 node_number;
+	enum node_status_info node_status;
+	struct stream_context element;
+	enum stream_errors error_code;
+	u32 reserved1;
+	u64 tail_pointer;
+	u64 strm_phy_addr;
+	struct list_head node;
+	u64 reserved2;
+};
+
+/**
+ * struct strm_info - struct to describe streamid node in streamid table
+ * @pid: identification number of stream
+ * @continuity_counter: counter to maintain packet count for a stream
+ * @usageflag: flag to know free or under use for allocating streamid node
+ * @strmtbl_update: struct to know enqueue or dequeue streamid in table
+ */
+struct stream_info {
+	u16 pid;
+	u8 continuity_counter;
+	bool usageflag;
+	enum strmtbl_cnxt strmtbl_update;
+};
+
+/* Enum for error handling of mux context */
+enum mux_op_errs {
+	MUXER_NO_ERROR = 0,
+	ERROR_OUTPUT_BUFFER_IS_NOT_ACCESIBLE,
+	ERROR_PARTIAL_PACKET_WRITTEN
+};
+
+/**
+ * struct muxer_context - struct to describe mux node in linked list
+ * @node_status: status of mux node
+ * @reserved: reserved for hardware align
+ * @dst_buf_start_addr: physical address of dst buf
+ * @dst_buf_size: size of the output buffer
+ * @dst_buf_written: size of data written in dst buf
+ * @num_of_pkts_written: number of packets in dst buf
+ * @error_code: error status of mux node updated by IP
+ * @mux_phy_addr: physical address of muxer
+ * @node: struct of linked list head
+ */
+struct muxer_context {
+	enum node_status_info node_status;
+	u32 reserved;
+	u64 dst_buf_start_addr;
+	u32 dst_buf_size;
+	u32 dst_buf_written;
+	u32 num_of_pkts_written;
+	enum mux_op_errs error_code;
+	u64 mux_phy_addr;
+	struct list_head node;
+};
+
+/**
+ * struct xlnx_tsmux_dmabufintl - dma buf internal info
+ * @dbuf: reference to a buffer's dmabuf struct
+ * @attach: attachment to the buffer's dmabuf
+ * @sgt: scatterlist info for the buffer's dmabuf
+ * @dmabuf_addr: buffer physical address
+ * @dmabuf_fd: dma buffer fd
+ * @buf_id: dma buffer reference id
+ */
+struct xlnx_tsmux_dmabufintl {
+	struct dma_buf *dbuf;
+	struct dma_buf_attachment *attach;
+	struct sg_table *sgt;
+	dma_addr_t dmabuf_addr;
+	s32 dmabuf_fd;
+	u16 buf_id;
+};
+
+/**
+ * struct xlnx_tsmux - xilinx mpeg2 TS muxer device
+ * @dev: pointer to struct device instance used by the driver
+ * @iomem: base address of the HW/IP
+ * @chdev: char device handle
+ * @user_count: count of users who have opened the device
+ * @lock: spinlock to protect driver data structures
+ * @waitq: wait queue used by the driver
+ * @irq: irq number
+ * @id: device instance ID
+ * @num_inbuf: number of input buffers allocated uisng DMA
+ * @num_outbuf: number of output buffers allocated uisng DMA
+ * @srcbuf_size: size of each source buffer
+ * @dstbuf_size: size of each destination buffer
+ * @strm_node: list containing descriptors of stream context
+ * @mux_node: list containing descriptors of mux context
+ * @stcxt_node_cnt: stream number used for maintaing list
+ * @num_strmnodes: number of stream nodes in the streamid table
+ * @intn_stream_count: internal count of streams added to stream context
+ * @outbuf_idx: index number to maintain output buffers
+ * @srcbuf_addrs: physical address of source buffer
+ * @dstbuf_addrs: physical address of destination buffer
+ * @src_kaddrs: kernel VA for source buffer allocated by the driver
+ * @dst_kaddrs: kernel VA for destination buffer allocated by the driver
+ * @strm_ctx_pool: dma pool to allocate stream context buffers
+ * @mux_ctx_pool: dma pool to allocate mux context buffers
+ * @strmtbl_addrs: physical address of streamid table
+ * @strmtbl_kaddrs: kernel VA for streamid table
+ * @intn_strmtbl_addrs: physical address of streamid table for internal
+ * @intn_strmtbl_kaddrs: kernel VA for streamid table for internal
+ * @ap_clk: interface clock
+ * @src_dmabufintl: array of src DMA buf allocated by user
+ * @dst_dmabufintl: array of src DMA buf allocated by user
+ * @outbuf_written: size in bytes written in output buffer
+ * @stream_count: stream count
+ */
+struct xlnx_tsmux {
+	struct device *dev;
+	void __iomem *iomem;
+	struct cdev chdev;
+	atomic_t user_count;
+	/* lock is used to protect access to sync_err and wdg_err */
+	spinlock_t lock;
+	wait_queue_head_t waitq;
+	s32 irq;
+	s32 id;
+	u32 num_inbuf;
+	u32 num_outbuf;
+	size_t srcbuf_size;
+	size_t dstbuf_size;
+	struct list_head strm_node;
+	struct list_head mux_node;
+	u32 stcxt_node_cnt;
+	u32 num_strmnodes;
+	atomic_t intn_stream_count;
+	atomic_t outbuf_idx;
+	dma_addr_t srcbuf_addrs[XTSMUX_MAXIN_TLSTRM];
+	dma_addr_t dstbuf_addrs[XTSMUX_MAXOUT_TLSTRM];
+	void *src_kaddrs[XTSMUX_MAXIN_TLSTRM];
+	void *dst_kaddrs[XTSMUX_MAXOUT_TLSTRM];
+	struct dma_pool *strm_ctx_pool;
+	struct dma_pool *mux_ctx_pool;
+	dma_addr_t strmtbl_addrs;
+	void *strmtbl_kaddrs;
+	dma_addr_t intn_strmtbl_addrs;
+	void *intn_strmtbl_kaddrs;
+	struct clk *ap_clk;
+	struct xlnx_tsmux_dmabufintl src_dmabufintl[XTSMUX_MAXIN_STRM];
+	struct xlnx_tsmux_dmabufintl dst_dmabufintl[XTSMUX_MAXOUT_STRM];
+	s32 outbuf_written;
+	atomic_t stream_count;
+};
+
+static inline u32 xlnx_tsmux_read(const struct xlnx_tsmux *mpgmuxts,
+				  const u32 reg)
+{
+	return ioread32(mpgmuxts->iomem + reg);
+}
+
+static inline void xlnx_tsmux_write(const struct xlnx_tsmux *mpgmuxts,
+				    const u32 reg, const u32 val)
+{
+	iowrite32(val, (void __iomem *)(mpgmuxts->iomem + reg));
+}
+
+/* TODO: Optimize using iowrite64 call */
+static inline void xlnx_tsmux_write64(const struct xlnx_tsmux *mpgmuxts,
+				      const u32 reg, const u64 val)
+{
+	iowrite32(lower_32_bits(val), (void __iomem *)(mpgmuxts->iomem + reg));
+	iowrite32(upper_32_bits(val), (void __iomem *)(mpgmuxts->iomem +
+						       reg + 4));
+}
+
+static int xlnx_tsmux_start_muxer(struct xlnx_tsmux *mpgmuxts)
+{
+	struct stream_context_node *new_strm_node;
+	struct muxer_context *new_mux_node;
+
+	new_mux_node = list_first_entry_or_null(&mpgmuxts->mux_node,
+						struct muxer_context, node);
+	if (!new_mux_node)
+		return -ENXIO;
+
+	xlnx_tsmux_write64(mpgmuxts, XTSMUX_MUXCONTEXT_ADDR,
+			   new_mux_node->mux_phy_addr);
+
+	new_strm_node = list_first_entry_or_null(&mpgmuxts->strm_node,
+						 struct stream_context_node,
+						 node);
+	if (!new_strm_node)
+		return -ENXIO;
+
+	xlnx_tsmux_write64(mpgmuxts, XTSMUX_STREAMCONTEXT_ADDR,
+			   new_strm_node->strm_phy_addr);
+
+	xlnx_tsmux_write(mpgmuxts, XTSMUX_NUM_DESC,
+			 atomic_read(&mpgmuxts->intn_stream_count));
+
+	xlnx_tsmux_write64(mpgmuxts, XTSMUX_STREAM_IDTBL_ADDR,
+			   (u64)mpgmuxts->intn_strmtbl_addrs);
+	xlnx_tsmux_write(mpgmuxts, XTSMUX_NUM_STREAM_IDTBL, 1);
+	xlnx_tsmux_write(mpgmuxts, XTSMUX_GLBL_IER,
+			 XTSMUX_GLBL_IER_ENABLE_MASK);
+	xlnx_tsmux_write(mpgmuxts, XTSMUX_IER_STAT,
+			 XTSMUX_IER_ENABLE_MASK);
+
+	xlnx_tsmux_write(mpgmuxts, XTSMUX_RST_CTRL,
+			 XTSMUX_RST_CTRL_START_MASK);
+
+	return 0;
+}
+
+static void xlnx_tsmux_stop_muxer(const struct xlnx_tsmux *mpgmuxts)
+{
+	xlnx_tsmux_write(mpgmuxts, XTSMUX_GLBL_IER, 0);
+	xlnx_tsmux_write(mpgmuxts, XTSMUX_IER_STAT, 0);
+	xlnx_tsmux_write(mpgmuxts, XTSMUX_RST_CTRL, 0);
+}
+
+static enum xlnx_tsmux_status xlnx_tsmux_get_status(const struct
+						    xlnx_tsmux * mpgmuxts)
+{
+	u32 status;
+
+	status = xlnx_tsmux_read(mpgmuxts, XTSMUX_RST_CTRL);
+
+	if (!status)
+		return MPG2MUX_ERROR;
+
+	if (status & XTSMUX_RST_CTRL_START_MASK)
+		return MPG2MUX_BUSY;
+
+	return MPG2MUX_READY;
+}
+
+static struct class *xlnx_tsmux_class;
+static dev_t xlnx_tsmux_devt;
+static atomic_t xlnx_tsmux_ndevs = ATOMIC_INIT(0);
+
+static int xlnx_tsmux_open(struct inode *pin, struct file *fptr)
+{
+	struct xlnx_tsmux *mpgtsmux;
+
+	mpgtsmux = container_of(pin->i_cdev, struct xlnx_tsmux, chdev);
+
+	fptr->private_data = mpgtsmux;
+	atomic_inc(&mpgtsmux->user_count);
+	atomic_set(&mpgtsmux->outbuf_idx, 0);
+	mpgtsmux->stcxt_node_cnt = 0;
+
+	return 0;
+}
+
+static int xlnx_tsmux_release(struct inode *pin, struct file *fptr)
+{
+	struct xlnx_tsmux *mpgtsmux = (struct xlnx_tsmux *)fptr->private_data;
+
+	if (!mpgtsmux)
+		return -EIO;
+
+	return 0;
+}
+
+/* TODO: Optimize buf alloc, dealloc API's to accommodate src, dst, strmtbl */
+static int xlnx_tsmux_ioctl_srcbuf_dealloc(struct xlnx_tsmux *mpgmuxts)
+{
+	unsigned int i;
+
+	for (i = 0; i < mpgmuxts->num_inbuf; i++) {
+		if (!mpgmuxts->src_kaddrs[i] || !mpgmuxts->srcbuf_addrs[i])
+			break;
+		dma_free_coherent(mpgmuxts->dev, mpgmuxts->srcbuf_size,
+				  mpgmuxts->src_kaddrs[i],
+				  mpgmuxts->srcbuf_addrs[i]);
+		mpgmuxts->src_kaddrs[i] = NULL;
+	}
+
+	return 0;
+}
+
+static int xlnx_tsmux_ioctl_srcbuf_alloc(struct xlnx_tsmux *mpgmuxts,
+					 void __user *arg)
+{
+	int ret;
+	unsigned int i;
+	struct strc_bufs_info buf_data;
+
+	ret = copy_from_user(&buf_data, arg, sizeof(struct strc_bufs_info));
+	if (ret < 0) {
+		dev_dbg(mpgmuxts->dev, "Failed to read input buffer info\n");
+		return ret;
+	}
+
+	if (buf_data.num_buf > XTSMUX_MAXIN_PLSTRM) {
+		dev_dbg(mpgmuxts->dev, "Excessive input payload. supported %d",
+			XTSMUX_MAXIN_PLSTRM);
+		return -EINVAL;
+	}
+
+	mpgmuxts->num_inbuf = buf_data.num_buf;
+	mpgmuxts->srcbuf_size = buf_data.buf_size;
+	/* buf_size & num_buf boundary conditions are handled in application
+	 * and initial version of driver tested with 32-bit addressing only
+	 */
+	for (i = 0; i < mpgmuxts->num_inbuf; i++) {
+		mpgmuxts->src_kaddrs[i] =
+			dma_alloc_coherent(mpgmuxts->dev,
+					   mpgmuxts->srcbuf_size,
+					   &mpgmuxts->srcbuf_addrs[i],
+					   GFP_KERNEL | GFP_DMA32);
+		if (!mpgmuxts->src_kaddrs[i]) {
+			dev_dbg(mpgmuxts->dev, "dma alloc fail %d buffer", i);
+			goto exit_free;
+		}
+	}
+
+	return 0;
+
+exit_free:
+	xlnx_tsmux_ioctl_srcbuf_dealloc(mpgmuxts);
+
+	return -ENOMEM;
+}
+
+static int xlnx_tsmux_ioctl_dstbuf_dealloc(struct xlnx_tsmux *mpgmuxts)
+{
+	unsigned int i;
+
+	for (i = 0; i < mpgmuxts->num_outbuf; i++) {
+		if (!mpgmuxts->dst_kaddrs[i] || !mpgmuxts->dstbuf_addrs[i])
+			break;
+		dma_free_coherent(mpgmuxts->dev, mpgmuxts->dstbuf_size,
+				  mpgmuxts->dst_kaddrs[i],
+				  mpgmuxts->dstbuf_addrs[i]);
+		mpgmuxts->dst_kaddrs[i] = NULL;
+	}
+
+	return 0;
+}
+
+static int xlnx_tsmux_ioctl_dstbuf_alloc(struct xlnx_tsmux *mpgmuxts,
+					 void __user *arg)
+{
+	int ret;
+	unsigned int i;
+	struct strc_bufs_info buf_data;
+
+	ret = copy_from_user(&buf_data, arg, sizeof(struct strc_bufs_info));
+	if (ret < 0) {
+		dev_dbg(mpgmuxts->dev, "%s: Failed to read output buffer info",
+			__func__);
+		return ret;
+	}
+
+	if (buf_data.num_buf > XTSMUX_MAXOUT_PLSTRM) {
+		dev_dbg(mpgmuxts->dev, "Excessive output payload supported %d",
+			XTSMUX_MAXOUT_PLSTRM);
+		return -EINVAL;
+	}
+
+	mpgmuxts->num_outbuf = buf_data.num_buf;
+	mpgmuxts->dstbuf_size = buf_data.buf_size;
+	/* buf_size & num_buf boundary conditions are handled in application*/
+	for (i = 0; i < mpgmuxts->num_outbuf; i++) {
+		mpgmuxts->dst_kaddrs[i] =
+			dma_alloc_coherent(mpgmuxts->dev,
+					   mpgmuxts->dstbuf_size,
+					   &mpgmuxts->dstbuf_addrs[i],
+					   GFP_KERNEL | GFP_DMA32);
+		if (!mpgmuxts->dst_kaddrs[i]) {
+			dev_dbg(mpgmuxts->dev, "dmamem alloc fail for %d", i);
+			goto exit_free;
+		}
+	}
+
+	return 0;
+
+exit_free:
+	xlnx_tsmux_ioctl_dstbuf_dealloc(mpgmuxts);
+
+	return -ENOMEM;
+}
+
+static int xlnx_tsmux_ioctl_strmtbl_dealloc(struct xlnx_tsmux *mpgmuxts)
+{
+	u32 buf_size;
+
+	buf_size = sizeof(struct stream_info) * mpgmuxts->num_strmnodes;
+	if (!mpgmuxts->strmtbl_kaddrs || !mpgmuxts->strmtbl_addrs)
+		return 0;
+
+	dma_free_coherent(mpgmuxts->dev, buf_size, mpgmuxts->strmtbl_kaddrs,
+			  mpgmuxts->strmtbl_addrs);
+	mpgmuxts->strmtbl_kaddrs = NULL;
+
+	if (!mpgmuxts->intn_strmtbl_kaddrs || !mpgmuxts->intn_strmtbl_addrs)
+		return 0;
+	dma_free_coherent(mpgmuxts->dev, buf_size,
+			  mpgmuxts->intn_strmtbl_kaddrs,
+			  mpgmuxts->intn_strmtbl_addrs);
+	mpgmuxts->intn_strmtbl_kaddrs = NULL;
+
+	return 0;
+}
+
+static int xlnx_tsmux_ioctl_strmtbl_alloc(struct xlnx_tsmux *mpgmuxts,
+					  void __user *arg)
+{
+	int ret, buf_size;
+	u16 num_nodes;
+
+	ret = copy_from_user(&num_nodes, arg, sizeof(u16));
+	if (ret < 0) {
+		dev_dbg(mpgmuxts->dev, "Failed to read streamid table info");
+		return ret;
+	}
+	mpgmuxts->num_strmnodes = num_nodes;
+	buf_size = sizeof(struct stream_info) * mpgmuxts->num_strmnodes;
+
+	mpgmuxts->strmtbl_kaddrs =
+		dma_alloc_coherent(mpgmuxts->dev,
+				   buf_size, &mpgmuxts->strmtbl_addrs,
+				   GFP_KERNEL | GFP_DMA32);
+	if (!mpgmuxts->strmtbl_kaddrs) {
+		dev_dbg(mpgmuxts->dev, "dmamem alloc fail for strm table");
+		return -ENOMEM;
+	}
+
+	/* Allocating memory for internal streamid table */
+	mpgmuxts->intn_strmtbl_kaddrs =
+		dma_alloc_coherent(mpgmuxts->dev,
+				   buf_size, &mpgmuxts->intn_strmtbl_addrs,
+				   GFP_KERNEL | GFP_DMA32);
+
+	if (!mpgmuxts->intn_strmtbl_kaddrs) {
+		dev_dbg(mpgmuxts->dev, "dmamem alloc fail for intr strm table");
+		goto exist_free;
+	}
+
+	return 0;
+exist_free:
+	xlnx_tsmux_ioctl_strmtbl_dealloc(mpgmuxts);
+
+	return -ENOMEM;
+}
+
+/**
+ * xlnx_tsmux_update_intstrm_tbl - updates stream id table
+ * @mpgmuxts: pointer to the device structure
+ *
+ * This function updates the stream id table
+ *
+ * Return: 0 on success and error value on failure.
+ *
+ */
+static int xlnx_tsmux_update_intstrm_tbl(struct xlnx_tsmux *mpgmuxts)
+{
+	struct stream_info *cptr, *intn_cptr;
+	int i, j;
+
+	cptr = (struct stream_info *)mpgmuxts->strmtbl_kaddrs;
+
+	if (!cptr->usageflag)
+		return 0;
+
+	for (i = 0; i < mpgmuxts->num_strmnodes && cptr->usageflag;
+	     i++, cptr++) {
+		intn_cptr = (struct stream_info *)mpgmuxts->intn_strmtbl_kaddrs;
+		/* Adding to table */
+		if (cptr->strmtbl_update == ADD_TO_TBL) {
+			for (j = 0; j < mpgmuxts->num_strmnodes;
+			     j++, intn_cptr++) {
+				if (!intn_cptr->usageflag) {
+					intn_cptr->pid = cptr->pid;
+					intn_cptr->continuity_counter = 0;
+					intn_cptr->usageflag = 1;
+					cptr->usageflag = 0;
+					break;
+				}
+			}
+			if (j == mpgmuxts->num_strmnodes)
+				return -EIO;
+		} else if (cptr->strmtbl_update == DEL_FR_TBL) {
+			/* deleting from table */
+			for (j = 0; j < mpgmuxts->num_strmnodes; j++,
+			     intn_cptr++) {
+				if (intn_cptr->usageflag) {
+					if (intn_cptr->pid == cptr->pid) {
+						intn_cptr->pid = 0;
+						intn_cptr->continuity_counter = 0;
+						intn_cptr->usageflag = 0;
+						cptr->usageflag = 0;
+						break;
+					}
+				}
+			}
+			if (j == mpgmuxts->num_strmnodes)
+				return -EIO;
+		} else {
+			return -EIO;
+		}
+	}
+
+	return 0;
+}
+
+static int xlnx_tsmux_update_strminfo_table(struct xlnx_tsmux *mpgmuxts,
+					    struct strc_strminfo new_strm_info)
+{
+	u32 i = 0;
+	struct stream_info *cptr;
+
+	cptr = (struct stream_info *)mpgmuxts->strmtbl_kaddrs;
+
+	/* Finding free memory block and writing input data into the block*/
+	for (i = 0; i < mpgmuxts->num_strmnodes; i++, cptr++) {
+		if (!cptr->usageflag) {
+			cptr->pid = new_strm_info.pid;
+			cptr->continuity_counter = 0;
+			cptr->usageflag = XTSMUX_STRMBL_BUSY;
+			cptr->strmtbl_update = new_strm_info.strmtbl_ctxt;
+			break;
+		}
+	}
+	if (i == mpgmuxts->num_strmnodes)
+		return -EIO;
+
+	return 0;
+}
+
+static int xlnx_tsmux_ioctl_update_strmtbl(struct xlnx_tsmux *mpgmuxts,
+					   void __user *arg)
+{
+	int ret;
+	struct strc_strminfo new_strm_info;
+
+	ret = copy_from_user(&new_strm_info, arg, sizeof(struct strc_strminfo));
+	if (ret < 0) {
+		dev_dbg(mpgmuxts->dev, "Reading strmInfo failed");
+		return ret;
+	}
+
+	return xlnx_tsmux_update_strminfo_table(mpgmuxts, new_strm_info);
+}
+
+static int xlnx_tsmux_enqueue_stream_context(struct xlnx_tsmux *mpgmuxts,
+					     struct
+					     stream_context_in * stream_data)
+{
+	struct stream_context_node *new_strm_node, *prev_strm_node;
+	void *kaddr_strm_node;
+	dma_addr_t strm_phy_addr;
+	unsigned long flags;
+	u32 i;
+
+	kaddr_strm_node = dma_pool_alloc(mpgmuxts->strm_ctx_pool,
+					 GFP_KERNEL | GFP_DMA32,
+					 &strm_phy_addr);
+
+	new_strm_node = (struct stream_context_node *)kaddr_strm_node;
+	if (!new_strm_node)
+		return -ENOMEM;
+
+	/* update the stream context node */
+	wmb();
+	new_strm_node->element.command = stream_data->command;
+	new_strm_node->element.is_pcr_stream = stream_data->is_pcr_stream;
+	new_strm_node->element.stream_id = stream_data->stream_id;
+	new_strm_node->element.extended_stream_id =
+				stream_data->extended_stream_id;
+	new_strm_node->element.pid = stream_data->pid;
+	new_strm_node->element.size_data_in = stream_data->size_data_in;
+	new_strm_node->element.pts = stream_data->pts;
+	new_strm_node->element.dts = stream_data->dts;
+	new_strm_node->element.insert_pcr = stream_data->insert_pcr;
+	new_strm_node->element.pcr_base = stream_data->pcr_base;
+	new_strm_node->element.pcr_extension = stream_data->pcr_extension;
+
+	/* Check for external dma buffer */
+	if (!stream_data->is_dmabuf) {
+		new_strm_node->element.in_buf_pointer =
+			mpgmuxts->srcbuf_addrs[stream_data->srcbuf_id];
+		new_strm_node->element.dmabuf_id = 0;
+	} else {
+		for (i = 0; i < XTSMUX_MAXIN_STRM; i++) {
+			/* Serching dma buf info based on srcbuf_id */
+			if (stream_data->srcbuf_id ==
+					mpgmuxts->src_dmabufintl[i].dmabuf_fd) {
+				new_strm_node->element.in_buf_pointer =
+					mpgmuxts->src_dmabufintl[i].dmabuf_addr;
+				new_strm_node->element.dmabuf_id =
+					mpgmuxts->src_dmabufintl[i].buf_id;
+				break;
+			}
+		}
+
+		/* No dma buf found with srcbuf_id*/
+		if (i == XTSMUX_MAXIN_STRM) {
+			dev_err(mpgmuxts->dev, "No DMA buffer with %d",
+				stream_data->srcbuf_id);
+			return -ENOMEM;
+		}
+	}
+
+	new_strm_node->strm_phy_addr = (u64)strm_phy_addr;
+	new_strm_node->node_number = mpgmuxts->stcxt_node_cnt + 1;
+	mpgmuxts->stcxt_node_cnt++;
+	new_strm_node->node_status = UPDATED_BY_DRIVER;
+	new_strm_node->error_code = NO_ERROR;
+	new_strm_node->tail_pointer = 0;
+
+	spin_lock_irqsave(&mpgmuxts->lock, flags);
+	/* If it is not first stream in stream node linked list find
+	 * physical address of current node and add to last node in list
+	 */
+	if (!list_empty_careful(&mpgmuxts->strm_node)) {
+		prev_strm_node = list_last_entry(&mpgmuxts->strm_node,
+						 struct stream_context_node,
+						 node);
+		prev_strm_node->tail_pointer = new_strm_node->strm_phy_addr;
+	}
+	/* update the list and stream count */
+	wmb();
+	list_add_tail(&new_strm_node->node, &mpgmuxts->strm_node);
+	atomic_inc(&mpgmuxts->stream_count);
+	spin_unlock_irqrestore(&mpgmuxts->lock, flags);
+
+	return 0;
+}
+
+static int xlnx_tsmux_set_stream_desc(struct xlnx_tsmux *mpgmuxts,
+				      void __user *arg)
+{
+	struct stream_context_in *stream_data;
+	int ret = 0;
+
+	stream_data = kzalloc(sizeof(*stream_data), GFP_KERNEL);
+	if (!stream_data)
+		return -ENOMEM;
+
+	ret = copy_from_user(stream_data, arg,
+			     sizeof(struct stream_context_in));
+	if (ret) {
+		dev_err(mpgmuxts->dev, "Failed to copy stream data from user");
+		goto error_free;
+	}
+
+	ret = xlnx_tsmux_enqueue_stream_context(mpgmuxts, stream_data);
+
+error_free:
+	kfree(stream_data);
+
+	return ret;
+}
+
+static int xlnx_tsmux_ioctl_set_stream_context(struct xlnx_tsmux *mpgmuxts,
+					       void __user *arg)
+{
+	int ret;
+
+	ret = xlnx_tsmux_set_stream_desc(mpgmuxts, arg);
+	if (ret < 0) {
+		dev_err(mpgmuxts->dev, "Setting stream descripter failed");
+		return ret;
+	}
+
+	return 0;
+}
+
+static enum xlnx_tsmux_status xlnx_tsmux_get_device_status(struct xlnx_tsmux *
+							   mpgmuxts)
+{
+	enum xlnx_tsmux_status ip_status;
+
+	ip_status = xlnx_tsmux_get_status(mpgmuxts);
+
+	if (ip_status == MPG2MUX_ERROR) {
+		dev_err(mpgmuxts->dev, "Failed to get device status");
+		return -EACCES;
+	}
+
+	if (ip_status == MPG2MUX_BUSY)
+		return -EBUSY;
+
+	return MPG2MUX_READY;
+}
+
+static int xlnx_tsmux_ioctl_start(struct xlnx_tsmux *mpgmuxts)
+{
+	enum xlnx_tsmux_status ip_stat;
+	int cnt, ret;
+
+	/* get IP status */
+	ip_stat = xlnx_tsmux_get_device_status(mpgmuxts);
+	if (ip_stat != MPG2MUX_READY) {
+		dev_err(mpgmuxts->dev, "device is busy");
+		return ip_stat;
+	}
+
+	if (list_empty(&mpgmuxts->mux_node) ||
+	    list_empty(&mpgmuxts->strm_node)) {
+		dev_err(mpgmuxts->dev, "No stream or mux to start device");
+		return -EIO;
+	}
+
+	cnt = atomic_read(&mpgmuxts->stream_count);
+	atomic_set(&mpgmuxts->intn_stream_count, cnt);
+
+	/* update streamid table */
+	ret = xlnx_tsmux_update_intstrm_tbl(mpgmuxts);
+
+	if (ret < 0) {
+		dev_err(mpgmuxts->dev, "Update streamid intn table failed\n");
+		return ret;
+	}
+	return xlnx_tsmux_start_muxer(mpgmuxts);
+}
+
+static void xlnx_tsmux_free_dmalloc(struct xlnx_tsmux *mpgmuxts)
+{
+	dma_pool_destroy(mpgmuxts->strm_ctx_pool);
+	dma_pool_destroy(mpgmuxts->mux_ctx_pool);
+}
+
+static int xlnx_tsmux_ioctl_stop(struct xlnx_tsmux *mpgmuxts)
+{
+	enum xlnx_tsmux_status ip_stat;
+	unsigned long flags;
+
+	ip_stat = xlnx_tsmux_get_device_status(mpgmuxts);
+	if (ip_stat != MPG2MUX_READY) {
+		dev_err(mpgmuxts->dev, "device is busy");
+		return ip_stat;
+	}
+
+	/* Free all driver allocated memory and reset linked list
+	 * Reset IP registers
+	 */
+	xlnx_tsmux_free_dmalloc(mpgmuxts);
+	spin_lock_irqsave(&mpgmuxts->lock, flags);
+	INIT_LIST_HEAD(&mpgmuxts->strm_node);
+	INIT_LIST_HEAD(&mpgmuxts->mux_node);
+	spin_unlock_irqrestore(&mpgmuxts->lock, flags);
+	xlnx_tsmux_stop_muxer(mpgmuxts);
+
+	return 0;
+}
+
+static int xlnx_tsmux_ioctl_get_status(struct xlnx_tsmux *mpgmuxts,
+				       void __user *arg)
+{
+	int ret;
+	enum xlnx_tsmux_status ip_stat;
+
+	ip_stat = xlnx_tsmux_get_device_status(mpgmuxts);
+
+	ret = copy_to_user(arg, (void *)&ip_stat,
+			   (unsigned long)(sizeof(enum xlnx_tsmux_status)));
+	if (ret) {
+		dev_err(mpgmuxts->dev, "Unable to copy device status to user");
+		return -EACCES;
+	}
+
+	return 0;
+}
+
+static int xlnx_tsmux_ioctl_get_outbufinfo(struct xlnx_tsmux *mpgmuxts,
+					   void __user *arg)
+{
+	int ret;
+	int out_index;
+	struct out_buffer out_info;
+
+	out_info.buf_write = mpgmuxts->outbuf_written;
+	mpgmuxts->outbuf_written = 0;
+	out_index = atomic_read(&mpgmuxts->outbuf_idx);
+	if (out_index)
+		out_info.buf_id = 0;
+	else
+		out_info.buf_id = 1;
+
+	ret = copy_to_user(arg, (void *)&out_info,
+			   (unsigned long)(sizeof(struct out_buffer)));
+	if (ret) {
+		dev_err(mpgmuxts->dev, "Unable to copy outbuf info");
+		return -EACCES;
+	}
+
+	return 0;
+}
+
+static int xlnx_tsmux_enqueue_mux_context(struct xlnx_tsmux *mpgmuxts,
+					  struct muxer_context_in *mux_data)
+{
+	struct muxer_context *new_mux_node;
+	u32 out_index;
+	void *kaddr_mux_node;
+	dma_addr_t mux_phy_addr;
+	unsigned long flags;
+	s32 i;
+
+	kaddr_mux_node = dma_pool_alloc(mpgmuxts->mux_ctx_pool,
+					GFP_KERNEL | GFP_DMA32,
+					&mux_phy_addr);
+
+	new_mux_node = (struct muxer_context *)kaddr_mux_node;
+	if (!new_mux_node)
+		return -EAGAIN;
+
+	new_mux_node->node_status = UPDATED_BY_DRIVER;
+	new_mux_node->mux_phy_addr = (u64)mux_phy_addr;
+
+	/* Check for external dma buffer */
+	if (!mux_data->is_dmabuf) {
+		out_index = 0;
+		new_mux_node->dst_buf_start_addr =
+			(u64)mpgmuxts->dstbuf_addrs[out_index];
+		new_mux_node->dst_buf_size = mpgmuxts->dstbuf_size;
+		if (out_index)
+			atomic_set(&mpgmuxts->outbuf_idx, 0);
+		else
+			atomic_set(&mpgmuxts->outbuf_idx, 1);
+	} else {
+		for (i = 0; i < XTSMUX_MAXOUT_STRM; i++) {
+			if (mux_data->dstbuf_id ==
+			   mpgmuxts->dst_dmabufintl[i].dmabuf_fd) {
+				new_mux_node->dst_buf_start_addr =
+					mpgmuxts->dst_dmabufintl[i].dmabuf_addr;
+				break;
+			}
+		}
+		if (i == XTSMUX_MAXOUT_STRM) {
+			dev_err(mpgmuxts->dev, "No DMA buffer with %d",
+				mux_data->dstbuf_id);
+			return -ENOMEM;
+		}
+		new_mux_node->dst_buf_size = mux_data->dmabuf_size;
+	}
+	new_mux_node->error_code = MUXER_NO_ERROR;
+
+	spin_lock_irqsave(&mpgmuxts->lock, flags);
+	list_add_tail(&new_mux_node->node, &mpgmuxts->mux_node);
+	spin_unlock_irqrestore(&mpgmuxts->lock, flags);
+
+	return 0;
+}
+
+static int xlnx_tsmux_set_mux_desc(struct xlnx_tsmux *mpgmuxts,
+				   void __user *arg)
+{
+	struct muxer_context_in *mux_data;
+	int ret = 0;
+
+	mux_data = kzalloc(sizeof(*mux_data), GFP_KERNEL);
+	if (!mux_data)
+		return -ENOMEM;
+
+	ret = copy_from_user(mux_data, arg,
+			     sizeof(struct muxer_context_in));
+	if (ret) {
+		dev_err(mpgmuxts->dev, "failed to copy muxer data from user");
+		goto kmem_free;
+	}
+
+	return xlnx_tsmux_enqueue_mux_context(mpgmuxts, mux_data);
+
+kmem_free:
+	kfree(mux_data);
+
+	return ret;
+}
+
+static int xlnx_tsmux_ioctl_set_mux_context(struct xlnx_tsmux *mpgmuxts,
+					    void __user *arg)
+{
+	int ret;
+
+	ret = xlnx_tsmux_set_mux_desc(mpgmuxts, arg);
+	if (ret < 0)
+		dev_dbg(mpgmuxts->dev, "Setting mux context failed");
+
+	return ret;
+}
+
+static int xlnx_tsmux_ioctl_verify_dmabuf(struct xlnx_tsmux *mpgmuxts,
+					  void __user *arg)
+{
+	struct dma_buf *dbuf;
+	struct dma_buf_attachment *attach;
+	struct sg_table *sgt;
+	struct xlnx_tsmux_dmabuf_info *dbuf_info;
+	s32 i;
+	int ret = 0;
+
+	dbuf_info = kzalloc(sizeof(*dbuf_info), GFP_KERNEL);
+	if (!dbuf_info)
+		return -ENOMEM;
+
+	ret = copy_from_user(dbuf_info, arg,
+			     sizeof(struct xlnx_tsmux_dmabuf_info));
+	if (ret) {
+		dev_err(mpgmuxts->dev, "Failed to copy from user");
+		goto dmak_free;
+	}
+	if (dbuf_info->dir != DMA_TO_MPG2MUX &&
+	    dbuf_info->dir != DMA_FROM_MPG2MUX) {
+		dev_err(mpgmuxts->dev, "Incorrect DMABUF direction %d",
+			dbuf_info->dir);
+		ret = -EINVAL;
+		goto dmak_free;
+	}
+	dbuf = dma_buf_get(dbuf_info->buf_fd);
+	if (IS_ERR(dbuf)) {
+		dev_err(mpgmuxts->dev, "dma_buf_get fail fd %d direction %d",
+			dbuf_info->buf_fd, dbuf_info->dir);
+		ret = PTR_ERR(dbuf);
+		goto dmak_free;
+	}
+	attach = dma_buf_attach(dbuf, mpgmuxts->dev);
+	if (IS_ERR(attach)) {
+		dev_err(mpgmuxts->dev, "dma_buf_attach fail fd %d dir %d",
+			dbuf_info->buf_fd, dbuf_info->dir);
+		ret = PTR_ERR(attach);
+		goto err_dmabuf_put;
+	}
+	sgt = dma_buf_map_attachment(attach,
+				     (enum dma_data_direction)(dbuf_info->dir));
+	if (IS_ERR(sgt)) {
+		dev_err(mpgmuxts->dev, "dma_buf_map_attach fail fd %d dir %d",
+			dbuf_info->buf_fd, dbuf_info->dir);
+		ret = PTR_ERR(sgt);
+		goto err_dmabuf_detach;
+	}
+
+	if (sgt->nents > 1) {
+		ret = -EIO;
+		dev_dbg(mpgmuxts->dev, "Not contig nents %d fd %d direction %d",
+			sgt->nents, dbuf_info->buf_fd, dbuf_info->dir);
+		goto err_dmabuf_unmap_attachment;
+	}
+	dev_dbg(mpgmuxts->dev, "dmabuf %s is physically contiguous",
+		(dbuf_info->dir ==
+		 DMA_TO_MPG2MUX ? "Source" : "Destination"));
+
+	if (dbuf_info->dir == DMA_TO_MPG2MUX) {
+		for (i = 0; i < XTSMUX_MAXIN_STRM; i++) {
+			if (!mpgmuxts->src_dmabufintl[i].buf_id) {
+				mpgmuxts->src_dmabufintl[i].dbuf = dbuf;
+				mpgmuxts->src_dmabufintl[i].attach = attach;
+				mpgmuxts->src_dmabufintl[i].sgt = sgt;
+				mpgmuxts->src_dmabufintl[i].dmabuf_addr =
+						sg_dma_address(sgt->sgl);
+				mpgmuxts->src_dmabufintl[i].dmabuf_fd =
+							dbuf_info->buf_fd;
+				mpgmuxts->src_dmabufintl[i].buf_id = i + 1;
+				dev_dbg(mpgmuxts->dev,
+					"%s: phy-addr=0x%llx for src dmabuf=%d",
+					__func__,
+					mpgmuxts->src_dmabufintl[i].dmabuf_addr,
+					mpgmuxts->src_dmabufintl[i].dmabuf_fd);
+				break;
+			}
+		}
+		/* External src streams more than XTSMUX_MAXIN_STRM
+		 * can not be handled
+		 */
+		if (i == XTSMUX_MAXIN_STRM) {
+			ret = -EIO;
+			dev_dbg(mpgmuxts->dev, "src DMA bufs more than %d",
+				XTSMUX_MAXIN_STRM);
+			goto err_dmabuf_unmap_attachment;
+		}
+	} else {
+		for (i = 0; i < XTSMUX_MAXOUT_STRM; i++) {
+			if (!mpgmuxts->dst_dmabufintl[i].buf_id) {
+				mpgmuxts->dst_dmabufintl[i].dbuf = dbuf;
+				mpgmuxts->dst_dmabufintl[i].attach = attach;
+				mpgmuxts->dst_dmabufintl[i].sgt = sgt;
+				mpgmuxts->dst_dmabufintl[i].dmabuf_addr =
+						sg_dma_address(sgt->sgl);
+				mpgmuxts->dst_dmabufintl[i].dmabuf_fd =
+						dbuf_info->buf_fd;
+				mpgmuxts->dst_dmabufintl[i].buf_id = i + 1;
+				dev_dbg(mpgmuxts->dev,
+					"phy-addr=0x%llx for src dmabuf=%d",
+					mpgmuxts->dst_dmabufintl[i].dmabuf_addr,
+					mpgmuxts->dst_dmabufintl[i].dmabuf_fd);
+				break;
+			}
+		}
+		/* External dst streams more than XTSMUX_MAXOUT_STRM
+		 * can not be handled
+		 */
+		if (i == XTSMUX_MAXOUT_STRM) {
+			ret = -EIO;
+			dev_dbg(mpgmuxts->dev, "dst DMA bufs more than %d",
+				XTSMUX_MAXOUT_STRM);
+			goto err_dmabuf_unmap_attachment;
+		}
+	}
+
+	return 0;
+
+err_dmabuf_unmap_attachment:
+	dma_buf_unmap_attachment(attach, sgt,
+				 (enum dma_data_direction)dbuf_info->dir);
+err_dmabuf_detach:
+	dma_buf_detach(dbuf, attach);
+err_dmabuf_put:
+	dma_buf_put(dbuf);
+dmak_free:
+	kfree(dbuf_info);
+
+	return ret;
+}
+
+static long xlnx_tsmux_ioctl(struct file *fptr,
+			     unsigned int cmd, unsigned long data)
+{
+	struct xlnx_tsmux *mpgmuxts;
+	void __user *arg;
+	int ret;
+
+	mpgmuxts = fptr->private_data;
+	if (!mpgmuxts)
+		return -EINVAL;
+
+	arg = (void __user *)data;
+	switch (cmd) {
+	case MPG2MUX_INBUFALLOC:
+		ret = xlnx_tsmux_ioctl_srcbuf_alloc(mpgmuxts, arg);
+		break;
+	case MPG2MUX_INBUFDEALLOC:
+		ret = xlnx_tsmux_ioctl_srcbuf_dealloc(mpgmuxts);
+		break;
+	case MPG2MUX_OUTBUFALLOC:
+		ret = xlnx_tsmux_ioctl_dstbuf_alloc(mpgmuxts, arg);
+		break;
+	case MPG2MUX_OUTBUFDEALLOC:
+		ret = xlnx_tsmux_ioctl_dstbuf_dealloc(mpgmuxts);
+		break;
+	case MPG2MUX_STBLALLOC:
+		ret = xlnx_tsmux_ioctl_strmtbl_alloc(mpgmuxts, arg);
+		break;
+	case MPG2MUX_STBLDEALLOC:
+		ret = xlnx_tsmux_ioctl_strmtbl_dealloc(mpgmuxts);
+		break;
+	case MPG2MUX_TBLUPDATE:
+		ret = xlnx_tsmux_ioctl_update_strmtbl(mpgmuxts, arg);
+		break;
+	case MPG2MUX_SETSTRM:
+		ret = xlnx_tsmux_ioctl_set_stream_context(mpgmuxts, arg);
+		break;
+	case MPG2MUX_START:
+		ret = xlnx_tsmux_ioctl_start(mpgmuxts);
+		break;
+	case MPG2MUX_STOP:
+		ret = xlnx_tsmux_ioctl_stop(mpgmuxts);
+		break;
+	case MPG2MUX_STATUS:
+		ret = xlnx_tsmux_ioctl_get_status(mpgmuxts, arg);
+		break;
+	case MPG2MUX_GETOUTBUF:
+		ret = xlnx_tsmux_ioctl_get_outbufinfo(mpgmuxts, arg);
+		break;
+	case MPG2MUX_SETMUX:
+		ret = xlnx_tsmux_ioctl_set_mux_context(mpgmuxts, arg);
+		break;
+	case MPG2MUX_VDBUF:
+		ret = xlnx_tsmux_ioctl_verify_dmabuf(mpgmuxts, arg);
+		break;
+	default:
+		return -EINVAL;
+	}
+	if (ret < 0)
+		dev_err(mpgmuxts->dev, "ioctl %d failed\n", cmd);
+
+	return ret;
+}
+
+static int xlnx_tsmux_mmap(struct file *fp, struct vm_area_struct *vma)
+{
+	struct xlnx_tsmux *mpgmuxts = fp->private_data;
+	int ret, buf_id;
+
+	if (!mpgmuxts)
+		return -ENODEV;
+
+	buf_id = vma->vm_pgoff;
+
+	if (buf_id < mpgmuxts->num_inbuf) {
+		if (!mpgmuxts->srcbuf_addrs[buf_id]) {
+			dev_err(mpgmuxts->dev, "Mem not allocated for src %d",
+				buf_id);
+			return -EINVAL;
+		}
+		vma->vm_page_prot = pgprot_writecombine(vma->vm_page_prot);
+		ret = remap_pfn_range(vma, vma->vm_start,
+				      mpgmuxts->srcbuf_addrs[buf_id] >>
+				      PAGE_SHIFT, vma->vm_end - vma->vm_start,
+				      vma->vm_page_prot);
+		if (ret) {
+			dev_err(mpgmuxts->dev, "mmap fail bufid = %d", buf_id);
+			return -EINVAL;
+		}
+	} else if (buf_id < (mpgmuxts->num_inbuf + mpgmuxts->num_outbuf)) {
+		buf_id -= mpgmuxts->num_inbuf;
+		if (!mpgmuxts->dstbuf_addrs[buf_id]) {
+			dev_err(mpgmuxts->dev, "Mem not allocated fordst %d",
+				buf_id);
+			return -EINVAL;
+		}
+		vma->vm_page_prot = pgprot_writecombine(vma->vm_page_prot);
+		ret =
+		remap_pfn_range(vma, vma->vm_start,
+				mpgmuxts->dstbuf_addrs[buf_id] >> PAGE_SHIFT,
+				vma->vm_end - vma->vm_start, vma->vm_page_prot);
+		if (ret) {
+			dev_err(mpgmuxts->dev, "mmap fail buf_id = %d", buf_id);
+			ret = -EINVAL;
+		}
+	} else {
+		dev_err(mpgmuxts->dev, "Wrong buffer id -> %d buf", buf_id);
+		return -EINVAL;
+	}
+	fp->private_data = mpgmuxts;
+	return 0;
+}
+
+static __poll_t xlnx_tsmux_poll(struct file *fptr, poll_table *wait)
+{
+	struct xlnx_tsmux *mpgmuxts = fptr->private_data;
+
+	poll_wait(fptr, &mpgmuxts->waitq, wait);
+
+	if (xlnx_tsmux_read(mpgmuxts, XTSMUX_LAST_NODE_PROCESSED))
+		return POLLIN | POLLPRI;
+
+	return 0;
+}
+
+static const struct file_operations mpg2mux_fops = {
+	.open = xlnx_tsmux_open,
+	.release = xlnx_tsmux_release,
+	.unlocked_ioctl = xlnx_tsmux_ioctl,
+	.mmap = xlnx_tsmux_mmap,
+	.poll = xlnx_tsmux_poll,
+};
+
+static void xlnx_tsmux_free_dmabufintl(struct xlnx_tsmux_dmabufintl
+				       *intl_dmabuf, u16 dmabuf_id,
+				       enum xlnx_tsmux_dma_dir dir)
+{
+	unsigned int i = dmabuf_id - 1;
+
+	if (intl_dmabuf[i].dmabuf_fd) {
+		dma_buf_unmap_attachment(intl_dmabuf[i].attach,
+					 intl_dmabuf[i].sgt,
+					 (enum dma_data_direction)dir);
+		dma_buf_detach(intl_dmabuf[i].dbuf, intl_dmabuf[i].attach);
+		dma_buf_put(intl_dmabuf[i].dbuf);
+		intl_dmabuf[i].dmabuf_fd = 0;
+		intl_dmabuf[i].buf_id = 0;
+	}
+}
+
+static int xlnx_tsmux_update_complete(struct xlnx_tsmux *mpgmuxts)
+{
+	struct stream_context_node *tstrm_node;
+	struct muxer_context *temp_mux;
+	u32 num_strm_node, i;
+	u32 num_strms;
+	unsigned long flags;
+
+	num_strm_node = xlnx_tsmux_read(mpgmuxts, XTSMUX_LAST_NODE_PROCESSED);
+	if (num_strm_node == 0)
+		return -1;
+
+	/* Removing completed stream nodes from the list  */
+	spin_lock_irqsave(&mpgmuxts->lock, flags);
+	num_strms = atomic_read(&mpgmuxts->intn_stream_count);
+	for (i = 0; i < num_strms; i++) {
+		tstrm_node =
+			list_first_entry(&mpgmuxts->strm_node,
+					 struct stream_context_node, node);
+		list_del(&tstrm_node->node);
+		atomic_dec(&mpgmuxts->stream_count);
+		if (tstrm_node->element.dmabuf_id)
+			xlnx_tsmux_free_dmabufintl
+				(mpgmuxts->src_dmabufintl,
+				 tstrm_node->element.dmabuf_id,
+				 DMA_TO_MPG2MUX);
+		if (tstrm_node->node_number == num_strm_node) {
+			dma_pool_free(mpgmuxts->strm_ctx_pool, tstrm_node,
+				      tstrm_node->strm_phy_addr);
+			break;
+		}
+	}
+
+	/* Removing completed mux nodes from the list  */
+	temp_mux = list_first_entry(&mpgmuxts->mux_node, struct muxer_context,
+				    node);
+	mpgmuxts->outbuf_written = temp_mux->dst_buf_written;
+
+	list_del(&temp_mux->node);
+	spin_unlock_irqrestore(&mpgmuxts->lock, flags);
+
+	return 0;
+}
+
+static irqreturn_t xlnx_tsmux_intr_handler(int irq, void *ctx)
+{
+	u32 status;
+	struct xlnx_tsmux *mpgmuxts = (struct xlnx_tsmux *)ctx;
+
+	status = xlnx_tsmux_read(mpgmuxts, XTSMUX_ISR_STAT);
+	status &= XTSMUX_IER_ENABLE_MASK;
+
+	if (status) {
+		xlnx_tsmux_write(mpgmuxts, XTSMUX_ISR_STAT, status);
+		xlnx_tsmux_update_complete(mpgmuxts);
+		if (mpgmuxts->outbuf_written)
+			wake_up_interruptible(&mpgmuxts->waitq);
+		return IRQ_HANDLED;
+	}
+
+	return IRQ_NONE;
+}
+
+static int xlnx_tsmux_probe(struct platform_device *pdev)
+{
+	struct xlnx_tsmux *mpgmuxts;
+	struct device *dev = &pdev->dev;
+	struct device *dev_crt;
+	struct resource *dev_resrc;
+	int ret = -1;
+	unsigned long flags;
+
+	/* DRIVER_MAX_DEV is to limit the number of instances, but
+	 * Initial version is tested with single instance only.
+	 * TODO: replace atomic_read with ida_simple_get
+	 */
+	if (atomic_read(&xlnx_tsmux_ndevs) >= DRIVER_MAX_DEV) {
+		dev_err(&pdev->dev, "Limit of %d number of device is reached",
+			DRIVER_MAX_DEV);
+		return -EIO;
+	}
+
+	mpgmuxts = devm_kzalloc(&pdev->dev, sizeof(struct xlnx_tsmux),
+				GFP_KERNEL);
+	if (!mpgmuxts)
+		return -ENOMEM;
+	mpgmuxts->dev = &pdev->dev;
+	dev_resrc = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	mpgmuxts->iomem = devm_ioremap_resource(mpgmuxts->dev, dev_resrc);
+	if (IS_ERR(mpgmuxts->iomem))
+		return PTR_ERR(mpgmuxts->iomem);
+
+	mpgmuxts->irq = irq_of_parse_and_map(mpgmuxts->dev->of_node, 0);
+	if (!mpgmuxts->irq) {
+		dev_err(mpgmuxts->dev, "Unable to get IRQ");
+		return -EINVAL;
+	}
+
+	mpgmuxts->ap_clk = devm_clk_get(dev, "ap_clk");
+	if (IS_ERR(mpgmuxts->ap_clk)) {
+		ret = PTR_ERR(mpgmuxts->ap_clk);
+		dev_err(dev, "failed to get ap clk %d\n", ret);
+		goto cdev_err;
+	}
+	ret = clk_prepare_enable(mpgmuxts->ap_clk);
+	if (ret) {
+		dev_err(dev, "failed to enable ap clk %d\n", ret);
+		goto err_disable_ap_clk;
+	}
+
+	/* Initializing variables used in Muxer */
+	spin_lock_irqsave(&mpgmuxts->lock, flags);
+	INIT_LIST_HEAD(&mpgmuxts->strm_node);
+	INIT_LIST_HEAD(&mpgmuxts->mux_node);
+	spin_unlock_irqrestore(&mpgmuxts->lock, flags);
+	mpgmuxts->strm_ctx_pool = dma_pool_create("strcxt_pool", mpgmuxts->dev,
+						  XTSMUX_POOL_SIZE,
+						  XTSMUX_POOL_ALIGN,
+						  XTSMUX_POOL_SIZE *
+						  XTSMUX_MAXIN_TLSTRM);
+	if (!mpgmuxts->strm_ctx_pool) {
+		dev_err(mpgmuxts->dev, "Allocation fail for strm ctx pool");
+		return -ENOMEM;
+	}
+
+	mpgmuxts->mux_ctx_pool = dma_pool_create("muxcxt_pool", mpgmuxts->dev,
+						 XTSMUX_POOL_SIZE,
+						 XTSMUX_POOL_SIZE,
+						 XTSMUX_POOL_SIZE *
+						 XTSMUX_MAXIN_TLSTRM);
+
+	if (!mpgmuxts->mux_ctx_pool) {
+		dev_err(mpgmuxts->dev, "Allocation fail for mux ctx pool");
+		goto mux_err;
+	}
+
+	init_waitqueue_head(&mpgmuxts->waitq);
+
+	ret = devm_request_irq(mpgmuxts->dev, mpgmuxts->irq,
+			       xlnx_tsmux_intr_handler, IRQF_SHARED,
+			       DRIVER_NAME, mpgmuxts);
+
+	if (ret < 0) {
+		dev_err(mpgmuxts->dev, "Unable to register IRQ");
+		goto mux_err;
+	}
+
+	cdev_init(&mpgmuxts->chdev, &mpg2mux_fops);
+	mpgmuxts->chdev.owner = THIS_MODULE;
+	mpgmuxts->id = atomic_read(&xlnx_tsmux_ndevs);
+	ret = cdev_add(&mpgmuxts->chdev, MKDEV(MAJOR(xlnx_tsmux_devt),
+					       mpgmuxts->id), 1);
+
+	if (ret < 0) {
+		dev_err(mpgmuxts->dev, "cdev_add failed");
+		goto cadd_err;
+	}
+
+	dev_crt = device_create(xlnx_tsmux_class, mpgmuxts->dev,
+				MKDEV(MAJOR(xlnx_tsmux_devt), mpgmuxts->id),
+				mpgmuxts, "mpgmuxts%d", mpgmuxts->id);
+
+	if (IS_ERR(dev_crt)) {
+		ret = PTR_ERR(dev_crt);
+		dev_err(mpgmuxts->dev, "Unable to create device");
+		goto cdev_err;
+	}
+
+	dev_info(mpgmuxts->dev,
+		 "Xilinx mpeg2 TS muxer device probe completed");
+
+	atomic_inc(&xlnx_tsmux_ndevs);
+
+	return 0;
+
+err_disable_ap_clk:
+	clk_disable_unprepare(mpgmuxts->ap_clk);
+cdev_err:
+	cdev_del(&mpgmuxts->chdev);
+	device_destroy(xlnx_tsmux_class, MKDEV(MAJOR(xlnx_tsmux_devt),
+					       mpgmuxts->id));
+cadd_err:
+	dma_pool_destroy(mpgmuxts->mux_ctx_pool);
+mux_err:
+	dma_pool_destroy(mpgmuxts->strm_ctx_pool);
+
+	return ret;
+}
+
+static int xlnx_tsmux_remove(struct platform_device *pdev)
+{
+	struct xlnx_tsmux *mpgmuxts;
+
+	mpgmuxts = platform_get_drvdata(pdev);
+	if (!mpgmuxts || !xlnx_tsmux_class)
+		return -EIO;
+	dma_pool_destroy(mpgmuxts->mux_ctx_pool);
+	dma_pool_destroy(mpgmuxts->strm_ctx_pool);
+
+	device_destroy(xlnx_tsmux_class, MKDEV(MAJOR(xlnx_tsmux_devt),
+					       mpgmuxts->id));
+	cdev_del(&mpgmuxts->chdev);
+	atomic_dec(&xlnx_tsmux_ndevs);
+	clk_disable_unprepare(mpgmuxts->ap_clk);
+
+	return 0;
+}
+
+static const struct of_device_id xlnx_tsmux_of_match[] = {
+	{ .compatible = "xlnx,tsmux-1.0", },
+	{ }
+};
+
+static struct platform_driver xlnx_tsmux_driver = {
+	.probe = xlnx_tsmux_probe,
+	.remove = xlnx_tsmux_remove,
+	.driver = {
+		.name = DRIVER_NAME,
+		.of_match_table = xlnx_tsmux_of_match,
+	},
+};
+
+static int __init xlnx_tsmux_mod_init(void)
+{
+	int err;
+
+	xlnx_tsmux_class = class_create(THIS_MODULE, DRIVER_NAME);
+	if (IS_ERR(xlnx_tsmux_class)) {
+		pr_err("%s : Unable to create driver class", __func__);
+		return PTR_ERR(xlnx_tsmux_class);
+	}
+
+	err = alloc_chrdev_region(&xlnx_tsmux_devt, 0, DRIVER_MAX_DEV,
+				  DRIVER_NAME);
+	if (err < 0) {
+		pr_err("%s : Unable to get major number", __func__);
+		goto err_class;
+	}
+
+	err = platform_driver_register(&xlnx_tsmux_driver);
+	if (err < 0) {
+		pr_err("%s : Unable to register %s driver", __func__,
+		       DRIVER_NAME);
+		goto err_driver;
+	}
+
+	return 0;
+
+err_driver:
+	unregister_chrdev_region(xlnx_tsmux_devt, DRIVER_MAX_DEV);
+err_class:
+	class_destroy(xlnx_tsmux_class);
+
+	return err;
+}
+
+static void __exit xlnx_tsmux_mod_exit(void)
+{
+	platform_driver_unregister(&xlnx_tsmux_driver);
+	unregister_chrdev_region(xlnx_tsmux_devt, DRIVER_MAX_DEV);
+	class_destroy(xlnx_tsmux_class);
+	xlnx_tsmux_class = NULL;
+}
+
+module_init(xlnx_tsmux_mod_init);
+module_exit(xlnx_tsmux_mod_exit);
+
+MODULE_AUTHOR("Xilinx Inc.");
+MODULE_DESCRIPTION("Xilinx mpeg2 transport stream muxer IP driver");
+MODULE_LICENSE("GPL v2");
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xlnxsync/Kconfig
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xlnxsync/Kconfig	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,11 @@
+config XLNX_SYNC
+	tristate "Xilinx Synchronizer"
+	depends on ARCH_ZYNQMP
+	help
+	  This driver is developed for Xilinx Synchronizer IP. It is used to
+	  monitor the AXI addresses of the producer and initiate the
+	  consumer to start earlier, thereby reducing the latency to process
+	  the data.
+
+	  To compile this driver as a module, choose M here.
+	  If unsure, choose N
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xlnxsync/MAINTAINERS
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xlnxsync/MAINTAINERS	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,4 @@
+XILINX SYNCHRONIZER DRIVER
+M:	Vishal Sagar <vishal.sagar@xilinx.com>
+S:	Maintained
+F:	drivers/staging/xlnxsync
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xlnxsync/Makefile
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xlnxsync/Makefile	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1 @@
+obj-$(CONFIG_XLNX_SYNC) += xlnxsync.o
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xlnxsync/dt-binding.txt
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xlnxsync/dt-binding.txt	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,34 @@
+Xilinx Synchronizer
+-------------------
+
+The Xilinx Synchronizer is used for buffer synchronization between
+producer and consumer blocks. It manages to do so by tapping onto the bus
+where the producer block is writing frame data to memory and consumer block is
+reading the frame data from memory.
+
+It can work on the encode path with max 4 channels or on decode path with
+max 2 channels.
+
+Required properties:
+- compatible : Must contain "xlnx,sync-ip-1.0"
+- reg: Physical base address and length of the registers set for the device.
+- interrupts: Contains the interrupt line number.
+- interrupt-parent: phandle to interrupt controller.
+- clock-names: The input clock names for axilite, producer and consumer clock.
+- clocks: Reference to the clock that drives the axi interface, producer and consumer.
+- xlnx,num-chan: Range from 1 to 2 for decode.
+		 Range from 1 to 4 for encode.
+
+Optional properties:
+- xlnx,encode: Present if IP configured for encoding path, else absent.
+
+v_sync_vcu: subframe_sync_vcu@a00e0000 {
+	compatible = "xlnx,sync-ip-1.0";
+	reg = <0x0 0xa00e0000 0x0 0x10000>;
+	interrupt-parent = <&gic>;
+	interrupts = <0 96 4>;
+	clock-names = "s_axi_ctrl_aclk", "s_axi_mm_p_aclk", "s_axi_mm_aclk";
+	clocks = <&vid_s_axi_clk>, <&vid_stream_clk>, <&vid_stream_clk>;
+	xlnx,num-chan = <4>;
+	xlnx,encode;
+};
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xlnxsync/xlnxsync.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xlnxsync/xlnxsync.c	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,1494 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx Synchronizer IP driver
+ *
+ * Copyright (C) 2019 Xilinx, Inc.
+ *
+ * Author: Vishal Sagar <vishal.sagar@xilinx.com>
+ *
+ * This driver is used to control the Xilinx Synchronizer IP
+ * to achieve sub frame latency for encode and decode with VCU.
+ * This is done by monitoring the address lines for specific values.
+ */
+
+#include <linux/cdev.h>
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-buf.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/ioctl.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_irq.h>
+#include <linux/platform_device.h>
+#include <linux/poll.h>
+#include <linux/uaccess.h>
+#include <linux/xlnxsync.h>
+
+/* Register offsets and bit masks */
+#define XLNXSYNC_CTRL_REG		0x00
+#define XLNXSYNC_ISR_REG		0x04
+/* Producer Luma/Chroma Start/End Address */
+#define XLNXSYNC_PL_START_LO_REG	0x08
+#define XLNXSYNC_PL_START_HI_REG	0x0C
+#define XLNXSYNC_PC_START_LO_REG	0x20
+#define XLNXSYNC_PC_START_HI_REG	0x24
+#define XLNXSYNC_PL_END_LO_REG		0x38
+#define XLNXSYNC_PL_END_HI_REG		0x3C
+#define XLNXSYNC_PC_END_LO_REG		0x50
+#define XLNXSYNC_PC_END_HI_REG		0x54
+#define XLNXSYNC_L_MARGIN_REG		0x68
+#define XLNXSYNC_C_MARGIN_REG		0x74
+#define XLNXSYNC_IMR_REG		0x80
+#define XLNXSYNC_DBG_REG		0x84
+/* Consumer Luma/Chroma Start/End Address */
+#define XLNXSYNC_CL_START_LO_REG	0x88
+#define XLNXSYNC_CL_START_HI_REG	0x8C
+#define XLNXSYNC_CC_START_LO_REG	0xA0
+#define XLNXSYNC_CC_START_HI_REG	0xA4
+#define XLNXSYNC_CL_END_LO_REG		0xB8
+#define XLNXSYNC_CL_END_HI_REG		0xBC
+#define XLNXSYNC_CC_END_LO_REG		0xD0
+#define XLNXSYNC_CC_END_HI_REG		0xD4
+
+/* Luma/Chroma Core offset registers */
+#define XLNXSYNC_LCOREOFF_REG		0x400
+#define XLNXSYNC_CCOREOFF_REG		0x410
+#define XLNXSYNC_COREOFF_NEXT		0x4
+
+#define XLNXSYNC_CTRL_ENCDEC_MASK	BIT(0)
+#define XLNXSYNC_CTRL_ENABLE_MASK	BIT(1)
+#define XLNXSYNC_CTRL_INTR_EN_MASK	BIT(2)
+#define XLNXSYNC_CTRL_SOFTRESET		BIT(3)
+
+#define XLNXSYNC_ISR_PROD_SYNC_FAIL_MASK BIT(0)
+#define XLNXSYNC_ISR_PROD_WDG_ERR_MASK	BIT(1)
+/* Producer related */
+#define XLNXSYNC_ISR_PLDONE_SHIFT	(2)
+#define XLNXSYNC_ISR_PLDONE_MASK	GENMASK(3, 2)
+#define XLNXSYNC_ISR_PLSKIP_MASK	BIT(4)
+#define XLNXSYNC_ISR_PLVALID_MASK	BIT(5)
+#define XLNXSYNC_ISR_PCDONE_SHIFT	(6)
+#define XLNXSYNC_ISR_PCDONE_MASK	GENMASK(7, 6)
+#define XLNXSYNC_ISR_PCSKIP_MASK	BIT(8)
+#define XLNXSYNC_ISR_PCVALID_MASK	BIT(9)
+/* Consumer related */
+#define XLNXSYNC_ISR_CLDONE_SHIFT	(10)
+#define XLNXSYNC_ISR_CLDONE_MASK	GENMASK(11, 10)
+#define XLNXSYNC_ISR_CLSKIP_MASK	BIT(12)
+#define XLNXSYNC_ISR_CLVALID_MASK	BIT(13)
+#define XLNXSYNC_ISR_CCDONE_SHIFT	(14)
+#define XLNXSYNC_ISR_CCDONE_MASK	GENMASK(15, 14)
+#define XLNXSYNC_ISR_CCSKIP_MASK	BIT(16)
+#define XLNXSYNC_ISR_CCVALID_MASK	BIT(17)
+
+#define XLNXSYNC_ISR_LDIFF		BIT(18)
+#define XLNXSYNC_ISR_CDIFF		BIT(19)
+#define XLNXSYNC_ISR_CONS_SYNC_FAIL_MASK BIT(20)
+#define XLNXSYNC_ISR_CONS_WDG_ERR_MASK	BIT(21)
+
+/* bit 44 of start address */
+#define XLNXSYNC_FB_VALID_MASK		BIT(12)
+#define XLNXSYNC_FB_HI_ADDR_MASK	GENMASK(11, 0)
+
+#define XLNXSYNC_IMR_PROD_SYNC_FAIL_MASK BIT(0)
+#define XLNXSYNC_IMR_PROD_WDG_ERR_MASK	BIT(1)
+/* Producer */
+#define XLNXSYNC_IMR_PLVALID_MASK	BIT(5)
+#define XLNXSYNC_IMR_PCVALID_MASK	BIT(9)
+/* Consumer */
+#define XLNXSYNC_IMR_CLVALID_MASK	BIT(13)
+#define XLNXSYNC_IMR_CCVALID_MASK	BIT(17)
+/* Diff */
+#define XLNXSYNC_IMR_LDIFF		BIT(18)
+#define XLNXSYNC_IMR_CDIFF		BIT(19)
+#define XLNXSYNC_IMR_CONS_SYNC_FAIL_MASK BIT(20)
+#define XLNXSYNC_IMR_CONS_WDG_ERR_MASK	BIT(21)
+
+#define XLNXSYNC_IMR_ALL_MASK		(XLNXSYNC_IMR_PROD_SYNC_FAIL_MASK |\
+					 XLNXSYNC_IMR_PROD_WDG_ERR_MASK |\
+					 XLNXSYNC_IMR_PLVALID_MASK |\
+					 XLNXSYNC_IMR_PCVALID_MASK |\
+					 XLNXSYNC_IMR_CLVALID_MASK |\
+					 XLNXSYNC_IMR_CCVALID_MASK |\
+					 XLNXSYNC_IMR_LDIFF |\
+					 XLNXSYNC_IMR_CDIFF |\
+					 XLNXSYNC_IMR_CONS_SYNC_FAIL_MASK |\
+					 XLNXSYNC_IMR_CONS_WDG_ERR_MASK)
+
+/* Other macros */
+#define XLNXSYNC_CHAN_OFFSET		0x100
+#define XLNXSYNC_BUF_ADDR_SHIFT	0x3
+#define XLNXSYNC_DEVNAME_LEN		(32)
+
+#define XLNXSYNC_DRIVER_NAME		"xlnxsync"
+#define XLNXSYNC_DRIVER_VERSION		"0.1"
+
+#define XLNXSYNC_DEV_MAX		256
+
+/* Module Parameters */
+static struct class *xlnxsync_class;
+static dev_t xlnxsync_devt;
+/* Used to keep track of sync devices */
+static DEFINE_IDA(xs_ida);
+
+/**
+ * struct xlnxsync_device - Xilinx Synchronizer struct
+ * @chdev: Character device driver struct
+ * @dev: Pointer to device
+ * @iomem: Pointer to the register space
+ * @sync_mutex: Serialize general device specific ioctl calls
+ * @axi_clk: Pointer to clock structure for axilite clock
+ * @p_clk: Pointer to clock structure for producer clock
+ * @c_clk: Pointer to clock structure for consumer clock
+ * @user_count: Usage count
+ * @irq: IRQ number
+ * @irq_lock: Spinlock used to protect access to sync and watchdog error
+ * @minor: Device id count
+ * @config: IP config struct
+ * @channels: List head for syncip channel linked list
+ * @chan_count : Active channel number count
+ * @reserved : Bitmap to track reserved channels
+ *
+ * This structure contains the device driver related parameters
+ */
+struct xlnxsync_device {
+	struct cdev chdev;
+	struct device *dev;
+	void __iomem *iomem;
+	/* sync_mutex is used to serialize general device ioctl calls */
+	struct mutex sync_mutex;
+	struct clk *axi_clk;
+	struct clk *p_clk;
+	struct clk *c_clk;
+	atomic_t user_count;
+	unsigned int irq;
+	/* irq_lock is used to protect access to sync_err and wdg_err */
+	spinlock_t irq_lock;
+	u32 minor;
+	struct xlnxsync_config config;
+	struct list_head channels;
+	u8 chan_count;
+	unsigned long reserved;
+};
+
+/**
+ * struct xlnxsync_channel - Synchronizer context struct
+ * @dev: Xilinx synchronizer device struct
+ * @mutex: Serialize channel specific ioctl calls
+ * @id: Channel id
+ * @last_buf_used: Save last buffer id used
+ * @channel: list entry into syncip channel lists
+ * @wq_fbdone: Wait queue for frame buffer done events
+ * @wq_error: Wait queue for error events
+ * @l_done: Luma done result array
+ * @c_done: Chroma done result array
+ * @prod_sync_err: Capture synchronization error per channel
+ * @prod_wdg_err: Capture watchdog error per channel
+ * @cons_sync_err: Consumer synchronization error per channel
+ * @cons_wdg_err: Consumer watchdog error per channel
+ * @ldiff_err: Luma buffer diff > 1
+ * @cdiff_err: Chroma buffer diff > 1
+ * @err_event: Error event per channel
+ * @framedone_event: Framebuffer done event per channel
+ *
+ * This structure contains the syncip channel specific parameters
+ */
+struct xlnxsync_channel {
+	struct xlnxsync_device *dev;
+	/* Serialize channel specific ioctl calls */
+	struct mutex mutex;
+	u32 id;
+	int last_buf_used;
+	struct list_head channel;
+	wait_queue_head_t wq_fbdone;
+	wait_queue_head_t wq_error;
+	u8 l_done[XLNXSYNC_BUF_PER_CHAN][XLNXSYNC_IO];
+	u8 c_done[XLNXSYNC_BUF_PER_CHAN][XLNXSYNC_IO];
+	u8 prod_sync_err : 1;
+	u8 prod_wdg_err : 1;
+	u8 cons_sync_err : 1;
+	u8 cons_wdg_err : 1;
+	u8 ldiff_err : 1;
+	u8 cdiff_err : 1;
+	u8 err_event : 1;
+	u8 framedone_event : 1;
+};
+
+static inline u32 xlnxsync_read(struct xlnxsync_device *dev, u32 chan, u32 reg)
+{
+	return ioread32(dev->iomem + (chan * XLNXSYNC_CHAN_OFFSET) + reg);
+}
+
+static inline void xlnxsync_write(struct xlnxsync_device *dev, u32 chan,
+				  u32 reg, u32 val)
+{
+	iowrite32(val, dev->iomem + (chan * XLNXSYNC_CHAN_OFFSET) + reg);
+}
+
+static inline void xlnxsync_clr(struct xlnxsync_device *dev, u32 chan, u32 reg,
+				u32 clr)
+{
+	xlnxsync_write(dev, chan, reg, xlnxsync_read(dev, chan, reg) & ~clr);
+}
+
+static inline void xlnxsync_set(struct xlnxsync_device *dev, u32 chan, u32 reg,
+				u32 set)
+{
+	xlnxsync_write(dev, chan, reg, xlnxsync_read(dev, chan, reg) | set);
+}
+
+static bool xlnxsync_is_buf_done(struct xlnxsync_device *dev,
+				 u32 channel, u32 buf, u32 io,
+				 u8 force_clr_valid)
+{
+	u32 luma_valid, chroma_valid, reg_laddr, reg_caddr, buf_offset;
+
+	switch (io) {
+	case XLNXSYNC_PROD:
+		reg_laddr = XLNXSYNC_PL_START_HI_REG;
+		reg_caddr = XLNXSYNC_PC_START_HI_REG;
+		break;
+	case XLNXSYNC_CONS:
+		reg_laddr = XLNXSYNC_CL_START_HI_REG;
+		reg_caddr = XLNXSYNC_CC_START_HI_REG;
+		break;
+	default:
+		return false;
+	}
+
+	buf_offset = buf << XLNXSYNC_BUF_ADDR_SHIFT;
+	if (force_clr_valid) {
+		xlnxsync_clr(dev, channel, reg_laddr + buf_offset,
+			     XLNXSYNC_FB_VALID_MASK);
+		xlnxsync_clr(dev, channel, reg_caddr + buf_offset,
+			     XLNXSYNC_FB_VALID_MASK);
+	}
+
+	luma_valid = xlnxsync_read(dev, channel, reg_laddr + buf_offset) &
+				   XLNXSYNC_FB_VALID_MASK;
+	chroma_valid = xlnxsync_read(dev, channel, reg_caddr + buf_offset) &
+				     XLNXSYNC_FB_VALID_MASK;
+	if (!luma_valid && !chroma_valid)
+		return true;
+
+	return false;
+}
+
+static void xlnxsync_reset_chan(struct xlnxsync_device *dev, u32 chan)
+{
+	u8 num_retries = 50;
+
+	xlnxsync_set(dev, chan, XLNXSYNC_CTRL_REG, XLNXSYNC_CTRL_SOFTRESET);
+	/* Wait for a maximum of ~100ms to flush pending transactions */
+	while (num_retries--) {
+		if (!(xlnxsync_read(dev, chan, XLNXSYNC_CTRL_REG) &
+				XLNXSYNC_CTRL_SOFTRESET))
+			break;
+		usleep_range(2000, 2100);
+	}
+}
+
+static void xlnxsync_reset(struct xlnxsync_device *dev)
+{
+	u32 i;
+
+	for (i = 0; i < dev->config.max_channels; i++)
+		xlnxsync_reset_chan(dev, i);
+}
+
+static dma_addr_t xlnxsync_get_phy_addr(struct xlnxsync_device *dev,
+					u32 fd)
+{
+	struct dma_buf *dbuf;
+	struct dma_buf_attachment *attach;
+	struct sg_table *sgt;
+	dma_addr_t phy_addr = 0;
+
+	dbuf = dma_buf_get(fd);
+	if (IS_ERR(dbuf)) {
+		dev_err(dev->dev, "%s : Failed to get dma buf\n", __func__);
+		goto get_phy_addr_err;
+	}
+
+	attach = dma_buf_attach(dbuf, dev->dev);
+	if (IS_ERR(attach)) {
+		dev_err(dev->dev, "%s : Failed to attach buf\n", __func__);
+		goto fail_attach;
+	}
+
+	sgt = dma_buf_map_attachment(attach, DMA_BIDIRECTIONAL);
+	if (IS_ERR(sgt)) {
+		dev_err(dev->dev, "%s : Failed to attach map\n", __func__);
+		goto fail_map;
+	}
+
+	phy_addr = sg_dma_address(sgt->sgl);
+	dma_buf_unmap_attachment(attach, sgt, DMA_BIDIRECTIONAL);
+
+fail_map:
+	dma_buf_detach(dbuf, attach);
+fail_attach:
+	dma_buf_put(dbuf);
+get_phy_addr_err:
+	return phy_addr;
+}
+
+static int xlnxsync_chan_config(struct xlnxsync_channel *channel,
+				void __user *arg)
+{
+	struct xlnxsync_chan_config cfg;
+	int ret, i = 0, j;
+	dma_addr_t phy_start_address;
+	u64 luma_start_address[XLNXSYNC_IO];
+	u64 chroma_start_address[XLNXSYNC_IO];
+	u64 luma_end_address[XLNXSYNC_IO];
+	u64 chroma_end_address[XLNXSYNC_IO];
+	struct xlnxsync_device *dev = channel->dev;
+
+	ret = copy_from_user(&cfg, arg, sizeof(cfg));
+	if (ret) {
+		dev_err(dev->dev, "%s : Failed to copy from user\n", __func__);
+		return ret;
+	}
+
+	if (cfg.hdr_ver != XLNXSYNC_IOCTL_HDR_VER) {
+		dev_err(dev->dev, "%s : ioctl version mismatch\n", __func__);
+		dev_err(dev->dev,
+			"ioctl ver = 0x%llx expected ver = 0x%llx\n",
+			cfg.hdr_ver, (u64)XLNXSYNC_IOCTL_HDR_VER);
+		return -EINVAL;
+	}
+
+	/* Calculate luma/chroma physical addresses */
+	phy_start_address = xlnxsync_get_phy_addr(dev, cfg.dma_fd);
+	if (!phy_start_address) {
+		dev_err(dev->dev, "%s : Failed to obtain physical address\n",
+			__func__);
+		return -EINVAL;
+	}
+
+	luma_start_address[XLNXSYNC_PROD] =
+		cfg.luma_start_offset[XLNXSYNC_PROD] + phy_start_address;
+	luma_start_address[XLNXSYNC_CONS] =
+		cfg.luma_start_offset[XLNXSYNC_CONS] + phy_start_address;
+	chroma_start_address[XLNXSYNC_PROD] =
+		cfg.chroma_start_offset[XLNXSYNC_PROD] + phy_start_address;
+	chroma_start_address[XLNXSYNC_CONS] =
+		cfg. chroma_start_offset[XLNXSYNC_CONS] + phy_start_address;
+	luma_end_address[XLNXSYNC_PROD] =
+		cfg.luma_end_offset[XLNXSYNC_PROD] + phy_start_address;
+	luma_end_address[XLNXSYNC_CONS] =
+		cfg.luma_end_offset[XLNXSYNC_CONS] + phy_start_address;
+	chroma_end_address[XLNXSYNC_PROD] =
+		cfg.chroma_end_offset[XLNXSYNC_PROD] + phy_start_address;
+	chroma_end_address[XLNXSYNC_CONS] =
+		cfg.chroma_end_offset[XLNXSYNC_CONS] + phy_start_address;
+
+	dev_dbg(dev->dev, "Channel id = %d", channel->id);
+	dev_dbg(dev->dev, "Producer address\n");
+	dev_dbg(dev->dev, "Luma Start Addr = 0x%llx End Addr = 0x%llx Margin = 0x%08x\n",
+		luma_start_address[XLNXSYNC_PROD],
+		luma_end_address[XLNXSYNC_PROD], cfg.luma_margin);
+	dev_dbg(dev->dev, "Chroma Start Addr = 0x%llx End Addr = 0x%llx Margin = 0x%08x\n",
+		chroma_start_address[XLNXSYNC_PROD],
+		chroma_end_address[XLNXSYNC_PROD], cfg.chroma_margin);
+	dev_dbg(dev->dev, "FB id = %d IsMono = %d\n",
+		cfg.fb_id[XLNXSYNC_PROD], cfg.ismono[XLNXSYNC_PROD]);
+	dev_dbg(dev->dev, "Consumer address\n");
+	dev_dbg(dev->dev, "Luma Start Addr = 0x%llx End Addr = 0x%llx\n",
+		luma_start_address[XLNXSYNC_CONS],
+		luma_end_address[XLNXSYNC_CONS]);
+	dev_dbg(dev->dev, "Chroma Start Addr = 0x%llx End Addr = 0x%llx\n",
+		chroma_start_address[XLNXSYNC_CONS],
+		chroma_end_address[XLNXSYNC_CONS]);
+	dev_dbg(dev->dev, "FB id = %d IsMono = %d\n",
+		cfg.fb_id[XLNXSYNC_CONS], cfg.ismono[XLNXSYNC_CONS]);
+
+	for (j = 0; j < XLNXSYNC_IO; j++) {
+		u32 l_start_reg, l_end_reg, c_start_reg, c_end_reg;
+
+		if (cfg.fb_id[j] == XLNXSYNC_AUTO_SEARCH) {
+			/*
+			 * When fb_id is 0xFF auto search for free fb
+			 * in a channel
+			 */
+			dev_dbg(dev->dev, "%s : auto search free fb\n",
+				__func__);
+			switch (channel->last_buf_used) {
+			case 0:
+				i = 1;
+				break;
+			case 1:
+				i = 2;
+				break;
+			case 2:
+			case -1:
+			default:
+				i = 0;
+				break;
+			}
+
+			if (xlnxsync_is_buf_done(dev, channel->id, i, j, false)) {
+				dev_dbg(dev->dev,
+					"Channel %d %s FB %d is assigned\n",
+					channel->id, j ? "cons" : "prod", i);
+				if (j)
+					channel->last_buf_used = i;
+			} else {
+				dev_dbg(dev->dev,
+					"Channel %d %s FB %d is busy\n",
+					channel->id, j ? "cons" : "prod", i);
+				if (j) {
+					/* Force clear producer buf valid */
+					if (!xlnxsync_is_buf_done(dev,
+								  channel->id,
+								  i, 0, true)) {
+						dev_err(dev->dev,
+							"prod:buffer valid:\t"
+							"resetting failed\n");
+					}
+				}
+				return -EBUSY;
+			}
+		} else if (cfg.fb_id[j] >= 0 &&
+			   cfg.fb_id[j] < XLNXSYNC_BUF_PER_CHAN) {
+			/* If fb_id is specified, check its availability */
+			if (!(xlnxsync_is_buf_done(dev, channel->id,
+						   cfg.fb_id[j], j, false))) {
+				dev_dbg(dev->dev,
+					"%s : %s FB %d in channel %d is busy!\n",
+					__func__, j ? "prod" : "cons",
+					i, channel->id);
+				return -EBUSY;
+			}
+			dev_dbg(dev->dev, "%s : Configure fb %d\n",
+				__func__, i);
+		} else {
+			/* Invalid fb_id passed */
+			dev_err(dev->dev, "Invalid FB id %d for configuration!\n",
+				cfg.fb_id[j]);
+			return -EINVAL;
+		}
+
+		if (j == XLNXSYNC_PROD) {
+			l_start_reg = XLNXSYNC_PL_START_LO_REG;
+			l_end_reg = XLNXSYNC_PL_END_LO_REG;
+			c_start_reg = XLNXSYNC_PC_START_LO_REG;
+			c_end_reg = XLNXSYNC_PC_END_LO_REG;
+		} else {
+			l_start_reg = XLNXSYNC_CL_START_LO_REG;
+			l_end_reg = XLNXSYNC_CL_END_LO_REG;
+			c_start_reg = XLNXSYNC_CC_START_LO_REG;
+			c_end_reg = XLNXSYNC_CC_END_LO_REG;
+		}
+
+		/* Start Address */
+		xlnxsync_write(dev, channel->id, l_start_reg + (i << 3),
+			       lower_32_bits(luma_start_address[j]));
+
+		xlnxsync_write(dev, channel->id,
+			       (l_start_reg + 4) + (i << 3),
+			       upper_32_bits(luma_start_address[j]) &
+			       XLNXSYNC_FB_HI_ADDR_MASK);
+
+		/* End Address */
+		xlnxsync_write(dev, channel->id, l_end_reg + (i << 3),
+			       lower_32_bits(luma_end_address[j]));
+		xlnxsync_write(dev, channel->id, l_end_reg + 4 + (i << 3),
+			       upper_32_bits(luma_end_address[j]));
+
+		/* Set margin */
+		xlnxsync_write(dev, channel->id,
+			       XLNXSYNC_L_MARGIN_REG + (i << 2),
+			       cfg.luma_margin);
+
+		if (!cfg.ismono[j]) {
+			dev_dbg(dev->dev, "%s : Not monochrome. Program Chroma\n",
+				__func__);
+
+			/* Chroma Start Address */
+			xlnxsync_write(dev, channel->id,
+				       c_start_reg + (i << 3),
+				       lower_32_bits(chroma_start_address[j]));
+
+			xlnxsync_write(dev, channel->id,
+				       c_start_reg + 4 + (i << 3),
+				       upper_32_bits(chroma_start_address[j]) &
+				       XLNXSYNC_FB_HI_ADDR_MASK);
+
+			/* Chroma End Address */
+			xlnxsync_write(dev, channel->id,
+				       c_end_reg + (i << 3),
+				       lower_32_bits(chroma_end_address[j]));
+
+			xlnxsync_write(dev, channel->id,
+				       c_end_reg + 4 + (i << 3),
+				       upper_32_bits(chroma_end_address[j]));
+
+			/* Chroma Margin */
+			xlnxsync_write(dev, channel->id,
+				       XLNXSYNC_C_MARGIN_REG + (i << 2),
+				       cfg.chroma_margin);
+
+			/* Set the Valid bit */
+			xlnxsync_set(dev, channel->id,
+				     c_start_reg + 4 + (i << 3),
+				     XLNXSYNC_FB_VALID_MASK);
+		}
+
+		/* Set the Valid bit */
+		xlnxsync_set(dev, channel->id, l_start_reg + 4 + (i << 3),
+			     XLNXSYNC_FB_VALID_MASK);
+	}
+
+	for (i = 0; i < XLNXSYNC_MAX_CORES; i++) {
+		iowrite32(cfg.luma_core_offset[i],
+			  dev->iomem + XLNXSYNC_LCOREOFF_REG +
+			  (i * XLNXSYNC_COREOFF_NEXT));
+
+		iowrite32(cfg.chroma_core_offset[i],
+			  dev->iomem + XLNXSYNC_CCOREOFF_REG +
+			  (i * XLNXSYNC_COREOFF_NEXT));
+	}
+
+	return 0;
+}
+
+static int xlnxsync_chan_get_status(struct xlnxsync_channel *channel,
+				    void __user *arg)
+{
+	int ret;
+	u32 i, j;
+	unsigned long flags;
+	struct xlnxsync_stat status;
+	struct xlnxsync_device *dev = channel->dev;
+
+	/* Update Buffers status */
+	for (i = 0; i < XLNXSYNC_BUF_PER_CHAN; i++) {
+		for (j = 0; j < XLNXSYNC_IO; j++) {
+			if (xlnxsync_is_buf_done(dev, channel->id, i, j, false))
+				status.fbdone[i][j] = true;
+			else
+				status.fbdone[i][j] = false;
+		}
+	}
+
+	/* Update channel enable status */
+	if (xlnxsync_read(dev, channel->id, XLNXSYNC_CTRL_REG) &
+	    XLNXSYNC_CTRL_ENABLE_MASK)
+		status.enable = true;
+
+	/* Update channel error status */
+	spin_lock_irqsave(&dev->irq_lock, flags);
+	status.err.prod_sync = channel->prod_sync_err;
+	status.err.prod_wdg = channel->prod_wdg_err;
+	status.err.cons_sync = channel->cons_sync_err;
+	status.err.cons_wdg = channel->cons_wdg_err;
+	status.err.ldiff = channel->ldiff_err;
+	status.err.cdiff = channel->cdiff_err;
+	spin_unlock_irqrestore(&dev->irq_lock, flags);
+
+	status.hdr_ver = XLNXSYNC_IOCTL_HDR_VER;
+
+	ret = copy_to_user(arg, &status, sizeof(status));
+	if (ret) {
+		dev_err(dev->dev, "%s: failed to copy result data to user\n",
+			__func__);
+	} else {
+		channel->prod_sync_err = 0;
+		channel->prod_wdg_err = 0;
+		channel->cons_sync_err = 0;
+		channel->cons_wdg_err = 0;
+		channel->ldiff_err = 0;
+		channel->cdiff_err = 0;
+	}
+
+	return ret;
+}
+
+static int xlnxsync_reset_slot(struct xlnxsync_channel *channel)
+{
+	struct xlnxsync_device *dev = channel->dev;
+	int slot;
+
+	if (dev->config.hdr_ver != XLNXSYNC_IOCTL_HDR_VER) {
+		dev_err(dev->dev, "ioctl not supported!\n");
+		return -EINVAL;
+	}
+
+	/* check channel v/s max from dt */
+	if (channel->id >= dev->config.max_channels) {
+		dev_err(dev->dev, "Invalid channel %d. Max channels = %d!\n",
+			channel->id, dev->config.max_channels);
+		return -EINVAL;
+	}
+
+	slot = channel->last_buf_used + 1;
+	if (slot == XLNXSYNC_BUF_PER_CHAN)
+		slot = 0;
+
+	xlnxsync_write(dev, channel->id,
+		       XLNXSYNC_CL_START_LO_REG + 4 + (slot << 3), 0);
+	xlnxsync_write(dev, channel->id,
+		       XLNXSYNC_CC_START_LO_REG + 4 + (slot << 3), 0);
+	xlnxsync_write(dev, channel->id,
+		       XLNXSYNC_PL_START_LO_REG + 4 + (slot << 3), 0);
+	xlnxsync_write(dev, channel->id,
+		       XLNXSYNC_PC_START_LO_REG + 4 + (slot << 3), 0);
+
+	return 0;
+}
+
+static int xlnxsync_chan_enable(struct xlnxsync_channel *channel, bool enable)
+{
+	struct xlnxsync_device *dev = channel->dev;
+	unsigned int i, j;
+
+	if (dev->config.hdr_ver != XLNXSYNC_IOCTL_HDR_VER) {
+		dev_err(dev->dev, "ioctl not supported!\n");
+		return -EINVAL;
+	}
+
+	/* check channel v/s max from dt */
+	if (channel->id >= dev->config.max_channels) {
+		dev_err(dev->dev, "Invalid channel %d. Max channels = %d!\n",
+			channel->id, dev->config.max_channels);
+		return -EINVAL;
+	}
+
+	if (enable) {
+		dev_dbg(dev->dev, "Enabling %d channel\n", channel->id);
+		xlnxsync_set(dev, channel->id, XLNXSYNC_CTRL_REG,
+			     XLNXSYNC_CTRL_ENABLE_MASK |
+			     XLNXSYNC_CTRL_INTR_EN_MASK);
+	} else {
+		dev_dbg(dev->dev, "Disabling %d channel\n", channel->id);
+		xlnxsync_reset_chan(dev, channel->id);
+		xlnxsync_clr(dev, channel->id, XLNXSYNC_CTRL_REG,
+			     XLNXSYNC_CTRL_ENABLE_MASK |
+			     XLNXSYNC_CTRL_INTR_EN_MASK);
+		channel->prod_sync_err = false;
+		channel->prod_wdg_err = false;
+		channel->cons_sync_err = false;
+		channel->cons_wdg_err = false;
+		channel->ldiff_err = false;
+		channel->cdiff_err = false;
+
+		for (i = 0; i < XLNXSYNC_BUF_PER_CHAN; i++) {
+			for (j = 0; j < XLNXSYNC_IO; j++) {
+				channel->l_done[i][j] = false;
+				channel->c_done[i][j] = false;
+			}
+		}
+	}
+
+	return 0;
+}
+
+static int xlnxsync_get_config(struct xlnxsync_channel *channel,
+			       void __user *arg)
+{
+	struct xlnxsync_config cfg;
+	int ret;
+	struct xlnxsync_device *dev = channel->dev;
+
+	cfg.encode = dev->config.encode;
+	cfg.max_channels = dev->config.max_channels;
+	cfg.active_channels = dev->chan_count;
+	cfg.hdr_ver = XLNXSYNC_IOCTL_HDR_VER;
+	cfg.reserved_id = channel->id;
+	dev_dbg(dev->dev, "IP Config : encode = %d max_channels = %d\n",
+		cfg.encode, cfg.max_channels);
+	dev_dbg(dev->dev, "IP Config : active channels = %d reserved id = %d\n",
+		cfg.active_channels, cfg.reserved_id);
+	dev_dbg(dev->dev, "ioctl version = 0x%llx\n", cfg.hdr_ver);
+	ret = copy_to_user(arg, &cfg, sizeof(cfg));
+	if (ret) {
+		dev_err(dev->dev, "%s: failed to copy result data to user\n",
+			__func__);
+		return ret;
+	}
+
+	return 0;
+}
+
+static int xlnxsync_chan_clr_err(struct xlnxsync_channel *channel,
+				 void __user *arg)
+{
+	struct xlnxsync_clr_err errcfg;
+	u32 intr_unmask_val = 0;
+	int ret;
+	unsigned long flags;
+	struct xlnxsync_device *dev = channel->dev;
+
+	ret = copy_from_user(&errcfg, arg, sizeof(errcfg));
+	if (ret) {
+		dev_err(dev->dev, "%s : Failed to copy from user\n", __func__);
+		return ret;
+	}
+
+	if (errcfg.hdr_ver != XLNXSYNC_IOCTL_HDR_VER) {
+		dev_err(dev->dev, "%s : ioctl version mismatch\n", __func__);
+		dev_err(dev->dev,
+			"ioctl ver = 0x%llx expected ver = 0x%llx\n",
+			errcfg.hdr_ver, (u64)XLNXSYNC_IOCTL_HDR_VER);
+		return -EINVAL;
+	}
+
+	dev_dbg(dev->dev, "%s : Clearing %d channel errors\n",
+		__func__, channel->id);
+	/* Clear channel error status */
+	spin_lock_irqsave(&dev->irq_lock, flags);
+	if (errcfg.err.prod_sync) {
+		dev_dbg(dev->dev, "Unmasking producer sync err\n");
+		intr_unmask_val |= XLNXSYNC_IMR_PROD_SYNC_FAIL_MASK;
+	}
+
+	if (errcfg.err.prod_wdg) {
+		dev_dbg(dev->dev, "Unmasking producer wdg err\n");
+		intr_unmask_val |= XLNXSYNC_IMR_PROD_WDG_ERR_MASK;
+	}
+
+	if (errcfg.err.cons_sync) {
+		dev_dbg(dev->dev, "Unmasking consumer sync err\n");
+		intr_unmask_val |= XLNXSYNC_IMR_CONS_SYNC_FAIL_MASK;
+	}
+
+	if (errcfg.err.cons_wdg) {
+		dev_dbg(dev->dev, "Unmasking consumer wdg err\n");
+		intr_unmask_val |= XLNXSYNC_IMR_CONS_WDG_ERR_MASK;
+	}
+
+	if (errcfg.err.ldiff) {
+		dev_dbg(dev->dev, "Unmasking ldiff_err err\n");
+		intr_unmask_val |= XLNXSYNC_IMR_LDIFF;
+	}
+
+	if (errcfg.err.cdiff) {
+		dev_dbg(dev->dev, "Unmasking cdiff_err err\n");
+		intr_unmask_val |= XLNXSYNC_IMR_CDIFF;
+	}
+
+	xlnxsync_clr(dev, channel->id, XLNXSYNC_IMR_REG, intr_unmask_val);
+
+	dev_dbg(dev->dev, "Channel num:%d IMR: %x\n", channel->id,
+		xlnxsync_read(dev, channel->id, XLNXSYNC_IMR_REG));
+
+	spin_unlock_irqrestore(&dev->irq_lock, flags);
+
+	return 0;
+}
+
+static int xlnxsync_chan_get_fbdone_status(struct xlnxsync_channel *channel,
+					   void __user *arg)
+{
+	struct xlnxsync_fbdone fbdone_stat;
+	int ret, i, j;
+	struct xlnxsync_device *dev = channel->dev;
+
+	fbdone_stat.hdr_ver = XLNXSYNC_IOCTL_HDR_VER;
+
+	for (i = 0; i < XLNXSYNC_BUF_PER_CHAN; i++)
+		for (j = 0; j < XLNXSYNC_IO; j++)
+			if (channel->l_done[i][j] &&
+			    channel->c_done[i][j])
+				fbdone_stat.status[i][j] = true;
+
+	ret = copy_to_user(arg, &fbdone_stat, sizeof(fbdone_stat));
+	if (ret)
+		dev_err(dev->dev, "%s: failed to copy result data to user\n",
+			__func__);
+
+	return ret;
+}
+
+static int xlnxsync_chan_clr_fbdone_status(struct xlnxsync_channel *channel,
+					   void __user *arg)
+{
+	struct xlnxsync_fbdone fbd;
+	int ret, i, j;
+	unsigned long flags;
+	struct xlnxsync_device *dev = channel->dev;
+
+	ret = copy_from_user(&fbd, arg, sizeof(fbd));
+	if (ret) {
+		dev_err(dev->dev, "%s : Failed to copy from user\n", __func__);
+		return ret;
+	}
+
+	if (fbd.hdr_ver != XLNXSYNC_IOCTL_HDR_VER) {
+		dev_err(dev->dev, "%s : ioctl version mismatch\n", __func__);
+		dev_err(dev->dev,
+			"ioctl ver = 0x%llx expected ver = 0x%llx\n",
+			fbd.hdr_ver, (u64)XLNXSYNC_IOCTL_HDR_VER);
+		return -EINVAL;
+	}
+
+	/* Clear channel error status */
+	spin_lock_irqsave(&dev->irq_lock, flags);
+	for (i = 0; i < XLNXSYNC_BUF_PER_CHAN; i++) {
+		for (j = 0; j < XLNXSYNC_IO; j++) {
+			fbd.status[i][j] = false;
+			channel->l_done[i][j] = false;
+			channel->c_done[i][j] = false;
+		}
+	}
+	spin_unlock_irqrestore(&dev->irq_lock, flags);
+
+	return 0;
+}
+
+static int xlnxsync_chan_set_int_mask(struct xlnxsync_channel *channel,
+				      void __user *arg)
+{
+	struct xlnxsync_device *dev = channel->dev;
+	struct xlnxsync_intr intr_mask;
+	u32 intr_mask_val = 0;
+	int ret;
+
+	ret = copy_from_user(&intr_mask, arg, sizeof(intr_mask));
+	if (ret) {
+		dev_err(dev->dev, "%s : Failed to copy from user\n", __func__);
+		return ret;
+	}
+
+	/* check driver header version */
+	if (intr_mask.hdr_ver != XLNXSYNC_IOCTL_HDR_VER) {
+		dev_err(dev->dev, "%s : ioctl version mismatch\n", __func__);
+		dev_err(dev->dev,
+			"ioctl ver = 0x%llx expected ver = 0x%llx\n",
+			intr_mask.hdr_ver, (u64)XLNXSYNC_IOCTL_HDR_VER);
+		return -EINVAL;
+	}
+
+	if (intr_mask.err.prod_sync)
+		intr_mask_val |= XLNXSYNC_IMR_PROD_SYNC_FAIL_MASK;
+	if (intr_mask.err.prod_wdg)
+		intr_mask_val |= XLNXSYNC_IMR_PROD_WDG_ERR_MASK;
+	if (intr_mask.err.cons_sync)
+		intr_mask_val |= XLNXSYNC_IMR_CONS_SYNC_FAIL_MASK;
+	if (intr_mask.err.cons_wdg)
+		intr_mask_val |= XLNXSYNC_IMR_CONS_WDG_ERR_MASK;
+	if (intr_mask.err.ldiff)
+		intr_mask_val |= XLNXSYNC_IMR_LDIFF;
+	if (intr_mask.err.cdiff)
+		intr_mask_val |= XLNXSYNC_IMR_CDIFF;
+	if (intr_mask.prod_lfbdone)
+		intr_mask_val |= XLNXSYNC_IMR_PLVALID_MASK;
+	if (intr_mask.prod_cfbdone)
+		intr_mask_val |= XLNXSYNC_IMR_PCVALID_MASK;
+	if (intr_mask.cons_lfbdone)
+		intr_mask_val |= XLNXSYNC_IMR_CLVALID_MASK;
+	if (intr_mask.cons_cfbdone)
+		intr_mask_val |= XLNXSYNC_IMR_CCVALID_MASK;
+
+	dev_dbg(dev->dev, "Set interrupt mask: 0x%x for channel: %d\n",
+		intr_mask_val, channel->id);
+
+	xlnxsync_write(dev, channel->id, XLNXSYNC_IMR_REG, intr_mask_val);
+
+	return ret;
+}
+
+static long xlnxsync_ioctl(struct file *fptr, unsigned int cmd,
+			   unsigned long data)
+{
+	int ret = -EINVAL;
+	void __user *arg = (void __user *)data;
+	struct xlnxsync_channel *channel = fptr->private_data;
+	struct xlnxsync_device *xlnxsync_dev;
+
+	xlnxsync_dev = channel->dev;
+	if (!xlnxsync_dev) {
+		pr_err("%s: File op error\n", __func__);
+		return -EIO;
+	}
+
+	dev_dbg(xlnxsync_dev->dev, "ioctl = 0x%08x\n", cmd);
+
+	switch (cmd) {
+	case XLNXSYNC_GET_CFG:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_get_config(channel, arg);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_CHAN_GET_STATUS:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_chan_get_status(channel, arg);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_CHAN_SET_CONFIG:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_chan_config(channel, arg);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_CHAN_ENABLE:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_chan_enable(channel, true);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_CHAN_DISABLE:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_chan_enable(channel, false);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_CHAN_CLR_ERR:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_chan_clr_err(channel, arg);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_CHAN_GET_FBDONE_STAT:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_chan_get_fbdone_status(channel, arg);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_CHAN_CLR_FBDONE_STAT:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_chan_clr_fbdone_status(channel, arg);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_CHAN_SET_INTR_MASK:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_chan_set_int_mask(channel, arg);
+		mutex_unlock(&channel->mutex);
+		break;
+	case XLNXSYNC_RESET_SLOT:
+		if (mutex_lock_interruptible(&channel->mutex))
+			return -ERESTARTSYS;
+		ret = xlnxsync_reset_slot(channel);
+		mutex_unlock(&channel->mutex);
+		break;
+	}
+
+	return ret;
+}
+
+static __poll_t xlnxsync_poll(struct file *fptr, poll_table *wait)
+{
+	__poll_t ret = 0, req_events = poll_requested_events(wait);
+	struct xlnxsync_channel *channel = fptr->private_data;
+	struct xlnxsync_device *dev;
+	unsigned long flags;
+
+	dev = channel->dev;
+	if (!dev) {
+		pr_err("%s: File op error\n", __func__);
+		return -EIO;
+	}
+
+	dev_dbg_ratelimited(dev->dev, "%s : entered req_events = 0x%x!\n",
+			    __func__, req_events);
+
+	if (!(req_events & (POLLPRI | POLLIN)))
+		return 0;
+
+	if (req_events & EPOLLPRI) {
+		poll_wait(fptr, &channel->wq_error, wait);
+		spin_lock_irqsave(&dev->irq_lock, flags);
+		if (channel->err_event) {
+			dev_dbg_ratelimited(dev->dev,
+					    "%s : error event in chan = %d!\n",
+					     __func__, channel->id);
+			ret |= POLLPRI;
+			channel->err_event = false;
+		}
+		spin_unlock_irqrestore(&dev->irq_lock, flags);
+	}
+
+	if (req_events & EPOLLIN) {
+		poll_wait(fptr, &channel->wq_fbdone, wait);
+		spin_lock_irqsave(&dev->irq_lock, flags);
+		if (channel->framedone_event) {
+			dev_dbg_ratelimited(dev->dev,
+					    "%s : fbdone event in chan = %d!\n",
+					    __func__, channel->id);
+			ret |= POLLIN;
+			channel->framedone_event = false;
+		}
+		spin_unlock_irqrestore(&dev->irq_lock, flags);
+	}
+
+	return ret;
+}
+
+static int xlnxsync_open(struct inode *iptr, struct file *fptr)
+{
+	struct xlnxsync_device *dev;
+	struct xlnxsync_channel *chan;
+	unsigned int i;
+
+	dev = container_of(iptr->i_cdev, struct xlnxsync_device, chdev);
+	if (!dev) {
+		pr_err("%s: failed to get xlnxsync driver handle\n", __func__);
+		return -EAGAIN;
+	}
+
+	chan = devm_kzalloc(dev->dev, sizeof(*chan), GFP_KERNEL);
+	if (!chan)
+		return -ENOMEM;
+
+	if (mutex_lock_interruptible(&dev->sync_mutex))
+		return -ERESTARTSYS;
+	i = find_first_zero_bit_le(&dev->reserved, dev->config.max_channels);
+	if (i >= dev->config.max_channels) {
+		dev_err(dev->dev, "No free channel available\n");
+		mutex_unlock(&dev->sync_mutex);
+		return -ENOSPC;
+	}
+	dev_dbg(dev->dev, "Reserving channel %d\n", i);
+	set_bit(i, &dev->reserved);
+	chan->id = i;
+	chan->last_buf_used = -1;
+	list_add_tail(&chan->channel, &dev->channels);
+	chan->dev = dev;
+	fptr->private_data = chan;
+	mutex_init(&chan->mutex);
+	init_waitqueue_head(&chan->wq_fbdone);
+	init_waitqueue_head(&chan->wq_error);
+	dev->chan_count++;
+	atomic_inc(&dev->user_count);
+	dev_dbg(dev->dev, "%s: tid=%d Opened with user count = %d\n",
+		__func__, current->pid, atomic_read(&dev->user_count));
+	mutex_unlock(&dev->sync_mutex);
+
+	return 0;
+}
+
+static int xlnxsync_release(struct inode *iptr, struct file *fptr)
+{
+	struct xlnxsync_device *dev;
+	struct xlnxsync_channel *channel = fptr->private_data;
+
+	dev = container_of(iptr->i_cdev, struct xlnxsync_device, chdev);
+	if (!dev) {
+		pr_err("%s: failed to get xlnxsync driver handle", __func__);
+		return -EAGAIN;
+	}
+
+	dev_dbg(dev->dev, "%s: tid=%d user count = %d id = %d\n",
+		__func__, current->pid, atomic_read(&dev->user_count),
+		channel->id);
+
+	if (xlnxsync_read(dev, channel->id, XLNXSYNC_CTRL_REG) &
+			XLNXSYNC_CTRL_ENABLE_MASK) {
+		dev_dbg(dev->dev, "Disabling %d channel\n", channel->id);
+		xlnxsync_reset_chan(dev, channel->id);
+		xlnxsync_clr(dev, channel->id, XLNXSYNC_CTRL_REG,
+			     XLNXSYNC_CTRL_ENABLE_MASK |
+			     XLNXSYNC_CTRL_INTR_EN_MASK);
+	}
+
+	if (mutex_lock_interruptible(&dev->sync_mutex))
+		return -ERESTARTSYS;
+	clear_bit(channel->id, &dev->reserved);
+	dev->chan_count--;
+	list_del(&channel->channel);
+	mutex_unlock(&dev->sync_mutex);
+	devm_kfree(dev->dev, channel);
+
+	if (atomic_dec_and_test(&dev->user_count)) {
+		xlnxsync_reset(dev);
+		dev_dbg(dev->dev,
+			"%s: tid=%d Stopping and clearing device",
+			__func__, current->pid);
+	}
+
+	return 0;
+}
+
+static const struct file_operations xlnxsync_fops = {
+	.open = xlnxsync_open,
+	.release = xlnxsync_release,
+	.unlocked_ioctl = xlnxsync_ioctl,
+	.poll = xlnxsync_poll,
+};
+
+static irqreturn_t xlnxsync_irq_handler(int irq, void *data)
+{
+	struct xlnxsync_device *xlnxsync = (struct xlnxsync_device *)data;
+	u32 val;
+	u32 intr_mask_val = 0;
+	struct xlnxsync_channel *chan;
+
+	/*
+	 * Use simple spin_lock (instead of spin_lock_irqsave) as interrupt
+	 * is registered with irqf_oneshot and !irqf_shared
+	 */
+	spin_lock(&xlnxsync->irq_lock);
+	list_for_each_entry(chan, &xlnxsync->channels, channel) {
+		u32 i, j;
+
+		val = xlnxsync_read(xlnxsync, chan->id, XLNXSYNC_ISR_REG);
+
+		if (val & XLNXSYNC_ISR_PROD_SYNC_FAIL_MASK) {
+			chan->prod_sync_err = true;
+			intr_mask_val |= XLNXSYNC_IMR_PROD_SYNC_FAIL_MASK;
+		}
+		if (val & XLNXSYNC_ISR_PROD_WDG_ERR_MASK) {
+			chan->prod_wdg_err = true;
+			intr_mask_val |= XLNXSYNC_IMR_PROD_WDG_ERR_MASK;
+		}
+		if (val & XLNXSYNC_ISR_LDIFF) {
+			chan->ldiff_err = true;
+			intr_mask_val |= XLNXSYNC_IMR_LDIFF;
+		}
+		if (val & XLNXSYNC_ISR_CDIFF) {
+			chan->cdiff_err = true;
+			intr_mask_val |= XLNXSYNC_IMR_CDIFF;
+		}
+		if (val & XLNXSYNC_ISR_CONS_SYNC_FAIL_MASK) {
+			chan->cons_sync_err = true;
+			intr_mask_val |= XLNXSYNC_IMR_CONS_SYNC_FAIL_MASK;
+		}
+		if (val & XLNXSYNC_ISR_CONS_WDG_ERR_MASK) {
+			chan->cons_wdg_err = true;
+			intr_mask_val |= XLNXSYNC_IMR_CONS_WDG_ERR_MASK;
+		}
+		if (chan->prod_sync_err || chan->prod_wdg_err ||
+		    chan->ldiff_err || chan->cdiff_err ||
+		    chan->cons_sync_err || chan->cons_wdg_err)
+			chan->err_event = true;
+
+		if (val & XLNXSYNC_ISR_PLVALID_MASK) {
+			i = (val & XLNXSYNC_ISR_PLDONE_MASK) >>
+				XLNXSYNC_ISR_PLDONE_SHIFT;
+
+			chan->l_done[i][XLNXSYNC_PROD] = true;
+		}
+
+		if (val & XLNXSYNC_ISR_PCVALID_MASK) {
+			i = (val & XLNXSYNC_ISR_PCDONE_MASK) >>
+				XLNXSYNC_ISR_PCDONE_SHIFT;
+
+			chan->c_done[i][XLNXSYNC_PROD] = true;
+		}
+
+		if (val & XLNXSYNC_ISR_CLVALID_MASK) {
+			i = (val & XLNXSYNC_ISR_CLDONE_MASK) >>
+				XLNXSYNC_ISR_CLDONE_SHIFT;
+
+			chan->l_done[i][XLNXSYNC_CONS] = true;
+		}
+
+		if (val & XLNXSYNC_ISR_CCVALID_MASK) {
+			i = (val & XLNXSYNC_ISR_CCDONE_MASK) >>
+				XLNXSYNC_ISR_CCDONE_SHIFT;
+
+			chan->c_done[i][XLNXSYNC_CONS] = true;
+		}
+
+		for (i = 0; i < XLNXSYNC_BUF_PER_CHAN; i++) {
+			for (j = 0; j < XLNXSYNC_IO; j++) {
+				if (chan->l_done[i][j] &&
+				    chan->c_done[i][j])
+					chan->framedone_event = true;
+			}
+		}
+
+		/* Mask corresponding interrupts */
+		if (intr_mask_val)
+			xlnxsync_set(xlnxsync, chan->id, XLNXSYNC_IMR_REG,
+				     intr_mask_val);
+
+		if (chan->err_event) {
+			dev_dbg(xlnxsync->dev, "%s : error occurred at channel->id = %d\n",
+				__func__, chan->id);
+			wake_up_interruptible(&chan->wq_error);
+		}
+
+		if (chan->framedone_event) {
+			dev_dbg_ratelimited(xlnxsync->dev, "%s : framedone occurred\n",
+					    __func__);
+			wake_up_interruptible(&chan->wq_fbdone);
+		}
+
+	}
+
+	spin_unlock(&xlnxsync->irq_lock);
+
+	return IRQ_HANDLED;
+}
+
+static int xlnxsync_parse_dt_prop(struct xlnxsync_device *xlnxsync)
+{
+	struct device_node *node = xlnxsync->dev->of_node;
+	int ret;
+
+	xlnxsync->config.encode = of_property_read_bool(node, "xlnx,encode");
+	dev_dbg(xlnxsync->dev, "synchronizer type = %s\n",
+		xlnxsync->config.encode ? "encode" : "decode");
+
+	ret = of_property_read_u32(node, "xlnx,num-chan",
+				   (u32 *)&xlnxsync->config.max_channels);
+	if (ret)
+		return ret;
+
+	dev_dbg(xlnxsync->dev, "max channels = %d\n",
+		xlnxsync->config.max_channels);
+
+	if (xlnxsync->config.max_channels == 0 ||
+	    xlnxsync->config.max_channels > XLNXSYNC_MAX_ENC_CHAN) {
+		dev_err(xlnxsync->dev, "Number of channels should be 1 to 4.\n");
+		dev_err(xlnxsync->dev, "Invalid number of channels : %d\n",
+			xlnxsync->config.max_channels);
+		return -EINVAL;
+	}
+
+	if (!xlnxsync->config.encode &&
+	    xlnxsync->config.max_channels > XLNXSYNC_MAX_DEC_CHAN) {
+		dev_err(xlnxsync->dev, "Decode can't have more than 2 channels.\n");
+		return -EINVAL;
+	}
+
+	return ret;
+}
+
+static int xlnxsync_clk_setup(struct xlnxsync_device *xlnxsync)
+{
+	int ret;
+
+	xlnxsync->axi_clk = devm_clk_get(xlnxsync->dev, "s_axi_ctrl_aclk");
+	if (IS_ERR(xlnxsync->axi_clk)) {
+		ret = PTR_ERR(xlnxsync->axi_clk);
+		dev_err(xlnxsync->dev, "failed to get axi_aclk (%d)\n", ret);
+		return ret;
+	}
+
+	xlnxsync->p_clk = devm_clk_get(xlnxsync->dev, "s_axi_mm_p_aclk");
+	if (IS_ERR(xlnxsync->p_clk)) {
+		ret = PTR_ERR(xlnxsync->p_clk);
+		dev_err(xlnxsync->dev, "failed to get p_aclk (%d)\n", ret);
+		return ret;
+	}
+
+	xlnxsync->c_clk = devm_clk_get(xlnxsync->dev, "s_axi_mm_aclk");
+	if (IS_ERR(xlnxsync->c_clk)) {
+		ret = PTR_ERR(xlnxsync->c_clk);
+		dev_err(xlnxsync->dev, "failed to get axi_mm (%d)\n", ret);
+		return ret;
+	}
+
+	ret = clk_prepare_enable(xlnxsync->axi_clk);
+	if (ret) {
+		dev_err(xlnxsync->dev, "failed to enable axi_clk (%d)\n", ret);
+		return ret;
+	}
+
+	ret = clk_prepare_enable(xlnxsync->p_clk);
+	if (ret) {
+		dev_err(xlnxsync->dev, "failed to enable p_clk (%d)\n", ret);
+		goto err_pclk;
+	}
+
+	ret = clk_prepare_enable(xlnxsync->c_clk);
+	if (ret) {
+		dev_err(xlnxsync->dev, "failed to enable axi_mm (%d)\n", ret);
+		goto err_cclk;
+	}
+
+	return ret;
+
+err_cclk:
+	clk_disable_unprepare(xlnxsync->p_clk);
+err_pclk:
+	clk_disable_unprepare(xlnxsync->axi_clk);
+
+	return ret;
+}
+
+static int xlnxsync_probe(struct platform_device *pdev)
+{
+	struct xlnxsync_device *xlnxsync;
+	struct device *dc;
+	struct resource *res;
+	int ret;
+
+	xlnxsync = devm_kzalloc(&pdev->dev, sizeof(*xlnxsync), GFP_KERNEL);
+	if (!xlnxsync)
+		return -ENOMEM;
+
+	xlnxsync->minor = ida_simple_get(&xs_ida, 0, XLNXSYNC_DEV_MAX,
+					 GFP_KERNEL);
+	if (xlnxsync->minor < 0)
+		return xlnxsync->minor;
+
+	xlnxsync->dev = &pdev->dev;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(&pdev->dev, "Failed to get resource.\n");
+		return -ENODEV;
+	}
+
+	xlnxsync->iomem = devm_ioremap(xlnxsync->dev, res->start,
+				       resource_size(res));
+	if (!xlnxsync->iomem) {
+		dev_err(&pdev->dev, "ip register mapping failed.\n");
+		return -ENOMEM;
+	}
+
+	ret = xlnxsync_parse_dt_prop(xlnxsync);
+	if (ret < 0)
+		return ret;
+
+	xlnxsync->config.hdr_ver = XLNXSYNC_IOCTL_HDR_VER;
+	dev_dbg(xlnxsync->dev, "ioctl header version = 0x%llx\n",
+		xlnxsync->config.hdr_ver);
+
+	xlnxsync->irq = irq_of_parse_and_map(xlnxsync->dev->of_node, 0);
+	if (!xlnxsync->irq) {
+		dev_err(xlnxsync->dev, "Unable to parse and get irq.\n");
+		return -EINVAL;
+	}
+	ret = devm_request_threaded_irq(xlnxsync->dev, xlnxsync->irq, NULL,
+					xlnxsync_irq_handler,
+					IRQF_ONESHOT | IRQF_TRIGGER_RISING,
+					dev_name(xlnxsync->dev), xlnxsync);
+
+	if (ret) {
+		dev_err(xlnxsync->dev, "Err = %d Interrupt handler reg failed!\n",
+			ret);
+		return ret;
+	}
+
+	ret = xlnxsync_clk_setup(xlnxsync);
+	if (ret) {
+		dev_err(xlnxsync->dev, "clock setup failed!\n");
+		return ret;
+	}
+
+	INIT_LIST_HEAD(&xlnxsync->channels);
+	spin_lock_init(&xlnxsync->irq_lock);
+
+	mutex_init(&xlnxsync->sync_mutex);
+
+	cdev_init(&xlnxsync->chdev, &xlnxsync_fops);
+	xlnxsync->chdev.owner = THIS_MODULE;
+	ret = cdev_add(&xlnxsync->chdev,
+		       MKDEV(MAJOR(xlnxsync_devt), xlnxsync->minor), 1);
+	if (ret < 0) {
+		dev_err(xlnxsync->dev, "cdev_add failed");
+		goto clk_err;
+	}
+
+	if (!xlnxsync_class) {
+		dev_err(xlnxsync->dev, "xvfsync device class not created");
+		goto cdev_err;
+	}
+	dc = device_create(xlnxsync_class, xlnxsync->dev,
+			   MKDEV(MAJOR(xlnxsync_devt), xlnxsync->minor),
+			   xlnxsync, "xlnxsync%d", xlnxsync->minor);
+	if (IS_ERR(dc)) {
+		ret = PTR_ERR(dc);
+		dev_err(xlnxsync->dev, "Unable to create device");
+		goto cdev_err;
+	}
+
+	ret = dma_set_mask(&pdev->dev, DMA_BIT_MASK(44));
+	if (ret) {
+		dev_err(&pdev->dev, "dma_set_mask: %d\n", ret);
+		return ret;
+	}
+
+	platform_set_drvdata(pdev, xlnxsync);
+	dev_info(xlnxsync->dev, "Xilinx Synchronizer probe successful!\n");
+
+	return 0;
+
+cdev_err:
+	cdev_del(&xlnxsync->chdev);
+clk_err:
+	clk_disable_unprepare(xlnxsync->c_clk);
+	clk_disable_unprepare(xlnxsync->p_clk);
+	clk_disable_unprepare(xlnxsync->axi_clk);
+	ida_simple_remove(&xs_ida, xlnxsync->minor);
+
+	return ret;
+}
+
+static int xlnxsync_remove(struct platform_device *pdev)
+{
+	struct xlnxsync_device *xlnxsync = platform_get_drvdata(pdev);
+
+	if (!xlnxsync || !xlnxsync_class)
+		return -EIO;
+
+	cdev_del(&xlnxsync->chdev);
+	clk_disable_unprepare(xlnxsync->c_clk);
+	clk_disable_unprepare(xlnxsync->p_clk);
+	clk_disable_unprepare(xlnxsync->axi_clk);
+	ida_simple_remove(&xs_ida, xlnxsync->minor);
+
+	return 0;
+}
+
+static const struct of_device_id xlnxsync_of_match[] = {
+	{ .compatible = "xlnx,sync-ip-1.0", },
+	{ /* end of table*/ }
+};
+MODULE_DEVICE_TABLE(of, xlnxsync_of_match);
+
+static struct platform_driver xlnxsync_driver = {
+	.driver = {
+		.name = XLNXSYNC_DRIVER_NAME,
+		.of_match_table = xlnxsync_of_match,
+	},
+	.probe = xlnxsync_probe,
+	.remove = xlnxsync_remove,
+};
+
+static int __init xlnxsync_init_mod(void)
+{
+	int err;
+
+	xlnxsync_class = class_create(THIS_MODULE, XLNXSYNC_DRIVER_NAME);
+	if (IS_ERR(xlnxsync_class)) {
+		pr_err("%s : Unable to create xlnxsync class", __func__);
+		return PTR_ERR(xlnxsync_class);
+	}
+	err = alloc_chrdev_region(&xlnxsync_devt, 0,
+				  XLNXSYNC_DEV_MAX, XLNXSYNC_DRIVER_NAME);
+	if (err < 0) {
+		pr_err("%s: Unable to get major number for xlnxsync", __func__);
+		goto err_class;
+	}
+	err = platform_driver_register(&xlnxsync_driver);
+	if (err < 0) {
+		pr_err("%s: Unable to register %s driver",
+		       __func__, XLNXSYNC_DRIVER_NAME);
+		goto err_pdrv;
+	}
+	return 0;
+err_pdrv:
+	unregister_chrdev_region(xlnxsync_devt, XLNXSYNC_DEV_MAX);
+err_class:
+	class_destroy(xlnxsync_class);
+	return err;
+}
+
+static void __exit xlnxsync_cleanup_mod(void)
+{
+	platform_driver_unregister(&xlnxsync_driver);
+	unregister_chrdev_region(xlnxsync_devt, XLNXSYNC_DEV_MAX);
+	class_destroy(xlnxsync_class);
+	xlnxsync_class = NULL;
+}
+module_init(xlnxsync_init_mod);
+module_exit(xlnxsync_cleanup_mod);
+
+MODULE_AUTHOR("Vishal Sagar");
+MODULE_DESCRIPTION("Xilinx Synchronizer IP Driver");
+MODULE_LICENSE("GPL v2");
+MODULE_VERSION(XLNXSYNC_DRIVER_VERSION);
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroeframer/Kconfig
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroeframer/Kconfig	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,18 @@
+#
+# Xilinx Radio over Ethernet Framer driver
+#
+
+config XROE_FRAMER
+	tristate "Xilinx Radio over Ethernet Framer driver"
+	help
+	  The "Radio Over Ethernet Framer" IP (roe_framer) ingests/generates
+	  Ethernet packet data, (de-)multiplexes packets based on protocol
+	  into/from various Radio Antenna data streams.
+
+	  It has 2 main, independent, data paths:
+
+	  - Downlink, from the BaseBand to the Phone, Ethernet to Antenna,
+	  we call this the De-Framer path, or defm on all related IP signals.
+
+	  - Uplink, from the Phone to the BaseBand, Antenna to Ethernet,
+	  we call this the Framer path, or fram on all related IP signals.
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroeframer/Makefile
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroeframer/Makefile	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,12 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Makefile for Radio over Ethernet Framer driver
+#
+obj-$(CONFIG_XROE_FRAMER)	:= framer.o
+
+framer-objs := 	xroe_framer.o \
+		sysfs_xroe.o \
+		sysfs_xroe_framer_ipv4.o \
+		sysfs_xroe_framer_ipv6.o \
+		sysfs_xroe_framer_udp.o \
+		sysfs_xroe_framer_stats.o
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroeframer/README
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroeframer/README	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,47 @@
+Xilinx Radio over Ethernet Framer driver
+=========================================
+
+About the RoE Framer
+
+The "Radio Over Ethernet Framer" IP (roe_framer) ingests/generates Ethernet
+packet data, (de-)multiplexes packets based on protocol into/from various
+Radio Antenna data streams.
+
+It has 2 main, independent, data paths
+
+- Downlink, from the BaseBand to the Phone, Ethernet to Antenna,
+we call this the De-Framer path, or defm on all related IP signals.
+
+- Uplink, from the Phone to the BaseBand, Antenna to Ethernet,
+we call this the Framer path, or fram on all related IP signals.
+
+Key points:
+
+- Apart from the AXI4-Lite configuration port and a handful of strobe/control
+signals all data interfaces are AXI Stream(AXIS).
+- The IP does not contain an Ethernet MAC IP, rather it routes, or creates
+packets based on the direction through the roe_framer.
+- Currently designed to work with
+	- 1, 2 or 4 10G Ethernet AXIS stream ports to/from 1, 2, 4, 8, 16,
+	or 32 antenna ports
+		Note: each Ethernet port is 64 bit data @ 156.25MHz
+	- 1 or 2 25G Ethernet AXIS stream ports to/from 1, 2, 4, 8, 16,
+	or 32 antenna ports
+		Note: each Ethernet port is 64 bit data @ 390.25MHz
+- Contains a filter so that all non-protocol packets, or non-hardware-IP
+processed packets can be forwarded to another block for processing. In general
+this in a Microprocessor, specifically the Zynq ARM in our case. This filter
+function can move into the optional switch when TSN is used.
+
+About the Linux Driver
+
+The RoE Framer Linux Driver provides sysfs access to the framer controls. The
+loading of the driver to the hardware is possible using Device Tree binding
+(see "dt-binding.txt" for more information). When the driver is loaded, the
+general controls (such as framing mode, enable, restart etc) are exposed
+under /sys/kernel/xroe. Furthermore, specific controls can be found under
+/sys/kernel/xroe/framer. These include protocol-specific settings, for
+IPv4, IPv6 & UDP.
+
+There is also the option of accessing the framer's register map using
+ioctl calls for both reading and writing (where permitted) directly.
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroeframer/dt-binding.txt
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroeframer/dt-binding.txt	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,17 @@
+* Xilinx Radio over Ethernet Framer driver
+
+Required properties:
+- compatible: must be "xlnx,roe-framer-1.0"
+- reg: physical base address of the framer and length of memory mapped region
+- clock-names: list of clock names
+- clocks: list of clock sources corresponding to the clock names
+
+Example:
+	roe_framer@a0000000 {
+		compatible = "xlnx,roe-framer-1.0";
+		reg = <0x0 0xa0000000 0x0 0x10000>;
+		clock-names = "s_axi_aclk", "m_axis_defm_aclk",
+			      "s_axis_fram_aclk", "tx0_eth_port_clk",
+			      "internal_bus_clk";
+		clocks = <0x43 0x44 0x44 0x45 0x45>;
+	};
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroeframer/roe_framer_ctrl.h
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroeframer/roe_framer_ctrl.h	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,1088 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+/*-----------------------------------------------------------------------------
+ * C Header bank BASE definitions
+ *-----------------------------------------------------------------------------
+ */
+#define ROE_FRAMER_V1_0_CFG_BASE_ADDR 0x0 /* 0 */
+#define ROE_FRAMER_V1_0_FRAM_BASE_ADDR 0x2000 /* 8192 */
+#define ROE_FRAMER_V1_0_FRAM_DRP_BASE_ADDR 0x4000 /* 16384 */
+#define ROE_FRAMER_V1_0_DEFM_BASE_ADDR 0x6000 /* 24576 */
+#define ROE_FRAMER_V1_0_DEFM_DRP_BASE_ADDR 0x8000 /* 32768 */
+#define ROE_FRAMER_V1_0_ETH_BASE_ADDR 0xa000 /* 40960 */
+#define ROE_FRAMER_V1_0_STATS_BASE_ADDR 0xc000 /* 49152 */
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_framer_v1_0_cfg
+ * with prefix cfg_ @ address 0x0
+ *-----------------------------------------------------------------------------
+ */
+/* Type = roInt */
+#define CFG_MAJOR_REVISION_ADDR 0x0 /* 0 */
+#define CFG_MAJOR_REVISION_MASK 0xff000000 /* 4278190080 */
+#define CFG_MAJOR_REVISION_OFFSET 0x18 /* 24 */
+#define CFG_MAJOR_REVISION_WIDTH 0x8 /* 8 */
+#define CFG_MAJOR_REVISION_DEFAULT 0x1 /* 1 */
+
+/* Type = roInt */
+#define CFG_MINOR_REVISION_ADDR 0x0 /* 0 */
+#define CFG_MINOR_REVISION_MASK 0xff0000 /* 16711680 */
+#define CFG_MINOR_REVISION_OFFSET 0x10 /* 16 */
+#define CFG_MINOR_REVISION_WIDTH 0x8 /* 8 */
+#define CFG_MINOR_REVISION_DEFAULT 0x0 /* 0 */
+
+/* Type = roInt */
+#define CFG_VERSION_REVISION_ADDR 0x0 /* 0 */
+#define CFG_VERSION_REVISION_MASK 0xff00 /* 65280 */
+#define CFG_VERSION_REVISION_OFFSET 0x8 /* 8 */
+#define CFG_VERSION_REVISION_WIDTH 0x8 /* 8 */
+#define CFG_VERSION_REVISION_DEFAULT 0x0 /* 0 */
+
+/* Type = roInt */
+#define CFG_INTERNAL_REVISION_ADDR 0x4 /* 4 */
+#define CFG_INTERNAL_REVISION_MASK 0xffffffff /* 4294967295 */
+#define CFG_INTERNAL_REVISION_OFFSET 0x0 /* 0 */
+#define CFG_INTERNAL_REVISION_WIDTH 0x20 /* 32 */
+#define CFG_INTERNAL_REVISION_DEFAULT 0x12345678 /* 305419896 */
+
+/* Type = rw */
+#define CFG_TIMEOUT_VALUE_ADDR 0x8 /* 8 */
+#define CFG_TIMEOUT_VALUE_MASK 0xfff /* 4095 */
+#define CFG_TIMEOUT_VALUE_OFFSET 0x0 /* 0 */
+#define CFG_TIMEOUT_VALUE_WIDTH 0xc /* 12 */
+#define CFG_TIMEOUT_VALUE_DEFAULT 0x80 /* 128 */
+
+/* Type = rw */
+#define CFG_USER_RW_OUT_ADDR 0xc /* 12 */
+#define CFG_USER_RW_OUT_MASK 0xff /* 255 */
+#define CFG_USER_RW_OUT_OFFSET 0x0 /* 0 */
+#define CFG_USER_RW_OUT_WIDTH 0x8 /* 8 */
+#define CFG_USER_RW_OUT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_USER_RO_IN_ADDR 0xc /* 12 */
+#define CFG_USER_RO_IN_MASK 0xff0000 /* 16711680 */
+#define CFG_USER_RO_IN_OFFSET 0x10 /* 16 */
+#define CFG_USER_RO_IN_WIDTH 0x8 /* 8 */
+#define CFG_USER_RO_IN_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define CFG_MASTER_INT_ENABLE_ADDR 0x10 /* 16 */
+#define CFG_MASTER_INT_ENABLE_MASK 0x1 /* 1 */
+#define CFG_MASTER_INT_ENABLE_OFFSET 0x0 /* 0 */
+#define CFG_MASTER_INT_ENABLE_WIDTH 0x1 /* 1 */
+#define CFG_MASTER_INT_ENABLE_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define CFG_FRAM_FIFO_OF_ENABLE_ADDR 0x14 /* 20 */
+#define CFG_FRAM_FIFO_OF_ENABLE_MASK 0x1 /* 1 */
+#define CFG_FRAM_FIFO_OF_ENABLE_OFFSET 0x0 /* 0 */
+#define CFG_FRAM_FIFO_OF_ENABLE_WIDTH 0x1 /* 1 */
+#define CFG_FRAM_FIFO_OF_ENABLE_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define CFG_FRAM_FIFO_UF_ENABLE_ADDR 0x14 /* 20 */
+#define CFG_FRAM_FIFO_UF_ENABLE_MASK 0x2 /* 2 */
+#define CFG_FRAM_FIFO_UF_ENABLE_OFFSET 0x1 /* 1 */
+#define CFG_FRAM_FIFO_UF_ENABLE_WIDTH 0x1 /* 1 */
+#define CFG_FRAM_FIFO_UF_ENABLE_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define CFG_AXI_TIMEOUT_ENABLE_ADDR 0x14 /* 20 */
+#define CFG_AXI_TIMEOUT_ENABLE_MASK 0x80000000 /* 2147483648 */
+#define CFG_AXI_TIMEOUT_ENABLE_OFFSET 0x1f /* 31 */
+#define CFG_AXI_TIMEOUT_ENABLE_WIDTH 0x1 /* 1 */
+#define CFG_AXI_TIMEOUT_ENABLE_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define CFG_INTERRUPT_STATUS_SAMPLE_ADDR 0x1c /* 28 */
+#define CFG_INTERRUPT_STATUS_SAMPLE_MASK 0x1 /* 1 */
+#define CFG_INTERRUPT_STATUS_SAMPLE_OFFSET 0x0 /* 0 */
+#define CFG_INTERRUPT_STATUS_SAMPLE_WIDTH 0x1 /* 1 */
+#define CFG_INTERRUPT_STATUS_SAMPLE_DEFAULT 0x1 /* 1 */
+
+/* Type = roSig */
+#define CFG_FRAM_RESET_STATUS_ADDR 0x18 /* 24 */
+#define CFG_FRAM_RESET_STATUS_MASK 0x1 /* 1 */
+#define CFG_FRAM_RESET_STATUS_OFFSET 0x0 /* 0 */
+#define CFG_FRAM_RESET_STATUS_WIDTH 0x1 /* 1 */
+#define CFG_FRAM_RESET_STATUS_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_DEFM_RESET_STATUS_ADDR 0x18 /* 24 */
+#define CFG_DEFM_RESET_STATUS_MASK 0x2 /* 2 */
+#define CFG_DEFM_RESET_STATUS_OFFSET 0x1 /* 1 */
+#define CFG_DEFM_RESET_STATUS_WIDTH 0x1 /* 1 */
+#define CFG_DEFM_RESET_STATUS_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_FRAM_ANT_OF_INTERRUPT_ADDR 0x18 /* 24 */
+#define CFG_FRAM_ANT_OF_INTERRUPT_MASK 0x100 /* 256 */
+#define CFG_FRAM_ANT_OF_INTERRUPT_OFFSET 0x8 /* 8 */
+#define CFG_FRAM_ANT_OF_INTERRUPT_WIDTH 0x1 /* 1 */
+#define CFG_FRAM_ANT_OF_INTERRUPT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_FRAM_ETH_OF_INTERRUPT_ADDR 0x18 /* 24 */
+#define CFG_FRAM_ETH_OF_INTERRUPT_MASK 0x200 /* 512 */
+#define CFG_FRAM_ETH_OF_INTERRUPT_OFFSET 0x9 /* 9 */
+#define CFG_FRAM_ETH_OF_INTERRUPT_WIDTH 0x1 /* 1 */
+#define CFG_FRAM_ETH_OF_INTERRUPT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_FRAM_ANT_UF_INTERRUPT_ADDR 0x18 /* 24 */
+#define CFG_FRAM_ANT_UF_INTERRUPT_MASK 0x400 /* 1024 */
+#define CFG_FRAM_ANT_UF_INTERRUPT_OFFSET 0xa /* 10 */
+#define CFG_FRAM_ANT_UF_INTERRUPT_WIDTH 0x1 /* 1 */
+#define CFG_FRAM_ANT_UF_INTERRUPT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_FRAM_ETH_UF_INTERRUPT_ADDR 0x18 /* 24 */
+#define CFG_FRAM_ETH_UF_INTERRUPT_MASK 0x800 /* 2048 */
+#define CFG_FRAM_ETH_UF_INTERRUPT_OFFSET 0xb /* 11 */
+#define CFG_FRAM_ETH_UF_INTERRUPT_WIDTH 0x1 /* 1 */
+#define CFG_FRAM_ETH_UF_INTERRUPT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_AXI_TIMEOUT_STATUS_ADDR 0x18 /* 24 */
+#define CFG_AXI_TIMEOUT_STATUS_MASK 0x80000000 /* 2147483648 */
+#define CFG_AXI_TIMEOUT_STATUS_OFFSET 0x1f /* 31 */
+#define CFG_AXI_TIMEOUT_STATUS_WIDTH 0x1 /* 1 */
+#define CFG_AXI_TIMEOUT_STATUS_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_CONFIG_NO_OF_FRAM_ANTS_ADDR 0x20 /* 32 */
+#define CFG_CONFIG_NO_OF_FRAM_ANTS_MASK 0xffff /* 65535 */
+#define CFG_CONFIG_NO_OF_FRAM_ANTS_OFFSET 0x0 /* 0 */
+#define CFG_CONFIG_NO_OF_FRAM_ANTS_WIDTH 0x10 /* 16 */
+#define CFG_CONFIG_NO_OF_FRAM_ANTS_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_CONFIG_NO_OF_DEFM_ANTS_ADDR 0x20 /* 32 */
+#define CFG_CONFIG_NO_OF_DEFM_ANTS_MASK 0xffff0000 /* 4294901760 */
+#define CFG_CONFIG_NO_OF_DEFM_ANTS_OFFSET 0x10 /* 16 */
+#define CFG_CONFIG_NO_OF_DEFM_ANTS_WIDTH 0x10 /* 16 */
+#define CFG_CONFIG_NO_OF_DEFM_ANTS_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_CONFIG_NO_OF_ETH_PORTS_ADDR 0x24 /* 36 */
+#define CFG_CONFIG_NO_OF_ETH_PORTS_MASK 0x3ff /* 1023 */
+#define CFG_CONFIG_NO_OF_ETH_PORTS_OFFSET 0x0 /* 0 */
+#define CFG_CONFIG_NO_OF_ETH_PORTS_WIDTH 0xa /* 10 */
+#define CFG_CONFIG_NO_OF_ETH_PORTS_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define CFG_CONFIG_ETH_SPEED_ADDR 0x24 /* 36 */
+#define CFG_CONFIG_ETH_SPEED_MASK 0x3ff0000 /* 67043328 */
+#define CFG_CONFIG_ETH_SPEED_OFFSET 0x10 /* 16 */
+#define CFG_CONFIG_ETH_SPEED_WIDTH 0xa /* 10 */
+#define CFG_CONFIG_ETH_SPEED_DEFAULT 0x0 /* 0 */
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_framer_v1_0_fram
+ * with prefix fram_ @ address 0x2000
+ *------------------------------------------------------------------------------
+ */
+/* Type = rwpdef */
+#define FRAM_DISABLE_ADDR 0x2000 /* 8192 */
+#define FRAM_DISABLE_MASK 0x1 /* 1 */
+#define FRAM_DISABLE_OFFSET 0x0 /* 0 */
+#define FRAM_DISABLE_WIDTH 0x1 /* 1 */
+#define FRAM_DISABLE_DEFAULT 0x1 /* 1 */
+
+/* Type = roSig */
+#define FRAM_READY_ADDR 0x2000 /* 8192 */
+#define FRAM_READY_MASK 0x2 /* 2 */
+#define FRAM_READY_OFFSET 0x1 /* 1 */
+#define FRAM_READY_WIDTH 0x1 /* 1 */
+#define FRAM_READY_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define FRAM_FIFO_FULL_INDICATOR_ADDR 0x2004 /* 8196 */
+#define FRAM_FIFO_FULL_INDICATOR_MASK 0xffffffff /* 4294967295 */
+#define FRAM_FIFO_FULL_INDICATOR_OFFSET 0x0 /* 0 */
+#define FRAM_FIFO_FULL_INDICATOR_WIDTH 0x20 /* 32 */
+#define FRAM_FIFO_FULL_INDICATOR_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_SN_DATA_LOW_CNT_MIN_ADDR 0x2020 /* 8224 */
+#define FRAM_SN_DATA_LOW_CNT_MIN_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_DATA_LOW_CNT_MIN_OFFSET 0x0 /* 0 */
+#define FRAM_SN_DATA_LOW_CNT_MIN_WIDTH 0x20 /* 32 */
+#define FRAM_SN_DATA_LOW_CNT_MIN_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define FRAM_SN_DATA_LOW_CNT_MAX_ADDR 0x2024 /* 8228 */
+#define FRAM_SN_DATA_LOW_CNT_MAX_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_DATA_LOW_CNT_MAX_OFFSET 0x0 /* 0 */
+#define FRAM_SN_DATA_LOW_CNT_MAX_WIDTH 0x20 /* 32 */
+#define FRAM_SN_DATA_LOW_CNT_MAX_DEFAULT 0x78 /* 120 */
+
+/* Type = rw */
+#define FRAM_SN_DATA_LOW_CNT_INITVAL_ADDR 0x2028 /* 8232 */
+#define FRAM_SN_DATA_LOW_CNT_INITVAL_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_DATA_LOW_CNT_INITVAL_OFFSET 0x0 /* 0 */
+#define FRAM_SN_DATA_LOW_CNT_INITVAL_WIDTH 0x20 /* 32 */
+#define FRAM_SN_DATA_LOW_CNT_INITVAL_DEFAULT 0x75 /* 117 */
+
+/* Type = rw */
+#define FRAM_SN_DATA_LOW_CNT_INCVAL_ADDR 0x202c /* 8236 */
+#define FRAM_SN_DATA_LOW_CNT_INCVAL_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_DATA_LOW_CNT_INCVAL_OFFSET 0x0 /* 0 */
+#define FRAM_SN_DATA_LOW_CNT_INCVAL_WIDTH 0x20 /* 32 */
+#define FRAM_SN_DATA_LOW_CNT_INCVAL_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define FRAM_SN_DATA_HIGH_CNT_MIN_ADDR 0x2030 /* 8240 */
+#define FRAM_SN_DATA_HIGH_CNT_MIN_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_DATA_HIGH_CNT_MIN_OFFSET 0x0 /* 0 */
+#define FRAM_SN_DATA_HIGH_CNT_MIN_WIDTH 0x20 /* 32 */
+#define FRAM_SN_DATA_HIGH_CNT_MIN_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_SN_DATA_HIGH_CNT_MAX_ADDR 0x2034 /* 8244 */
+#define FRAM_SN_DATA_HIGH_CNT_MAX_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_DATA_HIGH_CNT_MAX_OFFSET 0x0 /* 0 */
+#define FRAM_SN_DATA_HIGH_CNT_MAX_WIDTH 0x20 /* 32 */
+#define FRAM_SN_DATA_HIGH_CNT_MAX_DEFAULT 0x4f /* 79 */
+
+/* Type = rw */
+#define FRAM_SN_DATA_HIGH_CNT_INITVAL_ADDR 0x2038 /* 8248 */
+#define FRAM_SN_DATA_HIGH_CNT_INITVAL_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_DATA_HIGH_CNT_INITVAL_OFFSET 0x0 /* 0 */
+#define FRAM_SN_DATA_HIGH_CNT_INITVAL_WIDTH 0x20 /* 32 */
+#define FRAM_SN_DATA_HIGH_CNT_INITVAL_DEFAULT 0x4f /* 79 */
+
+/* Type = rw */
+#define FRAM_SN_DATA_HIGH_CNT_INCVAL_ADDR 0x203c /* 8252 */
+#define FRAM_SN_DATA_HIGH_CNT_INCVAL_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_DATA_HIGH_CNT_INCVAL_OFFSET 0x0 /* 0 */
+#define FRAM_SN_DATA_HIGH_CNT_INCVAL_WIDTH 0x20 /* 32 */
+#define FRAM_SN_DATA_HIGH_CNT_INCVAL_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define FRAM_SN_CTRL_LOW_CNT_MIN_ADDR 0x2050 /* 8272 */
+#define FRAM_SN_CTRL_LOW_CNT_MIN_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_CTRL_LOW_CNT_MIN_OFFSET 0x0 /* 0 */
+#define FRAM_SN_CTRL_LOW_CNT_MIN_WIDTH 0x20 /* 32 */
+#define FRAM_SN_CTRL_LOW_CNT_MIN_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define FRAM_SN_CTRL_LOW_CNT_MAX_ADDR 0x2054 /* 8276 */
+#define FRAM_SN_CTRL_LOW_CNT_MAX_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_CTRL_LOW_CNT_MAX_OFFSET 0x0 /* 0 */
+#define FRAM_SN_CTRL_LOW_CNT_MAX_WIDTH 0x20 /* 32 */
+#define FRAM_SN_CTRL_LOW_CNT_MAX_DEFAULT 0x78 /* 120 */
+
+/* Type = rw */
+#define FRAM_SN_CTRL_LOW_CNT_INITVAL_ADDR 0x2058 /* 8280 */
+#define FRAM_SN_CTRL_LOW_CNT_INITVAL_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_CTRL_LOW_CNT_INITVAL_OFFSET 0x0 /* 0 */
+#define FRAM_SN_CTRL_LOW_CNT_INITVAL_WIDTH 0x20 /* 32 */
+#define FRAM_SN_CTRL_LOW_CNT_INITVAL_DEFAULT 0x75 /* 117 */
+
+/* Type = rw */
+#define FRAM_SN_CTRL_LOW_CNT_INCVAL_ADDR 0x205c /* 8284 */
+#define FRAM_SN_CTRL_LOW_CNT_INCVAL_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_CTRL_LOW_CNT_INCVAL_OFFSET 0x0 /* 0 */
+#define FRAM_SN_CTRL_LOW_CNT_INCVAL_WIDTH 0x20 /* 32 */
+#define FRAM_SN_CTRL_LOW_CNT_INCVAL_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define FRAM_SN_CTRL_HIGH_CNT_MIN_ADDR 0x2060 /* 8288 */
+#define FRAM_SN_CTRL_HIGH_CNT_MIN_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_CTRL_HIGH_CNT_MIN_OFFSET 0x0 /* 0 */
+#define FRAM_SN_CTRL_HIGH_CNT_MIN_WIDTH 0x20 /* 32 */
+#define FRAM_SN_CTRL_HIGH_CNT_MIN_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_SN_CTRL_HIGH_CNT_MAX_ADDR 0x2064 /* 8292 */
+#define FRAM_SN_CTRL_HIGH_CNT_MAX_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_CTRL_HIGH_CNT_MAX_OFFSET 0x0 /* 0 */
+#define FRAM_SN_CTRL_HIGH_CNT_MAX_WIDTH 0x20 /* 32 */
+#define FRAM_SN_CTRL_HIGH_CNT_MAX_DEFAULT 0x4f /* 79 */
+
+/* Type = rw */
+#define FRAM_SN_CTRL_HIGH_CNT_INITVAL_ADDR 0x2068 /* 8296 */
+#define FRAM_SN_CTRL_HIGH_CNT_INITVAL_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_CTRL_HIGH_CNT_INITVAL_OFFSET 0x0 /* 0 */
+#define FRAM_SN_CTRL_HIGH_CNT_INITVAL_WIDTH 0x20 /* 32 */
+#define FRAM_SN_CTRL_HIGH_CNT_INITVAL_DEFAULT 0x4f /* 79 */
+
+/* Type = rw */
+#define FRAM_SN_CTRL_HIGH_CNT_INCVAL_ADDR 0x206c /* 8300 */
+#define FRAM_SN_CTRL_HIGH_CNT_INCVAL_MASK 0xffffffff /* 4294967295 */
+#define FRAM_SN_CTRL_HIGH_CNT_INCVAL_OFFSET 0x0 /* 0 */
+#define FRAM_SN_CTRL_HIGH_CNT_INCVAL_WIDTH 0x20 /* 32 */
+#define FRAM_SN_CTRL_HIGH_CNT_INCVAL_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define FRAM_PROTOCOL_DEFINITION_ADDR 0x2200 /* 8704 */
+#define FRAM_PROTOCOL_DEFINITION_MASK 0xf /* 15 */
+#define FRAM_PROTOCOL_DEFINITION_OFFSET 0x0 /* 0 */
+#define FRAM_PROTOCOL_DEFINITION_WIDTH 0x4 /* 4 */
+#define FRAM_PROTOCOL_DEFINITION_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_GEN_VLAN_TAG_ADDR 0x2200 /* 8704 */
+#define FRAM_GEN_VLAN_TAG_MASK 0x10 /* 16 */
+#define FRAM_GEN_VLAN_TAG_OFFSET 0x4 /* 4 */
+#define FRAM_GEN_VLAN_TAG_WIDTH 0x1 /* 1 */
+#define FRAM_GEN_VLAN_TAG_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_SEL_IPV_ADDRESS_TYPE_ADDR 0x2200 /* 8704 */
+#define FRAM_SEL_IPV_ADDRESS_TYPE_MASK 0x60 /* 96 */
+#define FRAM_SEL_IPV_ADDRESS_TYPE_OFFSET 0x5 /* 5 */
+#define FRAM_SEL_IPV_ADDRESS_TYPE_WIDTH 0x2 /* 2 */
+#define FRAM_SEL_IPV_ADDRESS_TYPE_DEFAULT 0x0 /* 0 */
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_framer_v1_0_fram_drp
+ * with prefix fram_drp @ address 0x4000
+ *------------------------------------------------------------------------------
+ */
+/* Type = rw */
+#define FRAM_DRPFRAM_DATA_PC_ID_ADDR 0x4000 /* 16384 */
+#define FRAM_DRPFRAM_DATA_PC_ID_MASK 0xffff /* 65535 */
+#define FRAM_DRPFRAM_DATA_PC_ID_OFFSET 0x0 /* 0 */
+#define FRAM_DRPFRAM_DATA_PC_ID_WIDTH 0x10 /* 16 */
+#define FRAM_DRPFRAM_DATA_PC_ID_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_DRPFRAM_DATA_MESSAGE_TYPE_ADDR 0x4000 /* 16384 */
+#define FRAM_DRPFRAM_DATA_MESSAGE_TYPE_MASK 0xff0000 /* 16711680 */
+#define FRAM_DRPFRAM_DATA_MESSAGE_TYPE_OFFSET 0x10 /* 16 */
+#define FRAM_DRPFRAM_DATA_MESSAGE_TYPE_WIDTH 0x8 /* 8 */
+#define FRAM_DRPFRAM_DATA_MESSAGE_TYPE_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_DRPFRAM_DATA_ETHERNET_PORT_ADDR 0x4000 /* 16384 */
+#define FRAM_DRPFRAM_DATA_ETHERNET_PORT_MASK 0xff000000 /* 4278190080 */
+#define FRAM_DRPFRAM_DATA_ETHERNET_PORT_OFFSET 0x18 /* 24 */
+#define FRAM_DRPFRAM_DATA_ETHERNET_PORT_WIDTH 0x8 /* 8 */
+#define FRAM_DRPFRAM_DATA_ETHERNET_PORT_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_DRPFRAM_CTRL_PC_ID_ADDR 0x4400 /* 17408 */
+#define FRAM_DRPFRAM_CTRL_PC_ID_MASK 0xffff /* 65535 */
+#define FRAM_DRPFRAM_CTRL_PC_ID_OFFSET 0x0 /* 0 */
+#define FRAM_DRPFRAM_CTRL_PC_ID_WIDTH 0x10 /* 16 */
+#define FRAM_DRPFRAM_CTRL_PC_ID_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_DRPFRAM_CTRL_MESSAGE_TYPE_ADDR 0x4400 /* 17408 */
+#define FRAM_DRPFRAM_CTRL_MESSAGE_TYPE_MASK 0xff0000 /* 16711680 */
+#define FRAM_DRPFRAM_CTRL_MESSAGE_TYPE_OFFSET 0x10 /* 16 */
+#define FRAM_DRPFRAM_CTRL_MESSAGE_TYPE_WIDTH 0x8 /* 8 */
+#define FRAM_DRPFRAM_CTRL_MESSAGE_TYPE_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define FRAM_DRPFRAM_CTRL_ETHERNET_PORT_ADDR 0x4400 /* 17408 */
+#define FRAM_DRPFRAM_CTRL_ETHERNET_PORT_MASK 0xff000000 /* 4278190080 */
+#define FRAM_DRPFRAM_CTRL_ETHERNET_PORT_OFFSET 0x18 /* 24 */
+#define FRAM_DRPFRAM_CTRL_ETHERNET_PORT_WIDTH 0x8 /* 8 */
+#define FRAM_DRPFRAM_CTRL_ETHERNET_PORT_DEFAULT 0x0 /* 0 */
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_framer_v1_0_defm
+ * with prefix defm_ @ address 0x6000
+ *------------------------------------------------------------------------------
+ */
+/* Type = rw */
+#define DEFM_RESTART_ADDR 0x6000 /* 24576 */
+#define DEFM_RESTART_MASK 0x1 /* 1 */
+#define DEFM_RESTART_OFFSET 0x0 /* 0 */
+#define DEFM_RESTART_WIDTH 0x1 /* 1 */
+#define DEFM_RESTART_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_READY_ADDR 0x6000 /* 24576 */
+#define DEFM_READY_MASK 0x2 /* 2 */
+#define DEFM_READY_OFFSET 0x1 /* 1 */
+#define DEFM_READY_WIDTH 0x1 /* 1 */
+#define DEFM_READY_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define DEFM_ERR_PACKET_FILTER_ADDR 0x6004 /* 24580 */
+#define DEFM_ERR_PACKET_FILTER_MASK 0x3 /* 3 */
+#define DEFM_ERR_PACKET_FILTER_OFFSET 0x0 /* 0 */
+#define DEFM_ERR_PACKET_FILTER_WIDTH 0x2 /* 2 */
+#define DEFM_ERR_PACKET_FILTER_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define DEFM_DATA_PKT_MESSAGE_TYPE_ADDR 0x6008 /* 24584 */
+#define DEFM_DATA_PKT_MESSAGE_TYPE_MASK 0xff /* 255 */
+#define DEFM_DATA_PKT_MESSAGE_TYPE_OFFSET 0x0 /* 0 */
+#define DEFM_DATA_PKT_MESSAGE_TYPE_WIDTH 0x8 /* 8 */
+#define DEFM_DATA_PKT_MESSAGE_TYPE_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define DEFM_CTRL_PKT_MESSAGE_TYPE_ADDR 0x600c /* 24588 */
+#define DEFM_CTRL_PKT_MESSAGE_TYPE_MASK 0xff /* 255 */
+#define DEFM_CTRL_PKT_MESSAGE_TYPE_OFFSET 0x0 /* 0 */
+#define DEFM_CTRL_PKT_MESSAGE_TYPE_WIDTH 0x8 /* 8 */
+#define DEFM_CTRL_PKT_MESSAGE_TYPE_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define DEFM_SN_DATA_LOW_CNT_MIN_ADDR 0x6020 /* 24608 */
+#define DEFM_SN_DATA_LOW_CNT_MIN_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_DATA_LOW_CNT_MIN_OFFSET 0x0 /* 0 */
+#define DEFM_SN_DATA_LOW_CNT_MIN_WIDTH 0x20 /* 32 */
+#define DEFM_SN_DATA_LOW_CNT_MIN_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define DEFM_SN_DATA_LOW_CNT_MAX_ADDR 0x6024 /* 24612 */
+#define DEFM_SN_DATA_LOW_CNT_MAX_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_DATA_LOW_CNT_MAX_OFFSET 0x0 /* 0 */
+#define DEFM_SN_DATA_LOW_CNT_MAX_WIDTH 0x20 /* 32 */
+#define DEFM_SN_DATA_LOW_CNT_MAX_DEFAULT 0x78 /* 120 */
+
+/* Type = rw */
+#define DEFM_SN_DATA_LOW_CNT_INCVAL_ADDR 0x602c /* 24620 */
+#define DEFM_SN_DATA_LOW_CNT_INCVAL_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_DATA_LOW_CNT_INCVAL_OFFSET 0x0 /* 0 */
+#define DEFM_SN_DATA_LOW_CNT_INCVAL_WIDTH 0x20 /* 32 */
+#define DEFM_SN_DATA_LOW_CNT_INCVAL_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define DEFM_SN_DATA_HIGH_CNT_MIN_ADDR 0x6030 /* 24624 */
+#define DEFM_SN_DATA_HIGH_CNT_MIN_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_DATA_HIGH_CNT_MIN_OFFSET 0x0 /* 0 */
+#define DEFM_SN_DATA_HIGH_CNT_MIN_WIDTH 0x20 /* 32 */
+#define DEFM_SN_DATA_HIGH_CNT_MIN_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define DEFM_SN_DATA_HIGH_CNT_MAX_ADDR 0x6034 /* 24628 */
+#define DEFM_SN_DATA_HIGH_CNT_MAX_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_DATA_HIGH_CNT_MAX_OFFSET 0x0 /* 0 */
+#define DEFM_SN_DATA_HIGH_CNT_MAX_WIDTH 0x20 /* 32 */
+#define DEFM_SN_DATA_HIGH_CNT_MAX_DEFAULT 0x4f /* 79 */
+
+/* Type = rw */
+#define DEFM_SN_DATA_HIGH_CNT_INCVAL_ADDR 0x603c /* 24636 */
+#define DEFM_SN_DATA_HIGH_CNT_INCVAL_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_DATA_HIGH_CNT_INCVAL_OFFSET 0x0 /* 0 */
+#define DEFM_SN_DATA_HIGH_CNT_INCVAL_WIDTH 0x20 /* 32 */
+#define DEFM_SN_DATA_HIGH_CNT_INCVAL_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define DEFM_SN_CTRL_LOW_CNT_MIN_ADDR 0x6050 /* 24656 */
+#define DEFM_SN_CTRL_LOW_CNT_MIN_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_CTRL_LOW_CNT_MIN_OFFSET 0x0 /* 0 */
+#define DEFM_SN_CTRL_LOW_CNT_MIN_WIDTH 0x20 /* 32 */
+#define DEFM_SN_CTRL_LOW_CNT_MIN_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define DEFM_SN_CTRL_LOW_CNT_MAX_ADDR 0x6054 /* 24660 */
+#define DEFM_SN_CTRL_LOW_CNT_MAX_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_CTRL_LOW_CNT_MAX_OFFSET 0x0 /* 0 */
+#define DEFM_SN_CTRL_LOW_CNT_MAX_WIDTH 0x20 /* 32 */
+#define DEFM_SN_CTRL_LOW_CNT_MAX_DEFAULT 0x78 /* 120 */
+
+/* Type = rw */
+#define DEFM_SN_CTRL_LOW_CNT_INCVAL_ADDR 0x605c /* 24668 */
+#define DEFM_SN_CTRL_LOW_CNT_INCVAL_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_CTRL_LOW_CNT_INCVAL_OFFSET 0x0 /* 0 */
+#define DEFM_SN_CTRL_LOW_CNT_INCVAL_WIDTH 0x20 /* 32 */
+#define DEFM_SN_CTRL_LOW_CNT_INCVAL_DEFAULT 0x1 /* 1 */
+
+/* Type = rw */
+#define DEFM_SN_CTRL_HIGH_CNT_MIN_ADDR 0x6060 /* 24672 */
+#define DEFM_SN_CTRL_HIGH_CNT_MIN_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_CTRL_HIGH_CNT_MIN_OFFSET 0x0 /* 0 */
+#define DEFM_SN_CTRL_HIGH_CNT_MIN_WIDTH 0x20 /* 32 */
+#define DEFM_SN_CTRL_HIGH_CNT_MIN_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define DEFM_SN_CTRL_HIGH_CNT_MAX_ADDR 0x6064 /* 24676 */
+#define DEFM_SN_CTRL_HIGH_CNT_MAX_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_CTRL_HIGH_CNT_MAX_OFFSET 0x0 /* 0 */
+#define DEFM_SN_CTRL_HIGH_CNT_MAX_WIDTH 0x20 /* 32 */
+#define DEFM_SN_CTRL_HIGH_CNT_MAX_DEFAULT 0x4f /* 79 */
+
+/* Type = rw */
+#define DEFM_SN_CTRL_HIGH_CNT_INCVAL_ADDR 0x606c /* 24684 */
+#define DEFM_SN_CTRL_HIGH_CNT_INCVAL_MASK 0xffffffff /* 4294967295 */
+#define DEFM_SN_CTRL_HIGH_CNT_INCVAL_OFFSET 0x0 /* 0 */
+#define DEFM_SN_CTRL_HIGH_CNT_INCVAL_WIDTH 0x20 /* 32 */
+#define DEFM_SN_CTRL_HIGH_CNT_INCVAL_DEFAULT 0x1 /* 1 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W0_31_0_ADDR 0x6100 /* 24832 */
+#define DEFM_USER_DATA_FILTER_W0_31_0_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W0_31_0_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W0_31_0_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W0_31_0_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W0_63_32_ADDR 0x6104 /* 24836 */
+#define DEFM_USER_DATA_FILTER_W0_63_32_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W0_63_32_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W0_63_32_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W0_63_32_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W0_95_64_ADDR 0x6108 /* 24840 */
+#define DEFM_USER_DATA_FILTER_W0_95_64_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W0_95_64_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W0_95_64_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W0_95_64_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W0_127_96_ADDR 0x610c /* 24844 */
+#define DEFM_USER_DATA_FILTER_W0_127_96_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W0_127_96_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W0_127_96_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W0_127_96_DEFAULT 0xfffffeae /* 4294966958 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W0_MASK_ADDR 0x6110 /* 24848 */
+#define DEFM_USER_DATA_FILTER_W0_MASK_MASK 0xffff /* 65535 */
+#define DEFM_USER_DATA_FILTER_W0_MASK_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W0_MASK_WIDTH 0x10 /* 16 */
+#define DEFM_USER_DATA_FILTER_W0_MASK_DEFAULT 0xcfff /* 53247 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W1_31_0_ADDR 0x6120 /* 24864 */
+#define DEFM_USER_DATA_FILTER_W1_31_0_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W1_31_0_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W1_31_0_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W1_31_0_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W1_63_32_ADDR 0x6124 /* 24868 */
+#define DEFM_USER_DATA_FILTER_W1_63_32_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W1_63_32_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W1_63_32_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W1_63_32_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W1_95_64_ADDR 0x6128 /* 24872 */
+#define DEFM_USER_DATA_FILTER_W1_95_64_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W1_95_64_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W1_95_64_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W1_95_64_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W1_127_96_ADDR 0x612c /* 24876 */
+#define DEFM_USER_DATA_FILTER_W1_127_96_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W1_127_96_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W1_127_96_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W1_127_96_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W1_MASK_ADDR 0x6130 /* 24880 */
+#define DEFM_USER_DATA_FILTER_W1_MASK_MASK 0xffff /* 65535 */
+#define DEFM_USER_DATA_FILTER_W1_MASK_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W1_MASK_WIDTH 0x10 /* 16 */
+#define DEFM_USER_DATA_FILTER_W1_MASK_DEFAULT 0xffff /* 65535 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W2_31_0_ADDR 0x6140 /* 24896 */
+#define DEFM_USER_DATA_FILTER_W2_31_0_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W2_31_0_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W2_31_0_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W2_31_0_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W2_63_32_ADDR 0x6144 /* 24900 */
+#define DEFM_USER_DATA_FILTER_W2_63_32_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W2_63_32_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W2_63_32_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W2_63_32_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W2_95_64_ADDR 0x6148 /* 24904 */
+#define DEFM_USER_DATA_FILTER_W2_95_64_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W2_95_64_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W2_95_64_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W2_95_64_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W2_127_96_ADDR 0x614c /* 24908 */
+#define DEFM_USER_DATA_FILTER_W2_127_96_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W2_127_96_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W2_127_96_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W2_127_96_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W2_MASK_ADDR 0x6150 /* 24912 */
+#define DEFM_USER_DATA_FILTER_W2_MASK_MASK 0xffff /* 65535 */
+#define DEFM_USER_DATA_FILTER_W2_MASK_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W2_MASK_WIDTH 0x10 /* 16 */
+#define DEFM_USER_DATA_FILTER_W2_MASK_DEFAULT 0xffff /* 65535 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W3_31_0_ADDR 0x6160 /* 24928 */
+#define DEFM_USER_DATA_FILTER_W3_31_0_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W3_31_0_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W3_31_0_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W3_31_0_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W3_63_32_ADDR 0x6164 /* 24932 */
+#define DEFM_USER_DATA_FILTER_W3_63_32_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W3_63_32_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W3_63_32_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W3_63_32_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W3_95_64_ADDR 0x6168 /* 24936 */
+#define DEFM_USER_DATA_FILTER_W3_95_64_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W3_95_64_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W3_95_64_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W3_95_64_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W3_127_96_ADDR 0x616c /* 24940 */
+#define DEFM_USER_DATA_FILTER_W3_127_96_MASK 0xffffffff /* 4294967295 */
+#define DEFM_USER_DATA_FILTER_W3_127_96_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W3_127_96_WIDTH 0x20 /* 32 */
+#define DEFM_USER_DATA_FILTER_W3_127_96_DEFAULT 0xffffffff /* 4294967295 */
+
+/* Type = rwpdef */
+#define DEFM_USER_DATA_FILTER_W3_MASK_ADDR 0x6170 /* 24944 */
+#define DEFM_USER_DATA_FILTER_W3_MASK_MASK 0xffff /* 65535 */
+#define DEFM_USER_DATA_FILTER_W3_MASK_OFFSET 0x0 /* 0 */
+#define DEFM_USER_DATA_FILTER_W3_MASK_WIDTH 0x10 /* 16 */
+#define DEFM_USER_DATA_FILTER_W3_MASK_DEFAULT 0xffff /* 65535 */
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_framer_v1_0_defm_drp
+ * with prefix defm_drp @ address 0x8000
+ *------------------------------------------------------------------------------
+ */
+/* Type = rw */
+#define DEFM_DRPDEFM_DATA_PC_ID_ADDR 0x8000 /* 32768 */
+#define DEFM_DRPDEFM_DATA_PC_ID_MASK 0xffff /* 65535 */
+#define DEFM_DRPDEFM_DATA_PC_ID_OFFSET 0x0 /* 0 */
+#define DEFM_DRPDEFM_DATA_PC_ID_WIDTH 0x10 /* 16 */
+#define DEFM_DRPDEFM_DATA_PC_ID_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define DEFM_DRPDEFM_CTRL_PC_ID_ADDR 0x8400 /* 33792 */
+#define DEFM_DRPDEFM_CTRL_PC_ID_MASK 0xffff /* 65535 */
+#define DEFM_DRPDEFM_CTRL_PC_ID_OFFSET 0x0 /* 0 */
+#define DEFM_DRPDEFM_CTRL_PC_ID_WIDTH 0x10 /* 16 */
+#define DEFM_DRPDEFM_CTRL_PC_ID_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_LATENCY_ADDR 0x8800 /* 34816 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_LATENCY_MASK 0xffffff /* 16777215 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_LATENCY_OFFSET 0x0 /* 0 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_LATENCY_WIDTH 0x18 /* 24 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_LATENCY_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_ALIGNMENT_ADDR 0x8800 /* 34816 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_ALIGNMENT_MASK 0x1000000 /* 16777216 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_ALIGNMENT_OFFSET 0x18 /* 24 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_ALIGNMENT_WIDTH 0x1 /* 1 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_ALIGNMENT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_OVERFLOW_ADDR 0x8800 /* 34816 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_OVERFLOW_MASK 0x2000000 /* 33554432 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_OVERFLOW_OFFSET 0x19 /* 25 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_OVERFLOW_WIDTH 0x1 /* 1 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_OVERFLOW_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_UNDERFLOW_ADDR 0x8800 /* 34816 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_UNDERFLOW_MASK 0x4000000 /* 67108864 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_UNDERFLOW_OFFSET 0x1a /* 26 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_UNDERFLOW_WIDTH 0x1 /* 1 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_UNDERFLOW_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_REGULAR_ADDR 0x8800 /* 34816 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_REGULAR_MASK 0x8000000 /* 134217728 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_REGULAR_OFFSET 0x1b /* 27 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_REGULAR_WIDTH 0x1 /* 1 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_REGULAR_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_RWIN_ADDR 0x8800 /* 34816 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_RWIN_MASK 0xf0000000 /* 4026531840 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_RWIN_OFFSET 0x1c /* 28 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_RWIN_WIDTH 0x4 /* 4 */
+#define DEFM_DRPDEFM_DATA_BUFFER_STATE_RWIN_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_LATENCY_ADDR 0x9800 /* 38912 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_LATENCY_MASK 0xffffff /* 16777215 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_LATENCY_OFFSET 0x0 /* 0 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_LATENCY_WIDTH 0x18 /* 24 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_LATENCY_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_ALIGNMENT_ADDR 0x9800 /* 38912 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_ALIGNMENT_MASK 0x1000000 /* 16777216 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_ALIGNMENT_OFFSET 0x18 /* 24 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_ALIGNMENT_WIDTH 0x1 /* 1 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_ALIGNMENT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_OVERFLOW_ADDR 0x9800 /* 38912 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_OVERFLOW_MASK 0x2000000 /* 33554432 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_OVERFLOW_OFFSET 0x19 /* 25 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_OVERFLOW_WIDTH 0x1 /* 1 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_OVERFLOW_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_UNDERFLOW_ADDR 0x9800 /* 38912 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_UNDERFLOW_MASK 0x4000000 /* 67108864 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_UNDERFLOW_OFFSET 0x1a /* 26 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_UNDERFLOW_WIDTH 0x1 /* 1 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_UNDERFLOW_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_REGULAR_ADDR 0x9800 /* 38912 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_REGULAR_MASK 0x8000000 /* 134217728 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_REGULAR_OFFSET 0x1b /* 27 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_REGULAR_WIDTH 0x1 /* 1 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_REGULAR_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_RWIN_ADDR 0x9800 /* 38912 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_RWIN_MASK 0xf0000000 /* 4026531840 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_RWIN_OFFSET 0x1c /* 28 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_RWIN_WIDTH 0x4 /* 4 */
+#define DEFM_DRPDEFM_CTRL_BUFFER_STATE_RWIN_DEFAULT 0x0 /* 0 */
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_framer_v1_0_eth
+ * with prefix eth_ @ address 0xa000
+ *------------------------------------------------------------------------------
+ */
+/* Type = rwpdef */
+#define ETH_DEST_ADDR_31_0_ADDR 0xa000 /* 40960 */
+#define ETH_DEST_ADDR_31_0_MASK 0xffffffff /* 4294967295 */
+#define ETH_DEST_ADDR_31_0_OFFSET 0x0 /* 0 */
+#define ETH_DEST_ADDR_31_0_WIDTH 0x20 /* 32 */
+#define ETH_DEST_ADDR_31_0_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_DEST_ADDR_47_32_ADDR 0xa004 /* 40964 */
+#define ETH_DEST_ADDR_47_32_MASK 0xffff /* 65535 */
+#define ETH_DEST_ADDR_47_32_OFFSET 0x0 /* 0 */
+#define ETH_DEST_ADDR_47_32_WIDTH 0x10 /* 16 */
+#define ETH_DEST_ADDR_47_32_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_SRC_ADDR_31_0_ADDR 0xa008 /* 40968 */
+#define ETH_SRC_ADDR_31_0_MASK 0xffffffff /* 4294967295 */
+#define ETH_SRC_ADDR_31_0_OFFSET 0x0 /* 0 */
+#define ETH_SRC_ADDR_31_0_WIDTH 0x20 /* 32 */
+#define ETH_SRC_ADDR_31_0_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_SRC_ADDR_47_32_ADDR 0xa00c /* 40972 */
+#define ETH_SRC_ADDR_47_32_MASK 0xffff /* 65535 */
+#define ETH_SRC_ADDR_47_32_OFFSET 0x0 /* 0 */
+#define ETH_SRC_ADDR_47_32_WIDTH 0x10 /* 16 */
+#define ETH_SRC_ADDR_47_32_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_VLAN_ID_ADDR 0xa010 /* 40976 */
+#define ETH_VLAN_ID_MASK 0xfff /* 4095 */
+#define ETH_VLAN_ID_OFFSET 0x0 /* 0 */
+#define ETH_VLAN_ID_WIDTH 0xc /* 12 */
+#define ETH_VLAN_ID_DEFAULT 0x1 /* 1 */
+
+/* Type = rwpdef */
+#define ETH_VLAN_DEI_ADDR 0xa010 /* 40976 */
+#define ETH_VLAN_DEI_MASK 0x1000 /* 4096 */
+#define ETH_VLAN_DEI_OFFSET 0xc /* 12 */
+#define ETH_VLAN_DEI_WIDTH 0x1 /* 1 */
+#define ETH_VLAN_DEI_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_VLAN_PCP_ADDR 0xa010 /* 40976 */
+#define ETH_VLAN_PCP_MASK 0xe000 /* 57344 */
+#define ETH_VLAN_PCP_OFFSET 0xd /* 13 */
+#define ETH_VLAN_PCP_WIDTH 0x3 /* 3 */
+#define ETH_VLAN_PCP_DEFAULT 0x7 /* 7 */
+
+/* Type = rw */
+#define ETH_IPV4_VERSION_ADDR 0xa030 /* 41008 */
+#define ETH_IPV4_VERSION_MASK 0xf /* 15 */
+#define ETH_IPV4_VERSION_OFFSET 0x0 /* 0 */
+#define ETH_IPV4_VERSION_WIDTH 0x4 /* 4 */
+#define ETH_IPV4_VERSION_DEFAULT 0x4 /* 4 */
+
+/* Type = rw */
+#define ETH_IPV4_IHL_ADDR 0xa030 /* 41008 */
+#define ETH_IPV4_IHL_MASK 0xf0 /* 240 */
+#define ETH_IPV4_IHL_OFFSET 0x4 /* 4 */
+#define ETH_IPV4_IHL_WIDTH 0x4 /* 4 */
+#define ETH_IPV4_IHL_DEFAULT 0x5 /* 5 */
+
+/* Type = rw */
+#define ETH_IPV4_DSCP_ADDR 0xa034 /* 41012 */
+#define ETH_IPV4_DSCP_MASK 0x3f /* 63 */
+#define ETH_IPV4_DSCP_OFFSET 0x0 /* 0 */
+#define ETH_IPV4_DSCP_WIDTH 0x6 /* 6 */
+#define ETH_IPV4_DSCP_DEFAULT 0x2e /* 46 */
+
+/* Type = rw */
+#define ETH_IPV4_ECN_ADDR 0xa034 /* 41012 */
+#define ETH_IPV4_ECN_MASK 0xc0 /* 192 */
+#define ETH_IPV4_ECN_OFFSET 0x6 /* 6 */
+#define ETH_IPV4_ECN_WIDTH 0x2 /* 2 */
+#define ETH_IPV4_ECN_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define ETH_IPV4_ID_ADDR 0xa038 /* 41016 */
+#define ETH_IPV4_ID_MASK 0xffff /* 65535 */
+#define ETH_IPV4_ID_OFFSET 0x0 /* 0 */
+#define ETH_IPV4_ID_WIDTH 0x10 /* 16 */
+#define ETH_IPV4_ID_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define ETH_IPV4_FLAGS_ADDR 0xa03c /* 41020 */
+#define ETH_IPV4_FLAGS_MASK 0x7 /* 7 */
+#define ETH_IPV4_FLAGS_OFFSET 0x0 /* 0 */
+#define ETH_IPV4_FLAGS_WIDTH 0x3 /* 3 */
+#define ETH_IPV4_FLAGS_DEFAULT 0x2 /* 2 */
+
+/* Type = rw */
+#define ETH_IPV4_FRAGMENT_OFFSET_ADDR 0xa03c /* 41020 */
+#define ETH_IPV4_FRAGMENT_OFFSET_MASK 0x1fff8 /* 131064 */
+#define ETH_IPV4_FRAGMENT_OFFSET_OFFSET 0x3 /* 3 */
+#define ETH_IPV4_FRAGMENT_OFFSET_WIDTH 0xe /* 14 */
+#define ETH_IPV4_FRAGMENT_OFFSET_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define ETH_IPV4_TIME_TO_LIVE_ADDR 0xa040 /* 41024 */
+#define ETH_IPV4_TIME_TO_LIVE_MASK 0xff /* 255 */
+#define ETH_IPV4_TIME_TO_LIVE_OFFSET 0x0 /* 0 */
+#define ETH_IPV4_TIME_TO_LIVE_WIDTH 0x8 /* 8 */
+#define ETH_IPV4_TIME_TO_LIVE_DEFAULT 0x40 /* 64 */
+
+/* Type = rw */
+#define ETH_IPV4_PROTOCOL_ADDR 0xa044 /* 41028 */
+#define ETH_IPV4_PROTOCOL_MASK 0xff /* 255 */
+#define ETH_IPV4_PROTOCOL_OFFSET 0x0 /* 0 */
+#define ETH_IPV4_PROTOCOL_WIDTH 0x8 /* 8 */
+#define ETH_IPV4_PROTOCOL_DEFAULT 0x11 /* 17 */
+
+/* Type = rwpdef */
+#define ETH_IPV4_SOURCE_ADD_ADDR 0xa048 /* 41032 */
+#define ETH_IPV4_SOURCE_ADD_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV4_SOURCE_ADD_OFFSET 0x0 /* 0 */
+#define ETH_IPV4_SOURCE_ADD_WIDTH 0x20 /* 32 */
+#define ETH_IPV4_SOURCE_ADD_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_IPV4_DESTINATION_ADD_ADDR 0xa04c /* 41036 */
+#define ETH_IPV4_DESTINATION_ADD_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV4_DESTINATION_ADD_OFFSET 0x0 /* 0 */
+#define ETH_IPV4_DESTINATION_ADD_WIDTH 0x20 /* 32 */
+#define ETH_IPV4_DESTINATION_ADD_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define ETH_UDP_SOURCE_PORT_ADDR 0xa050 /* 41040 */
+#define ETH_UDP_SOURCE_PORT_MASK 0xffff /* 65535 */
+#define ETH_UDP_SOURCE_PORT_OFFSET 0x0 /* 0 */
+#define ETH_UDP_SOURCE_PORT_WIDTH 0x10 /* 16 */
+#define ETH_UDP_SOURCE_PORT_DEFAULT 0x8000 /* 32768 */
+
+/* Type = rw */
+#define ETH_UDP_DESTINATION_PORT_ADDR 0xa050 /* 41040 */
+#define ETH_UDP_DESTINATION_PORT_MASK 0xffff0000 /* 4294901760 */
+#define ETH_UDP_DESTINATION_PORT_OFFSET 0x10 /* 16 */
+#define ETH_UDP_DESTINATION_PORT_WIDTH 0x10 /* 16 */
+#define ETH_UDP_DESTINATION_PORT_DEFAULT 0xc000 /* 49152 */
+
+/* Type = rw */
+#define ETH_IPV6_V_ADDR 0xa080 /* 41088 */
+#define ETH_IPV6_V_MASK 0xf /* 15 */
+#define ETH_IPV6_V_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_V_WIDTH 0x4 /* 4 */
+#define ETH_IPV6_V_DEFAULT 0x6 /* 6 */
+
+/* Type = rw */
+#define ETH_IPV6_TRAFFIC_CLASS_ADDR 0xa084 /* 41092 */
+#define ETH_IPV6_TRAFFIC_CLASS_MASK 0xff /* 255 */
+#define ETH_IPV6_TRAFFIC_CLASS_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_TRAFFIC_CLASS_WIDTH 0x8 /* 8 */
+#define ETH_IPV6_TRAFFIC_CLASS_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define ETH_IPV6_FLOW_LABEL_ADDR 0xa088 /* 41096 */
+#define ETH_IPV6_FLOW_LABEL_MASK 0xfffff /* 1048575 */
+#define ETH_IPV6_FLOW_LABEL_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_FLOW_LABEL_WIDTH 0x14 /* 20 */
+#define ETH_IPV6_FLOW_LABEL_DEFAULT 0x0 /* 0 */
+
+/* Type = rw */
+#define ETH_IPV6_NEXT_HEADER_ADDR 0xa08c /* 41100 */
+#define ETH_IPV6_NEXT_HEADER_MASK 0xff /* 255 */
+#define ETH_IPV6_NEXT_HEADER_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_NEXT_HEADER_WIDTH 0x8 /* 8 */
+#define ETH_IPV6_NEXT_HEADER_DEFAULT 0x11 /* 17 */
+
+/* Type = rw */
+#define ETH_IPV6_HOP_LIMIT_ADDR 0xa090 /* 41104 */
+#define ETH_IPV6_HOP_LIMIT_MASK 0xff /* 255 */
+#define ETH_IPV6_HOP_LIMIT_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_HOP_LIMIT_WIDTH 0x8 /* 8 */
+#define ETH_IPV6_HOP_LIMIT_DEFAULT 0x40 /* 64 */
+
+/* Type = rwpdef */
+#define ETH_IPV6_SOURCE_ADD_31_0_ADDR 0xa094 /* 41108 */
+#define ETH_IPV6_SOURCE_ADD_31_0_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV6_SOURCE_ADD_31_0_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_SOURCE_ADD_31_0_WIDTH 0x20 /* 32 */
+#define ETH_IPV6_SOURCE_ADD_31_0_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_IPV6_SOURCE_ADD_63_32_ADDR 0xa098 /* 41112 */
+#define ETH_IPV6_SOURCE_ADD_63_32_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV6_SOURCE_ADD_63_32_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_SOURCE_ADD_63_32_WIDTH 0x20 /* 32 */
+#define ETH_IPV6_SOURCE_ADD_63_32_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_IPV6_SOURCE_ADD_95_64_ADDR 0xa09c /* 41116 */
+#define ETH_IPV6_SOURCE_ADD_95_64_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV6_SOURCE_ADD_95_64_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_SOURCE_ADD_95_64_WIDTH 0x20 /* 32 */
+#define ETH_IPV6_SOURCE_ADD_95_64_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_IPV6_SOURCE_ADD_127_96_ADDR 0xa0a0 /* 41120 */
+#define ETH_IPV6_SOURCE_ADD_127_96_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV6_SOURCE_ADD_127_96_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_SOURCE_ADD_127_96_WIDTH 0x20 /* 32 */
+#define ETH_IPV6_SOURCE_ADD_127_96_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_IPV6_DEST_ADD_31_0_ADDR 0xa0a4 /* 41124 */
+#define ETH_IPV6_DEST_ADD_31_0_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV6_DEST_ADD_31_0_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_DEST_ADD_31_0_WIDTH 0x20 /* 32 */
+#define ETH_IPV6_DEST_ADD_31_0_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_IPV6_DEST_ADD_63_32_ADDR 0xa0a8 /* 41128 */
+#define ETH_IPV6_DEST_ADD_63_32_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV6_DEST_ADD_63_32_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_DEST_ADD_63_32_WIDTH 0x20 /* 32 */
+#define ETH_IPV6_DEST_ADD_63_32_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_IPV6_DEST_ADD_95_64_ADDR 0xa0ac /* 41132 */
+#define ETH_IPV6_DEST_ADD_95_64_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV6_DEST_ADD_95_64_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_DEST_ADD_95_64_WIDTH 0x20 /* 32 */
+#define ETH_IPV6_DEST_ADD_95_64_DEFAULT 0x0 /* 0 */
+
+/* Type = rwpdef */
+#define ETH_IPV6_DEST_ADD_127_96_ADDR 0xa0b0 /* 41136 */
+#define ETH_IPV6_DEST_ADD_127_96_MASK 0xffffffff /* 4294967295 */
+#define ETH_IPV6_DEST_ADD_127_96_OFFSET 0x0 /* 0 */
+#define ETH_IPV6_DEST_ADD_127_96_WIDTH 0x20 /* 32 */
+#define ETH_IPV6_DEST_ADD_127_96_DEFAULT 0x0 /* 0 */
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_framer_v1_0_stats
+ * with prefix stats_ @ address 0xc000
+ *------------------------------------------------------------------------------
+ */
+/* Type = roSig */
+#define STATS_TOTAL_RX_GOOD_PKT_CNT_ADDR 0xc000 /* 49152 */
+#define STATS_TOTAL_RX_GOOD_PKT_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_TOTAL_RX_GOOD_PKT_CNT_OFFSET 0x0 /* 0 */
+#define STATS_TOTAL_RX_GOOD_PKT_CNT_WIDTH 0x20 /* 32 */
+#define STATS_TOTAL_RX_GOOD_PKT_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_TOTAL_RX_BAD_PKT_CNT_ADDR 0xc004 /* 49156 */
+#define STATS_TOTAL_RX_BAD_PKT_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_TOTAL_RX_BAD_PKT_CNT_OFFSET 0x0 /* 0 */
+#define STATS_TOTAL_RX_BAD_PKT_CNT_WIDTH 0x20 /* 32 */
+#define STATS_TOTAL_RX_BAD_PKT_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_TOTAL_RX_BAD_FCS_CNT_ADDR 0xc008 /* 49160 */
+#define STATS_TOTAL_RX_BAD_FCS_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_TOTAL_RX_BAD_FCS_CNT_OFFSET 0x0 /* 0 */
+#define STATS_TOTAL_RX_BAD_FCS_CNT_WIDTH 0x20 /* 32 */
+#define STATS_TOTAL_RX_BAD_FCS_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_DATA_RX_PACKETS_CNT_ADDR 0xc00c /* 49164 */
+#define STATS_USER_DATA_RX_PACKETS_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_DATA_RX_PACKETS_CNT_OFFSET 0x0 /* 0 */
+#define STATS_USER_DATA_RX_PACKETS_CNT_WIDTH 0x20 /* 32 */
+#define STATS_USER_DATA_RX_PACKETS_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_DATA_RX_GOOD_PKT_CNT_ADDR 0xc010 /* 49168 */
+#define STATS_USER_DATA_RX_GOOD_PKT_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_DATA_RX_GOOD_PKT_CNT_OFFSET 0x0 /* 0 */
+#define STATS_USER_DATA_RX_GOOD_PKT_CNT_WIDTH 0x20 /* 32 */
+#define STATS_USER_DATA_RX_GOOD_PKT_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_DATA_RX_BAD_PKT_CNT_ADDR 0xc014 /* 49172 */
+#define STATS_USER_DATA_RX_BAD_PKT_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_DATA_RX_BAD_PKT_CNT_OFFSET 0x0 /* 0 */
+#define STATS_USER_DATA_RX_BAD_PKT_CNT_WIDTH 0x20 /* 32 */
+#define STATS_USER_DATA_RX_BAD_PKT_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_DATA_RX_BAD_FCS_CNT_ADDR 0xc018 /* 49176 */
+#define STATS_USER_DATA_RX_BAD_FCS_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_DATA_RX_BAD_FCS_CNT_OFFSET 0x0 /* 0 */
+#define STATS_USER_DATA_RX_BAD_FCS_CNT_WIDTH 0x20 /* 32 */
+#define STATS_USER_DATA_RX_BAD_FCS_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_CTRL_RX_PACKETS_CNT_ADDR 0xc01c /* 49180 */
+#define STATS_USER_CTRL_RX_PACKETS_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_CTRL_RX_PACKETS_CNT_OFFSET 0x0 /* 0 */
+#define STATS_USER_CTRL_RX_PACKETS_CNT_WIDTH 0x20 /* 32 */
+#define STATS_USER_CTRL_RX_PACKETS_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_CTRL_RX_GOOD_PKT_CNT_ADDR 0xc020 /* 49184 */
+#define STATS_USER_CTRL_RX_GOOD_PKT_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_CTRL_RX_GOOD_PKT_CNT_OFFSET 0x0 /* 0 */
+#define STATS_USER_CTRL_RX_GOOD_PKT_CNT_WIDTH 0x20 /* 32 */
+#define STATS_USER_CTRL_RX_GOOD_PKT_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_CTRL_RX_BAD_PKT_CNT_ADDR 0xc024 /* 49188 */
+#define STATS_USER_CTRL_RX_BAD_PKT_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_CTRL_RX_BAD_PKT_CNT_OFFSET 0x0 /* 0 */
+#define STATS_USER_CTRL_RX_BAD_PKT_CNT_WIDTH 0x20 /* 32 */
+#define STATS_USER_CTRL_RX_BAD_PKT_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_CTRL_RX_BAD_FCS_CNT_ADDR 0xc028 /* 49192 */
+#define STATS_USER_CTRL_RX_BAD_FCS_CNT_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_CTRL_RX_BAD_FCS_CNT_OFFSET 0x0 /* 0 */
+#define STATS_USER_CTRL_RX_BAD_FCS_CNT_WIDTH 0x20 /* 32 */
+#define STATS_USER_CTRL_RX_BAD_FCS_CNT_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_DATA_RX_PKTS_RATE_ADDR 0xc02c /* 49196 */
+#define STATS_USER_DATA_RX_PKTS_RATE_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_DATA_RX_PKTS_RATE_OFFSET 0x0 /* 0 */
+#define STATS_USER_DATA_RX_PKTS_RATE_WIDTH 0x20 /* 32 */
+#define STATS_USER_DATA_RX_PKTS_RATE_DEFAULT 0x0 /* 0 */
+
+/* Type = roSig */
+#define STATS_USER_CTRL_RX_PKTS_RATE_ADDR 0xc030 /* 49200 */
+#define STATS_USER_CTRL_RX_PKTS_RATE_MASK 0xffffffff /* 4294967295 */
+#define STATS_USER_CTRL_RX_PKTS_RATE_OFFSET 0x0 /* 0 */
+#define STATS_USER_CTRL_RX_PKTS_RATE_WIDTH 0x20 /* 32 */
+#define STATS_USER_CTRL_RX_PKTS_RATE_DEFAULT 0x0 /* 0 */
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroeframer/sysfs_xroe.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroeframer/sysfs_xroe.c	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,562 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+#include <linux/init.h>
+#include <linux/kobject.h>
+#include <linux/module.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+#include "xroe_framer.h"
+
+enum { XROE_SIZE_MAX = 15 };
+static int xroe_size;
+static char xroe_tmp[XROE_SIZE_MAX];
+
+/**
+ * version_show - Returns the block's revision number
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the revision string
+ *
+ * Returns the block's major, minor & version revision numbers
+ * in a %d.%d.%d format
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t version_show(struct kobject *kobj, struct kobj_attribute *attr,
+			    char *buff)
+{
+	u32 major_rev;
+	u32 minor_rev;
+	u32 version_rev;
+
+	major_rev = utils_sysfs_show_wrapper(CFG_MAJOR_REVISION_ADDR,
+					     CFG_MAJOR_REVISION_OFFSET,
+					     CFG_MAJOR_REVISION_MASK, kobj);
+	minor_rev = utils_sysfs_show_wrapper(CFG_MINOR_REVISION_ADDR,
+					     CFG_MINOR_REVISION_OFFSET,
+					     CFG_MINOR_REVISION_MASK, kobj);
+	version_rev = utils_sysfs_show_wrapper(CFG_VERSION_REVISION_ADDR,
+					       CFG_VERSION_REVISION_OFFSET,
+					       CFG_VERSION_REVISION_MASK, kobj);
+	sprintf(buff, "%d.%d.%d\n", major_rev, minor_rev, version_rev);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * version_store - Writes to the framer version sysfs entry (not permitted)
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the revision string
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the framer version sysfs entry (not permitted)
+ *
+ * Return: 0
+ */
+static ssize_t version_store(struct  kobject *kobj, struct kobj_attribute *attr,
+			     const char *buff, size_t count)
+{
+	return 0;
+}
+
+/**
+ * enable_show - Returns the framer's enable status
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the enable status
+ *
+ * Reads and writes the framer's enable status to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t enable_show(struct kobject *kobj, struct kobj_attribute *attr,
+			   char *buff)
+{
+	u32 enable;
+
+	enable = utils_sysfs_show_wrapper(CFG_MASTER_INT_ENABLE_ADDR,
+					  CFG_MASTER_INT_ENABLE_OFFSET,
+					  CFG_MASTER_INT_ENABLE_MASK, kobj);
+	if (enable)
+		sprintf(buff, "true\n");
+	else
+		sprintf(buff, "false\n");
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * version_store - Writes to the framer's enable status register
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the enable status
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the framer's enable status
+ * to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t enable_store(struct kobject *kobj, struct kobj_attribute *attr,
+			    const char *buff, size_t count)
+{
+	u32 enable = 0;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+	if (strncmp(xroe_tmp, "true", xroe_size) == 0)
+		enable = 1;
+	else if (strncmp(xroe_tmp, "false", xroe_size) == 0)
+		enable = 0;
+	utils_sysfs_store_wrapper(CFG_MASTER_INT_ENABLE_ADDR,
+				  CFG_MASTER_INT_ENABLE_OFFSET,
+				  CFG_MASTER_INT_ENABLE_MASK, enable, kobj);
+	return xroe_size;
+}
+
+/**
+ * framer_restart_show - Returns the framer's restart status
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the restart status
+ *
+ * Reads and writes the framer's restart status to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t framer_restart_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buff)
+{
+	u32 restart;
+
+	restart = utils_sysfs_show_wrapper(FRAM_DISABLE_ADDR,
+					   FRAM_DISABLE_OFFSET,
+					   FRAM_DISABLE_MASK, kobj);
+	if (restart)
+		sprintf(buff, "true\n");
+
+	else
+		sprintf(buff, "false\n");
+
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * framer_restart_store - Writes to the framer's restart status register
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the restart status
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the framer's restart status
+ * to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t framer_restart_store(struct  kobject *kobj,
+				    struct kobj_attribute *attr,
+				    const char *buff, size_t count)
+{
+	u32 restart = 0;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+	if (strncmp(xroe_tmp, "true", xroe_size) == 0)
+		restart = 0x01;
+	else if (strncmp(xroe_tmp, "false", xroe_size) == 0)
+		restart = 0x00;
+	utils_sysfs_store_wrapper(FRAM_DISABLE_ADDR, FRAM_DISABLE_OFFSET,
+				  FRAM_DISABLE_MASK, restart, kobj);
+	return xroe_size;
+}
+
+/**
+ * deframer_restart_show - Returns the deframer's restart status
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the restart status
+ *
+ * Reads and writes the deframer's restart status to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t deframer_restart_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buff)
+{
+	u32 offset = DEFM_RESTART_OFFSET;
+	u32 mask = DEFM_RESTART_MASK;
+	u32 buffer = 0;
+	u32 restart = 0;
+	void __iomem *working_address = ((u8 *)xroe_lp->base_addr
+	+ DEFM_RESTART_ADDR);
+
+	buffer = ioread32(working_address);
+	restart = (buffer & mask) >> offset;
+
+	if (restart)
+		sprintf(buff, "true\n");
+
+	else
+		sprintf(buff, "false\n");
+
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * deframer_restart_store - Writes to the deframer's restart status register
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the restart status
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the deframer's restart status
+ * to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t deframer_restart_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buff, size_t count)
+{
+	u32 offset = DEFM_RESTART_OFFSET;
+	u32 mask = DEFM_RESTART_MASK;
+	void __iomem *working_address = ((u8 *)xroe_lp->base_addr
+	+ DEFM_RESTART_ADDR);
+	u32 restart = 0;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+	if (strncmp(xroe_tmp, "true", xroe_size) == 0) {
+		restart = 0x01;
+		utils_write32withmask(working_address, restart,
+				      mask, offset);
+	} else if (strncmp(xroe_tmp, "false", xroe_size) == 0) {
+		restart = 0x00;
+		utils_write32withmask(working_address, restart,
+				      mask, offset);
+	}
+
+	return xroe_size;
+}
+
+/**
+ * xxv_reset_show - Returns the XXV's reset status
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the reset status
+ *
+ * Reads and writes the XXV's reset status to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t xxv_reset_show(struct kobject *kobj, struct kobj_attribute *attr,
+			      char *buff)
+{
+	u32 offset = CFG_USER_RW_OUT_OFFSET;
+	u32 mask = CFG_USER_RW_OUT_MASK;
+	u32 buffer = 0;
+	u32 restart = 0;
+	void __iomem *working_address = ((u8 *)xroe_lp->base_addr +
+	CFG_USER_RW_OUT_ADDR);
+
+	buffer = ioread32(working_address);
+	restart = (buffer & mask) >> offset;
+	if (restart)
+		sprintf(buff, "true\n");
+	else
+		sprintf(buff, "false\n");
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * xxv_reset_store - Writes to the XXV's reset register
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the reset status
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the XXV's reset status
+ * to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t xxv_reset_store(struct  kobject *kobj,
+			       struct kobj_attribute *attr,
+			       const char *buff, size_t count)
+{
+	u32 offset = CFG_USER_RW_OUT_OFFSET;
+	u32 mask = CFG_USER_RW_OUT_MASK;
+	void __iomem *working_address = ((u8 *)xroe_lp->base_addr +
+	CFG_USER_RW_OUT_ADDR);
+	u32 restart = 0;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+
+	if (strncmp(xroe_tmp, "true", xroe_size) == 0) {
+		restart = 0x01;
+		utils_write32withmask(working_address, restart,
+				      mask, offset);
+	} else if (strncmp(xroe_tmp, "false", xroe_size) == 0) {
+		restart = 0x00;
+		utils_write32withmask(working_address, restart,
+				      mask, offset);
+	}
+	return xroe_size;
+}
+
+/**
+ * framing_show - Returns the current framing
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the reset status
+ *
+ * Reads and writes the current framing type to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t framing_show(struct kobject *kobj, struct kobj_attribute *attr,
+			    char *buff)
+{
+	u32 offset = (DEFM_DATA_PKT_MESSAGE_TYPE_ADDR +
+	DEFM_DATA_PKT_MESSAGE_TYPE_OFFSET);
+	u8 buffer = 0;
+	u8 framing = 0xff;
+	void __iomem *working_address = ((u8 *)xroe_lp->base_addr + offset);
+
+	buffer = ioread8(working_address);
+	framing = buffer;
+	if (framing == 0)
+		sprintf(buff, "eCPRI\n");
+	else if (framing == 1)
+		sprintf(buff, "1914.3\n");
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * framing_store - Writes to the current framing register
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the reset status
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the current framing
+ * to the sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t framing_store(struct kobject *kobj, struct kobj_attribute *attr,
+			     const char *buff, size_t count)
+{
+	u32 offset = (DEFM_DATA_PKT_MESSAGE_TYPE_ADDR +
+	DEFM_DATA_PKT_MESSAGE_TYPE_OFFSET);
+	void __iomem *working_address = ((u8 *)xroe_lp->base_addr + offset);
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+	if (strncmp(xroe_tmp, "eCPRI", xroe_size) == 0)
+		iowrite8(0, working_address);
+	else if (strncmp(xroe_tmp, "1914.3", xroe_size) == 0)
+		iowrite8(1, working_address);
+	return xroe_size;
+}
+
+/* TODO Use DEVICE_ATTR/_RW/_RO macros */
+
+static struct kobj_attribute version_attribute =
+	__ATTR(version, 0444, version_show, version_store);
+
+static struct kobj_attribute enable_attribute =
+	__ATTR(enable, 0660, enable_show, enable_store);
+
+static struct kobj_attribute framer_restart =
+	__ATTR(framer_restart, 0660, framer_restart_show, framer_restart_store);
+
+static struct kobj_attribute deframer_restart =
+	__ATTR(deframer_restart, 0660, deframer_restart_show,
+	       deframer_restart_store);
+
+static struct kobj_attribute xxv_reset =
+	__ATTR(xxv_reset, 0660, xxv_reset_show, xxv_reset_store);
+
+static struct kobj_attribute framing_attribute =
+	__ATTR(framing, 0660, framing_show, framing_store);
+
+static struct attribute *attrs[] = {
+	&version_attribute.attr,
+	&enable_attribute.attr,
+	&framer_restart.attr,
+	&deframer_restart.attr,
+	&xxv_reset.attr,
+	&framing_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group attr_group = {
+	.attrs = attrs,
+};
+
+struct kobject *root_xroe_kobj;
+
+/**
+ * xroe_sysfs_init - Creates the xroe sysfs directory and entries
+ *
+ * Return: 0 on success, negative value in case of failure to
+ * create the sysfs group
+ *
+ * Creates the xroe sysfs directory and entries, as well as the
+ * subdirectories for IPv4, IPv6 & UDP
+ */
+int xroe_sysfs_init(void)
+{
+	int ret;
+
+	root_xroe_kobj = kobject_create_and_add("xroe", kernel_kobj);
+	if (!root_xroe_kobj)
+		return -ENOMEM;
+	ret = sysfs_create_group(root_xroe_kobj, &attr_group);
+	if (ret)
+		kobject_put(root_xroe_kobj);
+	ret = xroe_sysfs_ipv4_init();
+	if (ret)
+		return ret;
+	ret = xroe_sysfs_ipv6_init();
+	if (ret)
+		return ret;
+	ret = xroe_sysfs_udp_init();
+	if (ret)
+		return ret;
+	ret = xroe_sysfs_stats_init();
+	return ret;
+}
+
+/**
+ * xroe_sysfs_exit - Deletes the xroe sysfs directory and entries
+ *
+ * Deletes the xroe sysfs directory and entries, as well as the
+ * subdirectories for IPv4, IPv6 & UDP
+ *
+ */
+void xroe_sysfs_exit(void)
+{
+	int i;
+
+	xroe_sysfs_ipv4_exit();
+	xroe_sysfs_ipv6_exit();
+	xroe_sysfs_udp_exit();
+	xroe_sysfs_stats_exit();
+	for (i = 0; i < MAX_NUM_ETH_PORTS; i++)
+		kobject_put(kobj_eth_ports[i]);
+	kobject_put(kobj_framer);
+	kobject_put(root_xroe_kobj);
+}
+
+/**
+ * utils_write32withmask - Writes a masked 32-bit value
+ * @working_address:	The starting address to write
+ * @value:			The value to be written
+ * @mask:			The mask to be used
+ * @offset:			The offset from the provided starting address
+ *
+ * Writes a 32-bit value to the provided address with the input mask
+ *
+ * Return: 0 on success
+ */
+int utils_write32withmask(void __iomem *working_address, u32 value,
+			  u32 mask, u32 offset)
+{
+	u32 read_register_value = 0;
+	u32 register_value_to_write = 0;
+	u32 delta = 0, buffer = 0;
+
+	read_register_value = ioread32(working_address);
+	buffer = (value << offset);
+	register_value_to_write = read_register_value & ~mask;
+	delta = buffer & mask;
+	register_value_to_write |= delta;
+	iowrite32(register_value_to_write, working_address);
+	return 0;
+}
+
+/**
+ * utils_sysfs_path_to_eth_port_num - Get the current ethernet port
+ * @kobj:	The kobject of the entry calling the function
+ *
+ * Extracts the number of the current ethernet port instance
+ *
+ * Return: The number of the ethernet port instance (0 - MAX_NUM_ETH_PORTS) on
+ * success, -1 otherwise
+ */
+static int utils_sysfs_path_to_eth_port_num(struct kobject *kobj)
+{
+	char *current_path = NULL;
+	int port;
+	int ret;
+
+	current_path = kobject_get_path(kobj, GFP_KERNEL);
+	ret = sscanf(current_path, "/kernel/xroe/framer/eth_port_%d/", &port);
+	/* if sscanf() returns 0, no fields were assigned, therefore no
+	 * adjustments will be made for port number
+	 */
+	if (ret == 0)
+		port = 0;
+//	printk(KERN_ALERT "current_path: %s port: %d\n", current_path, port);
+	kfree(current_path);
+	return port;
+}
+
+/**
+ * utils_sysfs_store_wrapper - Wraps the storing function for sysfs entries
+ * @address:	The address of the register to be written
+ * @offset:	The offset from the address of the register
+ * @mask:	The mask to be used on the value to be written
+ * @value:	The value to be written to the register
+ * @kobj:	The kobject of the entry calling the function
+ *
+ * Wraps the core functionality of all "store" functions of sysfs entries.
+ * After calculating the ethernet port number (in N/A cases, it's 0), the value
+ * is written to the designated register
+ *
+ */
+void utils_sysfs_store_wrapper(u32 address, u32 offset, u32 mask, u32 value,
+			       struct kobject *kobj)
+{
+	int port;
+	void __iomem *working_address;
+
+	port = utils_sysfs_path_to_eth_port_num(kobj);
+	working_address = (void __iomem *)(xroe_lp->base_addr +
+			  (address + (0x100 * port)));
+	utils_write32withmask(working_address, value, mask, offset);
+}
+
+/**
+ * utils_sysfs_store_wrapper - Wraps the storing function for sysfs entries
+ * @address:	The address of the register to be read
+ * @offset:	The offset from the address of the register
+ * @mask:	The mask to be used on the value to be read
+ * @kobj:	The kobject of the entry calling the function
+ *
+ * Wraps the core functionality of all "show" functions of sysfs entries.
+ * After calculating the ethernet port number (in N/A cases, it's 0), the value
+ * is read from the designated register and returned.
+ *
+ * Return: The value designated by the address, offset and mask
+ */
+u32 utils_sysfs_show_wrapper(u32 address, u32 offset, u32 mask,
+			     struct kobject *kobj)
+{
+	int port;
+	void __iomem *working_address;
+	u32 buffer;
+
+	port = utils_sysfs_path_to_eth_port_num(kobj);
+	working_address = (void __iomem *)(xroe_lp->base_addr +
+			  (address + (0x100 * port)));
+	buffer = ioread32(working_address);
+	return (buffer & mask) >> offset;
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroeframer/sysfs_xroe_framer_ipv4.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroeframer/sysfs_xroe_framer_ipv4.c	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,718 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+
+#include <linux/init.h>
+#include <linux/kobject.h>
+#include <linux/module.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+#include "xroe_framer.h"
+
+enum { XROE_SIZE_MAX = 15 };
+static int xroe_size;
+static char xroe_tmp[XROE_SIZE_MAX];
+
+static void utils_ipv4addr_hextochar(u32 ip, unsigned char *bytes);
+static int utils_ipv4addr_chartohex(char *ip_addr, uint32_t *p_ip_addr);
+
+/**
+ * ipv4_version_show - Returns the IPv4 version number
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 version number
+ *
+ * Returns the IPv4 version number
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_version_show(struct kobject *kobj,
+				 struct kobj_attribute *attr, char *buff)
+{
+	u32 version;
+
+	version = utils_sysfs_show_wrapper(ETH_IPV4_VERSION_ADDR,
+					   ETH_IPV4_VERSION_OFFSET,
+					   ETH_IPV4_VERSION_MASK, kobj);
+	sprintf(buff, "%d\n", version);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_version_store - Writes to the IPv4 version number sysfs entry
+ * (not permitted)
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 version
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 version number sysfs entry (not permitted)
+ *
+ * Return: 0
+ */
+static ssize_t ipv4_version_store(struct kobject *kobj,
+				  struct kobj_attribute *attr, const char *buff,
+				  size_t count)
+{
+	return 0;
+}
+
+/**
+ * ipv4_ihl_show - Returns the IPv4 IHL
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 IHL
+ *
+ * Returns the IPv4 IHL
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_ihl_show(struct kobject *kobj,
+			     struct kobj_attribute *attr, char *buff)
+{
+	u32 ihl;
+
+	ihl = utils_sysfs_show_wrapper(ETH_IPV4_IHL_ADDR, ETH_IPV4_IHL_OFFSET,
+				       ETH_IPV4_IHL_MASK, kobj);
+	sprintf(buff, "%d\n", ihl);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_ihl_store - Writes to the IPv4 IHL sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 IHL
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 IHL sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_ihl_store(struct kobject *kobj,
+			      struct kobj_attribute *attr, const char *buff,
+			      size_t count)
+{
+	int ret;
+	u32 ihl;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &ihl);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV4_IHL_ADDR, ETH_IPV4_IHL_OFFSET,
+				  ETH_IPV4_IHL_MASK, ihl, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_dscp_show - Returns the IPv4 DSCP
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 DSCP
+ *
+ * Returns the IPv4 DSCP
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_dscp_show(struct kobject *kobj,
+			      struct kobj_attribute *attr, char *buff)
+{
+	u32 dscp;
+
+	dscp = utils_sysfs_show_wrapper(ETH_IPV4_DSCP_ADDR,
+					ETH_IPV4_DSCP_OFFSET,
+					ETH_IPV4_DSCP_MASK, kobj);
+	sprintf(buff, "%d\n", dscp);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_dscp_store - Writes to the IPv4 DSCP sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 DSCP
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 DSCP sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_dscp_store(struct kobject *kobj,
+			       struct kobj_attribute *attr, const char *buff,
+			       size_t count)
+{
+	int ret;
+	u32 dscp;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &dscp);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV4_DSCP_ADDR, ETH_IPV4_DSCP_OFFSET,
+				  ETH_IPV4_DSCP_MASK, dscp, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_ecn_show - Returns the IPv4 ECN
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 ECN
+ *
+ * Returns the IPv4 ECN
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_ecn_show(struct kobject *kobj,
+			     struct kobj_attribute *attr, char *buff)
+{
+	u32 ecn;
+
+	ecn = utils_sysfs_show_wrapper(ETH_IPV4_ECN_ADDR, ETH_IPV4_ECN_OFFSET,
+				       ETH_IPV4_ECN_MASK, kobj);
+	sprintf(buff, "%d\n", ecn);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_ecn_store - Writes to the IPv4 ECN sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 ECN
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 ECN sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_ecn_store(struct kobject *kobj,
+			      struct kobj_attribute *attr, const char *buff,
+			      size_t count)
+{
+	int ret;
+	u32 ecn;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &ecn);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV4_ECN_ADDR, ETH_IPV4_ECN_OFFSET,
+				  ETH_IPV4_ECN_MASK, ecn, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_id_show - Returns the IPv4 ID
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 ID
+ *
+ * Returns the IPv4 ID
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_id_show(struct kobject *kobj,
+			    struct kobj_attribute *attr, char *buff)
+{
+	u32 id;
+
+	id = utils_sysfs_show_wrapper(ETH_IPV4_ID_ADDR, ETH_IPV4_ID_OFFSET,
+				      ETH_IPV4_ID_MASK, kobj);
+	sprintf(buff, "%d\n", id);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_id_store - Writes to the IPv4 ID sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 ID
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 ID sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_id_store(struct kobject *kobj,
+			     struct kobj_attribute *attr, const char *buff,
+			     size_t count)
+{
+	int ret;
+	u32 id;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &id);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV4_ID_ADDR, ETH_IPV4_ID_OFFSET,
+				  ETH_IPV4_ID_MASK, id, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_flags_show - Returns the IPv4 flags
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 flags
+ *
+ * Returns the IPv4 flags
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_flags_show(struct kobject *kobj,
+			       struct kobj_attribute *attr, char *buff)
+{
+	u32 flags;
+
+	flags = utils_sysfs_show_wrapper(ETH_IPV4_FLAGS_ADDR,
+					 ETH_IPV4_FLAGS_OFFSET,
+					 ETH_IPV4_FLAGS_MASK, kobj);
+	sprintf(buff, "%d\n", flags);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_flags_store - Writes to the IPv4 flags sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 flags
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 flags sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_flags_store(struct kobject *kobj,
+				struct kobj_attribute *attr, const char *buff,
+				size_t count)
+{
+	int ret;
+	u32 flags;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &flags);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV4_FLAGS_ADDR, ETH_IPV4_FLAGS_OFFSET,
+				  ETH_IPV4_FLAGS_MASK, flags, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_fragment_offset_show - Returns the IPv4 fragment offset
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 fragment offset
+ *
+ * Returns the IPv4 fragment offset
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_fragment_offset_show
+(struct kobject *kobj, struct kobj_attribute *attr, char *buff)
+{
+	u32 fragment;
+
+	fragment = utils_sysfs_show_wrapper(ETH_IPV4_FRAGMENT_OFFSET_ADDR,
+					    ETH_IPV4_FRAGMENT_OFFSET_OFFSET,
+					    ETH_IPV4_FRAGMENT_OFFSET_MASK,
+					    kobj);
+	sprintf(buff, "%d\n", fragment);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_fragment_offset_store - Writes to the IPv4 fragment offset sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 fragment offset
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 fragment offset sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_fragment_offset_store
+(struct kobject *kobj, struct kobj_attribute *attr, const char *buff,
+size_t count)
+{
+	int ret;
+	u32 fragment;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &fragment);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV4_FRAGMENT_OFFSET_ADDR,
+				  ETH_IPV4_FRAGMENT_OFFSET_OFFSET,
+				  ETH_IPV4_FRAGMENT_OFFSET_MASK, fragment,
+				  kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_ttl_show - Returns the IPv4 TTL
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 TTL
+ *
+ * Returns the IPv4 TTL
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_ttl_show(struct kobject *kobj, struct kobj_attribute *attr,
+			     char *buff)
+{
+	u32 ttl;
+
+	ttl = utils_sysfs_show_wrapper(ETH_IPV4_TIME_TO_LIVE_ADDR,
+				       ETH_IPV4_TIME_TO_LIVE_OFFSET,
+				       ETH_IPV4_TIME_TO_LIVE_MASK, kobj);
+	sprintf(buff, "%d\n", ttl);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_ttl_store - Writes to the IPv4 TTL sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 TTL
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 TTL sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_ttl_store(struct kobject *kobj,
+			      struct kobj_attribute *attr, const char *buff,
+			      size_t count)
+{
+	int ret;
+	u32 ttl;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &ttl);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV4_TIME_TO_LIVE_ADDR,
+				  ETH_IPV4_TIME_TO_LIVE_OFFSET,
+				  ETH_IPV4_TIME_TO_LIVE_MASK, ttl, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_protocol_show - Returns the IPv4 protocol
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 protocol
+ *
+ * Returns the IPv4 protocol
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_protocol_show(struct kobject *kobj,
+				  struct kobj_attribute *attr, char *buff)
+{
+	u32 protocol;
+
+	protocol = utils_sysfs_show_wrapper(ETH_IPV4_PROTOCOL_ADDR,
+					    ETH_IPV4_PROTOCOL_OFFSET,
+					    ETH_IPV4_PROTOCOL_MASK, kobj);
+	sprintf(buff, "%d\n", protocol);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_protocol_store - Writes to the IPv4 protocol sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 protocol
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 protocol sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_protocol_store(struct kobject *kobj,
+				   struct kobj_attribute *attr,
+				   const char *buff, size_t count)
+{
+	int ret;
+	u32 protocol;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &protocol);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV4_PROTOCOL_ADDR,
+				  ETH_IPV4_PROTOCOL_OFFSET,
+				  ETH_IPV4_PROTOCOL_MASK, protocol, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_source_address_show - Returns the IPv4 source address
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 source address
+ *
+ * Returns the IPv4 source address in x.x.x.x format
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_source_address_show
+(struct kobject *kobj, struct kobj_attribute *attr, char *buff)
+{
+	u32 source_add = 0;
+	unsigned char ip_addr_char[4];
+
+	source_add = utils_sysfs_show_wrapper(ETH_IPV4_SOURCE_ADD_ADDR,
+					      ETH_IPV4_SOURCE_ADD_OFFSET,
+					      ETH_IPV4_SOURCE_ADD_MASK, kobj);
+	utils_ipv4addr_hextochar(source_add, ip_addr_char);
+	sprintf(buff, "%d.%d.%d.%d\n", ip_addr_char[3], ip_addr_char[2],
+		ip_addr_char[1], ip_addr_char[0]);
+
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_source_address_store - Writes to the IPv4 source address sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 source address
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 source address sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_source_address_store
+(struct kobject *kobj, struct kobj_attribute *attr, const char *buff,
+size_t count)
+{
+	u32 source_add = 0;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+	if (utils_ipv4addr_chartohex(xroe_tmp, &source_add) == 4)
+		utils_sysfs_store_wrapper(ETH_IPV4_SOURCE_ADD_ADDR,
+					  ETH_IPV4_SOURCE_ADD_OFFSET,
+					  ETH_IPV4_SOURCE_ADD_MASK, source_add,
+					  kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv4_destination_address_show - Returns the IPv4 destination address
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 destination address
+ *
+ * Returns the IPv4 destination address in x.x.x.x format
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv4_destination_address_show
+(struct kobject *kobj, struct kobj_attribute *attr, char *buff)
+{
+	u32 dest_add = 0;
+	unsigned char ip_addr_char[4];
+
+	dest_add = utils_sysfs_show_wrapper(ETH_IPV4_DESTINATION_ADD_ADDR,
+					    ETH_IPV4_DESTINATION_ADD_OFFSET,
+					    ETH_IPV4_DESTINATION_ADD_MASK,
+					    kobj);
+	utils_ipv4addr_hextochar(dest_add, ip_addr_char);
+	sprintf(buff, "%d.%d.%d.%d\n", ip_addr_char[3], ip_addr_char[2],
+		ip_addr_char[1], ip_addr_char[0]);
+
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv4_destination_address_store - Writes to the IPv4 destination address
+ * sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 destination address
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv4 destination address sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv4_destination_address_store
+(struct kobject *kobj, struct kobj_attribute *attr, const char *buff,
+size_t count)
+{
+	u32 dest_add = 0;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+	if (utils_ipv4addr_chartohex(xroe_tmp, &dest_add) == 4)
+		utils_sysfs_store_wrapper(ETH_IPV4_DESTINATION_ADD_ADDR,
+					  ETH_IPV4_DESTINATION_ADD_OFFSET,
+					  ETH_IPV4_DESTINATION_ADD_MASK,
+					  dest_add, kobj);
+	return xroe_size;
+}
+
+/* TODO Use DEVICE_ATTR/_RW/_RO macros */
+
+static struct kobj_attribute version_attribute =
+	__ATTR(version, 0444, ipv4_version_show, ipv4_version_store);
+static struct kobj_attribute ihl_attribute =
+	__ATTR(ihl, 0660, ipv4_ihl_show, ipv4_ihl_store);
+static struct kobj_attribute dscp_attribute =
+	__ATTR(dscp, 0660, ipv4_dscp_show, ipv4_dscp_store);
+static struct kobj_attribute ecn_attribute =
+	__ATTR(ecn, 0660, ipv4_ecn_show, ipv4_ecn_store);
+static struct kobj_attribute id_attribute =
+	__ATTR(id, 0660, ipv4_id_show, ipv4_id_store);
+static struct kobj_attribute flags_attribute =
+	__ATTR(flags, 0660, ipv4_flags_show, ipv4_flags_store);
+static struct kobj_attribute fragment_offset_attribute =
+	__ATTR(fragment_offset, 0660, ipv4_fragment_offset_show,
+	       ipv4_fragment_offset_store);
+static struct kobj_attribute ttl_attribute =
+	__ATTR(ttl, 0660, ipv4_ttl_show, ipv4_ttl_store);
+static struct kobj_attribute protocol_attribute =
+	__ATTR(protocol, 0660, ipv4_protocol_show, ipv4_protocol_store);
+static struct kobj_attribute source_add_attribute =
+	__ATTR(source_add, 0660, ipv4_source_address_show,
+	       ipv4_source_address_store);
+static struct kobj_attribute destination_add_attribute =
+	__ATTR(dest_add, 0660, ipv4_destination_address_show,
+	       ipv4_destination_address_store);
+
+static struct attribute *attrs[] = {
+	&version_attribute.attr,
+	&ihl_attribute.attr,
+	&dscp_attribute.attr,
+	&ecn_attribute.attr,
+	&id_attribute.attr,
+	&flags_attribute.attr,
+	&fragment_offset_attribute.attr,
+	&ttl_attribute.attr,
+	&protocol_attribute.attr,
+	&source_add_attribute.attr,
+	&destination_add_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group attr_group = {
+	.attrs = attrs,
+};
+
+struct kobject *kobj_framer;
+static struct kobject *kobj_ipv4[MAX_NUM_ETH_PORTS];
+struct kobject *kobj_eth_ports[MAX_NUM_ETH_PORTS];
+
+/**
+ * xroe_sysfs_ipv4_init - Creates the xroe sysfs "ipv4" subdirectory & entries
+ *
+ * Return: 0 on success, negative value in case of failure to
+ * create the sysfs group
+ *
+ * Creates the xroe sysfs "ipv4" subdirectory and entries under "xroe"
+ */
+int xroe_sysfs_ipv4_init(void)
+{
+	int ret;
+	int i;
+	char eth_port_dir_name[11];
+
+	kobj_framer = kobject_create_and_add("framer", root_xroe_kobj);
+	if (!kobj_framer)
+		return -ENOMEM;
+	for (i = 0; i < 4; i++) {
+		snprintf(eth_port_dir_name, sizeof(eth_port_dir_name),
+			 "eth_port_%d", i);
+		kobj_eth_ports[i] = kobject_create_and_add(eth_port_dir_name,
+							   kobj_framer);
+		if (!kobj_eth_ports[i])
+			return -ENOMEM;
+		kobj_ipv4[i] = kobject_create_and_add("ipv4",
+						      kobj_eth_ports[i]);
+		if (!kobj_ipv4[i])
+			return -ENOMEM;
+		ret = sysfs_create_group(kobj_ipv4[i], &attr_group);
+		if (ret)
+			kobject_put(kobj_ipv4[i]);
+	}
+	return ret;
+}
+
+/**
+ * xroe_sysfs_ipv4_exit - Deletes the xroe sysfs "ipv4" subdirectory & entries
+ *
+ * Deletes the xroe sysfs "ipv4" subdirectory and entries,
+ * under the "xroe" entry
+ */
+void xroe_sysfs_ipv4_exit(void)
+{
+	int i;
+
+	for (i = 0; i < MAX_NUM_ETH_PORTS; i++)
+		kobject_put(kobj_ipv4[i]);
+}
+
+/**
+ * utils_ipv4addr_hextochar - Integer to char array for IPv4 addresses
+ * @ip:		The IP address in integer format
+ * @bytes:	The IP address in a 4-byte array
+ *
+ * Coverts an IPv4 address given in unsigned integer format to a character array
+ */
+static void utils_ipv4addr_hextochar(u32 ip, unsigned char *bytes)
+{
+	bytes[0] = ip & 0xFF;
+	bytes[1] = (ip >> 8) & 0xFF;
+	bytes[2] = (ip >> 16) & 0xFF;
+	bytes[3] = (ip >> 24) & 0xFF;
+}
+
+/**
+ * utils_ipv4addr_chartohex - Character to char array for IPv4 addresses
+ * @ip_addr:	The character array containing the IP address
+ * @p_ip_addr:	The converted IPv4 address
+ *
+ * Coverts an IPv4 address given as a character array to integer format
+ *
+ * Return: 4 (the length of the resulting character array) on success,
+ * -1 in case of wrong input
+ */
+static int utils_ipv4addr_chartohex(char *ip_addr, uint32_t *p_ip_addr)
+{
+	int count = 0, ret = -1;
+	char *string;
+	unsigned char *found;
+	u32 byte_array[4];
+	u32 byte = 0;
+
+	string = ip_addr;
+	while ((found = (unsigned char *)strsep(&string, ".")) != NULL) {
+		if (count <= 4) {
+			ret = kstrtouint(found, 10, &byte);
+			if (ret)
+				return ret;
+			byte_array[count] = byte;
+		} else {
+			break;
+		}
+		count++;
+	}
+
+	if (count == 4) {
+		ret = count;
+		*p_ip_addr = byte_array[3] | (byte_array[2] << 8)
+		| (byte_array[1] << 16) | (byte_array[0] << 24);
+	}
+	return ret;
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroeframer/sysfs_xroe_framer_ipv6.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroeframer/sysfs_xroe_framer_ipv6.c	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,571 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+
+#include <linux/init.h>
+#include <linux/kobject.h>
+#include <linux/module.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+#include "xroe_framer.h"
+
+enum { XROE_SIZE_MAX = 60 };
+static int xroe_size;
+static char xroe_tmp[XROE_SIZE_MAX];
+
+static void utils_ipv6addr_32to16(u32 *ip32, uint16_t *ip16);
+static int utils_ipv6addr_chartohex(char *ip_addr, uint32_t *p_ip_addr);
+
+/**
+ * ipv6_version_show - Returns the IPv6 version number
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 version number
+ *
+ * Returns the IPv6 version number
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv6_version_show(struct kobject *kobj,
+				 struct kobj_attribute *attr, char *buff)
+{
+	u32 version;
+
+	version = utils_sysfs_show_wrapper(ETH_IPV6_V_ADDR, ETH_IPV6_V_OFFSET,
+					   ETH_IPV6_V_MASK, kobj);
+	sprintf(buff, "%d\n", version);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv6_version_store - Writes to the IPv6 version number sysfs entry
+ * (not permitted)
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 version
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv6 version number sysfs entry (not permitted)
+ *
+ * Return: 0
+ */
+static ssize_t ipv6_version_store(struct kobject *kobj,
+				  struct kobj_attribute *attr,
+				  const char *buff, size_t count)
+{
+	return 0;
+}
+
+/**
+ * ipv6_traffic_class_show - Returns the IPv6 traffic class
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 traffic class
+ *
+ * Returns the IPv6 traffic class
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv6_traffic_class_show(struct kobject *kobj,
+				       struct kobj_attribute *attr, char *buff)
+{
+	u32 traffic_class;
+
+	traffic_class = utils_sysfs_show_wrapper(ETH_IPV6_TRAFFIC_CLASS_ADDR,
+						 ETH_IPV6_TRAFFIC_CLASS_OFFSET,
+						 ETH_IPV6_TRAFFIC_CLASS_MASK,
+						 kobj);
+	sprintf(buff, "%d\n", traffic_class);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv6_traffic_class_store - Writes to the IPv6 traffic class
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 traffic class
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv6 traffic class sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv6_traffic_class_store(struct kobject *kobj,
+					struct kobj_attribute *attr,
+					const char *buff, size_t count)
+{
+	int ret;
+	u32 traffic_class;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &traffic_class);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV6_TRAFFIC_CLASS_ADDR,
+				  ETH_IPV6_TRAFFIC_CLASS_OFFSET,
+				  ETH_IPV6_TRAFFIC_CLASS_MASK, traffic_class,
+				  kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv6_flow_label_show - Returns the IPv6 flow label
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 flow label
+ *
+ * Returns the IPv6 flow label
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv6_flow_label_show(struct kobject *kobj,
+				    struct kobj_attribute *attr, char *buff)
+{
+	u32 flow_label;
+
+	flow_label = utils_sysfs_show_wrapper(ETH_IPV6_FLOW_LABEL_ADDR,
+					      ETH_IPV6_FLOW_LABEL_OFFSET,
+					      ETH_IPV6_FLOW_LABEL_MASK, kobj);
+	sprintf(buff, "%d\n", flow_label);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv6_flow_label_store - Writes to the IPv6 flow label
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 flow label
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv6 flow label sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv6_flow_label_store(struct kobject *kobj,
+				     struct kobj_attribute *attr,
+				     const char *buff, size_t count)
+{
+	int ret;
+	u32 flow_label;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &flow_label);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV6_FLOW_LABEL_ADDR,
+				  ETH_IPV6_FLOW_LABEL_OFFSET,
+				  ETH_IPV6_FLOW_LABEL_MASK, flow_label, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv6_next_header_show - Returns the IPv6 next header
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 next header
+ *
+ * Returns the IPv6 next header
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv6_next_header_show(struct kobject *kobj,
+				     struct kobj_attribute *attr,
+				     char *buff)
+{
+	u32 next_header;
+
+	next_header = utils_sysfs_show_wrapper(ETH_IPV6_NEXT_HEADER_ADDR,
+					       ETH_IPV6_NEXT_HEADER_OFFSET,
+					       ETH_IPV6_NEXT_HEADER_MASK, kobj);
+	sprintf(buff, "%d\n", next_header);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv6_next_header_store - Writes to the IPv6 next header
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 next header
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv6 next header sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv6_next_header_store(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      const char *buff, size_t count)
+{
+	int ret;
+	u32 next_header;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &next_header);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV6_NEXT_HEADER_ADDR,
+				  ETH_IPV6_NEXT_HEADER_OFFSET,
+				  ETH_IPV6_NEXT_HEADER_MASK, next_header, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv6_hop_limit_show - Returns the IPv6 hop limit
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 hop limit
+ *
+ * Returns the IPv6 hop limit
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv6_hop_limit_show(struct kobject *kobj,
+				   struct kobj_attribute *attr, char *buff)
+{
+	u32 hop_limit;
+
+	hop_limit = utils_sysfs_show_wrapper(ETH_IPV6_HOP_LIMIT_ADDR,
+					     ETH_IPV6_HOP_LIMIT_OFFSET,
+					     ETH_IPV6_HOP_LIMIT_MASK, kobj);
+	sprintf(buff, "%d\n", hop_limit);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv6_hop_limit_store - Writes to the IPv6 hop limit
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv6 hop limit
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv6 hop limit sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv6_hop_limit_store
+(struct kobject *kobj, struct kobj_attribute *attr, const char *buff,
+size_t count)
+{
+	int ret;
+	u32 hop_limit;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &hop_limit);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_IPV6_HOP_LIMIT_ADDR,
+				  ETH_IPV6_HOP_LIMIT_OFFSET,
+				  ETH_IPV6_HOP_LIMIT_MASK, hop_limit, kobj);
+	return xroe_size;
+}
+
+/**
+ * ipv6_source_address_show - Returns the IPv6 source address
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 source address
+ *
+ * Returns the IPv6 source address in xxxx.xxxx.xxxx.xxxx.xxxx.xxxx.xxxx.xxxx
+ * format
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv6_source_address_show
+(struct kobject *kobj, struct kobj_attribute *attr, char *buff)
+{
+	u32 source[4];
+	u16 source_add16[8];
+
+	source[0] = utils_sysfs_show_wrapper(ETH_IPV6_SOURCE_ADD_31_0_ADDR,
+					     ETH_IPV6_SOURCE_ADD_31_0_OFFSET,
+					     ETH_IPV6_SOURCE_ADD_31_0_MASK,
+					     kobj);
+	source[1] = utils_sysfs_show_wrapper(ETH_IPV6_SOURCE_ADD_63_32_ADDR,
+					     ETH_IPV6_SOURCE_ADD_63_32_OFFSET,
+					     ETH_IPV6_SOURCE_ADD_63_32_MASK,
+					     kobj);
+	source[2] = utils_sysfs_show_wrapper(ETH_IPV6_SOURCE_ADD_95_64_ADDR,
+					     ETH_IPV6_SOURCE_ADD_95_64_OFFSET,
+					     ETH_IPV6_SOURCE_ADD_95_64_MASK,
+					     kobj);
+	source[3] = utils_sysfs_show_wrapper(ETH_IPV6_SOURCE_ADD_127_96_ADDR,
+					     ETH_IPV6_SOURCE_ADD_127_96_OFFSET,
+					     ETH_IPV6_SOURCE_ADD_127_96_MASK,
+					     kobj);
+
+	utils_ipv6addr_32to16(source, source_add16);
+	sprintf(buff, "%04x:%04x:%04x:%04x:%04x:%04x:%04x:%04x\n",
+		source_add16[0], source_add16[1], source_add16[2],
+		source_add16[3],
+		source_add16[4], source_add16[5], source_add16[6],
+		source_add16[7]);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv6_source_address_store - Writes to the IPv6 source address sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 source address
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv6 source address sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv6_source_address_store
+(struct kobject *kobj, struct kobj_attribute *attr, const char *buff,
+size_t count)
+{
+	u32 source_add[4];
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+	if (utils_ipv6addr_chartohex(xroe_tmp, source_add) == 8) {
+		utils_sysfs_store_wrapper(ETH_IPV6_SOURCE_ADD_31_0_ADDR,
+					  ETH_IPV6_SOURCE_ADD_31_0_OFFSET,
+					  ETH_IPV6_SOURCE_ADD_31_0_MASK,
+					  source_add[0], kobj);
+		utils_sysfs_store_wrapper(ETH_IPV6_SOURCE_ADD_63_32_ADDR,
+					  ETH_IPV6_SOURCE_ADD_63_32_OFFSET,
+					  ETH_IPV6_SOURCE_ADD_63_32_MASK,
+					  source_add[1], kobj);
+		utils_sysfs_store_wrapper(ETH_IPV6_SOURCE_ADD_95_64_ADDR,
+					  ETH_IPV6_SOURCE_ADD_95_64_OFFSET,
+					  ETH_IPV6_SOURCE_ADD_95_64_MASK,
+					  source_add[2], kobj);
+		utils_sysfs_store_wrapper(ETH_IPV6_SOURCE_ADD_127_96_ADDR,
+					  ETH_IPV6_SOURCE_ADD_127_96_OFFSET,
+					  ETH_IPV6_SOURCE_ADD_127_96_MASK,
+					  source_add[3], kobj);
+	}
+	return xroe_size;
+}
+
+/**
+ * ipv6_destination_address_show - Returns the IPv6 destination address
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 destination address
+ *
+ * Returns the IPv6 destination address in
+ * xxxx.xxxx.xxxx.xxxx.xxxx.xxxx.xxxx.xxxx format
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t ipv6_destination_address_show
+(struct kobject *kobj, struct kobj_attribute *attr, char *buff)
+{
+	u32 dest[4];
+	u16 dest_add16[8];
+
+	dest[0] = utils_sysfs_show_wrapper(ETH_IPV6_DEST_ADD_31_0_ADDR,
+					   ETH_IPV6_DEST_ADD_31_0_OFFSET,
+					   ETH_IPV6_DEST_ADD_31_0_MASK,
+					   kobj);
+	dest[1] = utils_sysfs_show_wrapper(ETH_IPV6_DEST_ADD_63_32_ADDR,
+					   ETH_IPV6_DEST_ADD_63_32_OFFSET,
+					   ETH_IPV6_DEST_ADD_63_32_MASK,
+					   kobj);
+	dest[2] = utils_sysfs_show_wrapper(ETH_IPV6_DEST_ADD_95_64_ADDR,
+					   ETH_IPV6_DEST_ADD_95_64_OFFSET,
+					   ETH_IPV6_DEST_ADD_95_64_MASK,
+					   kobj);
+	dest[3] = utils_sysfs_show_wrapper(ETH_IPV6_DEST_ADD_127_96_ADDR,
+					   ETH_IPV6_DEST_ADD_127_96_OFFSET,
+					   ETH_IPV6_DEST_ADD_127_96_MASK,
+					   kobj);
+
+	utils_ipv6addr_32to16(dest, dest_add16);
+	sprintf(buff, "%04x:%04x:%04x:%04x:%04x:%04x:%04x:%04x\n",
+		dest_add16[0], dest_add16[1], dest_add16[2],
+		dest_add16[3],
+		dest_add16[4], dest_add16[5], dest_add16[6],
+		dest_add16[7]);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * ipv6_destination_address_store - Writes to the IPv6 destination address
+ * sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the IPv4 destination address
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the IPv6 destination address sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t ipv6_destination_address_store
+(struct kobject *kobj, struct kobj_attribute *attr, const char *buff,
+size_t count)
+{
+	u32 dest_add[4];
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	strncpy(xroe_tmp, buff, xroe_size);
+	if (utils_ipv6addr_chartohex(xroe_tmp, dest_add) == 8) {
+		utils_sysfs_store_wrapper(ETH_IPV6_DEST_ADD_31_0_ADDR,
+					  ETH_IPV6_DEST_ADD_31_0_OFFSET,
+					  ETH_IPV6_DEST_ADD_31_0_MASK,
+					  dest_add[0], kobj);
+		utils_sysfs_store_wrapper(ETH_IPV6_DEST_ADD_63_32_ADDR,
+					  ETH_IPV6_DEST_ADD_63_32_OFFSET,
+					  ETH_IPV6_DEST_ADD_63_32_MASK,
+					  dest_add[1], kobj);
+		utils_sysfs_store_wrapper(ETH_IPV6_DEST_ADD_95_64_ADDR,
+					  ETH_IPV6_DEST_ADD_95_64_OFFSET,
+					  ETH_IPV6_DEST_ADD_95_64_MASK,
+					  dest_add[2], kobj);
+		utils_sysfs_store_wrapper(ETH_IPV6_DEST_ADD_127_96_ADDR,
+					  ETH_IPV6_DEST_ADD_127_96_OFFSET,
+					  ETH_IPV6_DEST_ADD_127_96_MASK,
+					  dest_add[3], kobj);
+	}
+	return xroe_size;
+}
+
+/* TODO Use DEVICE_ATTR/_RW/_RO macros */
+
+static struct kobj_attribute version_attribute =
+	__ATTR(version, 0444, ipv6_version_show, ipv6_version_store);
+static struct kobj_attribute traffic_class =
+	__ATTR(traffic_class, 0660, ipv6_traffic_class_show,
+	       ipv6_traffic_class_store);
+static struct kobj_attribute flow_label =
+	__ATTR(flow_label, 0660, ipv6_flow_label_show, ipv6_flow_label_store);
+static struct kobj_attribute next_header =
+	__ATTR(next_header, 0660, ipv6_next_header_show,
+	       ipv6_next_header_store);
+static struct kobj_attribute hop_limit =
+	__ATTR(hop_limit, 0660, ipv6_hop_limit_show, ipv6_hop_limit_store);
+static struct kobj_attribute source_add_attribute =
+	__ATTR(source_add, 0660, ipv6_source_address_show,
+	       ipv6_source_address_store);
+static struct kobj_attribute dest_add_attribute =
+	__ATTR(dest_add, 0660, ipv6_destination_address_show,
+	       ipv6_destination_address_store);
+
+static struct attribute *attrs[] = {
+	&version_attribute.attr,
+	&traffic_class.attr,
+	&flow_label.attr,
+	&next_header.attr,
+	&hop_limit.attr,
+	&source_add_attribute.attr,
+	&dest_add_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group attr_group = {
+	.attrs = attrs,
+};
+
+static struct kobject *kobj_ipv6[MAX_NUM_ETH_PORTS];
+
+/**
+ * xroe_sysfs_ipv6_init - Creates the xroe sysfs "ipv6" subdirectory & entries
+ *
+ * Return: 0 on success, negative value in case of failure to
+ * create the sysfs group
+ *
+ * Creates the xroe sysfs "ipv6" subdirectory and entries under "xroe"
+ */
+int xroe_sysfs_ipv6_init(void)
+{
+	int ret;
+	int i;
+
+	for (i = 0; i < 4; i++) {
+		kobj_ipv6[i] = kobject_create_and_add("ipv6",
+						      kobj_eth_ports[i]);
+		if (!kobj_ipv6[i])
+			return -ENOMEM;
+		ret = sysfs_create_group(kobj_ipv6[i], &attr_group);
+		if (ret)
+			kobject_put(kobj_ipv6[i]);
+	}
+	return ret;
+}
+
+/**
+ * xroe_sysfs_ipv4_exit - Deletes the xroe sysfs "ipv6" subdirectory & entries
+ *
+ * Deletes the xroe sysfs "ipv6" subdirectory and entries,
+ * under the "xroe" entry
+ *
+ */
+void xroe_sysfs_ipv6_exit(void)
+{
+	int i;
+
+	for (i = 0; i < MAX_NUM_ETH_PORTS; i++)
+		kobject_put(kobj_ipv6[i]);
+}
+
+/**
+ * utils_ipv6addr_32to16 - uint32_t to uint16_t for IPv6 addresses
+ * @ip32:	The IPv6 address in uint32_t format
+ * @ip16:	The IPv6 address in uint16_t format
+ *
+ * Coverts an IPv6 address given in uint32_t format to uint16_t
+ */
+static void utils_ipv6addr_32to16(u32 *ip32, uint16_t *ip16)
+{
+	ip16[0] = ip32[0] >> 16;
+	ip16[1] = ip32[0] & 0x0000FFFF;
+	ip16[2] = ip32[1] >> 16;
+	ip16[3] = ip32[1] & 0x0000FFFF;
+	ip16[4] = ip32[2] >> 16;
+	ip16[5] = ip32[2] & 0x0000FFFF;
+	ip16[6] = ip32[3] >> 16;
+	ip16[7] = ip32[3] & 0x0000FFFF;
+}
+
+/**
+ * utils_ipv6addr_chartohex - Character to char array for IPv6 addresses
+ * @ip_addr:	The character array containing the IP address
+ * @p_ip_addr:	The converted IPv4 address
+ *
+ * Coverts an IPv6 address given as a character array to integer format
+ *
+ * Return: 8 (the length of the resulting character array) on success,
+ * -1 in case of wrong input
+ */
+static int utils_ipv6addr_chartohex(char *ip_addr, uint32_t *p_ip_addr)
+{
+	int ret;
+	int count;
+	char *string;
+	unsigned char *found;
+	u16 ip_array_16[8];
+	u32 field;
+
+	ret = -1;
+	count = 0;
+	string = ip_addr;
+	while ((found = (unsigned char *)strsep(&string, ":")) != NULL) {
+		if (count <= 8) {
+			ret = kstrtouint(found, 16, &field);
+			if (ret)
+				return ret;
+			ip_array_16[count] = (uint16_t)field;
+		} else {
+			break;
+		}
+		count++;
+	}
+	if (count == 8) {
+		p_ip_addr[0] = ip_array_16[1] | (ip_array_16[0] << 16);
+		p_ip_addr[1] = ip_array_16[3] | (ip_array_16[2] << 16);
+		p_ip_addr[2] = ip_array_16[5] | (ip_array_16[4] << 16);
+		p_ip_addr[3] = ip_array_16[7] | (ip_array_16[6] << 16);
+		ret = count;
+	}
+	return ret;
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroeframer/sysfs_xroe_framer_stats.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroeframer/sysfs_xroe_framer_stats.c	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,401 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2019 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+
+#include <linux/init.h>
+#include <linux/kobject.h>
+#include <linux/module.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+#include "xroe_framer.h"
+
+/**
+ * total_rx_good_pkt_show - Returns the total good rx packet count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total good rx packet count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_good_pkt_show(struct kobject *kobj,
+				      struct kobj_attribute *attr, char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_TOTAL_RX_GOOD_PKT_CNT_ADDR,
+					 STATS_TOTAL_RX_GOOD_PKT_CNT_OFFSET,
+					 STATS_TOTAL_RX_GOOD_PKT_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_bad_pkt_show - Returns the total bad rx packet count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total bad rx packet count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_bad_pkt_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_TOTAL_RX_BAD_PKT_CNT_ADDR,
+					 STATS_TOTAL_RX_BAD_PKT_CNT_OFFSET,
+					 STATS_TOTAL_RX_BAD_PKT_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_bad_fcs_show - Returns the total bad fcs count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total bad frame check sequences count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_bad_fcs_show(struct kobject *kobj,
+				     struct kobj_attribute *attr,
+				     char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_TOTAL_RX_BAD_FCS_CNT_ADDR,
+					 STATS_TOTAL_RX_BAD_FCS_CNT_OFFSET,
+					 STATS_TOTAL_RX_BAD_FCS_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_user_pkt_show - Returns the total user rx packet count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total user rx packet count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_user_pkt_show(struct kobject *kobj,
+				      struct kobj_attribute *attr,
+				      char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_USER_DATA_RX_PACKETS_CNT_ADDR,
+					 STATS_USER_DATA_RX_PACKETS_CNT_OFFSET,
+					 STATS_USER_DATA_RX_PACKETS_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_good_user_pkt_show - Returns the total good user rx packet count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total good user rx packet count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_good_user_pkt_show(struct kobject *kobj,
+					   struct kobj_attribute *attr,
+					   char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_USER_DATA_RX_GOOD_PKT_CNT_ADDR,
+					 STATS_USER_DATA_RX_GOOD_PKT_CNT_OFFSET,
+					 STATS_USER_DATA_RX_GOOD_PKT_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_bad_user_pkt_show - Returns the total bad user rx packet count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total bad user rx packet count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_bad_user_pkt_show(struct kobject *kobj,
+					  struct kobj_attribute *attr,
+					  char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_USER_DATA_RX_BAD_PKT_CNT_ADDR,
+					 STATS_USER_DATA_RX_BAD_PKT_CNT_OFFSET,
+					 STATS_USER_DATA_RX_BAD_PKT_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_bad_user_fcs_show - Returns the total bad user rx fcs count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total bad user frame check sequences count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_bad_user_fcs_show(struct kobject *kobj,
+					  struct kobj_attribute *attr,
+					  char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_USER_DATA_RX_BAD_FCS_CNT_ADDR,
+					 STATS_USER_DATA_RX_BAD_FCS_CNT_OFFSET,
+					 STATS_USER_DATA_RX_BAD_FCS_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_user_ctrl_pkt_show - Returns the total user rx control packet count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total user rx control packet count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_user_ctrl_pkt_show(struct kobject *kobj,
+					   struct kobj_attribute *attr,
+					   char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_USER_CTRL_RX_PACKETS_CNT_ADDR,
+					 STATS_USER_CTRL_RX_PACKETS_CNT_OFFSET,
+					 STATS_USER_CTRL_RX_PACKETS_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_good_user_ctrl_pkt_show - Returns the total good user rx
+ * control packet count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total good user rx control packet count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_good_user_ctrl_pkt_show(struct kobject *kobj,
+						struct kobj_attribute *attr,
+						char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_USER_CTRL_RX_GOOD_PKT_CNT_ADDR,
+					 STATS_USER_CTRL_RX_GOOD_PKT_CNT_OFFSET,
+					 STATS_USER_CTRL_RX_GOOD_PKT_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_bad_user_ctrl_pkt_show - Returns the total bad user rx
+ * control packet count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total bad user rx control packet count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_bad_user_ctrl_pkt_show(struct kobject *kobj,
+					       struct kobj_attribute *attr,
+					       char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_USER_CTRL_RX_BAD_PKT_CNT_ADDR,
+					 STATS_USER_CTRL_RX_BAD_PKT_CNT_OFFSET,
+					 STATS_USER_CTRL_RX_BAD_PKT_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * total_rx_bad_user_ctrl_fcs_show - Returns the total bad user rx
+ * control fcs count
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total bad user control frame check sequences count
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t total_rx_bad_user_ctrl_fcs_show(struct kobject *kobj,
+					       struct kobj_attribute *attr,
+					       char *buff)
+{
+	u32 count;
+
+	count = utils_sysfs_show_wrapper(STATS_USER_CTRL_RX_BAD_FCS_CNT_ADDR,
+					 STATS_USER_CTRL_RX_BAD_FCS_CNT_OFFSET,
+					 STATS_USER_CTRL_RX_BAD_FCS_CNT_MASK,
+					 kobj);
+	return sprintf(buff, "%d\n", count);
+}
+
+/**
+ * rx_user_pkt_rate_show - Returns the rate of user packets
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total user rx packet count
+ *
+ * Return: Returns the rate of user packets
+ */
+static ssize_t rx_user_pkt_rate_show(struct kobject *kobj,
+				     struct kobj_attribute *attr, char *buff)
+{
+	u32 rate;
+
+	rate = utils_sysfs_show_wrapper(STATS_USER_DATA_RX_PKTS_RATE_ADDR,
+					STATS_USER_DATA_RX_PKTS_RATE_OFFSET,
+					STATS_USER_DATA_RX_PKTS_RATE_MASK,
+					kobj);
+	return sprintf(buff, "%d\n", rate);
+}
+
+/**
+ * rx_user_ctrl_pkt_rate_show - Returns the rate of user control packets
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer the value will be written to
+ *
+ * Returns the total user rx packet count
+ *
+ * Return: Returns the rate of user control packets
+ */
+static ssize_t rx_user_ctrl_pkt_rate_show(struct kobject *kobj,
+					  struct kobj_attribute *attr,
+					  char *buff)
+{
+	u32 rate;
+
+	rate = utils_sysfs_show_wrapper(STATS_USER_CTRL_RX_PKTS_RATE_ADDR,
+					STATS_USER_CTRL_RX_PKTS_RATE_OFFSET,
+					STATS_USER_CTRL_RX_PKTS_RATE_MASK,
+					kobj);
+	return sprintf(buff, "%d\n", rate);
+}
+
+/* TODO Use DEVICE_ATTR/_RW/_RO macros */
+static struct kobj_attribute total_rx_good_pkt_attribute =
+	__ATTR(total_rx_good_pkt, 0444, total_rx_good_pkt_show, NULL);
+static struct kobj_attribute total_rx_bad_pkt_attribute =
+	__ATTR(total_rx_bad_pkt, 0444, total_rx_bad_pkt_show, NULL);
+static struct kobj_attribute total_rx_bad_fcs_attribute =
+	__ATTR(total_rx_bad_fcs, 0444, total_rx_bad_fcs_show, NULL);
+static struct kobj_attribute total_rx_user_pkt_attribute =
+	__ATTR(total_rx_user_pkt, 0444, total_rx_user_pkt_show, NULL);
+static struct kobj_attribute total_rx_good_user_pkt_attribute =
+	__ATTR(total_rx_good_user_pkt, 0444, total_rx_good_user_pkt_show, NULL);
+static struct kobj_attribute total_rx_bad_user_pkt_attribute =
+	__ATTR(total_rx_bad_user_pkt, 0444, total_rx_bad_user_pkt_show, NULL);
+static struct kobj_attribute total_rx_bad_user_fcs_attribute =
+	__ATTR(total_rx_bad_user_fcs, 0444, total_rx_bad_user_fcs_show, NULL);
+static struct kobj_attribute total_rx_user_ctrl_pkt_attribute =
+	__ATTR(total_rx_user_ctrl_pkt, 0444, total_rx_user_ctrl_pkt_show, NULL);
+static struct kobj_attribute total_rx_good_user_ctrl_pkt_attribute =
+	__ATTR(total_rx_good_user_ctrl_pkt, 0444,
+	       total_rx_good_user_ctrl_pkt_show, NULL);
+static struct kobj_attribute total_rx_bad_user_ctrl_pkt_attribute =
+	__ATTR(total_rx_bad_user_ctrl_pkt, 0444,
+	       total_rx_bad_user_ctrl_pkt_show, NULL);
+static struct kobj_attribute total_rx_bad_user_ctrl_fcs_attribute =
+	__ATTR(total_rx_bad_user_ctrl_fcs, 0444,
+	       total_rx_bad_user_ctrl_fcs_show, NULL);
+static struct kobj_attribute rx_user_pkt_rate_attribute =
+	__ATTR(rx_user_pkt_rate, 0444, rx_user_pkt_rate_show, NULL);
+static struct kobj_attribute rx_user_ctrl_pkt_rate_attribute =
+	__ATTR(rx_user_ctrl_pkt_rate, 0444, rx_user_ctrl_pkt_rate_show, NULL);
+
+static struct attribute *attrs[] = {
+	&total_rx_good_pkt_attribute.attr,
+	&total_rx_bad_pkt_attribute.attr,
+	&total_rx_bad_fcs_attribute.attr,
+	&total_rx_user_pkt_attribute.attr,
+	&total_rx_good_user_pkt_attribute.attr,
+	&total_rx_bad_user_pkt_attribute.attr,
+	&total_rx_bad_user_fcs_attribute.attr,
+	&total_rx_user_ctrl_pkt_attribute.attr,
+	&total_rx_good_user_ctrl_pkt_attribute.attr,
+	&total_rx_bad_user_ctrl_pkt_attribute.attr,
+	&total_rx_bad_user_ctrl_fcs_attribute.attr,
+	&rx_user_pkt_rate_attribute.attr,
+	&rx_user_ctrl_pkt_rate_attribute.attr,
+	NULL,
+};
+
+static struct attribute_group attr_group = {
+	.attrs = attrs,
+};
+
+struct kobject *kobj_stats;
+
+/**
+ * xroe_sysfs_stats_init - Creates the xroe sysfs "stats" subdirectory & entries
+ *
+ * Return: 0 on success, negative value in case of failure to
+ * create the sysfs group
+ *
+ * Creates the xroe sysfs "stats" subdirectory and entries under "xroe"
+ */
+int xroe_sysfs_stats_init(void)
+{
+	int ret;
+
+	kobj_stats = kobject_create_and_add("stats", root_xroe_kobj);
+	if (!kobj_stats)
+		return -ENOMEM;
+
+	ret = sysfs_create_group(kobj_stats, &attr_group);
+	if (ret)
+		kobject_put(kobj_stats);
+
+	return ret;
+}
+
+/**
+ * xroe_sysfs_stats_exit - Deletes the xroe sysfs "ipv4" subdirectory & entries
+ *
+ * Deletes the xroe sysfs "stats" subdirectory and entries,
+ * under the "xroe" entry
+ */
+void xroe_sysfs_stats_exit(void)
+{
+	kobject_put(kobj_stats);
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroeframer/sysfs_xroe_framer_udp.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroeframer/sysfs_xroe_framer_udp.c	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,181 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+
+#include <linux/init.h>
+#include <linux/kobject.h>
+#include <linux/module.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+#include "xroe_framer.h"
+
+enum { XROE_SIZE_MAX = 15 };
+static int xroe_size;
+
+/**
+ * udp_source_port_show - Returns the UDP source port
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the UDP source port
+ *
+ * Returns the UDP source port
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t udp_source_port_show(struct kobject *kobj,
+				    struct kobj_attribute *attr,
+				    char *buff)
+{
+	u32 source_port;
+
+	source_port = utils_sysfs_show_wrapper(ETH_UDP_SOURCE_PORT_ADDR,
+					       ETH_UDP_SOURCE_PORT_OFFSET,
+					       ETH_UDP_SOURCE_PORT_MASK, kobj);
+	sprintf(buff, "%d\n", source_port);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * udp_source_port_store - Writes to the UDP source port sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the UDP source port
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the UDP source port sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t udp_source_port_store(struct kobject *kobj,
+				     struct kobj_attribute *attr,
+				     const char *buff, size_t count)
+{
+	int ret;
+	u32 source_port;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &source_port);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_UDP_SOURCE_PORT_ADDR,
+				  ETH_UDP_SOURCE_PORT_OFFSET,
+				  ETH_UDP_SOURCE_PORT_MASK, source_port, kobj);
+	return xroe_size;
+}
+
+/**
+ * udp_destination_port_show - Returns the UDP destination port
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the UDP destination port
+ *
+ * Returns the UDP destination port
+ *
+ * Return: XROE_SIZE_MAX on success
+ */
+static ssize_t udp_destination_port_show(struct kobject *kobj,
+					 struct kobj_attribute *attr,
+					 char *buff)
+{
+	u32 dest_port;
+
+	dest_port = utils_sysfs_show_wrapper(ETH_UDP_DESTINATION_PORT_ADDR,
+					     ETH_UDP_DESTINATION_PORT_OFFSET,
+					     ETH_UDP_DESTINATION_PORT_MASK,
+					     kobj);
+	sprintf(buff, "%d\n", dest_port);
+	return XROE_SIZE_MAX;
+}
+
+/**
+ * udp_destination_port_store - Writes to the UDP destination port sysfs entry
+ * @kobj:	The kernel object of the entry
+ * @attr:	The attributes of the kernel object
+ * @buff:	The buffer containing the UDP destination port
+ * @count:	The number of characters typed by the user
+ *
+ * Writes to the UDP destination port sysfs entry
+ *
+ * Return: XROE_SIZE_MAX or the value of "count", if that's lesser, on success
+ */
+static ssize_t udp_destination_port_store(struct kobject *kobj,
+					  struct kobj_attribute *attr,
+					  const char *buff, size_t count)
+{
+	int ret;
+	u32 dest_port;
+
+	xroe_size = min_t(size_t, count, (size_t)XROE_SIZE_MAX);
+	ret = kstrtouint(buff, 10, &dest_port);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(ETH_UDP_DESTINATION_PORT_ADDR,
+				  ETH_UDP_DESTINATION_PORT_OFFSET,
+				  ETH_UDP_DESTINATION_PORT_MASK, dest_port,
+				  kobj);
+	return xroe_size;
+}
+
+/* TODO Use DEVICE_ATTR/_RW/_RO macros */
+
+static struct kobj_attribute source_port =
+	__ATTR(source_port, 0660, udp_source_port_show,
+	       udp_source_port_store);
+static struct kobj_attribute dest_port =
+	__ATTR(dest_port, 0660, udp_destination_port_show,
+	       udp_destination_port_store);
+
+static struct attribute *attrs[] = {
+	&source_port.attr,
+	&dest_port.attr,
+	NULL,
+};
+
+static struct attribute_group attr_group = {
+	.attrs = attrs,
+};
+
+static struct kobject *kobj_udp[MAX_NUM_ETH_PORTS];
+
+/**
+ * xroe_sysfs_udp_init - Creates the xroe sysfs "udp" subdirectory and entries
+ *
+ * Return: 0 on success, negative value in case of failure to
+ * create the sysfs group
+ *
+ * Creates the xroe sysfs "udp" subdirectory and entries under "xroe"
+ */
+int xroe_sysfs_udp_init(void)
+{
+	int ret;
+	int i;
+
+	for (i = 0; i < 4; i++) {
+		kobj_udp[i] = kobject_create_and_add("udp",  kobj_eth_ports[i]);
+		if (!kobj_udp[i])
+			return -ENOMEM;
+		ret = sysfs_create_group(kobj_udp[i], &attr_group);
+		if (ret)
+			kobject_put(kobj_udp[i]);
+	}
+	return ret;
+}
+
+/**
+ * xroe_sysfs_ipv6_exit - Deletes the xroe sysfs "udp" subdirectory & entries
+ *
+ * Deletes the xroe sysfs "udp" subdirectory and entries,
+ * under the "xroe" entry
+ *
+ */
+void xroe_sysfs_udp_exit(void)
+{
+	int i;
+
+	for (i = 0; i < MAX_NUM_ETH_PORTS; i++)
+		kobject_put(kobj_udp[i]);
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroeframer/xroe_framer.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroeframer/xroe_framer.c	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,155 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+#include <linux/cdev.h>
+#include <linux/device.h>
+#include <linux/errno.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include "xroe_framer.h"
+
+#define DRIVER_NAME "framer"
+
+/*
+ * TODO: to be made static as well, so that multiple instances can be used. As
+ * of now, the "xroe_lp" structure is shared among the multiple source files
+ */
+struct framer_local *xroe_lp;
+static struct platform_driver framer_driver;
+/*
+ * TODO: placeholder for the IRQ once it's been implemented
+ * in the framer block
+ */
+static irqreturn_t framer_irq(int irq, void *xroe_lp)
+{
+	return IRQ_HANDLED;
+}
+
+/**
+ * framer_probe - Probes the device tree to locate the framer block
+ * @pdev:	The structure containing the device's details
+ *
+ * Probes the device tree to locate the framer block and maps it to
+ * the kernel virtual memory space
+ *
+ * Return: 0 on success or a negative errno on error.
+ */
+static int framer_probe(struct platform_device *pdev)
+{
+	struct resource *r_mem; /* IO mem resources */
+	struct resource *r_irq;
+	struct device *dev = &pdev->dev;
+	int rc = 0;
+
+	dev_dbg(dev, "Device Tree Probing\n");
+	xroe_lp = devm_kzalloc(&pdev->dev, sizeof(*xroe_lp), GFP_KERNEL);
+	if (!xroe_lp)
+		return -ENOMEM;
+
+	/* Get iospace for the device */
+	r_mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	xroe_lp->base_addr = devm_ioremap_resource(&pdev->dev, r_mem);
+	if (IS_ERR(xroe_lp->base_addr))
+		return PTR_ERR(xroe_lp->base_addr);
+
+	dev_set_drvdata(dev, xroe_lp);
+	xroe_sysfs_init();
+	/* Get IRQ for the device */
+	/*
+	 * TODO: No IRQ *yet* in the DT from the framer block, as it's still
+	 * under development. To be added once it's in the block, and also
+	 * replace with platform_get_irq_byname()
+	 */
+	r_irq = platform_get_resource(pdev, IORESOURCE_IRQ, 0);
+	if (IS_ERR(r_irq)) {
+		dev_info(dev, "no IRQ found\n");
+		/*
+		 * TODO: Return non-zero (error) code on no IRQ found.
+		 * To be implemented once the IRQ is in the block
+		 */
+		return 0;
+	}
+	rc = devm_request_irq(dev, xroe_lp->irq, &framer_irq, 0, DRIVER_NAME, xroe_lp);
+	if (rc) {
+		dev_err(dev, "testmodule: Could not allocate interrupt %d.\n",
+			xroe_lp->irq);
+		/*
+		 * TODO: Return non-zero (error) code on no IRQ found.
+		 * To be implemented once the IRQ is in the block
+		 */
+		return 0;
+	}
+
+	return rc;
+}
+
+/**
+ * framer_init - Registers the driver
+ *
+ * Return: 0 on success, -1 on allocation error
+ *
+ * Registers the framer driver and creates character device drivers
+ * for the whole block, as well as separate ones for stats and
+ * radio control.
+ */
+static int __init framer_init(void)
+{
+	int ret;
+
+	pr_debug("XROE framer driver init\n");
+
+	ret = platform_driver_register(&framer_driver);
+
+	return ret;
+}
+
+/**
+ * framer_exit - Destroys the driver
+ *
+ * Unregisters the framer driver and destroys the character
+ * device driver for the whole block, as well as the separate ones
+ * for stats and radio control. Returns 0 upon successful execution
+ */
+static void __exit framer_exit(void)
+{
+	xroe_sysfs_exit();
+	platform_driver_unregister(&framer_driver);
+	pr_info("XROE Framer exit\n");
+}
+
+module_init(framer_init);
+module_exit(framer_exit);
+
+static const struct of_device_id framer_of_match[] = {
+	{ .compatible = "xlnx,roe-framer-1.0", },
+	{ /* end of list */ },
+};
+MODULE_DEVICE_TABLE(of, framer_of_match);
+
+static struct platform_driver framer_driver = {
+	.driver = {
+		/*
+		 * TODO: .name shouldn't be necessary, though removing
+		 * it results in kernel panic. To investigate further
+		 */
+		.name = DRIVER_NAME,
+		.of_match_table = framer_of_match,
+	},
+	.probe = framer_probe,
+};
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Xilinx Inc.");
+MODULE_DESCRIPTION("framer - Xilinx Radio over Ethernet Framer driver");
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroeframer/xroe_framer.h
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroeframer/xroe_framer.h	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,63 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+#include "roe_framer_ctrl.h"
+#include <linux/cdev.h>
+#include <linux/device.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/ioctl.h>
+#include <linux/kernel.h>
+#include <linux/kobject.h>
+#include <linux/module.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+#include <linux/uaccess.h>
+#include <uapi/linux/stat.h> /* S_IRUSR, S_IWUSR */
+
+/* TODO: Remove hardcoded value of number of Ethernet ports and read the value
+ * from the device tree.
+ */
+#define MAX_NUM_ETH_PORTS 0x4
+/* TODO: to be made static as well, so that multiple instances can be used. As
+ * of now, the following 3 structures are shared among the multiple
+ * source files
+ */
+extern struct framer_local *xroe_lp;
+extern struct kobject *root_xroe_kobj;
+extern struct kobject *kobj_framer;
+extern struct kobject *kobj_eth_ports[MAX_NUM_ETH_PORTS];
+struct framer_local {
+	int irq;
+	unsigned long mem_start;
+	unsigned long mem_end;
+	void __iomem *base_addr;
+};
+
+int xroe_sysfs_init(void);
+int xroe_sysfs_ipv4_init(void);
+int xroe_sysfs_ipv6_init(void);
+int xroe_sysfs_udp_init(void);
+int xroe_sysfs_stats_init(void);
+void xroe_sysfs_exit(void);
+void xroe_sysfs_ipv4_exit(void);
+void xroe_sysfs_ipv6_exit(void);
+void xroe_sysfs_udp_exit(void);
+void xroe_sysfs_stats_exit(void);
+int utils_write32withmask(void __iomem *working_address, u32 value,
+			  u32 mask, u32 offset);
+int utils_check_address_offset(u32 offset, size_t device_size);
+void utils_sysfs_store_wrapper(u32 address, u32 offset, u32 mask, u32 value,
+			       struct kobject *kobj);
+u32 utils_sysfs_show_wrapper(u32 address, u32 offset, u32 mask,
+			     struct kobject *kobj);
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroetrafficgen/Kconfig
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroetrafficgen/Kconfig	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,14 @@
+# SPDX-License-Identifier: GPL-2.0
+
+#
+# Xilinx Radio over Ethernet Traffic Generator driver
+#
+
+config XROE_TRAFFIC_GEN
+	tristate "Xilinx Radio over Ethernet Traffic Generator driver"
+	help
+	  The Traffic Generator is used for in testing of other RoE IP Blocks
+	  (currenty the XRoE Framer) and simulates an radio antenna interface.
+	  It generates rolling rampdata for eCPRI antenna paths.
+	  Each path is tagged with the antenna number. The sink locks to this
+	  ramp data, then checks the next value is as expected.
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroetrafficgen/Makefile
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroetrafficgen/Makefile	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,8 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Makefile for Radio over Ethernet Framer driver
+#
+obj-$(XROE_TRAFFIC_GEN)	:= xroe_traffic_gen.o
+
+framer-objs := 	xroe-traffic-gen.o \
+		xroe-traffic-gen-sysfs.o \
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroetrafficgen/README
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroetrafficgen/README	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,19 @@
+Xilinx Radio over Ethernet Traffic Generator driver
+===================================================
+
+About the RoE Framer Traffic Generator
+
+The Traffic Generator is used for in testing of other RoE IP Blocks (currenty
+the XRoE Framer) and simulates an radio antenna interface. It generates rolling
+rampdata for eCPRI antenna paths. Each path is tagged with the antenna number.
+The sink locks to this ramp data, then checks the next value is as expected.
+
+
+About the Linux Driver
+
+The RoE Traffic Generator Linux Driver provides sysfs access to control a
+simulated radio antenna interface.
+The loading of the driver to the hardware is possible using Device Tree binding
+(see "dt-binding.txt" for more information). When the driver is loaded, the
+general controls (such as sink lock, enable, loopback etc) are exposed
+under /sys/kernel/xroetrafficgen.
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroetrafficgen/dt-binding.txt
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroetrafficgen/dt-binding.txt	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,15 @@
+* Xilinx Radio over Ethernet Traffic Generator driver
+
+Required properties:
+- compatible: must be "xlnx,roe-framer-1.0"
+- reg: physical base address of the framer and length of memory mapped region
+- clock-names: list of clock names
+- clocks: list of clock sources corresponding to the clock names
+
+Example:
+	roe_radio_ctrl@a0060000 {
+		compatible = "xlnx,roe-traffic-gen-1.0";
+		reg = <0x0 0xa0060000 0x0 0x10000>;
+		clock-names = "s_axis_fram_aclk", "s_axi_aclk";
+		clocks = <0x44 0x43>;
+	};
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroetrafficgen/roe_radio_ctrl.h
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroetrafficgen/roe_radio_ctrl.h	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,183 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+/*-----------------------------------------------------------------------------
+ * C Header bank BASE definitions
+ *-----------------------------------------------------------------------------
+ */
+#define ROE_RADIO_CFG_BASE_ADDR 0x0
+#define ROE_RADIO_SOURCE_BASE_ADDR 0x1000
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_radio_cfg
+ * with prefix radio_ @ address 0x0
+ *-----------------------------------------------------------------------------
+ */
+/* Type = roInt */
+#define RADIO_ID_ADDR 0x0
+#define RADIO_ID_MASK 0xffffffff
+#define RADIO_ID_OFFSET 0x0
+#define RADIO_ID_WIDTH 0x20
+#define RADIO_ID_DEFAULT 0x120001
+
+/* Type = rw */
+#define RADIO_TIMEOUT_ENABLE_ADDR 0x4
+#define RADIO_TIMEOUT_ENABLE_MASK 0x1
+#define RADIO_TIMEOUT_ENABLE_OFFSET 0x0
+#define RADIO_TIMEOUT_ENABLE_WIDTH 0x1
+#define RADIO_TIMEOUT_ENABLE_DEFAULT 0x0
+
+/* Type = ro */
+#define RADIO_TIMEOUT_STATUS_ADDR 0x8
+#define RADIO_TIMEOUT_STATUS_MASK 0x1
+#define RADIO_TIMEOUT_STATUS_OFFSET 0x0
+#define RADIO_TIMEOUT_STATUS_WIDTH 0x1
+#define RADIO_TIMEOUT_STATUS_DEFAULT 0x1
+
+/* Type = rw */
+#define RADIO_TIMEOUT_VALUE_ADDR 0xc
+#define RADIO_TIMEOUT_VALUE_MASK 0xfff
+#define RADIO_TIMEOUT_VALUE_OFFSET 0x0
+#define RADIO_TIMEOUT_VALUE_WIDTH 0xc
+#define RADIO_TIMEOUT_VALUE_DEFAULT 0x80
+
+/* Type = rw */
+#define RADIO_GPIO_CDC_LEDMODE2_ADDR 0x10
+#define RADIO_GPIO_CDC_LEDMODE2_MASK 0x1
+#define RADIO_GPIO_CDC_LEDMODE2_OFFSET 0x0
+#define RADIO_GPIO_CDC_LEDMODE2_WIDTH 0x1
+#define RADIO_GPIO_CDC_LEDMODE2_DEFAULT 0x0
+
+/* Type = rw */
+#define RADIO_GPIO_CDC_LEDGPIO_ADDR 0x10
+#define RADIO_GPIO_CDC_LEDGPIO_MASK 0x30
+#define RADIO_GPIO_CDC_LEDGPIO_OFFSET 0x4
+#define RADIO_GPIO_CDC_LEDGPIO_WIDTH 0x2
+#define RADIO_GPIO_CDC_LEDGPIO_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_GPIO_CDC_DIPSTATUS_ADDR 0x14
+#define RADIO_GPIO_CDC_DIPSTATUS_MASK 0xff
+#define RADIO_GPIO_CDC_DIPSTATUS_OFFSET 0x0
+#define RADIO_GPIO_CDC_DIPSTATUS_WIDTH 0x8
+#define RADIO_GPIO_CDC_DIPSTATUS_DEFAULT 0x0
+
+/* Type = wPlsH */
+#define RADIO_SW_TRIGGER_ADDR 0x20
+#define RADIO_SW_TRIGGER_MASK 0x1
+#define RADIO_SW_TRIGGER_OFFSET 0x0
+#define RADIO_SW_TRIGGER_WIDTH 0x1
+#define RADIO_SW_TRIGGER_DEFAULT 0x0
+
+/* Type = rw */
+#define RADIO_CDC_ENABLE_ADDR 0x24
+#define RADIO_CDC_ENABLE_MASK 0x1
+#define RADIO_CDC_ENABLE_OFFSET 0x0
+#define RADIO_CDC_ENABLE_WIDTH 0x1
+#define RADIO_CDC_ENABLE_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_ERROR_ADDR 0x24
+#define RADIO_CDC_ERROR_MASK 0x2
+#define RADIO_CDC_ERROR_OFFSET 0x1
+#define RADIO_CDC_ERROR_WIDTH 0x1
+#define RADIO_CDC_ERROR_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_STATUS_ADDR 0x24
+#define RADIO_CDC_STATUS_MASK 0x4
+#define RADIO_CDC_STATUS_OFFSET 0x2
+#define RADIO_CDC_STATUS_WIDTH 0x1
+#define RADIO_CDC_STATUS_DEFAULT 0x0
+
+/* Type = rw */
+#define RADIO_CDC_LOOPBACK_ADDR 0x28
+#define RADIO_CDC_LOOPBACK_MASK 0x1
+#define RADIO_CDC_LOOPBACK_OFFSET 0x0
+#define RADIO_CDC_LOOPBACK_WIDTH 0x1
+#define RADIO_CDC_LOOPBACK_DEFAULT 0x0
+
+/* Type = rw */
+#define RADIO_SINK_ENABLE_ADDR 0x2c
+#define RADIO_SINK_ENABLE_MASK 0x1
+#define RADIO_SINK_ENABLE_OFFSET 0x0
+#define RADIO_SINK_ENABLE_WIDTH 0x1
+#define RADIO_SINK_ENABLE_DEFAULT 0x1
+
+/* Type = roSig */
+#define RADIO_CDC_ERROR_31_0_ADDR 0x30
+#define RADIO_CDC_ERROR_31_0_MASK 0xffffffff
+#define RADIO_CDC_ERROR_31_0_OFFSET 0x0
+#define RADIO_CDC_ERROR_31_0_WIDTH 0x20
+#define RADIO_CDC_ERROR_31_0_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_ERROR_63_32_ADDR 0x34
+#define RADIO_CDC_ERROR_63_32_MASK 0xffffffff
+#define RADIO_CDC_ERROR_63_32_OFFSET 0x0
+#define RADIO_CDC_ERROR_63_32_WIDTH 0x20
+#define RADIO_CDC_ERROR_63_32_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_ERROR_95_64_ADDR 0x38
+#define RADIO_CDC_ERROR_95_64_MASK 0xffffffff
+#define RADIO_CDC_ERROR_95_64_OFFSET 0x0
+#define RADIO_CDC_ERROR_95_64_WIDTH 0x20
+#define RADIO_CDC_ERROR_95_64_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_ERROR_127_96_ADDR 0x3c
+#define RADIO_CDC_ERROR_127_96_MASK 0xffffffff
+#define RADIO_CDC_ERROR_127_96_OFFSET 0x0
+#define RADIO_CDC_ERROR_127_96_WIDTH 0x20
+#define RADIO_CDC_ERROR_127_96_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_STATUS_31_0_ADDR 0x40
+#define RADIO_CDC_STATUS_31_0_MASK 0xffffffff
+#define RADIO_CDC_STATUS_31_0_OFFSET 0x0
+#define RADIO_CDC_STATUS_31_0_WIDTH 0x20
+#define RADIO_CDC_STATUS_31_0_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_STATUS_63_32_ADDR 0x44
+#define RADIO_CDC_STATUS_63_32_MASK 0xffffffff
+#define RADIO_CDC_STATUS_63_32_OFFSET 0x0
+#define RADIO_CDC_STATUS_63_32_WIDTH 0x20
+#define RADIO_CDC_STATUS_63_32_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_STATUS_95_64_ADDR 0x48
+#define RADIO_CDC_STATUS_95_64_MASK 0xffffffff
+#define RADIO_CDC_STATUS_95_64_OFFSET 0x0
+#define RADIO_CDC_STATUS_95_64_WIDTH 0x20
+#define RADIO_CDC_STATUS_95_64_DEFAULT 0x0
+
+/* Type = roSig */
+#define RADIO_CDC_STATUS_127_96_ADDR 0x4c
+#define RADIO_CDC_STATUS_127_96_MASK 0xffffffff
+#define RADIO_CDC_STATUS_127_96_OFFSET 0x0
+#define RADIO_CDC_STATUS_127_96_WIDTH 0x20
+#define RADIO_CDC_STATUS_127_96_DEFAULT 0x0
+
+/*-----------------------------------------------------------------------------
+ * C Header bank register definitions for bank roe_radio_source
+ * with prefix fram_ @ address 0x1000
+ *-----------------------------------------------------------------------------
+ */
+/* Type = rwpdef */
+#define FRAM_PACKET_DATA_SIZE_ADDR 0x1000
+#define FRAM_PACKET_DATA_SIZE_MASK 0x7f
+#define FRAM_PACKET_DATA_SIZE_OFFSET 0x0
+#define FRAM_PACKET_DATA_SIZE_WIDTH 0x7
+#define FRAM_PACKET_DATA_SIZE_DEFAULT 0x0
+
+/* Type = rwpdef */
+#define FRAM_PAUSE_DATA_SIZE_ADDR 0x1004
+#define FRAM_PAUSE_DATA_SIZE_MASK 0x7f
+#define FRAM_PAUSE_DATA_SIZE_OFFSET 0x0
+#define FRAM_PAUSE_DATA_SIZE_WIDTH 0x7
+#define FRAM_PAUSE_DATA_SIZE_DEFAULT 0x0
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroetrafficgen/xroe-traffic-gen-sysfs.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroetrafficgen/xroe-traffic-gen-sysfs.c	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,824 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+
+#include <linux/device.h>
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/kobject.h>
+#include <linux/module.h>
+#include <linux/stat.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+#include "roe_radio_ctrl.h"
+#include "xroe-traffic-gen.h"
+
+static int xroe_size;
+static char xroe_tmp[XROE_SIZE_MAX];
+
+/**
+ * utils_sysfs_store_wrapper - Wraps the storing function for sysfs entries
+ * @dev:	The structure containing the device's information
+ * @address:	The address of the register to be written
+ * @offset:	The offset from the address of the register
+ * @mask:	The mask to be used on the value to be written
+ * @value:	The value to be written to the register
+ *
+ * Wraps the core functionality of all "store" functions of sysfs entries.
+ */
+static void utils_sysfs_store_wrapper(struct device *dev, u32 address,
+				      u32 offset, u32 mask, u32 value)
+{
+	void __iomem *working_address;
+	u32 read_register_value = 0;
+	u32 register_value_to_write = 0;
+	u32 delta = 0;
+	u32 buffer = 0;
+	struct xroe_traffic_gen_local *lp = dev_get_drvdata(dev);
+
+	working_address = (void __iomem *)(lp->base_addr + address);
+	read_register_value = ioread32(working_address);
+	buffer = (value << offset);
+	register_value_to_write = read_register_value & ~mask;
+	delta = buffer & mask;
+	register_value_to_write |= delta;
+	iowrite32(register_value_to_write, working_address);
+}
+
+/**
+ * utils_sysfs_show_wrapper - Wraps the "show" function for sysfs entries
+ * @dev:	The structure containing the device's information
+ * @address:	The address of the register to be read
+ * @offset:	The offset from the address of the register
+ * @mask:	The mask to be used on the value to be read
+ *
+ * Wraps the core functionality of all "show" functions of sysfs entries.
+ *
+ * Return: The value designated by the address, offset and mask
+ */
+static u32 utils_sysfs_show_wrapper(struct device *dev, u32 address, u32 offset,
+				    u32 mask)
+{
+	void __iomem *working_address;
+	u32 buffer;
+	struct xroe_traffic_gen_local *lp = dev_get_drvdata(dev);
+
+	working_address = (void __iomem *)(lp->base_addr + address);
+	buffer = ioread32(working_address);
+	return (buffer & mask) >> offset;
+}
+
+/**
+ * radio_id_show - Returns the block's ID number
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the ID number string
+ *
+ * Returns the traffic gen's ID (0x1179649 by default)
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t radio_id_show(struct device *dev,
+			     struct device_attribute *attr, char *buf)
+{
+	u32 radio_id;
+
+	radio_id = utils_sysfs_show_wrapper(dev, RADIO_ID_ADDR,
+					    RADIO_ID_OFFSET,
+					    RADIO_ID_MASK);
+	return sprintf(buf, "%d\n", radio_id);
+}
+static DEVICE_ATTR_RO(radio_id);
+
+/**
+ * timeout_enable_show - Returns the traffic gen's timeout enable status
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the enable status
+ *
+ * Reads and writes the traffic gen's timeout enable status to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t timeout_enable_show(struct device *dev,
+				   struct device_attribute *attr, char *buf)
+{
+	u32 timeout_enable;
+
+	timeout_enable = utils_sysfs_show_wrapper(dev,
+						  RADIO_TIMEOUT_ENABLE_ADDR,
+						  RADIO_TIMEOUT_ENABLE_OFFSET,
+						  RADIO_TIMEOUT_ENABLE_MASK);
+	return sprintf(buf, "%d\n", timeout_enable);
+}
+
+/**
+ * timeout_enable_store - Writes to the traffic gens's timeout enable
+ * status register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the enable status
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's timeout enable
+ * status to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t timeout_enable_store(struct device *dev,
+				    struct device_attribute *attr,
+				    const char *buf, size_t count)
+{
+	u32 enable = 0;
+
+	strncpy(xroe_tmp, buf, xroe_size);
+	if (strncmp(xroe_tmp, "true", xroe_size) == 0)
+		enable = 1;
+	else if (strncmp(xroe_tmp, "false", xroe_size) == 0)
+		enable = 0;
+	utils_sysfs_store_wrapper(dev, RADIO_TIMEOUT_ENABLE_ADDR,
+				  RADIO_TIMEOUT_ENABLE_OFFSET,
+				  RADIO_TIMEOUT_ENABLE_MASK, enable);
+	return count;
+}
+static DEVICE_ATTR_RW(timeout_enable);
+
+/**
+ * timeout_status_show - Returns the timeout status
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the ID number string
+ *
+ * Returns the traffic gen's timeout status (0x1 by default)
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t timeout_status_show(struct device *dev,
+				   struct device_attribute *attr, char *buf)
+{
+	u32 timeout;
+
+	timeout = utils_sysfs_show_wrapper(dev, RADIO_TIMEOUT_STATUS_ADDR,
+					   RADIO_TIMEOUT_STATUS_OFFSET,
+					   RADIO_TIMEOUT_STATUS_MASK);
+	return sprintf(buf, "%d\n", timeout);
+}
+static DEVICE_ATTR_RO(timeout_status);
+
+/**
+ * timeout_enable_show - Returns the traffic gen's timeout value
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the enable status
+ *
+ * Reads and writes the traffic gen's timeout value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t timeout_value_show(struct device *dev,
+				  struct device_attribute *attr, char *buf)
+{
+	u32 timeout_value;
+
+	timeout_value = utils_sysfs_show_wrapper(dev, RADIO_TIMEOUT_VALUE_ADDR,
+						 RADIO_TIMEOUT_VALUE_OFFSET,
+						 RADIO_TIMEOUT_VALUE_MASK);
+	return sprintf(buf, "%d\n", timeout_value);
+}
+
+/**
+ * timeout_enable_store - Writes to the traffic gens's timeout value
+ * status register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's timeout value
+ * to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t timeout_value_store(struct device *dev,
+				   struct device_attribute *attr,
+				   const char *buf, size_t count)
+{
+	u32 timeout_value;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &timeout_value);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, RADIO_TIMEOUT_VALUE_ADDR,
+				  RADIO_TIMEOUT_VALUE_OFFSET,
+				  RADIO_TIMEOUT_VALUE_MASK, timeout_value);
+	return count;
+}
+static DEVICE_ATTR_RW(timeout_value);
+
+/**
+ * ledmode_show - Returns the current LED mode
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the enable status
+ *
+ * Reads and writes the traffic gen's LED mode value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t ledmode_show(struct device *dev, struct device_attribute *attr,
+			    char *buf)
+{
+	u32 ledmode;
+
+	ledmode = utils_sysfs_show_wrapper(dev, RADIO_GPIO_CDC_LEDMODE2_ADDR,
+					   RADIO_GPIO_CDC_LEDMODE2_OFFSET,
+					   RADIO_GPIO_CDC_LEDMODE2_MASK);
+	return sprintf(buf, "%d\n", ledmode);
+}
+
+/**
+ * ledmode_store - Writes to the current LED mode register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's LED mode value
+ * to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t ledmode_store(struct device *dev,
+			     struct device_attribute *attr,
+			     const char *buf, size_t count)
+{
+	u32 ledmode;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &ledmode);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, RADIO_GPIO_CDC_LEDMODE2_ADDR,
+				  RADIO_GPIO_CDC_LEDMODE2_OFFSET,
+				  RADIO_GPIO_CDC_LEDMODE2_MASK, ledmode);
+	return count;
+}
+static DEVICE_ATTR_RW(ledmode);
+
+/**
+ * ledgpio_show - Returns the current LED gpio
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the enable status
+ *
+ * Reads and writes the traffic gen's LED gpio value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t ledgpio_show(struct device *dev, struct device_attribute *attr,
+			    char *buf)
+{
+	u32 ledgpio;
+
+	ledgpio = utils_sysfs_show_wrapper(dev, RADIO_GPIO_CDC_LEDGPIO_ADDR,
+					   RADIO_GPIO_CDC_LEDGPIO_OFFSET,
+					   RADIO_GPIO_CDC_LEDGPIO_MASK);
+	return sprintf(buf, "%d\n", ledgpio);
+}
+
+/**
+ * ledgpio_store - Writes to the current LED gpio register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's LED gpio value
+ * to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t ledgpio_store(struct device *dev,
+			     struct device_attribute *attr,
+			     const char *buf, size_t count)
+{
+	u32 ledgpio;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &ledgpio);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, RADIO_GPIO_CDC_LEDGPIO_ADDR,
+				  RADIO_GPIO_CDC_LEDGPIO_OFFSET,
+				  RADIO_GPIO_CDC_LEDGPIO_MASK, ledgpio);
+	return count;
+}
+static DEVICE_ATTR_RW(ledgpio);
+
+/**
+ * dip_status_show - Returns the current DIP switch value
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the enable status
+ *
+ * Reads and writes the GPIO DIP switch value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t dip_status_show(struct device *dev,
+			       struct device_attribute *attr, char *buf)
+{
+	u32 dip_status;
+
+	dip_status = utils_sysfs_show_wrapper(dev, RADIO_GPIO_CDC_LEDGPIO_ADDR,
+					      RADIO_GPIO_CDC_LEDGPIO_OFFSET,
+					      RADIO_GPIO_CDC_LEDGPIO_MASK);
+	return sprintf(buf, "0x%08x\n", dip_status);
+}
+static DEVICE_ATTR_RO(dip_status);
+
+/**
+ * sw_trigger_show - Returns the current SW trigger status
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the enable status
+ *
+ * Reads and writes the traffic gen's SW trigger status value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t sw_trigger_show(struct device *dev,
+			       struct device_attribute *attr,
+			       char *buf)
+{
+	u32 sw_trigger;
+
+	sw_trigger = utils_sysfs_show_wrapper(dev, RADIO_SW_TRIGGER_ADDR,
+					      RADIO_SW_TRIGGER_OFFSET,
+					      RADIO_SW_TRIGGER_MASK);
+	return sprintf(buf, "%d\n", sw_trigger);
+}
+
+/**
+ * sw_trigger_store - Writes to the SW trigger status register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's SW trigger
+ * value to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t sw_trigger_store(struct device *dev,
+				struct device_attribute *attr,
+				const char *buf, size_t count)
+{
+	u32 sw_trigger;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &sw_trigger);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, RADIO_SW_TRIGGER_ADDR,
+				  RADIO_SW_TRIGGER_OFFSET,
+				  RADIO_SW_TRIGGER_MASK, sw_trigger);
+	return count;
+}
+static DEVICE_ATTR_RW(sw_trigger);
+
+/**
+ * radio_enable_show - Returns the current radio enable status
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the enable status
+ *
+ * Reads and writes the traffic gen's radio enable value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t radio_enable_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	u32 radio_enable;
+
+	radio_enable = utils_sysfs_show_wrapper(dev, RADIO_CDC_ENABLE_ADDR,
+						RADIO_CDC_ENABLE_OFFSET,
+						RADIO_CDC_ENABLE_MASK);
+	return sprintf(buf, "%d\n", radio_enable);
+}
+
+/**
+ * radio_enable_store - Writes to the radio enable register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's radio enable
+ * value to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t radio_enable_store(struct device *dev,
+				  struct device_attribute *attr,
+				  const char *buf, size_t count)
+{
+	u32 radio_enable;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &radio_enable);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, RADIO_CDC_ENABLE_ADDR,
+				  RADIO_CDC_ENABLE_OFFSET,
+				  RADIO_CDC_ENABLE_MASK,
+				  radio_enable);
+	return count;
+}
+static DEVICE_ATTR_RW(radio_enable);
+
+/**
+ * radio_error_show - Returns the current radio error status
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the error status
+ *
+ * Reads and writes the traffic gen's radio error value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t radio_error_show(struct device *dev,
+				struct device_attribute *attr, char *buf)
+{
+	u32 radio_error;
+
+	radio_error = utils_sysfs_show_wrapper(dev, RADIO_CDC_ERROR_ADDR,
+					       RADIO_CDC_STATUS_OFFSET,
+					       RADIO_CDC_STATUS_MASK);
+	return sprintf(buf, "%d\n", radio_error);
+}
+static DEVICE_ATTR_RO(radio_error);
+
+/**
+ * radio_status_show - Returns the current radio status
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the status
+ *
+ * Reads and writes the traffic gen's radio status value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t radio_status_show(struct device *dev,
+				 struct device_attribute *attr, char *buf)
+{
+	u32 radio_status;
+
+	radio_status = utils_sysfs_show_wrapper(dev, RADIO_CDC_STATUS_ADDR,
+						RADIO_CDC_STATUS_OFFSET,
+						RADIO_CDC_STATUS_MASK);
+	return sprintf(buf, "%d\n", radio_status);
+}
+static DEVICE_ATTR_RO(radio_status);
+
+/**
+ * radio_loopback_show - Returns the current radio loopback status
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the loopback status
+ *
+ * Reads and writes the traffic gen's radio loopback value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t radio_loopback_show(struct device *dev,
+				   struct device_attribute *attr, char *buf)
+{
+	u32 radio_loopback;
+
+	radio_loopback = utils_sysfs_show_wrapper(dev,
+						  RADIO_CDC_LOOPBACK_ADDR,
+						  RADIO_CDC_LOOPBACK_OFFSET,
+						  RADIO_CDC_LOOPBACK_MASK);
+	return sprintf(buf, "%d\n", radio_loopback);
+}
+
+/**
+ * radio_loopback_store - Writes to the radio loopback register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's radio loopback
+ * value to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t radio_loopback_store(struct device *dev,
+				    struct device_attribute *attr,
+				    const char *buf, size_t count)
+{
+	u32 radio_loopback;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &radio_loopback);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, RADIO_CDC_LOOPBACK_ADDR,
+				  RADIO_CDC_LOOPBACK_OFFSET,
+				  RADIO_CDC_LOOPBACK_MASK, radio_loopback);
+	return count;
+}
+static DEVICE_ATTR_RW(radio_loopback);
+
+/**
+ * radio_sink_enable_show - Returns the current radio sink enable status
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the loopback status
+ *
+ * Reads and writes the traffic gen's radio sink enable value to the sysfs entry
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t radio_sink_enable_show(struct device *dev,
+				      struct device_attribute *attr, char *buf)
+{
+	u32 sink_enable;
+
+	sink_enable = utils_sysfs_show_wrapper(dev, RADIO_SINK_ENABLE_ADDR,
+					       RADIO_SINK_ENABLE_OFFSET,
+					       RADIO_SINK_ENABLE_MASK);
+	return sprintf(buf, "%d\n", sink_enable);
+}
+
+/**
+ * radio_sink_enable_store - Writes to the radio sink enable register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's radio sink
+ * enable value to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t radio_sink_enable_store(struct device *dev,
+				       struct device_attribute *attr,
+				       const char *buf, size_t count)
+{
+	u32 sink_enable;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &sink_enable);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, RADIO_SINK_ENABLE_ADDR,
+				  RADIO_SINK_ENABLE_OFFSET,
+				  RADIO_SINK_ENABLE_MASK, sink_enable);
+	return count;
+}
+static DEVICE_ATTR_RW(radio_sink_enable);
+
+/**
+ * antenna_status_show - Returns the status for all antennas
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the ID number string
+ *
+ * Returns the traffic gen's status for all antennas
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t antenna_status_show(struct device *dev,
+				   struct device_attribute *attr, char *buf)
+{
+	u32 status_0_31;
+	u32 status_63_32;
+	u32 status_95_64;
+	u32 status_127_96;
+
+	status_0_31 =
+	utils_sysfs_show_wrapper(dev, RADIO_CDC_STATUS_31_0_ADDR,
+				 RADIO_CDC_STATUS_31_0_OFFSET,
+				 lower_32_bits(RADIO_CDC_STATUS_31_0_MASK));
+	status_63_32 =
+	utils_sysfs_show_wrapper(dev, RADIO_CDC_STATUS_63_32_ADDR,
+				 RADIO_CDC_STATUS_63_32_OFFSET,
+				 lower_32_bits(RADIO_CDC_STATUS_63_32_MASK));
+	status_95_64 =
+	utils_sysfs_show_wrapper(dev, RADIO_CDC_STATUS_95_64_ADDR,
+				 RADIO_CDC_STATUS_95_64_OFFSET,
+				 lower_32_bits(RADIO_CDC_STATUS_95_64_MASK));
+	status_127_96 =
+	utils_sysfs_show_wrapper(dev, RADIO_CDC_STATUS_127_96_ADDR,
+				 RADIO_CDC_STATUS_127_96_OFFSET,
+				 lower_32_bits(RADIO_CDC_STATUS_127_96_MASK));
+
+	return sprintf(buf, "0x%08x 0x%08x 0x%08x 0x%08x\n",
+		       status_0_31, status_63_32, status_95_64, status_127_96);
+}
+static DEVICE_ATTR_RO(antenna_status);
+
+/**
+ * antenna_error_show - Returns the error for all antennas
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the ID number string
+ *
+ * Returns the traffic gen's error for all antennas
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t antenna_error_show(struct device *dev,
+				  struct device_attribute *attr, char *buf)
+{
+	u32 error_0_31;
+	u32 error_63_32;
+	u32 error_95_64;
+	u32 error_127_96;
+
+	error_0_31 =
+	utils_sysfs_show_wrapper(dev, RADIO_CDC_ERROR_31_0_ADDR,
+				 RADIO_CDC_ERROR_31_0_OFFSET,
+				 lower_32_bits(RADIO_CDC_ERROR_31_0_MASK));
+	error_63_32 =
+	utils_sysfs_show_wrapper(dev, RADIO_CDC_ERROR_63_32_ADDR,
+				 RADIO_CDC_ERROR_63_32_OFFSET,
+				 lower_32_bits(RADIO_CDC_ERROR_63_32_MASK));
+	error_95_64 =
+	utils_sysfs_show_wrapper(dev, RADIO_CDC_ERROR_95_64_ADDR,
+				 RADIO_CDC_ERROR_95_64_OFFSET,
+				 lower_32_bits(RADIO_CDC_ERROR_95_64_MASK));
+	error_127_96 =
+	utils_sysfs_show_wrapper(dev, RADIO_CDC_ERROR_127_96_ADDR,
+				 RADIO_CDC_ERROR_127_96_OFFSET,
+				 lower_32_bits(RADIO_CDC_ERROR_127_96_MASK));
+
+	return sprintf(buf, "0x%08x 0x%08x 0x%08x 0x%08x\n",
+		       error_0_31, error_63_32, error_95_64, error_127_96);
+}
+static DEVICE_ATTR_RO(antenna_error);
+
+/**
+ * framer_packet_size_show - Returns the size of the framer's packet
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the loopback status
+ *
+ * Reads and writes the traffic gen's framer packet size value
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t framer_packet_size_show(struct device *dev,
+				       struct device_attribute *attr,
+				       char *buf)
+{
+	u32 packet_size;
+
+	packet_size = utils_sysfs_show_wrapper(dev, FRAM_PACKET_DATA_SIZE_ADDR,
+					       FRAM_PACKET_DATA_SIZE_OFFSET,
+					       FRAM_PACKET_DATA_SIZE_MASK);
+	return sprintf(buf, "%d\n", packet_size);
+}
+
+/**
+ * framer_packet_size_store - Writes to the framer's packet size register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's framer packet
+ * size value to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t framer_packet_size_store(struct device *dev,
+					struct device_attribute *attr,
+					const char *buf, size_t count)
+{
+	u32 packet_size;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &packet_size);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, FRAM_PACKET_DATA_SIZE_ADDR,
+				  FRAM_PACKET_DATA_SIZE_OFFSET,
+				  FRAM_PACKET_DATA_SIZE_MASK, packet_size);
+	return count;
+}
+static DEVICE_ATTR_RW(framer_packet_size);
+
+/**
+ * framer_pause_size_show - Returns the size of the framer's pause
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the loopback status
+ *
+ * Reads and writes the traffic gen's framer pause size value
+ *
+ * Return: The number of characters printed on success
+ */
+static ssize_t framer_pause_size_show(struct device *dev,
+				      struct device_attribute *attr, char *buf)
+{
+	u32 pause_size;
+
+	pause_size = utils_sysfs_show_wrapper(dev, FRAM_PAUSE_DATA_SIZE_ADDR,
+					      FRAM_PAUSE_DATA_SIZE_OFFSET,
+					      FRAM_PAUSE_DATA_SIZE_MASK);
+	return sprintf(buf, "%d\n", pause_size);
+}
+
+/**
+ * framer_pause_size_store - Writes to the framer's pause size register
+ * @dev:	The device's structure
+ * @attr:	The attributes of the kernel object
+ * @buf:	The buffer containing the timeout value
+ * @count:	The number of characters typed by the user
+ *
+ * Reads the user input and accordingly writes the traffic gens's framer pause
+ * size value to the sysfs entry
+ *
+ * Return: The number of characters of the entry (count) on success
+ */
+static ssize_t framer_pause_size_store(struct device *dev,
+				       struct device_attribute *attr,
+				       const char *buf, size_t count)
+{
+	u32 pause_size;
+	int ret;
+
+	ret = kstrtouint(buf, 10, &pause_size);
+	if (ret)
+		return ret;
+	utils_sysfs_store_wrapper(dev, FRAM_PAUSE_DATA_SIZE_ADDR,
+				  FRAM_PAUSE_DATA_SIZE_OFFSET,
+				  FRAM_PAUSE_DATA_SIZE_MASK, pause_size);
+	return count;
+}
+static DEVICE_ATTR_RW(framer_pause_size);
+
+static struct attribute *xroe_traffic_gen_attrs[] = {
+	&dev_attr_radio_id.attr,
+	&dev_attr_timeout_enable.attr,
+	&dev_attr_timeout_status.attr,
+	&dev_attr_timeout_value.attr,
+	&dev_attr_ledmode.attr,
+	&dev_attr_ledgpio.attr,
+	&dev_attr_dip_status.attr,
+	&dev_attr_sw_trigger.attr,
+	&dev_attr_radio_enable.attr,
+	&dev_attr_radio_error.attr,
+	&dev_attr_radio_status.attr,
+	&dev_attr_radio_loopback.attr,
+	&dev_attr_radio_sink_enable.attr,
+	&dev_attr_antenna_status.attr,
+	&dev_attr_antenna_error.attr,
+	&dev_attr_framer_packet_size.attr,
+	&dev_attr_framer_pause_size.attr,
+	NULL,
+};
+ATTRIBUTE_GROUPS(xroe_traffic_gen);
+
+/**
+ * xroe_traffic_gen_sysfs_init - Creates the xroe sysfs directory and entries
+ * @dev:	The device's structure
+ *
+ * Return: 0 on success, negative value in case of failure to
+ * create the sysfs group
+ *
+ * Creates the xroetrafficgen sysfs directory and entries
+ */
+int xroe_traffic_gen_sysfs_init(struct device *dev)
+{
+	int ret;
+
+	dev->groups = xroe_traffic_gen_groups;
+	ret = sysfs_create_group(&dev->kobj, *xroe_traffic_gen_groups);
+	if (ret)
+		dev_err(dev, "sysfs creation failed\n");
+
+	return ret;
+}
+
+/**
+ * xroe_traffic_gen_sysfs_exit - Deletes the xroe sysfs directory and entries
+ * @dev:	The device's structure
+ *
+ * Deletes the xroetrafficgen sysfs directory and entries
+ */
+void xroe_traffic_gen_sysfs_exit(struct device *dev)
+{
+	sysfs_remove_group(&dev->kobj, *xroe_traffic_gen_groups);
+}
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroetrafficgen/xroe-traffic-gen.c
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroetrafficgen/xroe-traffic-gen.c	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,124 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/stat.h>
+#include <linux/sysfs.h>
+#include "xroe-traffic-gen.h"
+
+#define DRIVER_NAME "xroe_traffic_gen"
+
+static struct platform_driver xroe_traffic_gen_driver;
+
+/**
+ * xroe_traffic_gen_probe - Probes the device tree to locate the traffic gen
+ * block
+ * @pdev:	The structure containing the device's details
+ *
+ * Probes the device tree to locate the traffic gen block and maps it to
+ * the kernel virtual memory space
+ *
+ * Return: 0 on success or a negative errno on error.
+ */
+static int xroe_traffic_gen_probe(struct platform_device *pdev)
+{
+	struct xroe_traffic_gen_local *lp;
+	struct resource *r_mem; /* IO mem resources */
+	struct device *dev = &pdev->dev;
+
+	lp = devm_kzalloc(&pdev->dev, sizeof(*lp), GFP_KERNEL);
+	if (!lp)
+		return -ENOMEM;
+
+	/* Get iospace for the device */
+	/*
+	 * TODO: Use platform_get_resource_byname() instead when the DT entry
+	 * of the traffic gen block has been finalised (when it gets out of
+	 * the development stage).
+	 */
+	r_mem = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	lp->base_addr = devm_ioremap_resource(&pdev->dev, r_mem);
+	if (IS_ERR(lp->base_addr))
+		return PTR_ERR(lp->base_addr);
+
+	dev_set_drvdata(dev, lp);
+	xroe_traffic_gen_sysfs_init(dev);
+	return 0;
+}
+
+/**
+ * xroe_traffic_gen_remove - Removes the sysfs entries created by the driver
+ * @pdev:	The structure containing the device's details
+ *
+ * Removes the sysfs entries created by the driver
+ *
+ * Return: 0
+ */
+static int xroe_traffic_gen_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+
+	xroe_traffic_gen_sysfs_exit(dev);
+	return 0;
+}
+
+/**
+ * xroe_traffic_gen_init - Registers the driver
+ *
+ * Return: 0 on success, -1 on allocation error
+ *
+ * Registers the traffic gen driver and creates the sysfs entries related
+ * to it
+ */
+static int __init xroe_traffic_gen_init(void)
+{
+	int ret;
+
+	pr_info("XROE traffic generator driver init\n");
+	ret = platform_driver_register(&xroe_traffic_gen_driver);
+	return ret;
+}
+
+/**
+ * xroe_traffic_gen_exit - Destroys the driver
+ *
+ * Unregisters the traffic gen driver
+ */
+static void __exit xroe_traffic_gen_exit(void)
+{
+	platform_driver_unregister(&xroe_traffic_gen_driver);
+	pr_debug("XROE traffic generator driver exit\n");
+}
+
+static const struct of_device_id xroe_traffic_gen_of_match[] = {
+	{ .compatible = "xlnx,roe-traffic-gen-1.0", },
+	{ /* end of list */ },
+};
+MODULE_DEVICE_TABLE(of, xroe_traffic_gen_of_match);
+
+static struct platform_driver xroe_traffic_gen_driver = {
+	.driver = {
+		.name = DRIVER_NAME,
+		.of_match_table	= xroe_traffic_gen_of_match,
+	},
+	.probe = xroe_traffic_gen_probe,
+	.remove = xroe_traffic_gen_remove,
+};
+
+module_init(xroe_traffic_gen_init);
+module_exit(xroe_traffic_gen_exit);
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Xilinx Inc.");
+MODULE_DESCRIPTION("Xilinx Radio over Ethernet Traffic Generator driver");
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/drivers/staging/xroetrafficgen/xroe-traffic-gen.h
--- /dev/null
+++ linux-xlnx-2023.1/drivers/staging/xroetrafficgen/xroe-traffic-gen.h	2023-07-05 08:33:32.312817500 +0900
@@ -0,0 +1,15 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2018 Xilinx, Inc.
+ *
+ * Vasileios Bimpikas <vasileios.bimpikas@xilinx.com>
+ */
+
+struct xroe_traffic_gen_local {
+	void __iomem *base_addr;
+};
+
+enum { XROE_SIZE_MAX = 15 };
+
+int xroe_traffic_gen_sysfs_init(struct device *dev);
+void xroe_traffic_gen_sysfs_exit(struct device *dev);
--- /dev/null
+++ linux-xlnx-2023.1/include/linux/xilinx-hdcp1x-cipher.h	2023-07-05 08:33:36.792262900 +0900
@@ -0,0 +1,105 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Xilinx HDCP1X Cipher driver
+ *
+ * Copyright (C) 2022 Xilinx, Inc.
+ *
+ * Author: Jagadeesh Banisetti <jagadeesh.banisetti@xilinx.com>
+ */
+
+#ifndef __XILINX_HDCP1X_CIPHER_H__
+#define __XILINX_HDCP1X_CIPHER_H__
+
+#include <linux/device.h>
+#include <linux/io.h>
+#include <linux/types.h>
+
+#if IS_ENABLED(CONFIG_XLNX_HDCP1X_CIPHER)
+void *xhdcp1x_cipher_init(struct device *dev, void __iomem *hdcp1x_base);
+int xhdcp1x_cipher_reset(void *ref);
+int xhdcp1x_cipher_enable(void *ref);
+int xhdcp1x_cipher_disable(void *ref);
+int xhdcp1x_cipher_set_num_lanes(void *ref, u8 num_lanes);
+int xhdcp1x_cipher_set_keyselect(void *ref, u8 keyselect);
+int xhdcp1x_cipher_load_bksv(void *ref, u8 *buf);
+int xhdcp1x_cipher_set_remoteksv(void *ref, u64 ksv);
+int xhdcp1x_cipher_get_ro(void *ref, u16 *ro);
+int xhdcp1x_cipher_set_b(void *ref, u64 an, bool is_repeater);
+int xhdcp1x_cipher_is_request_complete(void *ref);
+int xhdcp1x_cipher_set_link_state_check(void *ref, bool is_enabled);
+int xhdcp1x_cipher_get_interrupts(void *ref, u32 *interrupts);
+int xhdcp1x_cipher_is_linkintegrity_failed(void *ref);
+#else
+static inline void *xhdcp1x_cipher_init(struct device *dev,
+					void __iomem *hdcp1x_base)
+{
+	return NULL;
+}
+
+static inline int xhdcp1x_cipher_reset(void *ref)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_enable(void *ref)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_disable(void *ref)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_set_num_lanes(void *ref, u8 num_lanes)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_set_keyselect(void *ref, u8 keyselect)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_load_bksv(void *ref, u8 *buf)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_set_remoteksv(void *ref, u64 ksv)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_get_ro(void *ref, u16 *ro)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_set_b(void *ref, u64 value, bool is_repeater)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_is_request_complete(void *ref)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_set_link_state_check(void *ref, u8 is_enabled)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_get_interrupts(void *ref, u32 *interrupts)
+{
+	return -EINVAL;
+}
+
+static inline int xhdcp1x_cipher_is_linkintegrity_failed(void *ref)
+{
+	return -EINVAL;
+}
+#endif /* CONFIG_XLNX_HDCP1X_CIPHER */
+
+#endif /* __XILINX_HDCP1X_CIPHER_H__ */
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/include/linux/xlnx/xlnx_hdcp2x_cipher.h
--- /dev/null
+++ linux-xlnx-2023.1/include/linux/xlnx/xlnx_hdcp2x_cipher.h	2023-07-05 08:33:36.792262900 +0900
@@ -0,0 +1,93 @@
+/* SPDX-License-Identifier: GPL-2.0*/
+/*
+ * Xilinx HDCP2X Cipher driver
+ *
+ * Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Author: Lakshmi Prasanna Eachuri <lakshmi.prasanna.eachuri@amd.com>
+ */
+
+#ifndef _XLNX_HDCP2X_CIPHER_H_
+#define _XLNX_HDCP2X_CIPHER_H_
+
+#include <linux/types.h>
+
+#define XHDCP2X_CIPHER_VER_BASE			(0 * 64)
+#define XHDCP2X_CIPHER_VER_ID_OFFSET		((XHDCP2X_CIPHER_VER_BASE) + (0 * 4))
+#define XHDCP2X_CIPHER_VER_VERSION_OFFSET	((XHDCP2X_CIPHER_VER_BASE) + (1 * 4))
+
+#define XHDCP2X_CIPHER_REG_BASE			(1 * 64)
+#define XHDCP2X_CIPHER_REG_CTRL_OFFSET		((XHDCP2X_CIPHER_REG_BASE) + (0 * 4))
+#define XHDCP2X_CIPHER_REG_CTRL_SET_OFFSET	((XHDCP2X_CIPHER_REG_BASE) + (1 * 4))
+#define XHDCP2X_CIPHER_REG_CTRL_CLR_OFFSET	((XHDCP2X_CIPHER_REG_BASE) + (2 * 4))
+#define XHDCP2X_CIPHER_REG_STA_OFFSET		((XHDCP2X_CIPHER_REG_BASE) + (3 * 4))
+#define XHDCP2X_CIPHER_REG_KS_1_OFFSET		((XHDCP2X_CIPHER_REG_BASE) + (4 * 4))
+#define XHDCP2X_CIPHER_REG_KS_2_OFFSET		((XHDCP2X_CIPHER_REG_BASE) + (5 * 4))
+#define XHDCP2X_CIPHER_REG_KS_3_OFFSET		((XHDCP2X_CIPHER_REG_BASE) + (6 * 4))
+#define XHDCP2X_CIPHER_REG_KS_4_OFFSET		((XHDCP2X_CIPHER_REG_BASE) + (7 * 4))
+#define XHDCP2X_CIPHER_REG_LC128_1_OFFSET	((XHDCP2X_CIPHER_REG_BASE) + (8 * 4))
+#define XHDCP2X_CIPHER_REG_LC128_2_OFFSET	((XHDCP2X_CIPHER_REG_BASE) + (9 * 4))
+#define XHDCP2X_CIPHER_REG_LC128_3_OFFSET	((XHDCP2X_CIPHER_REG_BASE) + (10 * 4))
+#define XHDCP2X_CIPHER_REG_LC128_4_OFFSET	((XHDCP2X_CIPHER_REG_BASE) + (11 * 4))
+#define XHDCP2X_CIPHER_REG_RIV_1_OFFSET		((XHDCP2X_CIPHER_REG_BASE) + (12 * 4))
+#define XHDCP2X_CIPHER_REG_RIV_2_OFFSET		((XHDCP2X_CIPHER_REG_BASE) + (13 * 4))
+#define XHDCP2X_CIPHER_REG_INPUTCTR_1_OFFSET	((XHDCP2X_CIPHER_REG_BASE) + (14 * 4))
+#define XHDCP2X_CIPHER_REG_INPUTCTR_2_OFFSET	((XHDCP2X_CIPHER_REG_BASE) + (15 * 4))
+
+#define XHDCP2X_CIPHER_REG_CTRL_RUN_MASK	BIT(0)
+#define XHDCP2X_CIPHER_REG_CTRL_IE_MASK		BIT(1)
+#define XHDCP2X_CIPHER_REG_CTRL_ENCRYPT_MASK	BIT(3)
+#define XHDCP2X_CIPHER_REG_CTRL_BLANK_MASK	BIT(4)
+#define XHDCP2X_CIPHER_REG_CTRL_NOISE_MASK	BIT(5)
+#define XHDCP2X_CIPHER_REG_CTRL_LANE_CNT_MASK	GENMASK(9, 6)
+#define XHDCP2X_CIPHER_REG_CTRL_LANE_CNT_BIT_POS	6
+
+#define XHDCP2X_CIPHER_REG_STA_IRQ_MASK		BIT(0)
+#define XHDCP2X_CIPHER_REG_STA_EVT_MASK		BIT(1)
+#define XHDCP2X_CIPHER_REG_STA_ENCRYPTED_MASK	BIT(2)
+#define XHDCP2X_CIPHER_REG_CTRL_MODE_MASK	BIT(2)
+
+#define XHDCP2X_CIPHER_KEY_LENGTH		16
+#define XHDCP2X_CIPHER_SHIFT_16			16
+#define XHDCP2X_CIPHER_MASK_16			GENMASK(15, 0)
+#define XHDCP2X_CIPHER_VER_ID			0x2200
+
+/**
+ * struct xlnx_hdcp2x_cipher_hw - HDCP2X internal cipher engine hardware structure
+ * @cipher_coreaddress: HDCP2X cipher core address
+ */
+struct xlnx_hdcp2x_cipher_hw {
+	void __iomem *cipher_coreaddress;
+};
+
+#define xlnx_hdcp2x_cipher_write(coreaddress, reg_offset, data) \
+	writel(data, (coreaddress) + (reg_offset))
+
+#define xlnx_hdcp2x_cipher_read(coreaddress, reg_offset) \
+	readl((coreaddress) + (reg_offset))
+
+#define xlnx_hdcp2x_cipher_get_status(cipher_address) \
+	xlnx_hdcp2x_cipher_read(cipher_address, XHDCP2X_CIPHER_REG_STA_OFFSET)
+
+#define xlnx_hdcp2x_cipher_enable(cipher_address) \
+	xlnx_hdcp2x_cipher_write(cipher_address, \
+	(XHDCP2X_CIPHER_REG_CTRL_SET_OFFSET), (XHDCP2X_CIPHER_REG_CTRL_RUN_MASK))
+
+#define xlnx_hdcp2x_cipher_disable(cipher_address) \
+		xlnx_hdcp2x_cipher_write(cipher_address, \
+		(XHDCP2X_CIPHER_REG_CTRL_CLR_OFFSET), (XHDCP2X_CIPHER_REG_CTRL_RUN_MASK))
+
+#define xlnx_hdcp2x_cipher_set_txmode(cipher_address) \
+	xlnx_hdcp2x_cipher_write(cipher_address, \
+	(XHDCP2X_CIPHER_REG_CTRL_CLR_OFFSET), (XHDCP2X_CIPHER_REG_CTRL_MODE_MASK))
+
+int xlnx_hdcp2x_cipher_cfg_init(struct xlnx_hdcp2x_cipher_hw *cipher_cfg);
+void xlnx_hdcp2x_cipher_set_keys(struct xlnx_hdcp2x_cipher_hw *cipher_cfg,
+				 const u8 *buf, u32 offset, u16 len);
+void xlnx_hdcp2x_cipher_set_lanecount(struct xlnx_hdcp2x_cipher_hw *cipher_cfg,
+				      u8 lanecount);
+void xlnx_hdcp2x_cipher_init(struct xlnx_hdcp2x_cipher_hw *cipher_cfg);
+void  xlnx_hdcp2x_tx_cipher_update_encryption(struct xlnx_hdcp2x_cipher_hw *cipher_cfg,
+					      u8 enable);
+
+#endif
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/include/linux/xlnx/xlnx_hdcp_common.h
--- /dev/null
+++ linux-xlnx-2023.1/include/linux/xlnx/xlnx_hdcp_common.h	2023-07-05 08:33:36.792262900 +0900
@@ -0,0 +1,22 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Xilinx HDCP Common driver
+ *
+ * Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Author: Lakshmi Prasanna Eachuri <lakshmi.prasanna.eachuri@amd.com>
+ */
+
+#ifndef _XLNX_HDCP_COMMON_H_
+#define _XLNX_HDCP_COMMON_H_
+
+#include <linux/types.h>
+
+int mp_mod_exp(unsigned int y[], const unsigned int x[], const unsigned int n[],
+	       unsigned int d[], size_t ndigits);
+size_t mp_conv_to_octets(const unsigned int a[], size_t ndigits, unsigned char *c,
+			 size_t nbytes);
+size_t mp_conv_from_octets(unsigned int a[], size_t ndigits, const unsigned char *c,
+			   size_t nbytes);
+
+#endif
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/include/linux/xlnx/xlnx_hdcp_rng.h
--- /dev/null
+++ linux-xlnx-2023.1/include/linux/xlnx/xlnx_hdcp_rng.h	2023-07-05 08:33:36.792262900 +0900
@@ -0,0 +1,43 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Xilinx HDCP 2X Random Number Generator Driver
+ *
+ * Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Author: Lakshmi Prasanna Eachuri <lakshmi.prasanna.eachuri@amd.com>
+ */
+
+#ifndef _XLNX_HDCP_RNG_H_
+#define _XLNX_HDCP_RNG_H_
+
+#include <linux/types.h>
+
+/**
+ * struct xlnx_hdcp2x_rng_hw - HDCP 2X random number generator configuration structure
+ * @rng_coreaddress: HDCP 2X random number generator core address
+ */
+struct xlnx_hdcp2x_rng_hw {
+	void __iomem *rng_coreaddress;
+};
+
+#define XHDCP2X_RNG_VER_BASE			(0 * 64)
+#define XHDCP2X_RNG_VER_ID_OFFSET		((XHDCP2X_RNG_VER_BASE) + (0 * 4))
+#define XHDCP2X_RNG_VER_VERSION_OFFSET		((XHDCP2X_RNG_VER_BASE) + (1 * 4))
+#define XHDCP2X_RNG_REG_BASE			(1 * 64)
+#define XHDCP2X_RNG_REG_CTRL_OFFSET		((XHDCP2X_RNG_REG_BASE) + (0 * 4))
+#define XHDCP2X_RNG_REG_CTRL_SET_OFFSET		((XHDCP2X_RNG_REG_BASE) + (1 * 4))
+#define XHDCP2X_RNG_REG_CTRL_CLR_OFFSET		((XHDCP2X_RNG_REG_BASE) + (2 * 4))
+#define XHDCP2X_RNG_REG_STA_OFFSET		((XHDCP2X_RNG_REG_BASE) + (3 * 4))
+#define XHDCP2X_RNG_REG_RN_1_OFFSET		((XHDCP2X_RNG_REG_BASE) + (4 * 4))
+#define XHDCP2X_RNG_SHIFT_16			16
+#define XHDCP2X_RNG_MASK_16			GENMASK(15, 0)
+#define XHDCP2X_RNG_VER_ID			0x2200
+#define XHDCP2X_RNG_REG_CTRL_RUN_MASK		BIT(0)
+
+int xlnx_hdcp2x_rng_cfg_init(struct xlnx_hdcp2x_rng_hw *rng_cfg);
+void xlnx_hdcp2x_rng_get_random_number(struct xlnx_hdcp2x_rng_hw *rng_cfg,
+				       u8 *writeptr, u16 length, u16 randomlength);
+void xlnx_hdcp2x_rng_enable(struct xlnx_hdcp2x_rng_hw *rng_cfg);
+void xlnx_hdcp2x_rng_disable(struct xlnx_hdcp2x_rng_hw *rng_cfg);
+
+#endif
diff -urN '--label=/dev/null' /dev/null linux-xlnx-2023.1/include/linux/xlnx/xlnx_timer.h
--- /dev/null
+++ linux-xlnx-2023.1/include/linux/xlnx/xlnx_timer.h	2023-07-05 08:33:36.792262900 +0900
@@ -0,0 +1,108 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * The Xilinx timer/counter component. This component supports the Xilinx
+ * timer/counter which supports the following features:
+ *  - Polled mode.
+ *  - Interrupt driven mode
+ *  - enabling and disabling specific timers
+ *  - PWM operation
+ *  - Cascade Operation
+ *
+ * Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Author: Lakshmi Prasanna Eachuri <lakshmi.prasanna.eachuri@amd.com>
+ */
+
+#ifndef _XLNX_TIMER_H_
+#define _XLNX_TIMER_H_
+
+#include <linux/types.h>
+
+/**
+ * struct xlnx_hdcp_timer_hw - This structure contains hardware subcore configuration
+ * information about AXI Timer.
+ * @coreaddress: AXI Timer core address
+ * @sys_clock_freq: System Clock Frequency
+ */
+struct xlnx_hdcp_timer_hw {
+	void __iomem *coreaddress;
+	u32 sys_clock_freq;
+};
+
+/*
+ * Detailed register descriptions available in
+ * Programming Guide PG079.
+ * https://docs.xilinx.com/v/u/en-US/pg079-axi-timer
+ */
+#define XTC_DEVICE_TIMER_COUNT		2
+#define XTC_TIMER_COUNTER_OFFSET	16
+#define XTC_CASCADE_MODE_OPTION		BIT(7)
+#define XTC_ENABLE_ALL_OPTION		BIT(6)
+#define XTC_DOWN_COUNT_OPTION		BIT(5)
+#define XTC_CAPTURE_MODE_OPTION		BIT(4)
+#define XTC_INT_MODE_OPTION		BIT(3)
+#define XTC_AUTO_RELOAD_OPTION		BIT(2)
+#define XTC_EXT_COMPARE_OPTION		BIT(1)
+#define XTC_TIMER_0			0
+#define XTC_TIMER_1			1
+
+#define XTC_TCSR_OFFSET			0
+#define XTC_TLR_OFFSET			4
+#define XTC_TCR_OFFSET			8
+#define XTC_CSR_CASC_MASK		BIT(11)
+#define XTC_CSR_ENABLE_ALL_MASK		BIT(10)
+#define XTC_CSR_ENABLE_PWM_MASK		BIT(9)
+#define XTC_CSR_INT_OCCURED_MASK	BIT(8)
+#define XTC_CSR_ENABLE_TMR_MASK		BIT(7)
+#define XTC_CSR_ENABLE_INT_MASK		BIT(6)
+#define XTC_CSR_LOAD_MASK		BIT(5)
+#define XTC_CSR_AUTO_RELOAD_MASK	BIT(4)
+#define XTC_CSR_EXT_CAPTURE_MASK	BIT(3)
+#define XTC_CSR_EXT_GENERATE_MASK	BIT(2)
+#define XTC_CSR_DOWN_COUNT_MASK		BIT(1)
+#define XTC_CSR_CAPTURE_MODE_MASK	BIT(0)
+#define XTC_MAX_LOAD_VALUE		GENMASK(31, 0)
+#define XTC_COMPONENT_IS_READY		BIT(0)
+#define XTC_COMPONENT_IS_STARTED	BIT(1)
+
+typedef void (*xlnx_timer_cntr_handler) (void *callbackref, u8 tmr_cntr_number);
+
+/**
+ * struct xlnx_hdcp_timer_config - The user is required to allocate a
+ * variable of this type for every timer/counter device in the system.
+ * @hw_config: Configuration of timer hardware core
+ * @handler: Timer callback handler
+ * @callbackref: Timer callback reference
+ * @is_tmrcntr0_started: Timercnt0 is initialized and started
+ * @is_tmrcntr1_started: Timercnt1 is initialized and started
+ */
+struct xlnx_hdcp_timer_config {
+	struct xlnx_hdcp_timer_hw hw_config;
+	xlnx_timer_cntr_handler handler;
+	void *callbackref;
+	u32 is_tmrcntr0_started;
+	u32 is_tmrcntr1_started;
+};
+
+u32 xlnx_hdcp_tmrcntr_get_value(struct xlnx_hdcp_timer_config *xtimercntr,
+				u8 tmr_cntr_number);
+int xlnx_hdcp_tmrcntr_init(struct xlnx_hdcp_timer_config *xtimercntr);
+void xlnx_hdcp_tmrcntr_stop(struct xlnx_hdcp_timer_config *xtimercntr,
+			    u8 tmr_cntr_number);
+void xlnx_hdcp_tmrcntr_start(struct xlnx_hdcp_timer_config *xtimercntr,
+			     u8 tmr_cntr_number);
+void xlnx_hdcp_tmrcntr_cfg_init(struct xlnx_hdcp_timer_config *xtimercntr);
+void xlnx_hdcp_tmrcntr_start(struct xlnx_hdcp_timer_config *xtimercntr,
+			     u8 tmr_cntr_number);
+void xlnx_hdcp_tmrcntr_set_reset_value(struct xlnx_hdcp_timer_config *xtimercntr,
+				       u8 tmr_cntr_number, u32 reset_value);
+void xlnx_hdcp_tmrcntr_set_options(struct xlnx_hdcp_timer_config *xtimercntr,
+				   u8 tmr_cntr_number, u32 options);
+void xlnx_hdcp_tmrcntr_set_handler(struct xlnx_hdcp_timer_config *xtimercntr,
+				   xlnx_timer_cntr_handler funcptr,
+				   void *callbackref);
+void xlnx_hdcp_tmrcntr_interrupt_handler(struct xlnx_hdcp_timer_config *xtimercntr);
+void xlnx_hdcp_tmrcntr_reset(struct xlnx_hdcp_timer_config *xtimercntr,
+			     u8 tmr_cntr_number);
+
+#endif
--- /dev/null
+++ linux-xlnx-2023.1/include/uapi/linux/xlnx_mpg2tsmux_interface.h	2023-07-05 08:33:37.843151200 +0900
@@ -0,0 +1,253 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+
+/*
+ * Xilinx mpeg2 transport stream muxer ioctl calls
+ *
+ * Copyright (C) 2019 Xilinx, Inc.
+ *
+ * Author:	Venkateshwar Rao G <venkateshwar.rao.gannava@xilinx.com>
+ */
+
+#ifndef __XLNX_MPG2TSMUX_INTERFACE_H__
+#define __XLNX_MPG2TSMUX_INTERFACE_H__
+
+#include <linux/types.h>
+#include <linux/ioctl.h>
+
+/**
+ * enum ts_mux_command - command for stream context
+ * @CREATE_TS_MISC: create misc
+ * @CREATE_TS_VIDEO_KEYFRAME: create video key frame
+ * @CREATE_TS_VIDEO_NON_KEYFRAME: create non key frame
+ * @CREATE_TS_AUDIO: create audio
+ * @WRITE_PAT: write pat
+ * @WRITE_PMT: write pmt
+ * @WRITE_SI: write si
+ * @INVALID: invalid
+ */
+enum ts_mux_command {
+	CREATE_TS_MISC = 0,
+	CREATE_TS_VIDEO_KEYFRAME,
+	CREATE_TS_VIDEO_NON_KEYFRAME,
+	CREATE_TS_AUDIO,
+	WRITE_PAT,
+	WRITE_PMT,
+	WRITE_SI,
+	INVALID
+};
+
+/**
+ * struct stream_context_in - struct to enqueue a stream context descriptor
+ * @command: stream context type
+ * @stream_id: stream identification number
+ * @extended_stream_id: extended stream id
+ * @is_pcr_stream: flag for pcr stream
+ * @is_valid_pts: flag for valid pts
+ * @is_valid_dts: flag for valid dts
+ * @is_dmabuf: flag to set if external src buffer is DMA allocated
+ * @pid: packet id number
+ * @size_data_in: size in bytes of input buffer
+ * @pts: presentation time stamp
+ * @dts: display time stamp
+ * @srcbuf_id: source buffer id after mmap
+ * @insert_pcr: flag for inserting pcr in stream context
+ * @pcr_extension: pcr extension number
+ * @pcr_base: pcr base number
+ */
+struct stream_context_in {
+	enum ts_mux_command command;
+	__u8 stream_id;
+	__u8 extended_stream_id;
+	int is_pcr_stream;
+	int is_valid_pts;
+	int is_valid_dts;
+	int is_dmabuf;
+	__u16 pid;
+	__u64 size_data_in;
+	__u64 pts;
+	__u64 dts;
+	__u32 srcbuf_id;
+	int insert_pcr;
+	__u16 pcr_extension;
+	__u64 pcr_base;
+};
+
+/**
+ * struct mux_context_in - struct to enqueue a mux context descriptor
+ * @is_dmabuf: flag to set if external src buffer is DMA allocated
+ * @dstbuf_id: destination buffer id after mmap
+ * @dmabuf_size: size in bytes of output buffer
+ */
+struct muxer_context_in {
+	int is_dmabuf;
+	__u32 dstbuf_id;
+	__u32 dmabuf_size;
+};
+
+/**
+ * enum xlnx_tsmux_status - ip status
+ * @MPG2MUX_BUSY: device busy
+ * @MPG2MUX_READY: device ready
+ * @MPG2MUX_ERROR: error state
+ */
+enum xlnx_tsmux_status {
+	MPG2MUX_BUSY = 0,
+	MPG2MUX_READY,
+	MPG2MUX_ERROR
+};
+
+/**
+ * struct strc_bufs_info - struct to specify bufs requirement
+ * @num_buf: number of buffers
+ * @buf_size: size of each buffer
+ */
+struct strc_bufs_info {
+	__u32 num_buf;
+	__u32 buf_size;
+};
+
+/**
+ * struct strc_out_buf - struct to get output buffer info
+ * @buf_id: buf id into which output is written
+ * @buf_write: output bytes written in buf
+ */
+struct out_buffer {
+	__u32 buf_id;
+	__u32 buf_write;
+};
+
+/**
+ * enum strmtbl_cnxt - streamid table operation
+ * @NO_UPDATE: no table update
+ * @ADD_TO_TBL: add the entry to table
+ * @DEL_FR_TBL: delete the entry from table
+ */
+enum strmtbl_cnxt {
+	NO_UPDATE = 0,
+	ADD_TO_TBL,
+	DEL_FR_TBL,
+};
+
+/**
+ * struct strm_tbl_info - struct to enqueue/dequeue streamid in table
+ * @strmtbl_ctxt: enqueue/dequeue stream id
+ * @pid: stream id
+ */
+struct strc_strminfo {
+	enum strmtbl_cnxt strmtbl_ctxt;
+	__u16 pid;
+};
+
+/**
+ * enum xlnx_tsmux_dma_dir - dma direction
+ * @DMA_TO_MPG2MUX: memory to device
+ * @DMA_FROM_MPG2MUX: device to memory
+ */
+enum xlnx_tsmux_dma_dir {
+	DMA_TO_MPG2MUX = 1,
+	DMA_FROM_MPG2MUX,
+};
+
+/**
+ * enum xlnx_tsmux_dmabuf_flags - dma buffer handling
+ * @DMABUF_ERROR: buffer error
+ * @DMABUF_CONTIG: contig buffer
+ * @DMABUF_NON_CONTIG: non contigs buffer
+ * @DMABUF_ATTACHED: buffer attached
+ */
+enum xlnx_tsmux_dmabuf_flags {
+	DMABUF_ERROR = 1,
+	DMABUF_CONTIG = 2,
+	DMABUF_NON_CONTIG = 4,
+	DMABUF_ATTACHED = 8,
+};
+
+/**
+ * struct xlnx_tsmux_dmabuf_info - struct to verify dma buf before enque
+ * @buf_fd: file descriptor
+ * @dir: direction of the dma buffer
+ * @flags: flags returned by the driver
+ */
+struct xlnx_tsmux_dmabuf_info {
+	int buf_fd;
+	enum xlnx_tsmux_dma_dir dir;
+	enum xlnx_tsmux_dmabuf_flags flags;
+};
+
+/* MPG2MUX IOCTL CALL LIST */
+
+#define MPG2MUX_MAGIC 'M'
+
+/**
+ * MPG2MUX_INBUFALLOC - src buffer allocation
+ */
+#define MPG2MUX_INBUFALLOC _IOWR(MPG2MUX_MAGIC, 1, struct strc_bufs_info *)
+
+/**
+ * MPG2MUX_INBUFDEALLOC - deallocates the all src buffers
+ */
+#define MPG2MUX_INBUFDEALLOC _IO(MPG2MUX_MAGIC, 2)
+
+/**
+ * MPG2MUX_OUTBUFALLOC - allocates DMA able memory for dst
+ */
+#define MPG2MUX_OUTBUFALLOC _IOWR(MPG2MUX_MAGIC, 3, struct strc_bufs_info *)
+
+/**
+ * MPG2MUX_OUTBUFDEALLOC - deallocates the all dst buffers allocated
+ */
+#define MPG2MUX_OUTBUFDEALLOC _IO(MPG2MUX_MAGIC, 4)
+
+/**
+ * MPG2MUX_STBLALLOC - allocates DMA able memory for streamid table
+ */
+#define MPG2MUX_STBLALLOC _IOW(MPG2MUX_MAGIC, 5, unsigned short *)
+
+/**
+ * MPG2MUX_STBLDEALLOC - deallocates streamid table memory
+ */
+#define MPG2MUX_STBLDEALLOC _IO(MPG2MUX_MAGIC, 6)
+
+/**
+ * MPG2MUX_TBLUPDATE - enqueue or dequeue in streamid table
+ */
+#define MPG2MUX_TBLUPDATE _IOW(MPG2MUX_MAGIC, 7, struct strc_strminfo *)
+
+/**
+ * MPG2MUX_SETSTRM - enqueue a stream descriptor in stream context
+ *		linked list along with src buf address
+ */
+#define MPG2MUX_SETSTRM _IOW(MPG2MUX_MAGIC, 8, struct stream_context_in *)
+
+/**
+ * MPG2MUX_START - starts muxer IP after configuring stream
+ *		and mux context registers
+ */
+#define MPG2MUX_START _IO(MPG2MUX_MAGIC, 9)
+
+/**
+ * MPG2MUX_STOP - stops the muxer IP
+ */
+#define MPG2MUX_STOP _IO(MPG2MUX_MAGIC, 10)
+
+/**
+ * MPG2MUX_STATUS - command to get the status of IP
+ */
+#define MPG2MUX_STATUS _IOR(MPG2MUX_MAGIC, 11, unsigned short *)
+
+/**
+ * MPG2MUX_GETOUTBUF - get the output buffer id with size of output data
+ */
+#define MPG2MUX_GETOUTBUF _IOW(MPG2MUX_MAGIC, 12, struct out_buffer *)
+
+/**
+ * MPG2MUX_SETMUX - enqueue a mux descriptor with dst buf address
+ */
+#define MPG2MUX_SETMUX _IOW(MPG2MUX_MAGIC, 13, struct muxer_context_in *)
+
+/**
+ * MPG2MUX_VRFY_DMABUF - status of a given dma buffer fd
+ */
+#define MPG2MUX_VDBUF _IOWR(MPG2MUX_MAGIC, 14, struct xlnx_tsmux_dmabuf_info *)
+
+#endif
--- /dev/null
+++ linux-xlnx-2023.1/include/uapi/linux/xlnxsync.h	2023-07-05 08:33:37.843151200 +0900
@@ -0,0 +1,179 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+
+#ifndef __XLNXSYNC_H__
+#define __XLNXSYNC_H__
+
+#include <linux/types.h>
+
+#define XLNXSYNC_IOCTL_HDR_VER		0x10004
+
+/*
+ * This is set in the fb_id of struct xlnxsync_chan_config when
+ * configuring the channel. This makes the driver auto search for
+ * a free framebuffer slot.
+ */
+#define XLNXSYNC_AUTO_SEARCH		0xFF
+
+#define XLNXSYNC_MAX_ENC_CHAN		4
+#define XLNXSYNC_MAX_DEC_CHAN		2
+#define XLNXSYNC_BUF_PER_CHAN		3
+
+#define XLNXSYNC_PROD			0
+#define XLNXSYNC_CONS			1
+#define XLNXSYNC_IO			2
+
+#define XLNXSYNC_MAX_CORES		4
+
+/**
+ * struct xlnxsync_err_intr - Channel error interrupt types
+ * @prod_sync: Producer synchronization error interrupt
+ * @prod_wdg: Producer watchdog interrupt
+ * @cons_sync: Consumer synchronization error interrupt
+ * @cons_wdg: Consumer watchdog interrupt
+ * @ldiff: Luma buffer difference interrupt
+ * @cdiff: Chroma buffer difference interrupt
+ */
+struct xlnxsync_err_intr {
+	__u8 prod_sync : 1;
+	__u8 prod_wdg : 1;
+	__u8 cons_sync : 1;
+	__u8 cons_wdg : 1;
+	__u8 ldiff : 1;
+	__u8 cdiff : 1;
+};
+
+/**
+ * struct xlnxsync_intr - Channel Interrupt types
+ * @hdr_ver: IOCTL header version
+ * @err: Structure for error interrupts
+ * @prod_lfbdone: Producer luma frame buffer done interrupt
+ * @prod_cfbdone: Producer chroma frame buffer done interrupt
+ * @cons_lfbdone: Consumer luma frame buffer done interrupt
+ * @cons_cfbdone: Consumer chroma frame buffer done interrupt
+ */
+struct xlnxsync_intr {
+	__u64 hdr_ver;
+	struct xlnxsync_err_intr err;
+	__u8 prod_lfbdone : 1;
+	__u8 prod_cfbdone : 1;
+	__u8 cons_lfbdone : 1;
+	__u8 cons_cfbdone : 1;
+};
+
+/**
+ * struct xlnxsync_chan_config - Synchronizer channel configuration struct
+ * @hdr_ver: IOCTL header version
+ * @luma_start_offset: Start offset of Luma buffer
+ * @chroma_start_offset: Start offset of Chroma buffer
+ * @luma_end_offset: End offset of Luma buffer
+ * @chroma_end_offset: End offset of Chroma buffer
+ * @luma_margin: Margin for Luma buffer
+ * @chroma_margin: Margin for Chroma buffer
+ * @luma_core_offset: Array of 4 offsets for luma
+ * @chroma_core_offset: Array of 4 offsets for chroma
+ * @dma_fd: File descriptor of dma
+ * @fb_id: Framebuffer index. Valid values 0/1/2/XLNXSYNC_AUTO_SEARCH
+ * @ismono: Flag to indicate if buffer is Luma only.
+ * Valid 0..3 & XLNXSYNC_AUTO_SEARCH
+ *
+ * This structure contains the configuration for monitoring a particular
+ * framebuffer on a particular channel.
+ */
+struct xlnxsync_chan_config {
+	__u64 hdr_ver;
+	__u64 luma_start_offset[XLNXSYNC_IO];
+	__u64 chroma_start_offset[XLNXSYNC_IO];
+	__u64 luma_end_offset[XLNXSYNC_IO];
+	__u64 chroma_end_offset[XLNXSYNC_IO];
+	__u32 luma_margin;
+	__u32 chroma_margin;
+	__u32 luma_core_offset[XLNXSYNC_MAX_CORES];
+	__u32 chroma_core_offset[XLNXSYNC_MAX_CORES];
+	__u32 dma_fd;
+	__u8 fb_id[XLNXSYNC_IO];
+	__u8 ismono[XLNXSYNC_IO];
+};
+
+/**
+ * struct xlnxsync_clr_err - Clear channel error
+ * @hdr_ver: IOCTL header version
+ * @err: Structure for error interrupts
+ */
+struct xlnxsync_clr_err {
+	__u64 hdr_ver;
+	struct xlnxsync_err_intr err;
+};
+
+/**
+ * struct xlnxsync_fbdone - Framebuffer Done
+ * @hdr_ver: IOCTL header version
+ * @status: Framebuffer Done status
+ */
+struct xlnxsync_fbdone {
+	__u64 hdr_ver;
+	__u8 status[XLNXSYNC_BUF_PER_CHAN][XLNXSYNC_IO];
+};
+
+/**
+ * struct xlnxsync_config - Synchronizer IP configuration
+ * @hdr_ver: IOCTL header version
+ * @encode: true if encoder type, false for decoder type
+ * @max_channels: Maximum channels this IP supports
+ * @active_channels: Number of active IP channels
+ * @reserved_id: Reserved channel ID for instance
+ */
+struct xlnxsync_config {
+	__u64	hdr_ver;
+	__u8	encode;
+	__u8	max_channels;
+	__u8	active_channels;
+	__u8	reserved_id;
+	__u32	reserved[10];
+};
+
+/**
+ * struct xlnxsync_stat - Sync IP channel status
+ * @hdr_ver: IOCTL header version
+ * @fbdone: for every pair of luma/chroma buffer for every producer/consumer
+ * @enable: channel enable
+ * @err: Structure for error interrupts
+ */
+struct xlnxsync_stat {
+	__u64 hdr_ver;
+	__u8 fbdone[XLNXSYNC_BUF_PER_CHAN][XLNXSYNC_IO];
+	__u8 enable;
+	struct xlnxsync_err_intr err;
+};
+
+#define XLNXSYNC_MAGIC			'X'
+
+/*
+ * This ioctl is used to get the IP config (i.e. encode / decode)
+ * and max number of channels
+ */
+#define XLNXSYNC_GET_CFG		_IOR(XLNXSYNC_MAGIC, 1,\
+					     struct xlnxsync_config *)
+/* This ioctl is used to get the channel status */
+#define XLNXSYNC_CHAN_GET_STATUS	_IOR(XLNXSYNC_MAGIC, 2, __u32 *)
+/* This is used to set the framebuffer address for a channel */
+#define XLNXSYNC_CHAN_SET_CONFIG	_IOW(XLNXSYNC_MAGIC, 3,\
+					     struct xlnxsync_chan_config *)
+/* Enable a channel. */
+#define XLNXSYNC_CHAN_ENABLE		_IO(XLNXSYNC_MAGIC, 4)
+/* Disable a channel. */
+#define XLNXSYNC_CHAN_DISABLE		_IO(XLNXSYNC_MAGIC, 5)
+/* This is used to clear the Sync and Watchdog errors  for a channel */
+#define XLNXSYNC_CHAN_CLR_ERR		_IOW(XLNXSYNC_MAGIC, 6,\
+					     struct xlnxsync_clr_err *)
+/* This is used to get the framebuffer done status for a channel */
+#define XLNXSYNC_CHAN_GET_FBDONE_STAT	_IOR(XLNXSYNC_MAGIC, 7,\
+					     struct xlnxsync_fbdone *)
+/* This is used to clear the framebuffer done status for a channel */
+#define XLNXSYNC_CHAN_CLR_FBDONE_STAT	_IOW(XLNXSYNC_MAGIC, 8,\
+					     struct xlnxsync_fbdone *)
+/* This is used to set interrupt mask */
+#define XLNXSYNC_CHAN_SET_INTR_MASK	_IOW(XLNXSYNC_MAGIC, 9,\
+					     struct xlnxsync_intr *)
+/* This is used to reset the last programmed slot */
+#define XLNXSYNC_RESET_SLOT		_IO(XLNXSYNC_MAGIC, 10)
+#endif
