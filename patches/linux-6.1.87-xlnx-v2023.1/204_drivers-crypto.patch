diff --git a/Documentation/devicetree/bindings/crypto/xlnx,zynqmp-aes.yaml b/Documentation/devicetree/bindings/crypto/xlnx,zynqmp-aes.yaml
index 9e8fbd02b..6252c18bf 100644
--- a/Documentation/devicetree/bindings/crypto/xlnx,zynqmp-aes.yaml
+++ b/Documentation/devicetree/bindings/crypto/xlnx,zynqmp-aes.yaml
@@ -10,6 +10,8 @@ maintainers:
   - Kalyani Akula <kalyani.akula@xilinx.com>
   - Michal Simek <michal.simek@xilinx.com>
 
+deprecated: true
+
 description: |
   The ZynqMP AES-GCM hardened cryptographic accelerator is used to
   encrypt or decrypt the data with provided key and initialization vector.
diff --git a/Documentation/devicetree/bindings/crypto/zynqmp-rsa.txt b/Documentation/devicetree/bindings/crypto/zynqmp-rsa.txt
new file mode 100644
index 000000000..564392c20
--- /dev/null
+++ b/Documentation/devicetree/bindings/crypto/zynqmp-rsa.txt
@@ -0,0 +1,12 @@
+Xilinx ZynqMP RSA hw acceleration support
+
+The zynqmp PS-RSA hw accelerator is used to encrypt/decrypt
+the given user data.
+
+Required properties:
+- compatible:	should contain "xlnx,zynqmp-rsa" (deprecated)
+
+Example:
+	xlnx_rsa: zynqmp_rsa {
+		compatible = "xlnx,zynqmp-rsa";
+	};
diff --git a/drivers/crypto/Kconfig b/drivers/crypto/Kconfig
index db242234c..f107f7922 100644
--- a/drivers/crypto/Kconfig
+++ b/drivers/crypto/Kconfig
@@ -686,12 +686,25 @@ config CRYPTO_DEV_ROCKCHIP
 	  This driver interfaces with the hardware crypto accelerator.
 	  Supporting cbc/ecb chainmode, and aes/des/des3_ede cipher mode.
 
+config CRYPTO_DEV_XILINX_RSA
+	tristate "Support for Xilinx ZynqMP RSA hw accelerator"
+	depends on ARCH_ZYNQMP || COMPILE_TEST
+	select CRYPTO_AES
+	select CRYPTO_BLKCIPHER
+	help
+	  Xilinx processors have RSA hw accelerator used for signature
+	  generation and verification. This driver interfaces with RSA
+	  hw accelerator. Select this if you want to use the ZynqMP module
+	  for RSA algorithms.
+
 config CRYPTO_DEV_ZYNQMP_AES
 	tristate "Support for Xilinx ZynqMP AES hw accelerator"
 	depends on ZYNQMP_FIRMWARE || COMPILE_TEST
 	select CRYPTO_AES
 	select CRYPTO_ENGINE
 	select CRYPTO_AEAD
+	select CRYPTO_GCM
+	select CRYPTO_USER_API_AEAD
 	help
 	  Xilinx ZynqMP has AES-GCM engine used for symmetric key
 	  encryption and decryption. This driver interfaces with AES hw
diff --git a/drivers/crypto/xilinx/Makefile b/drivers/crypto/xilinx/Makefile
index 730feff5b..21c67f9a2 100644
--- a/drivers/crypto/xilinx/Makefile
+++ b/drivers/crypto/xilinx/Makefile
@@ -1,3 +1,4 @@
 # SPDX-License-Identifier: GPL-2.0-only
 obj-$(CONFIG_CRYPTO_DEV_ZYNQMP_AES) += zynqmp-aes-gcm.o
 obj-$(CONFIG_CRYPTO_DEV_ZYNQMP_SHA3) += zynqmp-sha.o
+obj-$(CONFIG_CRYPTO_DEV_XILINX_RSA) += zynqmp-rsa.o
diff --git a/drivers/crypto/xilinx/zynqmp-aes-gcm.c b/drivers/crypto/xilinx/zynqmp-aes-gcm.c
index 74bd3eb63..e907c8472 100644
--- a/drivers/crypto/xilinx/zynqmp-aes-gcm.c
+++ b/drivers/crypto/xilinx/zynqmp-aes-gcm.c
@@ -1,7 +1,8 @@
 // SPDX-License-Identifier: GPL-2.0
 /*
  * Xilinx ZynqMP AES Driver.
- * Copyright (c) 2020 Xilinx Inc.
+ * Copyright (C) 2020 - 2022 Xilinx Inc.
+ * Copyright (C) 2022 - 2023, Advanced Micro Devices, Inc.
  */
 
 #include <crypto/aes.h>
@@ -25,6 +26,7 @@
 #define ZYNQMP_AES_BLK_SIZE		1U
 #define ZYNQMP_AES_MIN_INPUT_BLK_SIZE	4U
 #define ZYNQMP_AES_WORD_LEN		4U
+#define VERSAL_AES_QWORD_LEN		16U
 
 #define ZYNQMP_AES_GCM_TAG_MISMATCH_ERR		0x01
 #define ZYNQMP_AES_WRONG_KEY_SRC_ERR		0x13
@@ -41,22 +43,39 @@ enum zynqmp_aead_keysrc {
 	ZYNQMP_AES_PUF_KEY
 };
 
-struct zynqmp_aead_drv_ctx {
-	union {
-		struct aead_alg aead;
-	} alg;
-	struct device *dev;
-	struct crypto_engine *engine;
+enum versal_aead_keysrc {
+	VERSAL_AES_BBRAM_KEY = 0,
+	VERSAL_AES_BBRAM_RED_KEY,
+	VERSAL_AES_BH_KEY,
+	VERSAL_AES_BH_RED_KEY,
+	VERSAL_AES_EFUSE_KEY,
+	VERSAL_AES_EFUSE_RED_KEY,
+	VERSAL_AES_EFUSE_USER_KEY_0,
+	VERSAL_AES_EFUSE_USER_KEY_1,
+	VERSAL_AES_EFUSE_USER_RED_KEY_0,
+	VERSAL_AES_EFUSE_USER_RED_KEY_1,
+	VERSAL_AES_KUP_KEY,
+	VERSAL_AES_PUF_KEY,
+	VERSAL_AES_USER_KEY_0,
+	VERSAL_AES_USER_KEY_1,
+	VERSAL_AES_USER_KEY_2,
+	VERSAL_AES_USER_KEY_3,
+	VERSAL_AES_USER_KEY_4,
+	VERSAL_AES_USER_KEY_5,
+	VERSAL_AES_USER_KEY_6,
+	VERSAL_AES_USER_KEY_7,
+	VERSAL_AES_EXPANDED_KEYS,
+	VERSAL_AES_ALL_KEYS,
 };
 
-struct zynqmp_aead_hw_req {
-	u64 src;
-	u64 iv;
-	u64 key;
-	u64 dst;
-	u64 size;
-	u64 op;
-	u64 keysrc;
+enum versal_aead_op {
+	VERSAL_AES_ENCRYPT = 0,
+	VERSAL_AES_DECRYPT
+};
+
+enum versal_aes_keysize {
+	AES_KEY_SIZE_128 = 0,
+	AES_KEY_SIZE_256 = 2,
 };
 
 struct zynqmp_aead_tfm_ctx {
@@ -66,14 +85,46 @@ struct zynqmp_aead_tfm_ctx {
 	u8 *iv;
 	u32 keylen;
 	u32 authsize;
-	enum zynqmp_aead_keysrc keysrc;
+	u8 keysrc;
 	struct crypto_aead *fbk_cipher;
 };
 
+struct xilinx_aead_drv_ctx {
+	struct aead_alg aead;
+	struct device *dev;
+	struct crypto_engine *engine;
+	int (*aes_aead_cipher)(struct aead_request *areq);
+	int (*fallback_check)(struct zynqmp_aead_tfm_ctx *ctx,
+			      struct aead_request *areq);
+};
+
+struct zynqmp_aead_hw_req {
+	u64 src;
+	u64 iv;
+	u64 key;
+	u64 dst;
+	u64 size;
+	u64 op;
+	u64 keysrc;
+};
+
 struct zynqmp_aead_req_ctx {
 	enum zynqmp_aead_op op;
 };
 
+struct versal_init_ops {
+	u64 iv;
+	u32 op;
+	u32 keysrc;
+	u32 size;
+};
+
+struct versal_in_params {
+	u64 in_data_addr;
+	u32 size;
+	u32 is_last;
+};
+
 static int zynqmp_aes_aead_cipher(struct aead_request *req)
 {
 	struct crypto_aead *aead = crypto_aead_reqtfm(req);
@@ -174,6 +225,128 @@ static int zynqmp_aes_aead_cipher(struct aead_request *req)
 	return err;
 }
 
+static int versal_aes_aead_cipher(struct aead_request *req)
+{
+	struct crypto_aead *aead = crypto_aead_reqtfm(req);
+	struct zynqmp_aead_tfm_ctx *tfm_ctx = crypto_aead_ctx(aead);
+	struct zynqmp_aead_req_ctx *rq_ctx = aead_request_ctx(req);
+	dma_addr_t dma_addr_data, dma_addr_hw_req, dma_addr_in;
+	u32 total_len = req->assoclen + req->cryptlen;
+	u32 key_offset = total_len + GCM_AES_IV_SIZE;
+	struct device *dev = tfm_ctx->dev;
+	struct versal_init_ops *hwreq;
+	struct versal_in_params *in;
+	u32 gcm_offset, out_len;
+	size_t dma_size;
+	char *kbuf;
+	int ret;
+
+	dma_size = key_offset + tfm_ctx->keylen;
+
+	kbuf = dma_alloc_coherent(dev, dma_size, &dma_addr_data, GFP_KERNEL);
+	if (!kbuf) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	/*
+	 * Allocated separate memory as separate structure for init ops
+	 * Also to avoid big continuous memory allocation
+	 */
+	hwreq = dma_alloc_coherent(dev, sizeof(struct versal_init_ops),
+				   &dma_addr_hw_req, GFP_KERNEL);
+	if (!hwreq) {
+		ret = -ENOMEM;
+		goto hwreq_fail;
+	}
+
+	/*
+	 * Allocated separate memory as separate structure for in params
+	 * Also to avoid big continuous memory allocation
+	 */
+	in = dma_alloc_coherent(dev, sizeof(struct versal_in_params),
+				&dma_addr_in, GFP_KERNEL);
+	if (!in) {
+		ret = -ENOMEM;
+		goto in_fail;
+	}
+
+	scatterwalk_map_and_copy(kbuf, req->src, 0, total_len, 0);
+	memcpy(kbuf + total_len, req->iv, GCM_AES_IV_SIZE);
+	hwreq->iv = dma_addr_data + total_len;
+	hwreq->keysrc = tfm_ctx->keysrc;
+
+	if (rq_ctx->op == ZYNQMP_AES_ENCRYPT) {
+		hwreq->op = VERSAL_AES_ENCRYPT;
+		out_len = total_len + ZYNQMP_AES_AUTH_SIZE;
+		in->size = req->cryptlen;
+	} else {
+		hwreq->op = VERSAL_AES_DECRYPT;
+		out_len = total_len - ZYNQMP_AES_AUTH_SIZE;
+		in->size = req->cryptlen - ZYNQMP_AES_AUTH_SIZE;
+	}
+
+	if (tfm_ctx->keylen == XSECURE_AES_KEY_SIZE_128)
+		hwreq->size = AES_KEY_SIZE_128;
+	else if (tfm_ctx->keylen == XSECURE_AES_KEY_SIZE_256)
+		hwreq->size = AES_KEY_SIZE_256;
+
+	memcpy(kbuf + key_offset,
+	       tfm_ctx->key, tfm_ctx->keylen);
+
+	ret = versal_pm_aes_key_write(hwreq->size, hwreq->keysrc,
+				      dma_addr_data + key_offset);
+	if (ret)
+		goto in_fail;
+
+	ret = versal_pm_aes_op_init(dma_addr_hw_req);
+	if (ret)
+		goto in_fail;
+
+	if (req->assoclen > 0) {
+		/* Currently GMAC is OFF by default */
+		ret = versal_pm_aes_update_aad(dma_addr_data, req->assoclen);
+		if (ret)
+			goto in_fail;
+	}
+
+	in->in_data_addr = dma_addr_data + req->assoclen;
+	in->is_last = 1;
+	gcm_offset = req->assoclen + in->size;
+
+	if (rq_ctx->op == ZYNQMP_AES_ENCRYPT) {
+		ret = versal_pm_aes_enc_update(dma_addr_in,
+					       dma_addr_data + req->assoclen);
+		if (ret)
+			goto in_fail;
+
+		ret = versal_pm_aes_enc_final(dma_addr_data + gcm_offset);
+		if (ret)
+			goto in_fail;
+	} else {
+		ret = versal_pm_aes_dec_update(dma_addr_in,
+					       dma_addr_data + req->assoclen);
+		if (ret)
+			goto in_fail;
+
+		ret = versal_pm_aes_dec_final(dma_addr_data + gcm_offset);
+		if (ret)
+			goto in_fail;
+	}
+
+	sg_copy_from_buffer(req->dst, sg_nents(req->dst),
+			    kbuf, out_len);
+
+in_fail:
+	memzero_explicit(hwreq, sizeof(struct zynqmp_aead_hw_req));
+	dma_free_coherent(dev, sizeof(struct versal_init_ops), hwreq, dma_addr_hw_req);
+hwreq_fail:
+	memzero_explicit(kbuf, dma_size);
+	dma_free_coherent(dev, dma_size, kbuf, dma_addr_data);
+err:
+	return ret;
+}
+
 static int zynqmp_fallback_check(struct zynqmp_aead_tfm_ctx *tfm_ctx,
 				 struct aead_request *req)
 {
@@ -201,19 +374,54 @@ static int zynqmp_fallback_check(struct zynqmp_aead_tfm_ctx *tfm_ctx,
 	return need_fallback;
 }
 
-static int zynqmp_handle_aes_req(struct crypto_engine *engine,
-				 void *req)
+static int versal_fallback_check(struct zynqmp_aead_tfm_ctx *tfm_ctx,
+				 struct aead_request *req)
+{
+	int need_fallback = 0;
+	struct zynqmp_aead_req_ctx *rq_ctx = aead_request_ctx(req);
+
+	if (tfm_ctx->authsize != ZYNQMP_AES_AUTH_SIZE) {
+		need_fallback = 1;
+		goto fallback;
+	}
+
+	if (tfm_ctx->keylen != XSECURE_AES_KEY_SIZE_128 &&
+	    tfm_ctx->keylen != XSECURE_AES_KEY_SIZE_256) {
+		need_fallback = 1;
+		goto fallback;
+	}
+
+	if (req->cryptlen < ZYNQMP_AES_MIN_INPUT_BLK_SIZE ||
+	    req->cryptlen % ZYNQMP_AES_WORD_LEN ||
+	    req->assoclen % VERSAL_AES_QWORD_LEN) {
+		need_fallback = 1;
+		goto fallback;
+	}
+	if (rq_ctx->op == ZYNQMP_AES_DECRYPT &&
+	    req->cryptlen <= ZYNQMP_AES_AUTH_SIZE) {
+		need_fallback = 1;
+		goto fallback;
+	}
+fallback:
+	return need_fallback;
+}
+
+static int handle_aes_req(struct crypto_engine *engine, void *req)
 {
 	struct aead_request *areq =
 				container_of(req, struct aead_request, base);
 	struct crypto_aead *aead = crypto_aead_reqtfm(req);
 	struct zynqmp_aead_tfm_ctx *tfm_ctx = crypto_aead_ctx(aead);
+	struct aead_alg *alg = crypto_aead_alg(aead);
+	struct xilinx_aead_drv_ctx *drv_ctx;
+
 	struct zynqmp_aead_req_ctx *rq_ctx = aead_request_ctx(areq);
 	struct aead_request *subreq = aead_request_ctx(req);
 	int need_fallback;
 	int err;
 
-	need_fallback = zynqmp_fallback_check(tfm_ctx, areq);
+	drv_ctx = container_of(alg, struct xilinx_aead_drv_ctx, aead);
+	need_fallback = drv_ctx->fallback_check(tfm_ctx, areq);
 
 	if (need_fallback) {
 		aead_request_set_tfm(subreq, tfm_ctx->fbk_cipher);
@@ -228,7 +436,7 @@ static int zynqmp_handle_aes_req(struct crypto_engine *engine,
 		else
 			err = crypto_aead_decrypt(subreq);
 	} else {
-		err = zynqmp_aes_aead_cipher(areq);
+		err = drv_ctx->aes_aead_cipher(areq);
 	}
 
 	local_bh_disable();
@@ -251,10 +459,9 @@ static int zynqmp_aes_aead_setkey(struct crypto_aead *aead, const u8 *key,
 		if (keysrc == ZYNQMP_AES_KUP_KEY ||
 		    keysrc == ZYNQMP_AES_DEV_KEY ||
 		    keysrc == ZYNQMP_AES_PUF_KEY) {
-			tfm_ctx->keysrc = (enum zynqmp_aead_keysrc)keysrc;
-		} else {
-			tfm_ctx->keylen = keylen;
+			tfm_ctx->keysrc = keysrc;
 		}
+		return 0;
 	} else {
 		tfm_ctx->keylen = keylen;
 		if (keylen == ZYNQMP_AES_KEY_SIZE) {
@@ -270,6 +477,44 @@ static int zynqmp_aes_aead_setkey(struct crypto_aead *aead, const u8 *key,
 	return crypto_aead_setkey(tfm_ctx->fbk_cipher, key, keylen);
 }
 
+static int versal_aes_aead_setkey(struct crypto_aead *aead, const u8 *key,
+				  unsigned int keylen)
+{
+	struct crypto_tfm *tfm = crypto_aead_tfm(aead);
+	struct zynqmp_aead_tfm_ctx *tfm_ctx =
+			(struct zynqmp_aead_tfm_ctx *)crypto_tfm_ctx(tfm);
+
+	if (keylen == ZYNQMP_KEY_SRC_SEL_KEY_LEN) {
+		unsigned char keysrc = VERSAL_AES_USER_KEY_0;
+
+		keysrc += *key;
+		if (keysrc >= VERSAL_AES_USER_KEY_0 &&
+		    keysrc  <= VERSAL_AES_USER_KEY_7)
+			tfm_ctx->keysrc = keysrc;
+		else
+			tfm_ctx->keysrc = VERSAL_AES_USER_KEY_0;
+
+		return 0;
+	} else {
+		tfm_ctx->keylen = keylen;
+		if (keylen == XSECURE_AES_KEY_SIZE_256 ||
+		    keylen == XSECURE_AES_KEY_SIZE_128) {
+			memcpy(tfm_ctx->key, key, keylen);
+		}
+
+		if (tfm_ctx->keysrc < VERSAL_AES_USER_KEY_0 ||
+		    tfm_ctx->keysrc > VERSAL_AES_USER_KEY_7) {
+			tfm_ctx->keysrc = VERSAL_AES_USER_KEY_0;
+		}
+	}
+
+	tfm_ctx->fbk_cipher->base.crt_flags &= ~CRYPTO_TFM_REQ_MASK;
+	tfm_ctx->fbk_cipher->base.crt_flags |= (aead->base.crt_flags &
+					CRYPTO_TFM_REQ_MASK);
+
+	return crypto_aead_setkey(tfm_ctx->fbk_cipher, key, keylen);
+}
+
 static int zynqmp_aes_aead_setauthsize(struct crypto_aead *aead,
 				       unsigned int authsize)
 {
@@ -283,52 +528,52 @@ static int zynqmp_aes_aead_setauthsize(struct crypto_aead *aead,
 
 static int zynqmp_aes_aead_encrypt(struct aead_request *req)
 {
-	struct zynqmp_aead_drv_ctx *drv_ctx;
+	struct xilinx_aead_drv_ctx *drv_ctx;
 	struct crypto_aead *aead = crypto_aead_reqtfm(req);
 	struct aead_alg *alg = crypto_aead_alg(aead);
 	struct zynqmp_aead_req_ctx *rq_ctx = aead_request_ctx(req);
 
 	rq_ctx->op = ZYNQMP_AES_ENCRYPT;
-	drv_ctx = container_of(alg, struct zynqmp_aead_drv_ctx, alg.aead);
+	drv_ctx = container_of(alg, struct xilinx_aead_drv_ctx, aead);
 
 	return crypto_transfer_aead_request_to_engine(drv_ctx->engine, req);
 }
 
 static int zynqmp_aes_aead_decrypt(struct aead_request *req)
 {
-	struct zynqmp_aead_drv_ctx *drv_ctx;
+	struct xilinx_aead_drv_ctx *drv_ctx;
 	struct crypto_aead *aead = crypto_aead_reqtfm(req);
 	struct aead_alg *alg = crypto_aead_alg(aead);
 	struct zynqmp_aead_req_ctx *rq_ctx = aead_request_ctx(req);
 
 	rq_ctx->op = ZYNQMP_AES_DECRYPT;
-	drv_ctx = container_of(alg, struct zynqmp_aead_drv_ctx, alg.aead);
+	drv_ctx = container_of(alg, struct xilinx_aead_drv_ctx, aead);
 
 	return crypto_transfer_aead_request_to_engine(drv_ctx->engine, req);
 }
 
-static int zynqmp_aes_aead_init(struct crypto_aead *aead)
+static int aes_aead_init(struct crypto_aead *aead)
 {
 	struct crypto_tfm *tfm = crypto_aead_tfm(aead);
 	struct zynqmp_aead_tfm_ctx *tfm_ctx =
 		(struct zynqmp_aead_tfm_ctx *)crypto_tfm_ctx(tfm);
-	struct zynqmp_aead_drv_ctx *drv_ctx;
+	struct xilinx_aead_drv_ctx *drv_ctx;
 	struct aead_alg *alg = crypto_aead_alg(aead);
 
-	drv_ctx = container_of(alg, struct zynqmp_aead_drv_ctx, alg.aead);
+	drv_ctx = container_of(alg, struct xilinx_aead_drv_ctx, aead);
 	tfm_ctx->dev = drv_ctx->dev;
 
-	tfm_ctx->engine_ctx.op.do_one_request = zynqmp_handle_aes_req;
+	tfm_ctx->engine_ctx.op.do_one_request = handle_aes_req;
 	tfm_ctx->engine_ctx.op.prepare_request = NULL;
 	tfm_ctx->engine_ctx.op.unprepare_request = NULL;
 
-	tfm_ctx->fbk_cipher = crypto_alloc_aead(drv_ctx->alg.aead.base.cra_name,
+	tfm_ctx->fbk_cipher = crypto_alloc_aead(drv_ctx->aead.base.cra_name,
 						0,
 						CRYPTO_ALG_NEED_FALLBACK);
 
 	if (IS_ERR(tfm_ctx->fbk_cipher)) {
 		pr_err("%s() Error: failed to allocate fallback for %s\n",
-		       __func__, drv_ctx->alg.aead.base.cra_name);
+		       __func__, drv_ctx->aead.base.cra_name);
 		return PTR_ERR(tfm_ctx->fbk_cipher);
 	}
 
@@ -352,20 +597,22 @@ static void zynqmp_aes_aead_exit(struct crypto_aead *aead)
 	memzero_explicit(tfm_ctx, sizeof(struct zynqmp_aead_tfm_ctx));
 }
 
-static struct zynqmp_aead_drv_ctx aes_drv_ctx = {
-	.alg.aead = {
+static struct xilinx_aead_drv_ctx zynqmp_aes_drv_ctx = {
+	.fallback_check = zynqmp_fallback_check,
+	.aes_aead_cipher = zynqmp_aes_aead_cipher,
+	.aead = {
 		.setkey		= zynqmp_aes_aead_setkey,
 		.setauthsize	= zynqmp_aes_aead_setauthsize,
 		.encrypt	= zynqmp_aes_aead_encrypt,
 		.decrypt	= zynqmp_aes_aead_decrypt,
-		.init		= zynqmp_aes_aead_init,
+		.init		= aes_aead_init,
 		.exit		= zynqmp_aes_aead_exit,
 		.ivsize		= GCM_AES_IV_SIZE,
 		.maxauthsize	= ZYNQMP_AES_AUTH_SIZE,
 		.base = {
 		.cra_name		= "gcm(aes)",
-		.cra_driver_name	= "xilinx-zynqmp-aes-gcm",
-		.cra_priority		= 200,
+		.cra_driver_name	= "zynqmp-aes-gcm",
+		.cra_priority		= 300,
 		.cra_flags		= CRYPTO_ALG_TYPE_AEAD |
 					  CRYPTO_ALG_ASYNC |
 					  CRYPTO_ALG_ALLOCATES_MEMORY |
@@ -378,75 +625,150 @@ static struct zynqmp_aead_drv_ctx aes_drv_ctx = {
 	}
 };
 
+static struct xilinx_aead_drv_ctx versal_aes_drv_ctx = {
+	.fallback_check		= versal_fallback_check,
+	.aes_aead_cipher	= versal_aes_aead_cipher,
+	.aead = {
+		.setkey		= versal_aes_aead_setkey,
+		.setauthsize	= zynqmp_aes_aead_setauthsize,
+		.encrypt	= zynqmp_aes_aead_encrypt,
+		.decrypt	= zynqmp_aes_aead_decrypt,
+		.init		= aes_aead_init,
+		.exit		= zynqmp_aes_aead_exit,
+		.ivsize		= GCM_AES_IV_SIZE,
+		.maxauthsize	= ZYNQMP_AES_AUTH_SIZE,
+		.base = {
+		.cra_name		= "gcm(aes)",
+		.cra_driver_name	= "versal-aes-gcm",
+		.cra_priority		= 300,
+		.cra_flags		= CRYPTO_ALG_TYPE_AEAD |
+					  CRYPTO_ALG_ASYNC |
+					  CRYPTO_ALG_ALLOCATES_MEMORY |
+					  CRYPTO_ALG_KERN_DRIVER_ONLY |
+					  CRYPTO_ALG_NEED_FALLBACK,
+		.cra_blocksize		= ZYNQMP_AES_BLK_SIZE,
+		.cra_ctxsize		= sizeof(struct zynqmp_aead_tfm_ctx),
+		.cra_module		= THIS_MODULE,
+		}
+	}
+};
+
+static struct xlnx_feature aes_feature_map[] = {
+	{
+		.family = ZYNQMP_FAMILY_CODE,
+		.subfamily = ALL_SUB_FAMILY_CODE,
+		.feature_id = PM_SECURE_AES,
+		.data = &zynqmp_aes_drv_ctx,
+	},
+	{
+		.family = VERSAL_FAMILY_CODE,
+		.subfamily = VERSAL_SUB_FAMILY_CODE,
+		.feature_id = XSECURE_API_AES_OP_INIT,
+		.data = &versal_aes_drv_ctx,
+	},
+	{ /* sentinel */ }
+};
+
 static int zynqmp_aes_aead_probe(struct platform_device *pdev)
 {
+	struct xilinx_aead_drv_ctx *aes_drv_ctx;
 	struct device *dev = &pdev->dev;
 	int err;
 
+	/* Verify the hardware is present */
+	aes_drv_ctx = xlnx_get_crypto_dev_data(aes_feature_map);
+	if (IS_ERR(aes_drv_ctx)) {
+		dev_err(dev, "AES is not supported on the platform\n");
+		return PTR_ERR(aes_drv_ctx);
+	}
+
 	/* ZynqMP AES driver supports only one instance */
-	if (!aes_drv_ctx.dev)
-		aes_drv_ctx.dev = dev;
+	if (!aes_drv_ctx->dev)
+		aes_drv_ctx->dev = dev;
 	else
 		return -ENODEV;
 
+	platform_set_drvdata(pdev, aes_drv_ctx);
+
 	err = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(ZYNQMP_DMA_BIT_MASK));
 	if (err < 0) {
 		dev_err(dev, "No usable DMA configuration\n");
 		return err;
 	}
 
-	aes_drv_ctx.engine = crypto_engine_alloc_init(dev, 1);
-	if (!aes_drv_ctx.engine) {
+	aes_drv_ctx->engine = crypto_engine_alloc_init(dev, 1);
+	if (!aes_drv_ctx->engine) {
 		dev_err(dev, "Cannot alloc AES engine\n");
 		err = -ENOMEM;
 		goto err_engine;
 	}
 
-	err = crypto_engine_start(aes_drv_ctx.engine);
+	err = crypto_engine_start(aes_drv_ctx->engine);
 	if (err) {
 		dev_err(dev, "Cannot start AES engine\n");
 		goto err_engine;
 	}
 
-	err = crypto_register_aead(&aes_drv_ctx.alg.aead);
+	err = crypto_register_aead(&aes_drv_ctx->aead);
 	if (err < 0) {
 		dev_err(dev, "Failed to register AEAD alg.\n");
-		goto err_aead;
+		goto err_engine;
 	}
 	return 0;
 
-err_aead:
-	crypto_unregister_aead(&aes_drv_ctx.alg.aead);
-
 err_engine:
-	if (aes_drv_ctx.engine)
-		crypto_engine_exit(aes_drv_ctx.engine);
+	if (aes_drv_ctx->engine)
+		crypto_engine_exit(aes_drv_ctx->engine);
 
 	return err;
 }
 
 static int zynqmp_aes_aead_remove(struct platform_device *pdev)
 {
-	crypto_engine_exit(aes_drv_ctx.engine);
-	crypto_unregister_aead(&aes_drv_ctx.alg.aead);
+	struct xilinx_aead_drv_ctx *aes_drv_ctx;
+
+	aes_drv_ctx = platform_get_drvdata(pdev);
+
+	crypto_engine_exit(aes_drv_ctx->engine);
+
+	crypto_unregister_aead(&aes_drv_ctx->aead);
 
 	return 0;
 }
 
-static const struct of_device_id zynqmp_aes_dt_ids[] = {
-	{ .compatible = "xlnx,zynqmp-aes" },
-	{ /* sentinel */ }
-};
-MODULE_DEVICE_TABLE(of, zynqmp_aes_dt_ids);
-
 static struct platform_driver zynqmp_aes_driver = {
 	.probe	= zynqmp_aes_aead_probe,
 	.remove = zynqmp_aes_aead_remove,
 	.driver = {
 		.name		= "zynqmp-aes",
-		.of_match_table = zynqmp_aes_dt_ids,
 	},
 };
 
-module_platform_driver(zynqmp_aes_driver);
+static int __init aes_driver_init(void)
+{
+	struct platform_device *pdev;
+	int ret;
+
+	ret = platform_driver_register(&zynqmp_aes_driver);
+	if (ret)
+		return ret;
+
+	pdev = platform_device_register_simple(zynqmp_aes_driver.driver.name,
+					       0, NULL, 0);
+	if (IS_ERR(pdev)) {
+		ret = PTR_ERR(pdev);
+		platform_driver_unregister(&zynqmp_aes_driver);
+	}
+
+	return ret;
+}
+
+static void __exit aes_driver_exit(void)
+{
+	platform_driver_unregister(&zynqmp_aes_driver);
+}
+
+device_initcall(aes_driver_init);
+module_exit(aes_driver_exit);
+
 MODULE_LICENSE("GPL");
diff --git a/drivers/crypto/xilinx/zynqmp-rsa.c b/drivers/crypto/xilinx/zynqmp-rsa.c
new file mode 100644
index 000000000..c914e4156
--- /dev/null
+++ b/drivers/crypto/xilinx/zynqmp-rsa.c
@@ -0,0 +1,274 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright (C) 2017 - 2022 Xilinx, Inc.
+ * Copyright (C) 2022 - 2023, Advanced Micro Devices, Inc.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/crypto.h>
+#include <linux/spinlock.h>
+#include <crypto/algapi.h>
+#include <crypto/aes.h>
+#include <crypto/internal/skcipher.h>
+#include <linux/io.h>
+#include <linux/device.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/dma-mapping.h>
+#include <linux/scatterlist.h>
+#include <crypto/scatterwalk.h>
+#include <linux/firmware/xlnx-zynqmp.h>
+
+#define ZYNQMP_RSA_QUEUE_LENGTH	1
+#define ZYNQMP_RSA_MAX_KEY_SIZE	1024
+#define ZYNQMP_RSA_BLOCKSIZE	64
+
+/* Key size in bytes */
+#define XSECURE_RSA_2048_KEY_SIZE	(2048U / 8U)
+#define XSECURE_RSA_3072_KEY_SIZE	(3072U / 8U)
+#define XSECURE_RSA_4096_KEY_SIZE	(4096U / 8U)
+
+static struct zynqmp_rsa_dev *rsa_dd;
+
+struct zynqmp_rsa_op {
+	struct zynqmp_rsa_dev    *dd;
+	void *src;
+	void *dst;
+	int len;
+	u8 key[ZYNQMP_RSA_MAX_KEY_SIZE];
+	u8 *iv;
+	u32 keylen;
+};
+
+struct zynqmp_rsa_dev {
+	struct list_head        list;
+	struct device           *dev;
+	/* the lock protects queue and dev list*/
+	spinlock_t              lock;
+	struct crypto_queue     queue;
+	struct skcipher_alg	*alg;
+};
+
+struct zynqmp_rsa_drv {
+	struct list_head        dev_list;
+	/* the lock protects queue and dev list*/
+	spinlock_t              lock;
+};
+
+static struct zynqmp_rsa_drv zynqmp_rsa = {
+	.dev_list = LIST_HEAD_INIT(zynqmp_rsa.dev_list),
+	.lock = __SPIN_LOCK_UNLOCKED(zynqmp_rsa.lock),
+};
+
+static struct zynqmp_rsa_dev *zynqmp_rsa_find_dev(struct zynqmp_rsa_op *ctx)
+{
+	struct zynqmp_rsa_dev *dd = rsa_dd;
+
+	spin_lock_bh(&zynqmp_rsa.lock);
+	if (!ctx->dd)
+		ctx->dd = dd;
+	else
+		dd = ctx->dd;
+	spin_unlock_bh(&zynqmp_rsa.lock);
+
+	return dd;
+}
+
+static int zynqmp_setkey_blk(struct crypto_skcipher *tfm, const u8 *key,
+			     unsigned int len)
+{
+	struct zynqmp_rsa_op *op = crypto_skcipher_ctx(tfm);
+
+	op->keylen = len;
+	memcpy(op->key, key, len);
+	return 0;
+}
+
+static int zynqmp_rsa_xcrypt(struct skcipher_request *req, unsigned int flags)
+{
+	struct crypto_skcipher *tfm = crypto_skcipher_reqtfm(req);
+	struct zynqmp_rsa_op *op = crypto_skcipher_ctx(tfm);
+	struct zynqmp_rsa_dev *dd = zynqmp_rsa_find_dev(op);
+	int err, datasize, src_data = 0, dst_data = 0;
+	struct skcipher_walk walk = {0};
+	unsigned int nbytes;
+	char *kbuf;
+	size_t dma_size;
+	dma_addr_t dma_addr;
+
+	nbytes = req->cryptlen;
+	if (nbytes != XSECURE_RSA_2048_KEY_SIZE &&
+	    nbytes != XSECURE_RSA_3072_KEY_SIZE &&
+	    nbytes != XSECURE_RSA_4096_KEY_SIZE) {
+		return -EOPNOTSUPP;
+	}
+
+	dma_size = nbytes + op->keylen;
+	kbuf = dma_alloc_coherent(dd->dev, dma_size, &dma_addr, GFP_KERNEL);
+	if (!kbuf)
+		return -ENOMEM;
+
+	err = skcipher_walk_virt(&walk, req, false);
+	if (err)
+		goto out;
+
+	while ((datasize = walk.nbytes)) {
+		op->src = walk.src.virt.addr;
+		memcpy(kbuf + src_data, op->src, datasize);
+		src_data = src_data + datasize;
+		err = skcipher_walk_done(&walk, 0);
+		if (err)
+			goto out;
+	}
+	memcpy(kbuf + nbytes, op->key, op->keylen);
+
+	zynqmp_pm_rsa(dma_addr, nbytes, flags);
+
+	err = skcipher_walk_virt(&walk, req, false);
+	if (err)
+		goto out;
+
+	while ((datasize = walk.nbytes)) {
+		memcpy(walk.dst.virt.addr, kbuf + dst_data, datasize);
+		dst_data = dst_data + datasize;
+		err = skcipher_walk_done(&walk, 0);
+		if (err)
+			goto out;
+	}
+
+out:
+	dma_free_coherent(dd->dev, dma_size, kbuf, dma_addr);
+	return err;
+}
+
+static int zynqmp_rsa_decrypt(struct skcipher_request *req)
+{
+	return zynqmp_rsa_xcrypt(req, 0);
+}
+
+static int zynqmp_rsa_encrypt(struct skcipher_request *req)
+{
+	return zynqmp_rsa_xcrypt(req, 1);
+}
+
+static struct skcipher_alg zynqmp_alg = {
+	.base.cra_name		=	"xilinx-zynqmp-rsa",
+	.base.cra_driver_name	=	"zynqmp-rsa",
+	.base.cra_priority	=	400,
+	.base.cra_flags		=	CRYPTO_ALG_TYPE_SKCIPHER |
+					CRYPTO_ALG_KERN_DRIVER_ONLY,
+	.base.cra_blocksize	=	ZYNQMP_RSA_BLOCKSIZE,
+	.base.cra_ctxsize	=	sizeof(struct zynqmp_rsa_op),
+	.base.cra_alignmask	=	15,
+	.base.cra_module	=	THIS_MODULE,
+	.min_keysize		=	0,
+	.max_keysize		=	ZYNQMP_RSA_MAX_KEY_SIZE,
+	.setkey			=	zynqmp_setkey_blk,
+	.encrypt		=	zynqmp_rsa_encrypt,
+	.decrypt		=	zynqmp_rsa_decrypt,
+	.ivsize			=	1,
+};
+
+static struct xlnx_feature rsa_feature_map[] = {
+	{
+		.family = ZYNQMP_FAMILY_CODE,
+		.subfamily = ALL_SUB_FAMILY_CODE,
+		.feature_id = PM_SECURE_RSA,
+		.data = &zynqmp_alg,
+	},
+	{ /* sentinel */ }
+};
+
+static int zynqmp_rsa_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	int ret;
+
+	rsa_dd = devm_kzalloc(&pdev->dev, sizeof(*rsa_dd), GFP_KERNEL);
+	if (!rsa_dd)
+		return -ENOMEM;
+
+	rsa_dd->alg = xlnx_get_crypto_dev_data(rsa_feature_map);
+	if (IS_ERR(rsa_dd->alg)) {
+		dev_err(dev, "RSA is not supported on the platform\n");
+		return PTR_ERR(rsa_dd->alg);
+	}
+
+	rsa_dd->dev = dev;
+	platform_set_drvdata(pdev, rsa_dd);
+
+	ret = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(32));
+	if (ret < 0)
+		dev_err(dev, "no usable DMA configuration");
+
+	INIT_LIST_HEAD(&rsa_dd->list);
+	spin_lock_init(&rsa_dd->lock);
+	crypto_init_queue(&rsa_dd->queue, ZYNQMP_RSA_QUEUE_LENGTH);
+	spin_lock(&zynqmp_rsa.lock);
+	list_add_tail(&rsa_dd->list, &zynqmp_rsa.dev_list);
+	spin_unlock(&zynqmp_rsa.lock);
+
+	ret = crypto_register_skcipher(rsa_dd->alg);
+	if (ret)
+		goto err_algs;
+
+	return 0;
+
+err_algs:
+	spin_lock(&zynqmp_rsa.lock);
+	list_del(&rsa_dd->list);
+	spin_unlock(&zynqmp_rsa.lock);
+	dev_err(dev, "initialization failed.\n");
+	return ret;
+}
+
+static int zynqmp_rsa_remove(struct platform_device *pdev)
+{
+	struct zynqmp_rsa_dev *drv_ctx;
+
+	drv_ctx = platform_get_drvdata(pdev);
+
+	crypto_unregister_skcipher(drv_ctx->alg);
+
+	return 0;
+}
+
+static struct platform_driver xilinx_rsa_driver = {
+	.probe = zynqmp_rsa_probe,
+	.remove = zynqmp_rsa_remove,
+	.driver = {
+		.name = "zynqmp_rsa",
+	},
+};
+
+static int __init rsa_driver_init(void)
+{
+	struct platform_device *pdev;
+	int ret;
+
+	ret = platform_driver_register(&xilinx_rsa_driver);
+	if (ret)
+		return ret;
+
+	pdev = platform_device_register_simple(xilinx_rsa_driver.driver.name,
+					       0, NULL, 0);
+	if (IS_ERR(pdev)) {
+		ret = PTR_ERR(pdev);
+		platform_driver_unregister(&xilinx_rsa_driver);
+	}
+
+	return ret;
+}
+
+static void __exit rsa_driver_exit(void)
+{
+	platform_driver_unregister(&xilinx_rsa_driver);
+}
+
+device_initcall(rsa_driver_init);
+module_exit(rsa_driver_exit);
+
+MODULE_DESCRIPTION("ZynqMP RSA hw acceleration support.");
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Nava kishore Manne <navam@xilinx.com>");
diff --git a/drivers/crypto/xilinx/zynqmp-sha.c b/drivers/crypto/xilinx/zynqmp-sha.c
index 43ff170ff..5ec65050d 100644
--- a/drivers/crypto/xilinx/zynqmp-sha.c
+++ b/drivers/crypto/xilinx/zynqmp-sha.c
@@ -2,6 +2,7 @@
 /*
  * Xilinx ZynqMP SHA Driver.
  * Copyright (c) 2022 Xilinx Inc.
+ * Copyright (C) 2022-2023, Advanced Micro Devices, Inc.
  */
 #include <linux/cacheflush.h>
 #include <crypto/hash.h>
@@ -18,6 +19,11 @@
 #include <linux/of_device.h>
 #include <linux/platform_device.h>
 
+#define CONTINUE_PACKET		BIT(31)
+#define FIRST_PACKET		BIT(30)
+#define FINAL_PACKET		0
+#define RESET			0
+
 #define ZYNQMP_DMA_BIT_MASK		32U
 #define ZYNQMP_DMA_ALLOC_FIXED_SIZE	0x1000U
 
@@ -27,7 +33,7 @@ enum zynqmp_sha_op {
 	ZYNQMP_SHA3_FINAL = 4,
 };
 
-struct zynqmp_sha_drv_ctx {
+struct xilinx_sha_drv_ctx {
 	struct shash_alg sha3_384;
 	struct device *dev;
 };
@@ -50,9 +56,9 @@ static int zynqmp_sha_init_tfm(struct crypto_shash *hash)
 	struct zynqmp_sha_tfm_ctx *tfm_ctx = crypto_shash_ctx(hash);
 	struct shash_alg *alg = crypto_shash_alg(hash);
 	struct crypto_shash *fallback_tfm;
-	struct zynqmp_sha_drv_ctx *drv_ctx;
+	struct xilinx_sha_drv_ctx *drv_ctx;
 
-	drv_ctx = container_of(alg, struct zynqmp_sha_drv_ctx, sha3_384);
+	drv_ctx = container_of(alg, struct xilinx_sha_drv_ctx, sha3_384);
 	tfm_ctx->dev = drv_ctx->dev;
 
 	/* Allocate a fallback and abort if it failed. */
@@ -160,7 +166,48 @@ static int zynqmp_sha_digest(struct shash_desc *desc, const u8 *data, unsigned i
 	return ret;
 }
 
-static struct zynqmp_sha_drv_ctx sha3_drv_ctx = {
+static int versal_sha_digest(struct shash_desc *desc, const u8 *data,
+			     unsigned int len, u8 *out)
+{
+	int update_size, ret, flag = FIRST_PACKET;
+	unsigned int remaining_len = len;
+
+	while (remaining_len != 0) {
+		memzero_explicit(ubuf, ZYNQMP_DMA_ALLOC_FIXED_SIZE);
+		if (remaining_len >= ZYNQMP_DMA_ALLOC_FIXED_SIZE) {
+			update_size = ZYNQMP_DMA_ALLOC_FIXED_SIZE;
+			remaining_len -= ZYNQMP_DMA_ALLOC_FIXED_SIZE;
+		} else {
+			update_size = remaining_len;
+			remaining_len = 0;
+		}
+
+		memcpy(ubuf, data, update_size);
+		flush_icache_range((unsigned long)ubuf,
+				   (unsigned long)ubuf + update_size);
+
+		flag |= CONTINUE_PACKET;
+		ret = versal_pm_sha_hash(update_dma_addr, 0,
+					 update_size | flag);
+		if (ret)
+			return ret;
+
+		data += update_size;
+		flag = RESET;
+	}
+
+	flag |= FINAL_PACKET;
+	ret = versal_pm_sha_hash(0, final_dma_addr, flag);
+	if (ret)
+		return ret;
+
+	memcpy(out, fbuf, SHA3_384_DIGEST_SIZE);
+	memzero_explicit(fbuf, SHA3_384_DIGEST_SIZE);
+
+	return 0;
+}
+
+static struct xilinx_sha_drv_ctx zynqmp_sha3_drv_ctx = {
 	.sha3_384 = {
 		.init = zynqmp_sha_init,
 		.update = zynqmp_sha_update,
@@ -189,17 +236,63 @@ static struct zynqmp_sha_drv_ctx sha3_drv_ctx = {
 	}
 };
 
+static struct xilinx_sha_drv_ctx versal_sha3_drv_ctx = {
+	.sha3_384 = {
+		.init = zynqmp_sha_init,
+		.update = zynqmp_sha_update,
+		.final = zynqmp_sha_final,
+		.finup = zynqmp_sha_finup,
+		.export = zynqmp_sha_export,
+		.import = zynqmp_sha_import,
+		.digest = versal_sha_digest,
+		.init_tfm = zynqmp_sha_init_tfm,
+		.exit_tfm = zynqmp_sha_exit_tfm,
+		.descsize = sizeof(struct zynqmp_sha_desc_ctx),
+		.statesize = sizeof(struct sha3_state),
+		.digestsize = SHA3_384_DIGEST_SIZE,
+		.base = {
+			.cra_name = "sha3-384",
+			.cra_driver_name = "versal-sha3-384",
+			.cra_priority = 300,
+			.cra_flags = CRYPTO_ALG_KERN_DRIVER_ONLY |
+				CRYPTO_ALG_ALLOCATES_MEMORY |
+				CRYPTO_ALG_NEED_FALLBACK,
+			.cra_blocksize = SHA3_384_BLOCK_SIZE,
+			.cra_ctxsize = sizeof(struct zynqmp_sha_tfm_ctx),
+			.cra_alignmask = 3,
+			.cra_module = THIS_MODULE,
+		}
+	}
+};
+
+static struct xlnx_feature sha_feature_map[] = {
+	{
+		.family = ZYNQMP_FAMILY_CODE,
+		.subfamily = ALL_SUB_FAMILY_CODE,
+		.feature_id = PM_SECURE_SHA,
+		.data = &zynqmp_sha3_drv_ctx,
+	},
+	{
+		.family = VERSAL_FAMILY_CODE,
+		.subfamily = VERSAL_SUB_FAMILY_CODE,
+		.feature_id = XSECURE_API_SHA3_UPDATE,
+		.data = &versal_sha3_drv_ctx,
+	},
+	{ /* sentinel */ }
+};
+
 static int zynqmp_sha_probe(struct platform_device *pdev)
 {
+	struct xilinx_sha_drv_ctx *sha3_drv_ctx;
 	struct device *dev = &pdev->dev;
 	int err;
-	u32 v;
 
 	/* Verify the hardware is present */
-	err = zynqmp_pm_get_api_version(&v);
-	if (err)
-		return err;
-
+	sha3_drv_ctx = xlnx_get_crypto_dev_data(sha_feature_map);
+	if (IS_ERR(sha3_drv_ctx)) {
+		dev_err(dev, "SHA is not supported on the platform\n");
+		return PTR_ERR(sha3_drv_ctx);
+	}
 
 	err = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(ZYNQMP_DMA_BIT_MASK));
 	if (err < 0) {
@@ -207,19 +300,13 @@ static int zynqmp_sha_probe(struct platform_device *pdev)
 		return err;
 	}
 
-	err = crypto_register_shash(&sha3_drv_ctx.sha3_384);
-	if (err < 0) {
-		dev_err(dev, "Failed to register shash alg.\n");
-		return err;
-	}
-
-	sha3_drv_ctx.dev = dev;
-	platform_set_drvdata(pdev, &sha3_drv_ctx);
+	sha3_drv_ctx->dev = dev;
+	platform_set_drvdata(pdev, sha3_drv_ctx);
 
 	ubuf = dma_alloc_coherent(dev, ZYNQMP_DMA_ALLOC_FIXED_SIZE, &update_dma_addr, GFP_KERNEL);
 	if (!ubuf) {
 		err = -ENOMEM;
-		goto err_shash;
+		return err;
 	}
 
 	fbuf = dma_alloc_coherent(dev, SHA3_384_DIGEST_SIZE, &final_dma_addr, GFP_KERNEL);
@@ -228,24 +315,33 @@ static int zynqmp_sha_probe(struct platform_device *pdev)
 		goto err_mem;
 	}
 
+	err = crypto_register_shash(&sha3_drv_ctx->sha3_384);
+	if (err < 0) {
+		dev_err(dev, "Failed to register shash alg.\n");
+		goto err_mem1;
+	}
 	return 0;
 
-err_mem:
-	dma_free_coherent(sha3_drv_ctx.dev, ZYNQMP_DMA_ALLOC_FIXED_SIZE, ubuf, update_dma_addr);
+err_mem1:
+	dma_free_coherent(dev, SHA3_384_DIGEST_SIZE, fbuf, final_dma_addr);
 
-err_shash:
-	crypto_unregister_shash(&sha3_drv_ctx.sha3_384);
+err_mem:
+	dma_free_coherent(dev, ZYNQMP_DMA_ALLOC_FIXED_SIZE, ubuf, update_dma_addr);
 
 	return err;
 }
 
 static int zynqmp_sha_remove(struct platform_device *pdev)
 {
-	sha3_drv_ctx.dev = platform_get_drvdata(pdev);
+	struct xilinx_sha_drv_ctx *sha3_drv_ctx;
 
-	dma_free_coherent(sha3_drv_ctx.dev, ZYNQMP_DMA_ALLOC_FIXED_SIZE, ubuf, update_dma_addr);
-	dma_free_coherent(sha3_drv_ctx.dev, SHA3_384_DIGEST_SIZE, fbuf, final_dma_addr);
-	crypto_unregister_shash(&sha3_drv_ctx.sha3_384);
+	sha3_drv_ctx = platform_get_drvdata(pdev);
+
+	dma_free_coherent(sha3_drv_ctx->dev,
+			  ZYNQMP_DMA_ALLOC_FIXED_SIZE, ubuf, update_dma_addr);
+	dma_free_coherent(sha3_drv_ctx->dev,
+			  SHA3_384_DIGEST_SIZE, fbuf, final_dma_addr);
+	crypto_unregister_shash(&sha3_drv_ctx->sha3_384);
 
 	return 0;
 }
@@ -258,7 +354,33 @@ static struct platform_driver zynqmp_sha_driver = {
 	},
 };
 
-module_platform_driver(zynqmp_sha_driver);
+static int __init sha_driver_init(void)
+{
+	struct platform_device *pdev;
+	int ret;
+
+	ret = platform_driver_register(&zynqmp_sha_driver);
+	if (ret)
+		return ret;
+
+	pdev = platform_device_register_simple(zynqmp_sha_driver.driver.name,
+					       0, NULL, 0);
+	if (IS_ERR(pdev)) {
+		ret = PTR_ERR(pdev);
+		platform_driver_unregister(&zynqmp_sha_driver);
+	}
+
+	return ret;
+}
+
+static void __exit sha_driver_exit(void)
+{
+	platform_driver_unregister(&zynqmp_sha_driver);
+}
+
+device_initcall(sha_driver_init);
+module_exit(sha_driver_exit);
+
 MODULE_DESCRIPTION("ZynqMP SHA3 hardware acceleration support.");
 MODULE_LICENSE("GPL v2");
 MODULE_AUTHOR("Harsha <harsha.harsha@xilinx.com>");
