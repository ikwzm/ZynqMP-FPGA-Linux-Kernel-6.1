diff --git a/Documentation/ABI/testing/sysfs-driver-xilinx-tmr-inject b/Documentation/ABI/testing/sysfs-driver-xilinx-tmr-inject
new file mode 100644
index 000000000..a3f65da1c
--- /dev/null
+++ b/Documentation/ABI/testing/sysfs-driver-xilinx-tmr-inject
@@ -0,0 +1,12 @@
+What:		/sys/devices/platform/amba_pl/<dev>/inject_err
+Date:		Feb 2022
+Contact:	appana.durga.rao@xilinx.com
+Description:	This control file allows to inject fault using tmr inject.
+		This file is write only.
+
+What:		/sys/devices/platform/amba_pl/<dev>/inject_cpuid
+Date:		Feb 2022
+Contact:	appana.durga.rao@xilinx.com
+Description:	This control file allows to configure the CPU identifier
+		to enable fault injection.
+		This file is write only.
diff --git a/Documentation/ABI/testing/sysfs-driver-xilinx-tmr-manager b/Documentation/ABI/testing/sysfs-driver-xilinx-tmr-manager
new file mode 100644
index 000000000..5194f2f96
--- /dev/null
+++ b/Documentation/ABI/testing/sysfs-driver-xilinx-tmr-manager
@@ -0,0 +1,18 @@
+What:		/sys/devices/platform/amba_pl/<dev>/status
+Date:		Feb 2022
+Contact:	appana.durga.rao@xilinx.com
+Description:	This control file provides the status of the tmr manager
+		useful for getting the status of fault.
+		This file cannot be written.
+
+What:		/sys/devices/platform/amba_pl/<dev>/errcnt
+Date:		Feb 2022
+Contact:	appana.durga.rao@xilinx.com
+Description:	This control file provides the fault detection count.
+		This file cannot be written.
+
+What:		/sys/devices/platform/amba_pl/<dev>/dis_block_break
+Date:		Feb 2022
+Contact:	appana.durga.rao@xilinx.com
+Description:	This control file enables the break signal.
+		This file is write only.
diff --git a/Documentation/devicetree/bindings/misc/xlnx,dpu.yaml b/Documentation/devicetree/bindings/misc/xlnx,dpu.yaml
new file mode 100644
index 000000000..eb7e82298
--- /dev/null
+++ b/Documentation/devicetree/bindings/misc/xlnx,dpu.yaml
@@ -0,0 +1,91 @@
+# SPDX-License-Identifier: (GPL-2.0 OR BSD-2-Clause)
+%YAML 1.2
+---
+$id: http://devicetree.org/schemas/misc/xlnx,dpu.yaml#
+$schema: http://devicetree.org/meta-schemas/core.yaml#
+
+title: Xilinx Deep learning Processing Unit (DPU) Vivado Flow DT Bindings
+
+maintainers:
+  - Ye Yang <ye.yang@xilinx.com>
+
+description: |+
+    The DPUCZDX8G is the deep learning processing unit (DPU) designed to
+    support the Zynq UltraScale+ MPSoC and MicroBlaze. It is a configurable
+    computation engine optimized for convolutional neural networks.
+
+    The driver supports up to 4 DPU cores with 40-bit addressing and 1
+    softmax core with 32-bit addressing, depending on how the user config
+    the IP in Vivado TRD Flow.
+
+    For more details refer to PG338 "DPUCZDX8G for Zynq UltraScale+ MPSoCs"
+    (https://www.xilinx.com/content/dam/xilinx/support/documentation/ip_documentation/dpu/v3_4/pg338-dpu.pdf)
+
+properties:
+  compatible:
+    enum:
+      - xlnx,dpuczdx8g-3.4
+      - xlnx,dpuczdx8g-4.0
+      - xlnx,dpuczdx8g-4.1
+
+  reg:
+    maxItems: 1
+
+  clocks:
+    description: List of clock specifiers.
+    items:
+      - description: AXI Lite clock
+      - description: dsp clock, used for DSP blocks in the DPUCZDX8G. The frequency is twice that of dpu clock
+      - description: dpu clock, used for DPUCZDX8G general logic
+
+  clock-names:
+    items:
+      - const: s_axi_aclk
+      - const: dpu_2x_clk
+      - const: m_axi_dpu_aclk
+
+  interrupts:
+    description: One interrupt per available DPU core(up to 4) or 1 softmax core
+    minItems: 1
+    maxItems: 5
+
+  interrupt-names:
+    minItems: 1
+    maxItems: 5
+    items:
+      pattern: "^(dpu[0-3]|sfm)_interrupt$"
+
+required:
+  - compatible
+  - reg
+  - interrupts
+  - interrupt-names
+
+additionalProperties: false
+
+examples:
+  # Example for ZynqMP (2 DPU and 1 SOFTMAX)
+  - |
+    #include <dt-bindings/interrupt-controller/arm-gic.h>
+
+    dpuczdx8g@8f000000 {
+        compatible = "xlnx,dpuczdx8g-3.4";
+        reg = <0x8f000000 0x1000000>;
+        interrupt-parent = <&gic>;
+        interrupts = <GIC_SPI 88 IRQ_TYPE_LEVEL_HIGH>,
+                     <GIC_SPI 89 IRQ_TYPE_LEVEL_HIGH>,
+                     <GIC_SPI 92 IRQ_TYPE_LEVEL_HIGH>;
+        interrupt-names = "dpu0_interrupt", "dpu1_interrupt", "sfm_interrupt";
+        clocks = <&zynqmp_clk 71>, <&misc_clk_0>, <&misc_clk_1>;
+        clock-names = "s_axi_aclk", "dpu_2x_clk", "m_axi_dpu_aclk";
+    };
+
+  # Example for Microblaze (1 DPU)
+  - |
+    dpuczdx8g@1000000 {
+        compatible = "xlnx,dpuczdx8g-3.4";
+        reg = <0x01000000 0x1000000>;
+        interrupt-parent = <&microblaze_0_axi_intc>;
+        interrupts = <8 2>;
+        interrupt-names = "dpu0_interrupt";
+    };
diff --git a/Documentation/devicetree/bindings/misc/xlnx,tmr-inject.yaml b/Documentation/devicetree/bindings/misc/xlnx,tmr-inject.yaml
new file mode 100644
index 000000000..f6b4e179d
--- /dev/null
+++ b/Documentation/devicetree/bindings/misc/xlnx,tmr-inject.yaml
@@ -0,0 +1,47 @@
+# SPDX-License-Identifier: (GPL-2.0 OR BSD-2-Clause)
+%YAML 1.2
+---
+$id: http://devicetree.org/schemas/misc/xlnx,tmr-inject.yaml#
+$schema: http://devicetree.org/meta-schemas/core.yaml#
+
+title: Xilinx TMR Inject Device Tree Bindings
+
+maintainers:
+  - Appana Durga Kedareswara rao <appana.durga.rao@xilinx.com>
+
+description: |
+  The Triple Modular Redundancy(TMR) Inject core provides functional
+  fault injection by changing selected MicroBlaze instructions,
+  which provides the possibility to verify that the TMR
+  subsystem error detection and fault recovery logic
+  is working properly.
+
+properties:
+  compatible:
+    enum:
+      - xlnx,tmr-inject-1.0
+
+  reg:
+    maxItems: 1
+
+  xlnx,magic:
+    description: |
+      Magic number used when injecting faults. The fault inject write
+      data least significant byte (bits 7:0) must match this number
+      to have any effect. The value shall be in the range 0-255.
+    $ref: /schemas/types.yaml#/definitions/uint32
+
+required:
+  - compatible
+  - reg
+  - xlnx,magic
+
+additionalProperties: false
+
+examples:
+  - |
+    tmr-inject@44a30000 {
+            compatible = "xlnx,tmr-inject-1.0";
+            reg = <0x44a10000 0x10000>;
+            xlnx,magic = <0x46>;
+    };
diff --git a/Documentation/devicetree/bindings/misc/xlnx,tmr-manager.yaml b/Documentation/devicetree/bindings/misc/xlnx,tmr-manager.yaml
new file mode 100644
index 000000000..be8b59027
--- /dev/null
+++ b/Documentation/devicetree/bindings/misc/xlnx,tmr-manager.yaml
@@ -0,0 +1,48 @@
+# SPDX-License-Identifier: (GPL-2.0 OR BSD-2-Clause)
+%YAML 1.2
+---
+$id: http://devicetree.org/schemas/misc/xlnx,tmr-manager.yaml#
+$schema: http://devicetree.org/meta-schemas/core.yaml#
+
+title: Xilinx TMR Microblaze Device Tree Bindings
+
+maintainers:
+  - Appana Durga Kedareswara rao <appana.durga.rao@xilinx.com>
+
+description: |
+  The Triple Modular Redundancy(TMR) Manager is responsible
+  for handling the TMR subsystem state, including fault detection
+  and error recovery. The core is triplicated in each of the
+  sub-blocks in the TMR subsystem, and provides majority
+  voting of its internal state.
+
+properties:
+  compatible:
+    enum:
+      - xlnx,tmr-manager-1.0
+
+  reg:
+    maxItems: 1
+
+  xlnx,magic1:
+    description:
+      Magic number 1, When writing to the control register the first
+      write data byte (bits 7:0) must match this value in order to
+      have any effect on the nominal recovery function. the value
+      shall be in the range 0-255.
+    $ref: /schemas/types.yaml#/definitions/uint32
+
+required:
+  - compatible
+  - reg
+  - xlnx,magic1
+
+additionalProperties: false
+
+examples:
+  - |
+    tmr-manager@44a10000 {
+            compatible = "xlnx,tmr-manager-1.0";
+            reg = <0x44a10000 0x10000>;
+            xlnx,magic1 = <0x46>;
+    };
diff --git a/drivers/misc/Kconfig b/drivers/misc/Kconfig
index 0cef98319..87a0c88b7 100644
--- a/drivers/misc/Kconfig
+++ b/drivers/misc/Kconfig
@@ -457,6 +457,45 @@ config XILINX_SDFEC
 
 	  If unsure, say N.
 
+config XILINX_DPU
+	tristate "Xilinx Deep learning Processing Unit (DPU) Driver"
+	depends on HAS_IOMEM && COMMON_CLK
+	depends on ARCH_ZYNQMP || MICROBLAZE
+	help
+	  This option enables support for the Xilinx DPUCZDX8G (Deep learning
+	  Processing Unit) Vivado flow driver.
+	  It is a configurable computation engine dedicated for convolutional
+	  neural networks. The degree of parallelism utilized in the engine
+	  is a design parameter and application. It includes a set of optimized
+	  instructions, and supports most convolutional neural networks.
+
+	  If unsure, say N.
+
+config XILINX_AIE
+	tristate "Xilinx AI engine"
+	depends on ARM64 || COMPILE_TEST
+	depends on ZYNQMP_FIRMWARE
+	select FPGA_BRIDGE
+	help
+	  This option enables support for the Xilinx AI engine driver.
+	  One Xilinx AI engine device can have multiple partitions (groups of
+	  AI engine tiles). Xilinx AI engine device driver instance manages
+	  AI engine partitions. User application access its partitions through
+	  AI engine partition instance file operations.
+
+	  If unsure, say N
+
+config XILINX_PUF
+	tristate "Xilinx PUF driver"
+	depends on ZYNQMP_FIRMWARE
+	help
+	  This option enables support for the Xilinx Physical unclonable function
+	  (PUF) driver.
+	  It is a configurable driver to generate PUF KEK source either by using
+	  PUF registration or regeneration command.
+
+	  If unsure, say N.
+
 config MISC_RTSX
 	tristate
 	default MISC_RTSX_PCI || MISC_RTSX_USB
@@ -497,6 +536,26 @@ config VCPU_STALL_DETECTOR
 
 	  If you do not intend to run this kernel as a guest, say N.
 
+config TMR_MANAGER
+	bool "Select TMR Manager"
+	depends on MICROBLAZE && MB_MANAGER
+	help
+	  This option enables the driver developed for TMR Manager. The Triple
+	  Modular Redundancy(TMR) manager provides support for fault detection
+	  via sysfs interface.
+
+	  Say N here unless you know what you are doing.
+
+config TMR_INJECT
+	bool "Select TMR Inject"
+	depends on TMR_MANAGER
+	help
+	  This option enables the driver developed for TMR Inject.
+	  The Triple Modular Redundancy(TMR) Inject provides
+	  fault injection.
+
+	  Say N here unless you know what you are doing.
+
 source "drivers/misc/c2port/Kconfig"
 source "drivers/misc/eeprom/Kconfig"
 source "drivers/misc/cb710/Kconfig"
diff --git a/drivers/misc/Makefile b/drivers/misc/Makefile
index ac9b3e757..119e3622a 100644
--- a/drivers/misc/Makefile
+++ b/drivers/misc/Makefile
@@ -57,8 +57,13 @@ obj-$(CONFIG_PVPANIC)   	+= pvpanic/
 obj-$(CONFIG_HABANA_AI)		+= habanalabs/
 obj-$(CONFIG_UACCE)		+= uacce/
 obj-$(CONFIG_XILINX_SDFEC)	+= xilinx_sdfec.o
+obj-$(CONFIG_XILINX_DPU)	+= xlnx_dpu.o
 obj-$(CONFIG_HISI_HIKEY_USB)	+= hisi_hikey_usb.o
+obj-$(CONFIG_XILINX_AIE)	+= xilinx-ai-engine/
 obj-$(CONFIG_HI6421V600_IRQ)	+= hi6421v600-irq.o
 obj-$(CONFIG_OPEN_DICE)		+= open-dice.o
 obj-$(CONFIG_GP_PCI1XXXX)	+= mchp_pci1xxxx/
 obj-$(CONFIG_VCPU_STALL_DETECTOR)	+= vcpu_stall_detector.o
+obj-$(CONFIG_TMR_MANAGER)	+= xilinx_tmr_manager.o
+obj-$(CONFIG_TMR_INJECT)	+= xilinx_tmr_inject.o
+obj-$(CONFIG_XILINX_PUF)        += xilinx_puf.o
diff --git a/drivers/misc/xilinx-ai-engine/Makefile b/drivers/misc/xilinx-ai-engine/Makefile
new file mode 100644
index 000000000..2d0b51074
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/Makefile
@@ -0,0 +1,31 @@
+# SPDX-License-Identifier: GPL-2.0-only
+#
+# Makefile for Xilinx AI engine device driver
+#
+
+obj-$(CONFIG_XILINX_AIE)	+= xilinx-aie.o
+
+xilinx-aie-$(CONFIG_XILINX_AIE) := ai-engine-aie.o		\
+				   ai-engine-aieml.o		\
+				   ai-engine-aperture.o		\
+				   ai-engine-clock.o		\
+				   ai-engine-dev.o		\
+				   ai-engine-dev-v1_0.o		\
+				   ai-engine-dma.o		\
+				   ai-engine-interrupt.o	\
+				   ai-engine-mem.o		\
+				   ai-engine-overlay.o		\
+				   ai-engine-part.o		\
+				   ai-engine-res.o		\
+				   ai-engine-reset.o		\
+				   ai-engine-rscmgr.o		\
+				   ai-engine-sysfs.o		\
+				   ai-engine-sysfs-clock.o	\
+				   ai-engine-sysfs-core.o	\
+				   ai-engine-sysfs-dma.o	\
+				   ai-engine-sysfs-error.o	\
+				   ai-engine-sysfs-event.o	\
+				   ai-engine-sysfs-info.o	\
+				   ai-engine-sysfs-lock.o	\
+				   ai-engine-sysfs-status.o	\
+				   ai-engine-status-dump.o
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-aie.c b/drivers/misc/xilinx-ai-engine/ai-engine-aie.c
new file mode 100644
index 000000000..ba1253513
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-aie.c
@@ -0,0 +1,2600 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine driver AIE device specific implementation
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include <linux/bitfield.h>
+#include <linux/firmware/xlnx-zynqmp.h>
+#include <linux/io.h>
+#include <linux/slab.h>
+#include <linux/xlnx-ai-engine.h>
+
+#include "ai-engine-internal.h"
+
+#define AIE_ARRAY_SHIFT		30U
+#define AIE_COL_SHIFT		23U
+#define AIE_ROW_SHIFT		18U
+
+#define NUM_MEMS_PER_TILE	2U
+
+#define NUM_MODS_CORE_TILE	2U
+#define NUM_MODS_SHIMPL_TILE	1U
+
+#define NUM_UTIL_EVENTS		4U
+
+/*
+ * Number of resources per module
+ */
+#define AIE_NUM_PERF_CORE_MOD		4U
+#define AIE_NUM_USEREVENT_CORE_MOD	4U
+#define AIE_NUM_TRACECONTROL_CORE_MOD	1U
+#define AIE_NUM_PCEVENT_CORE_MOD	4U
+#define AIE_NUM_SSSELECT_CORE_MOD	8U
+#define AIE_NUM_BROADCAST_CORE_MOD	16U
+#define AIE_NUM_COMBOEVENT_CORE_MOD	4U
+#define AIE_NUM_GROUPEVENTS_CORE_MOD	9U
+
+#define AIE_NUM_PERF_MEM_MOD		2U
+#define AIE_NUM_USEREVENT_MEM_MOD	4U
+#define AIE_NUM_TRACECONTROL_MEM_MOD	1U
+#define AIE_NUM_PCEVENT_MEM_MOD		0U
+#define AIE_NUM_SSSELECT_MEM_MOD	0U
+#define AIE_NUM_BROADCAST_MEM_MOD	16U
+#define AIE_NUM_COMBOEVENT_MEM_MOD	4U
+#define AIE_NUM_GROUPEVENTS_MEM_MOD	8U
+
+#define AIE_NUM_PERF_PL_MOD		2U
+#define AIE_NUM_USEREVENT_PL_MOD	4U
+#define AIE_NUM_TRACECONTROL_PL_MOD	1U
+#define AIE_NUM_PCEVENT_PL_MOD		0U
+#define AIE_NUM_SSSELECT_PL_MOD		8U
+#define AIE_NUM_BROADCAST_PL_MOD	16U
+#define AIE_NUM_COMBOEVENT_PL_MOD	4U
+#define AIE_NUM_GROUPEVENTS_PL_MOD	7U
+
+/*
+ * Registers offsets
+ */
+#define AIE_SHIMNOC_L2INTR_MASK_REGOFF		0x00015000U
+#define AIE_SHIMNOC_L2INTR_INTR_REGOFF		0x00015010U
+#define AIE_SHIMNOC_DMA_BD0_ADDRLOW_REGOFF	0x0001d000U
+#define AIE_SHIMNOC_DMA_BD15_PACKET_REGOFF	0x0001d13cU
+#define AIE_SHIMNOC_AXIMM_REGOFF		0x0001e020U
+#define AIE_SHIMPL_BISR_CACHE_CTRL_REGOFF	0x00036000U
+#define AIE_SHIMPL_L1INTR_MASK_A_REGOFF		0x00035000U
+#define AIE_SHIMPL_L1INTR_BLOCK_NORTH_B_REGOFF	0x00035050U
+#define AIE_SHIMPL_TILECTRL_REGOFF		0x00036030U
+#define AIE_SHIMPL_CLKCNTR_REGOFF		0x00036040U
+#define AIE_SHIMPL_COLRESET_REGOFF		0x00036048U
+#define AIE_SHIMPL_RESET_REGOFF			0x0003604cU
+#define AIE_SHIMPL_GROUP_ERROR_REGOFF		0x0003450cU
+#define AIE_TILE_MEM_DMA_BD0_ADDR_A		0x0001D000U
+#define AIE_TILE_CORE_TILECTRL_REGOFF		0x00036030U
+#define AIE_TILE_CORE_CLKCNTR_REGOFF		0x00036040U
+#define AIE_TILE_CORE_GROUP_ERROR_REGOFF	0x00034510U
+#define AIE_TILE_MEM_GROUP_ERROR_REGOFF		0x00014514U
+#define AIE_TILE_CORE_R0_REGOFF			0x00030000U
+#define AIE_TILE_CORE_LC_REGOFF			0x00030520U
+#define AIE_TILE_CORE_VRL0_REGOFF		0x00030530U
+#define AIE_TILE_CORE_AMH3_PART3_REGOFF		0x000307a0U
+#define AIE_TILE_CORE_PERFCTRL_REGOFF		0x00031000U
+#define AIE_TILE_CORE_PERFCTRL_RESET_REGOFF	0x00031008U
+#define AIE_TILE_CORE_PERFCNT0_REGOFF		0x00031020U
+#define AIE_TILE_CORE_EVNTGEN_REGOFF		0x00034008U
+
+/*
+ * Register masks
+ */
+#define AIE_SHIMPL_SHIMRST_MASK			0x1U
+#define AIE_SHIMPL_COLRST_MASK			0x1U
+#define AIE_SHIMPL_CLKCNTR_COLBUF_MASK		0x1U
+#define AIE_SHIMPL_CLKCNTR_NEXTCLK_MASK		BIT(1)
+#define AIE_TILE_CLKCNTR_COLBUF_MASK		BIT(0)
+#define AIE_TILE_CLKCNTR_NEXTCLK_MASK		BIT(1)
+#define AIE_TILE_PERFCTRL_CNT0_MASK		0x7F7FU
+#define AIE_TILE_PERFCTRL_RESET_MASK		0x7FU
+#define AIE_TILE_CORE_PERFCNT0_MASK		0xFFFFFFFFU
+#define AIE_TILE_CORE_EVNTGEN_MASK		0x7F
+
+/*
+ * AI engine SHIM reset ID.
+ * TODO: it should follow the Linux reset framework. The ID should be in the
+ * device tree. However, as versal resets is not ready, we hardcode it in the
+ * driver.
+ */
+#define VERSAL_PM_RST_AIE_SHIM_ID			0xc10405fU
+
+/* Macros to define size of a sysfs binary attribute */
+#define AIE_PART_SYSFS_CORE_BINA_SIZE		0x4000		/* 16KB */
+#define AIE_PART_SYSFS_DMA_BINA_SIZE		0xC800		/* 50KB */
+#define AIE_PART_SYSFS_LOCK_BINA_SIZE		0x28000		/* 160KB */
+#define AIE_PART_SYSFS_ERROR_BINA_SIZE		0x4000		/* 16KB */
+#define AIE_PART_SYSFS_STATUS_BINA_SIZE		0x3c000		/* 240KB */
+
+static const struct aie_tile_regs aie_kernel_regs[] = {
+	/* SHIM AXI MM Config */
+	{.attribute = AIE_TILE_TYPE_MASK_SHIMNOC << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMNOC_AXIMM_REGOFF,
+	 .eoff = AIE_SHIMNOC_AXIMM_REGOFF,
+	},
+	/* SHIM DMA ADDRESS range */
+	{.attribute = AIE_TILE_TYPE_MASK_SHIMNOC << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMNOC_DMA_BD0_ADDRLOW_REGOFF,
+	 .eoff = AIE_SHIMNOC_DMA_BD15_PACKET_REGOFF,
+	},
+	/* SHIM 2nd level interrupt controller */
+	{.attribute = AIE_TILE_TYPE_MASK_SHIMNOC << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMNOC_L2INTR_MASK_REGOFF,
+	 .eoff = AIE_SHIMNOC_L2INTR_INTR_REGOFF,
+	},
+	/* SHIM 1st level interrupt controller */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		      AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMPL_L1INTR_MASK_A_REGOFF,
+	 .eoff = AIE_SHIMPL_L1INTR_BLOCK_NORTH_B_REGOFF,
+	},
+	/* SHIM column reset */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		      AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMPL_COLRESET_REGOFF,
+	 .eoff = AIE_SHIMPL_COLRESET_REGOFF,
+	},
+	/* SHIM reset Enable */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		      AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMPL_RESET_REGOFF,
+	 .eoff = AIE_SHIMPL_RESET_REGOFF,
+	},
+	/* SHIM tile control */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		      AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMPL_TILECTRL_REGOFF,
+	 .eoff = AIE_SHIMPL_TILECTRL_REGOFF,
+	},
+	/* SHIM clock control */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		      AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMPL_CLKCNTR_REGOFF,
+	 .eoff = AIE_SHIMPL_CLKCNTR_REGOFF,
+	},
+	/* SHIM BISR cache control */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		      AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMPL_BISR_CACHE_CTRL_REGOFF,
+	 .eoff = AIE_SHIMPL_BISR_CACHE_CTRL_REGOFF,
+	},
+	/* SHIM group error enable */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		      AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_SHIMPL_GROUP_ERROR_REGOFF,
+	 .eoff = AIE_SHIMPL_GROUP_ERROR_REGOFF,
+	},
+	/* Core tile control */
+	{.attribute = AIE_TILE_TYPE_MASK_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_TILE_CORE_TILECTRL_REGOFF,
+	 .eoff = AIE_TILE_CORE_TILECTRL_REGOFF,
+	},
+	/* Tile clock control */
+	{.attribute = AIE_TILE_TYPE_MASK_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_TILE_CORE_CLKCNTR_REGOFF,
+	 .eoff = AIE_TILE_CORE_CLKCNTR_REGOFF,
+	},
+	/* Tile group error for core module */
+	{.attribute = AIE_TILE_TYPE_MASK_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_TILE_CORE_GROUP_ERROR_REGOFF,
+	 .eoff = AIE_TILE_CORE_GROUP_ERROR_REGOFF,
+	},
+	/* Tile group error for memory module */
+	{.attribute = AIE_TILE_TYPE_MASK_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIE_TILE_MEM_GROUP_ERROR_REGOFF,
+	 .eoff = AIE_TILE_MEM_GROUP_ERROR_REGOFF,
+	},
+};
+
+static const struct aie_tile_regs aie_core_32bit_regs = {
+	.attribute = AIE_TILE_TYPE_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	.soff = AIE_TILE_CORE_R0_REGOFF,
+	.eoff = AIE_TILE_CORE_LC_REGOFF,
+};
+
+static const struct aie_tile_regs aie_core_128bit_regs = {
+	.attribute = AIE_TILE_TYPE_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	.soff = AIE_TILE_CORE_VRL0_REGOFF,
+	.eoff = AIE_TILE_CORE_AMH3_PART3_REGOFF,
+};
+
+static const struct aie_core_regs_attr aie_core_regs[] = {
+	{.core_regs = &aie_core_32bit_regs,
+	 .width = 1,
+	},
+	{.core_regs = &aie_core_128bit_regs,
+	 .width = 4,
+	},
+};
+
+static const struct aie_single_reg_field aie_col_rst = {
+	.mask = AIE_SHIMPL_COLRST_MASK,
+	.regoff = AIE_SHIMPL_COLRESET_REGOFF,
+};
+
+static const struct aie_single_reg_field aie_col_clkbuf = {
+	.mask = AIE_SHIMPL_CLKCNTR_COLBUF_MASK,
+	.regoff = AIE_SHIMPL_CLKCNTR_REGOFF,
+};
+
+static const struct aie_bd_lock_attr aie_tile_a_lockbd = {
+	.lock_acq_id = {
+		.mask = GENMASK(25, 22),
+		.regoff = 0x0U,
+	},
+	.lock_acq_val = {
+		.mask = BIT(17),
+		.regoff = 0x0U,
+	},
+	.lock_acq_en = {
+		.mask = BIT(18),
+		.regoff = 0x0U,
+	},
+	.lock_acq_val_en = {
+		.mask = BIT(16),
+		.regoff = 0x0U,
+	},
+	.lock_rel_id = {
+		.mask = GENMASK(25, 22),
+		.regoff = 0x0U,
+	},
+	.lock_rel_val = {
+		.mask = BIT(20),
+		.regoff = 0x0U,
+	},
+	.lock_rel_en = {
+		.mask = BIT(21),
+		.regoff = 0x0U,
+	},
+	.lock_rel_val_en = {
+		.mask = BIT(19),
+		.regoff = 0x0U,
+	},
+};
+
+static const struct aie_bd_lock_attr aie_tile_b_lockbd = {
+	.lock_acq_id = {
+		.mask = GENMASK(25, 22),
+		.regoff = 0x4U,
+	},
+	.lock_acq_val = {
+		.mask = BIT(17),
+		.regoff = 0x4U,
+	},
+	.lock_acq_en = {
+		.mask = BIT(18),
+		.regoff = 0x4U,
+	},
+	.lock_acq_val_en = {
+		.mask = BIT(16),
+		.regoff = 0x4U,
+	},
+	.lock_rel_id = {
+		.mask = GENMASK(25, 22),
+		.regoff = 0x4U,
+	},
+	.lock_rel_val = {
+		.mask = BIT(20),
+		.regoff = 0x4U,
+	},
+	.lock_rel_en = {
+		.mask = BIT(21),
+		.regoff = 0x4U,
+	},
+	.lock_rel_val_en = {
+		.mask = BIT(19),
+		.regoff = 0x4U,
+	},
+};
+
+static const struct aie_bd_lock_attr aie_shim_lockbd = {
+	.lock_acq_id = {
+		.mask = GENMASK(10, 7),
+		.regoff = 0x8U,
+	},
+	.lock_acq_val = {
+		.mask = BIT(2),
+		.regoff = 0x8U,
+	},
+	.lock_acq_en = {
+		.mask = BIT(3),
+		.regoff = 0x8U,
+	},
+	.lock_acq_val_en = {
+		.mask = BIT(1),
+		.regoff = 0x8U,
+	},
+	.lock_rel_id = {
+		.mask = GENMASK(10, 7),
+		.regoff = 0x8U,
+	},
+	.lock_rel_val = {
+		.mask = BIT(5),
+		.regoff = 0x8U,
+	},
+	.lock_rel_en = {
+		.mask = BIT(6),
+		.regoff = 0x8U,
+	},
+	.lock_rel_val_en = {
+		.mask = BIT(4),
+		.regoff = 0x8U,
+	},
+};
+
+static const struct aie_bd_pkt_attr aie_tile_pktbd = {
+	.pkt_en = {
+		.mask = BIT(27),
+		.regoff = 0x18U,
+	},
+	.pkt_type = {
+		.mask = GENMASK(14, 12),
+		.regoff = 0x10U,
+	},
+	.pkt_id = {
+		.mask = GENMASK(4, 0),
+		.regoff = 0x10U,
+	},
+};
+
+static const struct aie_bd_pkt_attr aie_shim_pktbd = {
+	.pkt_en = {
+		.mask = BIT(31),
+		.regoff = 0x10U,
+	},
+	.pkt_type = {
+		.mask = GENMASK(14, 12),
+		.regoff = 0x10U,
+	},
+	.pkt_id = {
+		.mask = GENMASK(4, 0),
+		.regoff = 0x10U,
+	},
+};
+
+static const struct aie_bd_axi_attr aie_shim_axibd = {
+	.smid = {
+		.mask = GENMASK(31, 28),
+		.regoff = 0xCU,
+	},
+	.cache = {
+		.mask = GENMASK(3, 0),
+		.regoff = 0xCU,
+	},
+	.qos = {
+		.mask = GENMASK(8, 5),
+		.regoff = 0xCU,
+	},
+	.secure_en = {
+		.mask = BIT(4),
+		.regoff = 0xCU,
+	},
+	.burst_len = {
+		.mask = GENMASK(10, 9),
+		.regoff = 0xCU,
+	},
+};
+
+static const struct aie_bd_aie_dim_attr aie_tile_dimbd = {
+	.x_incr = {
+		.mask = GENMASK(31, 24),
+		.regoff = 0x8U,
+	},
+	.x_wrap = {
+		.mask = GENMASK(23, 16),
+		.regoff = 0x8U,
+	},
+	.x_off = {
+		.mask = GENMASK(12, 0),
+		.regoff = 0x8U,
+	},
+	.y_incr = {
+		.mask = GENMASK(31, 24),
+		.regoff = 0xCU,
+	},
+	.y_wrap = {
+		.mask = GENMASK(23, 16),
+		.regoff = 0xCU,
+	},
+	.y_off = {
+		.mask = GENMASK(12, 0),
+		.regoff = 0xCU,
+	},
+};
+
+static const struct aie_bd_attr aie_tilebd = {
+	.valid_bd = {
+		.mask = BIT(31),
+		.regoff = 0x18U,
+	},
+	.next_bd = {
+		.mask = GENMASK(16, 13),
+		.regoff = 0x18U,
+	},
+	.use_next = {
+		.mask = BIT(17),
+		.regoff = 0x18U,
+	},
+	.addr = {
+		.addr = {
+			.mask = GENMASK(12, 0),
+			.regoff = 0x0U,
+		},
+		.length = {
+			.mask = GENMASK(12, 0),
+			.regoff = 0x18U,
+		},
+	},
+	.addr_2 = {
+		.addr = {
+			.mask = GENMASK(12, 0),
+			.regoff = 0x4U,
+		},
+		.length = {
+			.mask = GENMASK(12, 0),
+			.regoff = 0x18U,
+		},
+	},
+	.lock = aie_tile_a_lockbd,
+	.lock_2 = aie_tile_b_lockbd,
+	.packet = aie_tile_pktbd,
+	.aie_dim = aie_tile_dimbd,
+	.buf_sel = {
+		.mask = BIT(16),
+		.regoff = 0x14U,
+	},
+	.curr_ptr = {
+		.mask = GENMASK(12, 0),
+		.regoff = 0x14U,
+	},
+	.interleave_en = {
+		.mask = BIT(26),
+		.regoff = 0x18U,
+	},
+	.interleave_cnt = {
+		.mask = GENMASK(25, 18),
+		.regoff = 0x18U,
+	},
+	.double_buff_en = {
+		.mask = BIT(30),
+		.regoff = 0x18U,
+	},
+	.fifo_mode = {
+		.mask = GENMASK(29, 28),
+		.regoff = 0x18U,
+	},
+	.bd_idx_off = 0x20U,
+};
+
+static const struct aie_bd_attr aie_shimbd = {
+	.valid_bd = {
+		.mask = BIT(0),
+		.regoff = 0x8U,
+	},
+	.next_bd = {
+		.mask = GENMASK(14, 11),
+		.regoff = 0x8U,
+	},
+	.use_next = {
+		.mask = BIT(15),
+		.regoff = 0x8U,
+	},
+	.addr = {
+		.addr = {
+			.mask = GENMASK(31, 0),
+			.regoff = 0x0U,
+		},
+		.length = {
+			.mask = GENMASK(31, 0),
+			.regoff = 0x4U,
+		},
+	},
+	.addr_2 = {
+		.addr = {
+			.mask = GENMASK(31, 16),
+			.regoff = 0x8U,
+		},
+		.length = {
+			.mask = GENMASK(31, 0),
+			.regoff = 0x4U,
+		},
+	},
+	.lock = aie_shim_lockbd,
+	.packet = aie_shim_pktbd,
+	.axi = aie_shim_axibd,
+	.bd_idx_off = 0x14U,
+};
+
+static const struct aie_single_reg_field aie_core_perfctrl = {
+	.mask = AIE_TILE_PERFCTRL_CNT0_MASK,
+	.regoff = AIE_TILE_CORE_PERFCTRL_REGOFF,
+};
+
+static const struct aie_single_reg_field aie_core_perfctrl_reset = {
+	.mask = AIE_TILE_PERFCTRL_RESET_MASK,
+	.regoff = AIE_TILE_CORE_PERFCTRL_RESET_REGOFF,
+};
+
+static const struct aie_single_reg_field aie_core_perfcnt = {
+	.mask = AIE_TILE_CORE_PERFCNT0_MASK,
+	.regoff = AIE_TILE_CORE_PERFCNT0_REGOFF,
+};
+
+static const struct aie_single_reg_field aie_core_evntgen = {
+	.mask = AIE_TILE_CORE_EVNTGEN_MASK,
+	.regoff = AIE_TILE_CORE_EVNTGEN_REGOFF,
+};
+
+static const struct aie_dma_attr aie_shimdma = {
+	.laddr = {
+		.mask = 0xffffffffU,
+		.regoff = 0U,
+	},
+	.haddr = {
+		.mask = 0xffff0000U,
+		.regoff = 0x8U,
+	},
+	.buflen = {
+		.mask = 0xffffffffU,
+		.regoff = 0x4U,
+	},
+	.sts = {
+		.mask = GENMASK(1, 0),
+		.regoff = 2U,
+	},
+	.stall = {
+		.mask = BIT(4),
+		.regoff = 1U,
+	},
+	.qsize = {
+		.mask = GENMASK(8, 6),
+		.regoff = 3U,
+	},
+	.curbd = {
+		.mask = GENMASK(19, 16),
+		.regoff = 4U,
+	},
+	.qsts = {
+		.mask = BIT(28),
+		.regoff = 1U,
+	},
+	.fifo_cnt = {
+		.mask = GENMASK(12, 0),
+		.regoff = 16U,
+	},
+	.bd_regoff = AIE_SHIMNOC_DMA_BD0_ADDRLOW_REGOFF,
+	.mm2s_sts_regoff = 0x1d164U,
+	.s2mm_sts_regoff = 0x1d160U,
+	.fifo_cnt_regoff = 0x1DF20U,
+	.num_bds = 16,
+	.num_mm2s_chan = 2U,
+	.num_s2mm_chan = 2U,
+	.bd_len = 0x14U,
+};
+
+static const struct aie_dma_attr aie_tiledma = {
+	.sts = {
+		.mask = GENMASK(1, 0),
+		.regoff = 2U,
+	},
+	.stall = {
+		.mask = BIT(4),
+		.regoff = 1U,
+	},
+	.qsize = {
+		.mask = GENMASK(8, 6),
+		.regoff = 3U,
+	},
+	.curbd = {
+		.mask = GENMASK(19, 16),
+		.regoff = 4U,
+	},
+	.qsts = {
+		.mask = BIT(28),
+		.regoff = 1U,
+	},
+	.bd_regoff = AIE_TILE_MEM_DMA_BD0_ADDR_A,
+	.mm2s_sts_regoff = 0x1df10U,
+	.s2mm_sts_regoff = 0x1df00U,
+	.num_bds = 16,
+	.num_mm2s_chan = 2U,
+	.num_s2mm_chan = 2U,
+	.bd_len = 0x1CU,
+};
+
+static char *aie_dma_status_str[] = {
+	"idle",
+	"starting",
+	"running",
+	"stalled_on_requesting_lock",
+	"invalid_status",
+};
+
+static char *aie_queue_status_str[] = {
+	"okay",
+	"overflow",
+};
+
+static const struct aie_event_attr aie_pl_event = {
+	.bc_event = {
+		.mask = GENMASK(6, 0),
+		.regoff = 0x0U,
+	},
+	.group_error = {
+		.mask = GENMASK(10, 0),
+		.regoff = 0xcU,
+	},
+	.bc_regoff = 0x34010U,
+	.status_regoff = 0x34200U,
+	.group_regoff = 0x34500U,
+	.base_error_event = 62U,
+	.num_broadcasts = 16U,
+	.base_bc_event = 107U,
+	.num_events = 128U,
+};
+
+static const struct aie_event_attr aie_mem_event = {
+	.bc_event = {
+		.mask = GENMASK(6, 0),
+		.regoff = 0x0U,
+	},
+	.group_error = {
+		.mask = GENMASK(13, 0),
+		.regoff = 0x14U,
+	},
+	.bc_regoff = 0x14010U,
+	.status_regoff = 0x14200U,
+	.group_regoff = 0x14500U,
+	.base_error_event = 87U,
+	.num_broadcasts = 16U,
+	.base_bc_event = 107U,
+	.num_events = 128U,
+};
+
+static const struct aie_event_attr aie_core_event = {
+	.bc_event = {
+		.mask = GENMASK(6, 0),
+		.regoff = 0x0U,
+	},
+	.group_error = {
+		.mask = GENMASK(21, 0),
+		.regoff = 0x10U,
+	},
+	.bc_regoff = 0x34010U,
+	.status_regoff = 0x34200U,
+	.group_regoff = 0x34500U,
+	.base_error_event = 48U,
+	.num_broadcasts = 16U,
+	.base_bc_event = 107U,
+	.num_events = 128U,
+};
+
+static const struct aie_l1_intr_ctrl_attr aie_l1_intr_ctrl = {
+	.swa_status = {
+		.mask = GENMASK(19, 0),
+		.regoff = 0xcU,
+	},
+	.swb_status = {
+		.mask = GENMASK(19, 0),
+		.regoff = 0x3cU,
+	},
+	.swa_event = {
+		.mask = GENMASK(6, 0),
+		.regoff = 0x14U,
+	},
+	.swb_event = {
+		.mask = GENMASK(6, 0),
+		.regoff = 0x44U,
+	},
+	.regoff = 0x35000U,
+	.event_lsb = 8,
+	.num_broadcasts = 0x14U,
+};
+
+static const struct aie_l2_intr_ctrl_attr aie_l2_intr_ctrl = {
+	.mask = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0x0U,
+	},
+	.enable = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0x4U,
+	},
+	.disable = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0x8U,
+	},
+	.status = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0xcU,
+	},
+	.regoff = 0x15000U,
+	.num_broadcasts = 0x10U,
+};
+
+static const struct aie_event_prop aie_core_stream_error_prop[] = {
+	{
+		.event = 54U,
+		.event_str = "tlast_in_wss_words_0-2",
+	},
+	{
+		.event = 57U,
+		.event_str = "control_packet_error",
+	},
+	{
+		.event = 56U,
+		.event_str = "stream_packet_parity_error",
+	},
+};
+
+static const struct aie_event_prop aie_core_inst_error_prop[] = {
+	{
+		.event = 59U,
+		.event_str = "instruction_decompression_error",
+	},
+};
+
+static const struct aie_event_prop aie_core_ecc_error_prop[] = {
+	{
+		.event = 64U,
+		.event_str = "pm_ecc_error_2-bit",
+	},
+	{
+		.event = 62U,
+		.event_str = "pm_ecc_error_scrub_2-bit",
+	},
+};
+
+static const struct aie_event_prop aie_core_access_error_prop[] = {
+	{
+		.event = 55U,
+		.event_str = "pm_reg_access_failure",
+	},
+	{
+		.event = 66U,
+		.event_str = "dm_access_to_unavailable",
+	},
+	{
+		.event = 65U,
+		.event_str = "pm_address_out_of_range",
+	},
+	{
+		.event = 60U,
+		.event_str = "dm_address_out_of_range",
+	},
+};
+
+static const struct aie_event_prop aie_core_lock_error_prop[] = {
+	{
+		.event = 67U,
+		.event_str = "lock_access_to_unavailable",
+	},
+};
+
+static const struct aie_event_prop aie_core_bus_error_prop[] = {
+	{
+		.event = 58U,
+		.event_str = "axi-mm_slave_error",
+	},
+};
+
+static const struct aie_event_prop aie_mem_ecc_error_prop[] = {
+	{
+		.event = 88U,
+		.event_str = "dm_ecc_error_scrub_2-bit",
+	},
+	{
+		.event = 90U,
+		.event_str = "dm_ecc_error_2-bit",
+	},
+};
+
+static const struct aie_event_prop aie_mem_parity_error_prop[] = {
+	{
+		.event = 91U,
+		.event_str = "dm_parity_error_bank_2",
+	},
+	{
+		.event = 92U,
+		.event_str = "dm_parity_error_bank_3",
+	},
+	{
+		.event = 93U,
+		.event_str = "dm_parity_error_bank_4",
+	},
+	{
+		.event = 94U,
+		.event_str = "dm_parity_error_bank_5",
+	},
+	{
+		.event = 95U,
+		.event_str = "dm_parity_error_bank_6",
+	},
+	{
+		.event = 96U,
+		.event_str = "dm_parity_error_bank_7",
+	},
+};
+
+static const struct aie_event_prop aie_mem_dma_error_prop[] = {
+	{
+		.event = 97U,
+		.event_str = "dma_s2mm_0_error",
+	},
+	{
+		.event = 98U,
+		.event_str = "dma_s2mm_1_error",
+	},
+	{
+		.event = 99U,
+		.event_str = "dma_mm2s_0_error",
+	},
+	{
+		.event = 100U,
+		.event_str = "dma_mm2s_1_error",
+	},
+};
+
+static const struct aie_event_prop aie_shim_bus_error_prop[] = {
+	{
+		.event = 62U,
+		.event_str = "axi-mm_slave_tile_error",
+	},
+};
+
+static const struct aie_event_prop aie_shim_stream_error_prop[] = {
+	{
+		.event = 63U,
+		.event_str = "control_packet_error",
+	},
+	{
+		.event = 64U,
+		.event_str = "axi-mm_decode_nsu_error",
+	},
+	{
+		.event = 65U,
+		.event_str = "axi-mm_slave_nsu_error",
+	},
+	{
+		.event = 66U,
+		.event_str = "axi-mm_unsupported_traffic",
+	},
+	{
+		.event = 67U,
+		.event_str = "axi-mm_unsecure_access_in_secure_mode",
+	},
+	{
+		.event = 68U,
+		.event_str = "axi-mm_byte_strobe_error",
+	},
+};
+
+static const struct aie_event_prop aie_shim_dma_error_prop[] = {
+	{
+		.event = 69U,
+		.event_str = "dma_s2mm_0_error",
+	},
+	{
+		.event = 70U,
+		.event_str = "dma_s2mm_1_error",
+	},
+	{
+		.event = 71U,
+		.event_str = "dma_mm2s_0_error",
+	},
+	{
+		.event = 72U,
+		.event_str = "dma_mm2s_1_error",
+	},
+};
+
+static const struct aie_err_category aie_core_err_category[] = {
+	{
+		/* AIE_ERROR_CATEGORY_STREAM */
+		.err_category = AIE_ERROR_CATEGORY_STREAM,
+		.num_events = ARRAY_SIZE(aie_core_stream_error_prop),
+		.prop = aie_core_stream_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_ACCESS */
+		.err_category = AIE_ERROR_CATEGORY_ACCESS,
+		.num_events = ARRAY_SIZE(aie_core_access_error_prop),
+		.prop = aie_core_access_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_BUS */
+		.err_category = AIE_ERROR_CATEGORY_BUS,
+		.num_events = ARRAY_SIZE(aie_core_bus_error_prop),
+		.prop = aie_core_bus_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_INSTRUCTION */
+		.err_category = AIE_ERROR_CATEGORY_INSTRUCTION,
+		.num_events = ARRAY_SIZE(aie_core_inst_error_prop),
+		.prop = aie_core_inst_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_ECC */
+		.err_category = AIE_ERROR_CATEGORY_ECC,
+		.num_events = ARRAY_SIZE(aie_core_ecc_error_prop),
+		.prop = aie_core_ecc_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_LOCK */
+		.err_category = AIE_ERROR_CATEGORY_LOCK,
+		.num_events = ARRAY_SIZE(aie_core_lock_error_prop),
+		.prop = aie_core_lock_error_prop,
+	},
+};
+
+static const struct aie_err_category aie_mem_err_category[] = {
+	{
+		/* AIE_ERROR_CATEGORY_ECC */
+		.err_category = AIE_ERROR_CATEGORY_ECC,
+		.num_events = ARRAY_SIZE(aie_mem_ecc_error_prop),
+		.prop = aie_mem_ecc_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_MEM_PARITY */
+		.err_category = AIE_ERROR_CATEGORY_MEM_PARITY,
+		.num_events = ARRAY_SIZE(aie_mem_parity_error_prop),
+		.prop = aie_mem_parity_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_DMA */
+		.err_category = AIE_ERROR_CATEGORY_DMA,
+		.num_events = ARRAY_SIZE(aie_mem_dma_error_prop),
+		.prop = aie_mem_dma_error_prop,
+	},
+};
+
+static const struct aie_err_category aie_shim_err_category[] = {
+	{
+		/* AIE_ERROR_CATEGORY_BUS */
+		.err_category = AIE_ERROR_CATEGORY_BUS,
+		.num_events = ARRAY_SIZE(aie_shim_bus_error_prop),
+		.prop = aie_shim_bus_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_STREAM */
+		.err_category = AIE_ERROR_CATEGORY_STREAM,
+		.num_events = ARRAY_SIZE(aie_shim_stream_error_prop),
+		.prop = aie_shim_stream_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_DMA */
+		.err_category = AIE_ERROR_CATEGORY_DMA,
+		.num_events = ARRAY_SIZE(aie_shim_dma_error_prop),
+		.prop = aie_shim_dma_error_prop,
+	},
+};
+
+static const struct aie_error_attr aie_core_error = {
+	.num_err_categories = ARRAY_SIZE(aie_core_err_category),
+	.err_category = aie_core_err_category,
+};
+
+static const struct aie_error_attr aie_mem_error = {
+	.num_err_categories = ARRAY_SIZE(aie_mem_err_category),
+	.err_category = aie_mem_err_category,
+};
+
+static const struct aie_error_attr aie_shim_error = {
+	.num_err_categories = ARRAY_SIZE(aie_shim_err_category),
+	.err_category = aie_shim_err_category,
+};
+
+/* resource attributes for core tile type */
+static const
+struct aie_tile_rsc_attr aie_core_tile_rscs_attr[AIE_RSCTYPE_MAX] =  {
+	{
+		/* perf counter */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_PERF_MEM_MOD,},
+			{.num_rscs = AIE_NUM_PERF_CORE_MOD,},
+		},
+	},
+	{
+		/* user event */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_USEREVENT_MEM_MOD,},
+			{.num_rscs = AIE_NUM_USEREVENT_CORE_MOD,},
+		},
+	},
+	{
+		/* trace control */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_TRACECONTROL_MEM_MOD,},
+			{.num_rscs = AIE_NUM_TRACECONTROL_CORE_MOD,},
+		},
+	},
+	{
+		/* pc event */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_PCEVENT_MEM_MOD,},
+			{.num_rscs = AIE_NUM_PCEVENT_CORE_MOD,},
+		},
+	},
+	{
+		/* stream switch port select */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_SSSELECT_MEM_MOD,},
+			{.num_rscs = AIE_NUM_SSSELECT_CORE_MOD,},
+		},
+	},
+	{
+		/* broadcast */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_BROADCAST_MEM_MOD,},
+			{.num_rscs = AIE_NUM_BROADCAST_CORE_MOD,},
+		},
+	},
+	{
+		/* combo events */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_COMBOEVENT_MEM_MOD,},
+			{.num_rscs = AIE_NUM_COMBOEVENT_CORE_MOD,},
+		},
+	},
+	{
+		/* group events */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_GROUPEVENTS_MEM_MOD,},
+			{.num_rscs = AIE_NUM_GROUPEVENTS_CORE_MOD,},
+		},
+	},
+};
+
+/* resource attributes for SHIM PL tile type */
+static const
+struct aie_tile_rsc_attr aie_shimpl_tile_rscs_attr[AIE_RSCTYPE_MAX] =  {
+	{
+		/* perf counter */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_PERF_PL_MOD,},
+		},
+	},
+	{
+		/* user event */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_USEREVENT_PL_MOD,},
+		},
+	},
+	{
+		/* trace control */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_TRACECONTROL_PL_MOD},
+		},
+	},
+	{
+		/* pc event */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_PCEVENT_PL_MOD},
+		},
+	},
+	{
+		/* stream switch port select */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_SSSELECT_PL_MOD},
+		},
+	},
+	{
+		/* broadcast */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_BROADCAST_PL_MOD},
+		},
+	},
+	{
+		/* combo events */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_COMBOEVENT_PL_MOD},
+		},
+	},
+	{
+		/* group events */
+		.mod_attr = {
+			{.num_rscs = AIE_NUM_GROUPEVENTS_PL_MOD},
+		},
+	},
+};
+
+/* Events needed for core tile utilization */
+static const
+enum aie_events aie_core_util_events[NUM_UTIL_EVENTS] = {
+		[AIE_EVENT_CORE_ACTIVE] = 28,
+		[AIE_EVENT_CORE_DISABLED] = 29,
+		[AIE_EVENT_CORE_USER_EVNT_0] = 124,
+		[AIE_EVENT_CORE_USER_EVNT_1] = 125,
+};
+
+/* modules types array of CORE tile */
+static const
+enum aie_module_type aie_core_tile_module_types[NUM_MODS_CORE_TILE] = {
+	AIE_MEM_MOD,
+	AIE_CORE_MOD,
+};
+
+/* modules types array of SHIM PL tile */
+static const
+enum aie_module_type aie_shimpl_tile_module_types[NUM_MODS_SHIMPL_TILE] = {
+	AIE_PL_MOD,
+};
+
+static const struct aie_single_reg_field aie_core_sts = {
+	.mask = GENMASK(20, 0),
+	.regoff = 0x32004U,
+};
+
+static const struct aie_single_reg_field aie_core_done = {
+	.mask = BIT(20),
+	.regoff = 0x32004U,
+};
+
+static const struct aie_single_reg_field aie_core_disable_event_sts = {
+	.mask = BIT(15),
+	.regoff = 0x32008U,
+};
+
+static const struct aie_single_reg_field aie_core_pc = {
+	.mask = GENMASK(19, 0),
+	.regoff = 0x30280U,
+};
+
+static const struct aie_single_reg_field aie_core_lr = {
+	.mask = GENMASK(19, 0),
+	.regoff = 0x302B0U,
+};
+
+static const struct aie_single_reg_field aie_core_sp = {
+	.mask = GENMASK(19, 0),
+	.regoff = 0x302A0U,
+};
+
+static char *aie_core_status_str[] = {
+	"enabled",
+	"reset",
+	"south_memory_stall",
+	"west_memory_stall",
+	"north_memory_stall",
+	"east_memory_stall",
+	"south_lock_stall",
+	"west_lock_stall",
+	"north_lock_stall",
+	"east_lock_stall",
+	"stream_stall_ss0",
+	"stream_stall_ss1",
+	"stream_stall_ms0",
+	"stream_stall_ms1",
+	"cascade_stall_scd",
+	"cascade_stall_mcd",
+	"debug_halt",
+	"ecc_error_stall",
+	"ecc_scrubbing_stall",
+	"error_halt",
+	"core_done",
+};
+
+static const struct aie_lock_attr aie_pl_lock = {
+	.sts = {
+		.mask = GENMASK(1, 0),
+		.regoff = 2U,
+	},
+	.sts_regoff = 0x14F00,
+	.num_locks = 16U,
+};
+
+static const struct aie_lock_attr aie_mem_lock = {
+	.sts = {
+		.mask = GENMASK(1, 0),
+		.regoff = 2U,
+	},
+	.sts_regoff = 0x1EF00,
+	.num_locks = 16U,
+};
+
+static char *aie_lock_status_str[] = {
+	"released_for_write",
+	"acquired_for_write",
+	"released_for_read",
+	"acquired_for_read",
+};
+
+static const struct aie_dev_attr aie_tile_dev_attr[] = {
+	AIE_TILE_DEV_ATTR_RO(bd, AIE_TILE_TYPE_MASK_TILE |
+			     AIE_TILE_TYPE_MASK_SHIMNOC),
+	AIE_TILE_DEV_ATTR_RO(core, AIE_TILE_TYPE_MASK_TILE),
+	AIE_TILE_DEV_ATTR_RO(dma, AIE_TILE_TYPE_MASK_TILE |
+			     AIE_TILE_TYPE_MASK_SHIMNOC),
+	AIE_TILE_DEV_ATTR_RO(error, AIE_TILE_TYPE_MASK_TILE |
+			     AIE_TILE_TYPE_MASK_SHIMNOC |
+			     AIE_TILE_TYPE_MASK_SHIMPL),
+	AIE_TILE_DEV_ATTR_RO(event, AIE_TILE_TYPE_MASK_TILE |
+			     AIE_TILE_TYPE_MASK_SHIMNOC |
+			     AIE_TILE_TYPE_MASK_SHIMPL),
+	AIE_TILE_DEV_ATTR_RO(lock, AIE_TILE_TYPE_MASK_TILE |
+			     AIE_TILE_TYPE_MASK_SHIMNOC),
+};
+
+static const struct aie_dev_attr aie_aperture_dev_attr[] = {
+	AIE_APERTURE_ATTR_RO(hardware_info),
+};
+
+static const struct aie_dev_attr aie_part_dev_attr[] = {
+	AIE_PART_DEV_ATTR_RO(error_stat),
+	AIE_PART_DEV_ATTR_RO(current_freq),
+};
+
+static const struct aie_bin_attr aie_part_bin_attr[] = {
+	AIE_PART_BIN_ATTR_RO(core, AIE_PART_SYSFS_CORE_BINA_SIZE),
+	AIE_PART_BIN_ATTR_RO(dma, AIE_PART_SYSFS_DMA_BINA_SIZE),
+	AIE_PART_BIN_ATTR_RO(error, AIE_PART_SYSFS_ERROR_BINA_SIZE),
+	AIE_PART_BIN_ATTR_RO(lock, AIE_PART_SYSFS_LOCK_BINA_SIZE),
+	AIE_PART_BIN_ATTR_RO(status, AIE_PART_SYSFS_STATUS_BINA_SIZE),
+};
+
+static const struct aie_sysfs_attr aie_aperture_sysfs_attr = {
+	.dev_attr = aie_aperture_dev_attr,
+	.bin_attr = NULL,
+	.num_dev_attrs = ARRAY_SIZE(aie_aperture_dev_attr),
+	.num_bin_attrs = 0U,
+};
+
+static const struct aie_sysfs_attr aie_part_sysfs_attr = {
+	.dev_attr = aie_part_dev_attr,
+	.bin_attr = aie_part_bin_attr,
+	.num_dev_attrs = ARRAY_SIZE(aie_part_dev_attr),
+	.num_bin_attrs = ARRAY_SIZE(aie_part_bin_attr),
+};
+
+static const struct aie_sysfs_attr aie_tile_sysfs_attr = {
+	.dev_attr = aie_tile_dev_attr,
+	.bin_attr = NULL,
+	.num_dev_attrs = ARRAY_SIZE(aie_tile_dev_attr),
+	.num_bin_attrs = 0U,
+};
+
+static u32 aie_get_tile_type(struct aie_device *adev, struct aie_location *loc)
+{
+	if (loc->row)
+		return AIE_TILE_TYPE_TILE;
+	/* SHIM row */
+	if ((loc->col % 4) < 2)
+		return AIE_TILE_TYPE_SHIMPL;
+
+	if (adev->device_name == AIE_DEV_GEN_S100 ||
+	    adev->device_name == AIE_DEV_GEN_S200) {
+		if (loc->col == 58)
+			return AIE_TILE_TYPE_SHIMPL;
+	}
+
+	return AIE_TILE_TYPE_SHIMNOC;
+}
+
+static unsigned int aie_get_mem_info(struct aie_device *adev,
+				     struct aie_range *range,
+				     struct aie_part_mem *pmem)
+{
+	unsigned int i;
+	u8 start_row, num_rows;
+
+	if (range->start.row + range->size.row <= 1) {
+		/* SHIM row only, no memories in this range */
+		return 0;
+	}
+	if (!pmem)
+		return NUM_MEMS_PER_TILE;
+
+	for (i = 0; i < NUM_MEMS_PER_TILE; i++) {
+		struct aie_mem *mem = &pmem[i].mem;
+
+		memcpy(&mem->range, range, sizeof(*range));
+	}
+
+	start_row = adev->ttype_attr[AIE_TILE_TYPE_TILE].start_row;
+	num_rows = adev->ttype_attr[AIE_TILE_TYPE_TILE].num_rows;
+	/* Setup tile data memory information */
+	pmem[0].mem.offset = 0;
+	pmem[0].mem.size = KBYTES(32);
+	pmem[0].mem.range.start.row = start_row;
+	pmem[0].mem.range.size.row = num_rows;
+	/* Setup program memory information */
+	pmem[1].mem.offset = 0x20000;
+	pmem[1].mem.size = KBYTES(16);
+	pmem[1].mem.range.start.row = start_row;
+	pmem[1].mem.range.size.row = num_rows;
+
+	return NUM_MEMS_PER_TILE;
+}
+
+static int aie_init_part_clk_state(struct aie_partition *apart)
+{
+	int ret, num_tiles;
+
+	num_tiles = apart->range.size.col * (apart->range.size.row - 1);
+
+	ret = aie_resource_initialize(&apart->cores_clk_state, num_tiles);
+	if (ret) {
+		dev_err(&apart->dev,
+			"failed to initialize cores clock state resource.\n");
+		return ret;
+	}
+
+	ret = aie_resource_initialize(&apart->tiles_inuse, num_tiles);
+	if (ret) {
+		dev_err(&apart->dev,
+			"failed to initialize tiles in use resource.\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+static int aie_scan_part_clocks(struct aie_partition *apart)
+{
+	struct aie_device *adev = apart->adev;
+	struct aie_aperture *aperture = apart->aperture;
+	struct aie_range *range = &apart->range;
+	struct aie_location loc;
+
+	/* Clear the bitmap of cores and memories clock state */
+	aie_resource_put_region(&apart->cores_clk_state, 0,
+				apart->cores_clk_state.total);
+
+	for (loc.col = range->start.col;
+	     loc.col < range->start.col + range->size.col;
+	     loc.col++) {
+		for (loc.row = range->start.row;
+		     loc.row < range->start.row + range->size.row - 1;
+		     loc.row++) {
+			void __iomem *va;
+			u32 val, nbitpos;
+
+			/*
+			 * Reading registers of the current tile to see the next
+			 * tile is clock gated.
+			 */
+			nbitpos = loc.col * (range->size.row - 1) + loc.row;
+
+			if (aie_get_tile_type(adev, &loc) !=
+					AIE_TILE_TYPE_TILE) {
+				/* Checks shim tile for next core tile */
+				va = aperture->base +
+				     aie_cal_regoff(adev, loc,
+						    AIE_SHIMPL_CLKCNTR_REGOFF);
+				val = ioread32(va);
+
+				/*
+				 * check if the clock buffer and the next clock
+				 * tile is set, if one of them is not set, the
+				 * tiles of the column are clock gated.
+				 */
+				if (!(val & AIE_SHIMPL_CLKCNTR_COLBUF_MASK) ||
+				    !(val & AIE_SHIMPL_CLKCNTR_NEXTCLK_MASK))
+					break;
+
+				/* Set next tile in the row clock state on */
+				aie_resource_set(&apart->cores_clk_state,
+						 nbitpos, 1);
+				continue;
+			}
+
+			/* Checks core tile for next tile */
+			va = aperture->base +
+			     aie_cal_regoff(adev, loc,
+					    AIE_TILE_CORE_CLKCNTR_REGOFF);
+			val = ioread32(va);
+
+			/*
+			 * If the next tile is gated, skip the rest of the
+			 * column.
+			 */
+			if (!(val & AIE_TILE_CLKCNTR_NEXTCLK_MASK))
+				break;
+
+			aie_resource_set(&apart->cores_clk_state, nbitpos, 1);
+		}
+	}
+
+	/*
+	 * Set the tiles in use bitmap.
+	 * In case of scanning, tiles which are powered on are considered as
+	 * tiles in use.
+	 */
+	bitmap_copy(apart->tiles_inuse.bitmap, apart->cores_clk_state.bitmap,
+		    apart->tiles_inuse.total);
+
+	return 0;
+}
+
+/* aie_set_col_clocks() - set clocks of a range of tiles of a column
+ * @apart: AI engine partition
+ * @range: range of tiles of a column
+ * @enable: true to enable the clock, false to disable
+ * @return: 0 for success, negative value of errors.
+ */
+static int aie_set_col_clocks(struct aie_partition *apart,
+			      struct aie_range *range, bool enable)
+{
+	struct aie_location ploc;
+	u32 startbit;
+
+	/*
+	 * check if the range is of single colum. only single column is allowed.
+	 * check if the start row is tile row, only tile rows are allowed.
+	 */
+	if (range->size.col != 1 || range->start.row < 1)
+		return -EINVAL;
+
+	ploc.col = range->start.col;
+	for (ploc.row = range->start.row - 1;
+	     ploc.row < range->start.row + range->size.row - 1;
+	     ploc.row++) {
+		struct aie_device *adev = apart->adev;
+		struct aie_aperture *aperture = apart->aperture;
+
+		if (!ploc.row) {
+			void __iomem *va;
+			u32 val = 0;
+
+			/*
+			 * Configure SHIM clock registers to gate or
+			 * ungate next tile.
+			 */
+			if (enable)
+				val = AIE_SHIMPL_CLKCNTR_COLBUF_MASK |
+				      AIE_SHIMPL_CLKCNTR_NEXTCLK_MASK;
+			va = aperture->base +
+			     aie_cal_regoff(adev, ploc,
+					    AIE_SHIMPL_CLKCNTR_REGOFF);
+			iowrite32(val, va);
+		} else {
+			void __iomem *va;
+			u32 val = 0;
+
+			/*
+			 * Configure core tile clock registers to gate
+			 * or ungate next tile.
+			 */
+			if (enable)
+				val = AIE_TILE_CLKCNTR_COLBUF_MASK |
+				      AIE_TILE_CLKCNTR_NEXTCLK_MASK;
+			va = aperture->base +
+			     aie_cal_regoff(adev, ploc,
+					    AIE_TILE_CORE_CLKCNTR_REGOFF);
+			iowrite32(val, va);
+		}
+
+		/* If the tile clock is not on, jump to next column */
+		if (!enable)
+			break;
+	}
+
+	/* Update clock state bitmap */
+	startbit = (range->start.col - apart->range.start.col) *
+		   (apart->range.size.row - 1) + range->start.row - 1;
+	if (enable)
+		aie_resource_set(&apart->cores_clk_state, startbit,
+				 range->size.row);
+	else
+		aie_resource_clear(&apart->cores_clk_state, startbit,
+				   range->size.row);
+
+	return 0;
+}
+
+static int aie_set_part_clocks(struct aie_partition *apart)
+{
+	struct aie_range *range = &apart->range, lrange;
+	struct aie_location rloc;
+
+	/*
+	 * The tiles below the highest tile whose clock is on, need to have the
+	 * clock on. The first for loop is to scan the clock states bitmap to
+	 * see which tiles are required to be clocked on, and update the bitmap
+	 * to make sure the tiles below are also required to be clocked on.
+	 */
+	for (rloc.col = 0; rloc.col < range->size.col; rloc.col++) {
+		u32 startbit, inuse_toprow = 0, clk_toprow = 0;
+
+		startbit = rloc.col * (range->size.row - 1);
+
+		for (rloc.row = range->start.row + 1;
+		     rloc.row < range->start.row + range->size.row;
+		     rloc.row++) {
+			u32 bit = startbit + rloc.row - 1;
+
+			if (aie_resource_testbit(&apart->tiles_inuse, bit))
+				inuse_toprow = rloc.row;
+			if (aie_resource_testbit(&apart->cores_clk_state, bit))
+				clk_toprow = rloc.row;
+		}
+
+		/* Update clock states of a column */
+		lrange.start.col = rloc.col + range->start.col;
+		lrange.size.col = 1;
+		if (inuse_toprow < clk_toprow) {
+			lrange.start.row = inuse_toprow + 1;
+			lrange.size.row = clk_toprow - inuse_toprow;
+			aie_set_col_clocks(apart, &lrange, false);
+		} else  if (inuse_toprow > clk_toprow) {
+			lrange.start.row = clk_toprow + 1;
+			lrange.size.row = inuse_toprow - clk_toprow;
+			aie_set_col_clocks(apart, &lrange, true);
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * aie_get_core_status() - read the AI engine core status register.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine core.
+ * @return: 32-bit register value.
+ */
+static u32 aie_get_core_status(struct aie_partition *apart,
+			       struct aie_location *loc)
+{
+	u32 regoff, regvalue, eventval;
+
+	regoff = aie_cal_regoff(apart->adev, *loc, aie_core_sts.regoff);
+	regvalue = ioread32(apart->aperture->base + regoff);
+
+	/* Apply core done workaround */
+	if (!FIELD_GET(aie_core_done.mask, regvalue)) {
+		regoff = aie_cal_regoff(apart->adev, *loc,
+					aie_core_disable_event_sts.regoff);
+		eventval = ioread32(apart->aperture->base + regoff);
+
+		if (FIELD_GET(aie_core_disable_event_sts.mask, eventval))
+			regvalue |= aie_core_done.mask;
+	}
+	return regvalue;
+}
+
+/**
+ * aie_part_clear_mems() - clear memories of every tile in a partition
+ * @apart: AI engine partition
+ * @return: return 0 always.
+ */
+static int aie_part_clear_mems(struct aie_partition *apart)
+{
+	struct aie_device *adev = apart->adev;
+	struct aie_part_mem *pmems = apart->pmems;
+	u32 i, num_mems;
+
+	/* Get the number of different types of memories */
+	num_mems = adev->ops->get_mem_info(adev, &apart->range, NULL);
+	if (!num_mems)
+		return 0;
+
+	/* Clear each type of memories in the partition */
+	for (i = 0; i < num_mems; i++) {
+		struct aie_mem *mem = &pmems[i].mem;
+		struct aie_range *range = &mem->range;
+		u32 c, r;
+
+		for (c = range->start.col;
+		     c < range->start.col + range->size.col; c++) {
+			for (r = range->start.row;
+			     r < range->start.row + range->size.row; r++) {
+				struct aie_location loc;
+				u32 memoff;
+
+				loc.col = c;
+				loc.row = r;
+				memoff = aie_cal_regoff(adev, loc, mem->offset);
+				memset_io(apart->aperture->base + memoff, 0,
+					  mem->size);
+			}
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * aie_set_tile_isolation() - Set isolation boundary of AI engine tile
+ * @apart: AI engine partition
+ * @loc: Location of tile
+ * @dir: Direction to block
+ * @return: return 0 if success negative value for failure.
+ *
+ * Possible direction values are:
+ *	- AIE_ISOLATE_EAST_MASK
+ *	- AIE_ISOLATE_NORTH_MASK
+ *	- AIE_ISOLATE_WEST_MASK
+ *	- AIE_ISOLATE_SOUTH_MASK
+ *	- AIE_ISOLATE_ALL_MASK
+ *	- or "OR" of multiple values
+ */
+static int aie_set_tile_isolation(struct aie_partition *apart,
+				  struct aie_location *loc, u8 dir)
+{
+	struct aie_device *adev = apart->adev;
+	struct aie_aperture *aperture = apart->aperture;
+	void __iomem *va;
+	u32 ttype, val;
+
+	/* For AIE device, dir input will match register masks */
+	val = (u32)dir;
+	ttype = aie_get_tile_type(adev, loc);
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		va = aperture->base +
+		     aie_cal_regoff(adev, *loc, AIE_TILE_CORE_TILECTRL_REGOFF);
+	} else {
+		va = aperture->base +
+		     aie_cal_regoff(adev, *loc, AIE_SHIMPL_TILECTRL_REGOFF);
+	}
+	iowrite32(val, va);
+
+	return 0;
+}
+
+/**
+ * aie_get_lock_status() - reads the lock status.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @return: 32-bit register value.
+ */
+static u32 aie_get_lock_status(struct aie_partition *apart,
+			       struct aie_location *loc)
+{
+	u32 ttype, stsoff, regoff;
+
+	ttype = aie_get_tile_type(apart->adev, loc);
+	if (ttype != AIE_TILE_TYPE_TILE)
+		stsoff = aie_pl_lock.sts_regoff;
+	else
+		stsoff = aie_mem_lock.sts_regoff;
+
+	regoff = aie_cal_regoff(apart->adev, *loc, stsoff);
+
+	return ioread32(apart->aperture->base + regoff);
+}
+
+/**
+ * aie_get_lock_status_str() - returns the string value corresponding to
+ *			       lock status value.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine lock.
+ * @status: status value of lock.
+ * @lock: lock ID.
+ * @buffer: location to return lock status string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+static ssize_t aie_get_lock_status_str(struct aie_partition *apart,
+				       struct aie_location *loc, u32 status,
+				       u32 lock, char *buffer, ssize_t size)
+{
+	char **str = aie_lock_status_str;
+	u32 ttype, mask;
+	u8 value, shift;
+
+	ttype = aie_get_tile_type(apart->adev, loc);
+	if (ttype != AIE_TILE_TYPE_TILE) {
+		shift = lock * aie_pl_lock.sts.regoff;
+		mask = aie_pl_lock.sts.mask << shift;
+	} else {
+		shift = lock * aie_mem_lock.sts.regoff;
+		mask = aie_mem_lock.sts.mask << shift;
+	}
+
+	value = (status & mask) >> shift;
+
+	return scnprintf(buffer, max(0L, size), str[value]);
+}
+
+static ssize_t aie_get_tile_sysfs_lock_status(struct aie_partition *apart,
+					      struct aie_location *loc,
+					      char *buffer, ssize_t size)
+{
+	u32 i, ttype, num_locks;
+	unsigned long status;
+	ssize_t len = 0;
+
+	ttype = aie_get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_SHIMPL)
+		return len;
+
+	if (ttype == AIE_TILE_TYPE_TILE)
+		num_locks = aie_mem_lock.num_locks;
+	else
+		num_locks = aie_pl_lock.num_locks;
+
+	if (!aie_part_check_clk_enable_loc(apart, loc)) {
+		for (i = 0; i < num_locks; i++) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%d: clock_gated\n", i);
+		}
+		return len;
+	}
+
+	status = aie_get_lock_status(apart, loc);
+	for (i = 0; i < num_locks; i++) {
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d: ", i);
+		len += aie_get_lock_status_str(apart, loc, status, i,
+					       &buffer[len], size - len);
+		len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	}
+
+	return len;
+}
+
+static ssize_t aie_get_part_sysfs_lock_status(struct aie_partition *apart,
+					      struct aie_location *loc,
+					      char *buffer, ssize_t size)
+{
+	u32 i, ttype, num_locks;
+	unsigned long status;
+	ssize_t len = 0;
+
+	ttype = aie_get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_SHIMPL)
+		return len;
+
+	if (!aie_part_check_clk_enable_loc(apart, loc)) {
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "clock_gated");
+		return len;
+	}
+
+	if (ttype == AIE_TILE_TYPE_TILE)
+		num_locks = aie_mem_lock.num_locks;
+	else
+		num_locks = aie_pl_lock.num_locks;
+
+	status = aie_get_lock_status(apart, loc);
+	for (i = 0; i < num_locks; i++) {
+		len += aie_get_lock_status_str(apart, loc, status, i,
+					       &buffer[len], size - len);
+		if (i < num_locks - 1) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 DELIMITER_LEVEL0);
+		}
+	}
+	return len;
+}
+
+/*
+ * aie_get_tile_bd_attr() - gets tile bd attribute for AIE
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @attr: pointer of attribute to assign
+ */
+static void aie_get_tile_bd_attr(struct aie_partition *apart,
+				 struct aie_location *loc,
+				 const struct aie_bd_attr **attr)
+{
+	u32 ttype;
+
+	ttype = aie_get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_TILE)
+		*attr = &aie_tilebd;
+	else
+		*attr = &aie_shimbd;
+}
+
+/*
+ * aie_get_tile_dma_attr() - gets tile dma attribute for AIE
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @attr: pointer of attribute to assign
+ */
+static void aie_get_tile_dma_attr(struct aie_partition *apart,
+				  struct aie_location *loc,
+				  const struct aie_dma_attr **attr)
+{
+	u32 ttype;
+
+	ttype = aie_get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_TILE)
+		*attr = &aie_tiledma;
+	else
+		*attr = &aie_shimdma;
+}
+
+/**
+ * aie_get_dma_s2mm_status() - reads the DMA stream to memory map status.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @return: 32-bit register value.
+ */
+static u32 aie_get_dma_s2mm_status(struct aie_partition *apart,
+				   struct aie_location *loc)
+{
+	u32 stsoff, regoff, ttype;
+
+	ttype = aie_get_tile_type(apart->adev, loc);
+	if (ttype != AIE_TILE_TYPE_TILE)
+		stsoff = aie_shimdma.s2mm_sts_regoff;
+	else
+		stsoff = aie_tiledma.s2mm_sts_regoff;
+
+	regoff = aie_cal_regoff(apart->adev, *loc, stsoff);
+
+	return ioread32(apart->aperture->base + regoff);
+}
+
+/**
+ * aie_get_dma_mm2s_status() - reads the DMA memory map to stream status.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @return: 32-bit register value.
+ */
+static u32 aie_get_dma_mm2s_status(struct aie_partition *apart,
+				   struct aie_location *loc)
+{
+	u32 stsoff, regoff, ttype;
+
+	ttype = aie_get_tile_type(apart->adev, loc);
+	if (ttype != AIE_TILE_TYPE_TILE)
+		stsoff = aie_shimdma.mm2s_sts_regoff;
+	else
+		stsoff = aie_tiledma.mm2s_sts_regoff;
+
+	regoff = aie_cal_regoff(apart->adev, *loc, stsoff);
+
+	return ioread32(apart->aperture->base + regoff);
+}
+
+/**
+ * aie_get_chan_status() - reads the DMA channel status.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @status: status value of DMA.
+ * @chanid: DMA channel ID.
+ * @return: 8-bit status value.
+ */
+static u8 aie_get_chan_status(struct aie_partition *apart,
+			      struct aie_location *loc, u32 status, u8 chanid)
+{
+	const struct aie_single_reg_field *sts, *stall;
+	u32 mask, chan_shift, shift, value, ttype;
+
+	ttype = aie_get_tile_type(apart->adev, loc);
+	if (ttype != AIE_TILE_TYPE_TILE) {
+		sts = &aie_shimdma.sts;
+		stall = &aie_shimdma.stall;
+	} else {
+		sts = &aie_tiledma.sts;
+		stall = &aie_tiledma.stall;
+	}
+
+	/* Calculate channel status bit */
+	chan_shift = sts->regoff;
+	mask = sts->mask << (chan_shift * chanid);
+	shift = ffs(mask) - 1;
+	value = (status & mask) >> shift;
+
+	/* Calculate stall status bit */
+	chan_shift = stall->regoff;
+	mask = stall->mask << (chan_shift * chanid);
+	shift = ffs(mask) - 1;
+	value |= (status & mask) >> shift;
+
+	/* If invalid value set to "invalid_status" */
+	if (value >= ARRAY_SIZE(aie_dma_status_str))
+		value = ARRAY_SIZE(aie_dma_status_str) - 1;
+
+	return value;
+}
+
+/**
+ * aie_get_queue_size() - reads the DMA queue size.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @status: status value of DMA.
+ * @chanid: DMA channel ID.
+ * @return: 8-bit value.
+ */
+static u8 aie_get_queue_size(struct aie_partition *apart,
+			     struct aie_location *loc, u32 status, u8 chanid)
+{
+	const struct aie_single_reg_field *qsize;
+	u32 mask, chan_shift, shift, ttype;
+
+	ttype = aie_get_tile_type(apart->adev, loc);
+	if (ttype != AIE_TILE_TYPE_TILE)
+		qsize = &aie_shimdma.qsize;
+	else
+		qsize = &aie_tiledma.qsize;
+
+	/* Calculate queue size bit */
+	chan_shift = qsize->regoff;
+	mask = qsize->mask << (chan_shift * chanid);
+	shift = ffs(mask) - 1;
+
+	return (status & mask) >> shift;
+}
+
+/**
+ * aie_get_queue_status() - reads the DMA queue status.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @status: status value of DMA.
+ * @chanid: DMA channel ID.
+ * @return: 8-bit status value.
+ */
+static u8 aie_get_queue_status(struct aie_partition *apart,
+			       struct aie_location *loc, u32 status, u8 chanid)
+{
+	const struct aie_single_reg_field *qsts;
+	u32 mask, chan_shift, shift, ttype;
+
+	ttype = aie_get_tile_type(apart->adev, loc);
+	if (ttype != AIE_TILE_TYPE_TILE)
+		qsts = &aie_shimdma.qsts;
+	else
+		qsts = &aie_tiledma.qsts;
+
+	/* Get queue status bit */
+	chan_shift = qsts->regoff;
+	mask = qsts->mask << (chan_shift * chanid);
+	shift = ffs(mask) - 1;
+
+	return (status & mask) >> shift;
+}
+
+/**
+ * aie_get_current_bd() - reads the current buffer descriptor being processed
+ *			  by DMA channel.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @status: status value of DMA.
+ * @chanid: DMA channel ID.
+ * @return: 8-bit buffer descriptor value.
+ */
+static u8 aie_get_current_bd(struct aie_partition *apart,
+			     struct aie_location *loc, u32 status, u8 chanid)
+{
+	const struct aie_single_reg_field *curbd;
+	u32 mask, chan_shift, shift, ttype;
+
+	ttype = aie_get_tile_type(apart->adev, loc);
+	if (ttype != AIE_TILE_TYPE_TILE)
+		curbd = &aie_shimdma.curbd;
+	else
+		curbd = &aie_tiledma.curbd;
+
+	/* Get current buffer descriptor */
+	chan_shift = curbd->regoff;
+	mask = curbd->mask << (chan_shift * chanid);
+	shift = ffs(mask) - 1;
+
+	return (status & mask) >> shift;
+}
+
+/**
+ * aie_get_fifo_status() - reads the current value of DMA FIFO counters.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @return: concatenated value of counters for AIE tiles and 0 for shim tiles.
+ */
+static u32 aie_get_fifo_status(struct aie_partition *apart,
+			       struct aie_location *loc)
+{
+	u32 fifo_off, regoff, ttype;
+
+	ttype = aie_get_tile_type(apart->adev, loc);
+	if (ttype != AIE_TILE_TYPE_TILE)
+		return 0U;
+
+	fifo_off = aie_tiledma.fifo_cnt_regoff;
+
+	regoff = aie_cal_regoff(apart->adev, *loc, fifo_off);
+
+	return ioread32(apart->aperture->base + regoff);
+}
+
+/**
+ * aie_get_fifo_count() - returns the value of a DMA FIFO counter from its
+ *			  concatenated register value.
+ * @apart: AI engine partition.
+ * @status: register value of DMA FIFO counter.
+ * @counterid: counter ID.
+ * @return: DMA FIFO count.
+ */
+static u32 aie_get_fifo_count(struct aie_partition *apart, u32 status,
+			      u8 counterid)
+{
+	status >>= (aie_tiledma.fifo_cnt.regoff * counterid);
+
+	return (status & aie_tiledma.fifo_cnt.mask);
+}
+
+/**
+ * aie_get_part_sysfs_dma_status() - returns the status of DMA in string format
+ *				     with MM2S and S2MM type channel separated
+ *				     by a ',' symbol. Channels with a given
+ *				     type are separated by a '|' symbol.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @buffer: location to return DMA status string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+static ssize_t aie_get_part_sysfs_dma_status(struct aie_partition *apart,
+					     struct aie_location *loc,
+					     char *buffer, ssize_t size)
+{
+	u32 i, ttype, num_s2mm_chan, num_mm2s_chan;
+	bool is_delimit_req = false;
+	unsigned long status;
+	ssize_t len = 0;
+
+	ttype = aie_get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_SHIMPL)
+		return len;
+
+	if (!aie_part_check_clk_enable_loc(apart, loc)) {
+		len += scnprintf(buffer, max(0L, size - len),
+				 "mm2s: clock_gated%ss2mm: clock_gated",
+				 DELIMITER_LEVEL1);
+		return len;
+	}
+
+	if (ttype != AIE_TILE_TYPE_TILE) {
+		num_mm2s_chan = aie_shimdma.num_mm2s_chan;
+		num_s2mm_chan = aie_shimdma.num_s2mm_chan;
+	} else {
+		num_mm2s_chan = aie_tiledma.num_mm2s_chan;
+		num_s2mm_chan = aie_tiledma.num_s2mm_chan;
+	}
+
+	/* MM2S */
+	len += scnprintf(&buffer[len], max(0L, size - len), "mm2s: ");
+	status = aie_get_dma_mm2s_status(apart, loc);
+
+	for (i = 0; i < num_mm2s_chan; i++) {
+		u32 value = aie_get_chan_status(apart, loc, status, i);
+
+		if (is_delimit_req) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 DELIMITER_LEVEL0);
+		}
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 aie_dma_status_str[value]);
+		is_delimit_req = true;
+	}
+
+	/* S2MM */
+	is_delimit_req = false;
+	len += scnprintf(&buffer[len], max(0L, size - len), "%ss2mm: ",
+			 DELIMITER_LEVEL1);
+	status = aie_get_dma_s2mm_status(apart, loc);
+
+	for (i = 0; i < num_s2mm_chan; i++) {
+		u32 value = aie_get_chan_status(apart, loc, status, i);
+
+		if (is_delimit_req) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 DELIMITER_LEVEL0);
+		}
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 aie_dma_status_str[value]);
+		is_delimit_req = true;
+	}
+	return len;
+}
+
+/*
+ * aie_get_tile_sysfs_dma_status() - exports AI engine DMA channel status,
+ *				     queue size, queue status, and current
+ *				     buffer descriptor ID being processed by
+ *				     DMA channel to a tile level sysfs node.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @buffer: location to return DMA status string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+static ssize_t aie_get_tile_sysfs_dma_status(struct aie_partition *apart,
+					     struct aie_location *loc,
+					     char *buffer, ssize_t size)
+{
+	u32 ttype, chan, mm2s, s2mm, num_s2mm_chan, num_mm2s_chan, fifo;
+	char **qsts_str = aie_queue_status_str;
+	bool is_delimit_req = false;
+	ssize_t len = 0;
+
+	if (!aie_part_check_clk_enable_loc(apart, loc)) {
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "channel_status: mm2s: clock_gated%ss2mm: clock_gated\n",
+				 DELIMITER_LEVEL1);
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "queue_size: mm2s: clock_gated%ss2mm: clock_gated\n",
+				 DELIMITER_LEVEL1);
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "queue_status: mm2s: clock_gated%ss2mm: clock_gated\n",
+				 DELIMITER_LEVEL1);
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "current_bd: mm2s: clock_gated%ss2mm: clock_gated\n",
+				 DELIMITER_LEVEL1);
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "fifo_len: clock_gated\n");
+		return len;
+	}
+
+	ttype = aie_get_tile_type(apart->adev, loc);
+	if (ttype != AIE_TILE_TYPE_TILE) {
+		num_mm2s_chan = aie_shimdma.num_mm2s_chan;
+		num_s2mm_chan = aie_shimdma.num_s2mm_chan;
+	} else {
+		num_mm2s_chan = aie_tiledma.num_mm2s_chan;
+		num_s2mm_chan = aie_tiledma.num_s2mm_chan;
+	}
+
+	len += scnprintf(&buffer[len], max(0L, size - len), "channel_status: ");
+	len += aie_get_part_sysfs_dma_status(apart, loc, &buffer[len],
+					     max(0L, size - len));
+
+	mm2s = aie_get_dma_mm2s_status(apart, loc);
+	s2mm = aie_get_dma_s2mm_status(apart, loc);
+
+	/* Queue size */
+	len += scnprintf(&buffer[len], max(0L, size - len),
+			 "\nqueue_size: mm2s: ");
+
+	for (chan = 0; chan < num_mm2s_chan; chan++) {
+		if (is_delimit_req) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					DELIMITER_LEVEL0);
+		}
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d",
+				 aie_get_queue_size(apart, loc, mm2s, chan));
+		is_delimit_req = true;
+	}
+
+	len += scnprintf(&buffer[len], max(0L, size - len), "%ss2mm: ",
+			 DELIMITER_LEVEL1);
+
+	is_delimit_req = false;
+	for (chan = 0; chan < num_s2mm_chan; chan++) {
+		if (is_delimit_req) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					DELIMITER_LEVEL0);
+		}
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d",
+				 aie_get_queue_size(apart, loc, s2mm, chan));
+		is_delimit_req = true;
+	}
+
+	/* Queue status */
+	len += scnprintf(&buffer[len], max(0L, size - len),
+			 "\nqueue_status: mm2s: ");
+
+	is_delimit_req = false;
+	for (chan = 0; chan < num_mm2s_chan; chan++) {
+		if (is_delimit_req) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					DELIMITER_LEVEL0);
+		}
+
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 qsts_str[aie_get_queue_status(apart, loc, mm2s,
+					  chan)]);
+		is_delimit_req = true;
+	}
+
+	len += scnprintf(&buffer[len], max(0L, size - len), "%ss2mm: ",
+			 DELIMITER_LEVEL1);
+
+	is_delimit_req = false;
+	for (chan = 0; chan < num_s2mm_chan; chan++) {
+		if (is_delimit_req) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					DELIMITER_LEVEL0);
+		}
+
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 qsts_str[aie_get_queue_status(apart, loc, s2mm,
+					  chan)]);
+		is_delimit_req = true;
+	}
+
+	/* Current BD */
+	len += scnprintf(&buffer[len], max(0L, size - len),
+			 "\ncurrent_bd: mm2s: ");
+
+	is_delimit_req = false;
+	for (chan = 0; chan < num_mm2s_chan; chan++) {
+		if (is_delimit_req) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					DELIMITER_LEVEL0);
+		}
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d",
+				 aie_get_current_bd(apart, loc, mm2s, chan));
+		is_delimit_req = true;
+	}
+
+	len += scnprintf(&buffer[len], max(0L, size - len), "%ss2mm: ",
+			 DELIMITER_LEVEL1);
+
+	is_delimit_req = false;
+	for (chan = 0; chan < num_s2mm_chan; chan++) {
+		if (is_delimit_req) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					DELIMITER_LEVEL0);
+		}
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d",
+				 aie_get_current_bd(apart, loc, s2mm, chan));
+		is_delimit_req = true;
+	}
+
+	/* FIFO length */
+	len += scnprintf(&buffer[len], max(0L, size - len), "\nfifo_len: ");
+
+	fifo = aie_get_fifo_status(apart, loc);
+	len += scnprintf(&buffer[len], max(0L, size - len), "%d%s%d\n",
+			 aie_get_fifo_count(apart, fifo, 0), DELIMITER_LEVEL0,
+			 aie_get_fifo_count(apart, fifo, 1));
+	return len;
+}
+
+/**
+ * aie_get_tile_sysfs_bd_metadata() - exports AI engine DMA buffer descriptor
+ *				      metadata for all buffer descriptors to
+ *				      a tile level sysfs node.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA buffer descriptors.
+ * @buffer: location to return DMA buffer descriptor metadata string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+static ssize_t aie_get_tile_sysfs_bd_metadata(struct aie_partition *apart,
+					      struct aie_location *loc,
+					      char *buffer, ssize_t size)
+{
+	const struct aie_dma_attr *dma_attr;
+	const struct aie_bd_attr *bd_attr;
+	u32 enabled, ttype;
+	ssize_t len = 0;
+
+	aie_get_tile_dma_attr(apart, loc, &dma_attr);
+	aie_get_tile_bd_attr(apart, loc, &bd_attr);
+
+	ttype = aie_get_tile_type(apart->adev, loc);
+	enabled = aie_part_check_clk_enable_loc(apart, loc);
+	for (u32 bd = 0; bd < dma_attr->num_bds; bd++) {
+		u32 bd_data[AIE_MAX_BD_SIZE];
+		u32 i, index, base_bdoff;
+		u64 value;
+
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "%d: ", bd);
+		if (!enabled) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "clock_gated\n");
+			continue;
+		}
+
+		base_bdoff = dma_attr->bd_regoff + (bd_attr->bd_idx_off * bd);
+		memset(bd_data, 0, sizeof(bd_data));
+		for (i = 0; i < dma_attr->bd_len / sizeof(u32); i++) {
+			u32 regoff;
+
+			regoff = aie_cal_regoff(apart->adev, *loc,
+						base_bdoff + (i * 4U));
+			bd_data[i] = ioread32(apart->aperture->base + regoff);
+		}
+
+		/* address and length */
+		if (ttype == AIE_TILE_TYPE_TILE) {
+			index = bd_attr->addr.addr.regoff / sizeof(u32);
+			value = aie_get_reg_field(&bd_attr->addr.addr,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%llx%s", value, DELIMITER_LEVEL0);
+			index = bd_attr->addr_2.addr.regoff / sizeof(u32);
+			value = aie_get_reg_field(&bd_attr->addr_2.addr,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%llx%s", value, DELIMITER_LEVEL0);
+		} else {
+			u32 h_addr;
+
+			index = bd_attr->addr.addr.regoff / sizeof(u32);
+			value = aie_get_reg_field(&bd_attr->addr.addr,
+						  bd_data[index]);
+			h_addr = bd_data[bd_attr->addr_2.addr.regoff / sizeof(u32)];
+			h_addr = aie_get_reg_field(&bd_attr->addr_2.addr, h_addr);
+			value |= (u64)h_addr << 32;
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%llx%s", value, DELIMITER_LEVEL0);
+		}
+
+		index = bd_attr->addr.length.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->addr.length, bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+
+		/* locks */
+		index = bd_attr->lock.lock_acq_id.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->lock.lock_acq_id,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->lock.lock_acq_val,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->lock.lock_acq_en,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->lock.lock_acq_val_en,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->lock.lock_rel_val,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->lock.lock_rel_en,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->lock.lock_rel_val_en,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+
+		if (ttype == AIE_TILE_TYPE_TILE) {
+			index = bd_attr->lock_2.lock_acq_id.regoff / sizeof(u32);
+			value = aie_get_reg_field(&bd_attr->lock_2.lock_acq_id,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+			value = aie_get_reg_field(&bd_attr->lock_2.lock_acq_val,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+			value = aie_get_reg_field(&bd_attr->lock_2.lock_acq_en,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+			value = aie_get_reg_field(&bd_attr->lock_2.lock_acq_val_en,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+			value = aie_get_reg_field(&bd_attr->lock_2.lock_rel_val,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+			value = aie_get_reg_field(&bd_attr->lock_2.lock_rel_en,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+			value = aie_get_reg_field(&bd_attr->lock_2.lock_rel_val_en,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+		}
+
+		/* packet */
+		index = bd_attr->packet.pkt_en.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->packet.pkt_en,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		index = bd_attr->packet.pkt_id.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->packet.pkt_id,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		index = bd_attr->packet.pkt_type.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->packet.pkt_type,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+
+		/* control */
+		index = bd_attr->valid_bd.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->valid_bd, bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		index = bd_attr->use_next.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->use_next, bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		index = bd_attr->next_bd.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->next_bd, bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+
+		/* axi settings */
+		if (ttype == AIE_TILE_TYPE_SHIMNOC) {
+			index = bd_attr->axi.smid.regoff / sizeof(u32);
+			value = aie_get_reg_field(&bd_attr->axi.smid,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+			value = aie_get_reg_field(&bd_attr->axi.cache,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+			value = aie_get_reg_field(&bd_attr->axi.qos,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+			value = aie_get_reg_field(&bd_attr->axi.secure_en,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+			value = aie_get_reg_field(&bd_attr->axi.burst_len,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld\n", value);
+			continue;
+		}
+
+		index = bd_attr->buf_sel.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->buf_sel, bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		index = bd_attr->curr_ptr.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->curr_ptr, bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		index = bd_attr->double_buff_en.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->double_buff_en,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		index = bd_attr->interleave_en.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->interleave_en,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		index = bd_attr->interleave_cnt.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->interleave_cnt,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		index = bd_attr->fifo_mode.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->fifo_mode, bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+
+		/* dimensions */
+		index = bd_attr->aie_dim.x_incr.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->aie_dim.x_incr,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->aie_dim.x_wrap,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->aie_dim.x_off,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		index = bd_attr->aie_dim.y_incr.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->aie_dim.y_incr,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->aie_dim.y_wrap,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->aie_dim.y_off,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld\n",
+				 value);
+	}
+
+	return len;
+}
+
+static const struct aie_tile_operations aie_ops = {
+	.get_tile_type = aie_get_tile_type,
+	.get_mem_info = aie_get_mem_info,
+	.get_core_status = aie_get_core_status,
+	.get_part_sysfs_lock_status = aie_get_part_sysfs_lock_status,
+	.get_tile_sysfs_lock_status = aie_get_tile_sysfs_lock_status,
+	.get_part_sysfs_dma_status = aie_get_part_sysfs_dma_status,
+	.get_tile_sysfs_dma_status = aie_get_tile_sysfs_dma_status,
+	.get_tile_sysfs_bd_metadata = aie_get_tile_sysfs_bd_metadata,
+	.init_part_clk_state = aie_init_part_clk_state,
+	.scan_part_clocks = aie_scan_part_clocks,
+	.set_part_clocks = aie_set_part_clocks,
+	.set_tile_isolation = aie_set_tile_isolation,
+	.mem_clear = aie_part_clear_mems,
+};
+
+/**
+ * aie_device_init_rscs_attr() - initialize AI engine device resources
+ *				 attributes
+ * @adev: AI engine device
+ */
+static void aie_device_init_rscs_attr(struct aie_device *adev)
+{
+	struct aie_tile_attr *tattr;
+
+	tattr = &adev->ttype_attr[AIE_TILE_TYPE_TILE];
+	tattr->num_mods = NUM_MODS_CORE_TILE;
+	tattr->rscs_attr = aie_core_tile_rscs_attr;
+	tattr->mods = aie_core_tile_module_types;
+
+	tattr = &adev->ttype_attr[AIE_TILE_TYPE_SHIMPL];
+	tattr->num_mods = NUM_MODS_SHIMPL_TILE;
+	tattr->rscs_attr = aie_shimpl_tile_rscs_attr;
+	tattr->mods = aie_shimpl_tile_module_types;
+
+	/*
+	 * For now, SHIMNOC is the same as SHIMPL as there is
+	 * no SHIMNOC specific resources managed by kernel
+	 * driver yet.
+	 */
+	tattr = &adev->ttype_attr[AIE_TILE_TYPE_SHIMNOC];
+	tattr->num_mods = NUM_MODS_SHIMPL_TILE;
+	tattr->rscs_attr = aie_shimpl_tile_rscs_attr;
+	tattr->mods = aie_shimpl_tile_module_types;
+}
+
+/**
+ * aie_device_init() - Initialize AI engine device struct AIE specific
+ * @adev: AI engine device
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function initialize the AI engine device structure device version
+ * specific elements such as register addressing related array shift,
+ * column shift, and row shift; AIE device specific device operations, device
+ * columns resource.
+ */
+int aie_device_init(struct aie_device *adev)
+{
+	adev->array_shift = AIE_ARRAY_SHIFT;
+	adev->col_shift = AIE_COL_SHIFT;
+	adev->row_shift = AIE_ROW_SHIFT;
+	adev->ops = &aie_ops;
+	adev->num_kernel_regs = ARRAY_SIZE(aie_kernel_regs);
+	adev->kernel_regs = aie_kernel_regs;
+	adev->num_core_regs = ARRAY_SIZE(aie_core_regs);
+	adev->core_regs = aie_core_regs;
+	adev->col_rst = &aie_col_rst;
+	adev->col_clkbuf = &aie_col_clkbuf;
+	adev->shim_bd = &aie_shimbd;
+	adev->tile_bd = &aie_tilebd;
+	adev->shim_dma = &aie_shimdma;
+	adev->tile_dma = &aie_tiledma;
+	adev->pl_events = &aie_pl_event;
+	adev->mem_events = &aie_mem_event;
+	adev->core_events = &aie_core_event;
+	adev->l1_ctrl = &aie_l1_intr_ctrl;
+	adev->l2_ctrl = &aie_l2_intr_ctrl;
+	adev->core_errors = &aie_core_error;
+	adev->mem_errors = &aie_mem_error;
+	adev->shim_errors = &aie_shim_error;
+	adev->aperture_sysfs_attr = &aie_aperture_sysfs_attr;
+	adev->part_sysfs_attr = &aie_part_sysfs_attr;
+	adev->tile_sysfs_attr = &aie_tile_sysfs_attr;
+	adev->core_status_str = aie_core_status_str;
+	adev->core_pc = &aie_core_pc;
+	adev->core_lr = &aie_core_lr;
+	adev->core_sp = &aie_core_sp;
+	adev->core_perfctrl = &aie_core_perfctrl;
+	adev->core_perfctrl_reset = &aie_core_perfctrl_reset;
+	adev->core_perfcnt = &aie_core_perfcnt;
+	adev->core_evntgen = &aie_core_evntgen;
+	adev->core_util_events = aie_core_util_events;
+
+	aie_device_init_rscs_attr(adev);
+
+	return 0;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-aieml.c b/drivers/misc/xilinx-ai-engine/ai-engine-aieml.c
new file mode 100644
index 000000000..4fcb06e9e
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-aieml.c
@@ -0,0 +1,2864 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine driver AIE-ML device specific implementation
+ *
+ * Copyright (C) 2022 Xilinx, Inc.
+ */
+
+#include <linux/bitfield.h>
+#include <linux/firmware/xlnx-zynqmp.h>
+#include <linux/io.h>
+#include <linux/slab.h>
+#include <linux/xlnx-ai-engine.h>
+
+#include "ai-engine-internal.h"
+
+#define AIEML_ARRAY_SHIFT	32U
+#define AIEML_COL_SHIFT		25U
+#define AIEML_ROW_SHIFT		20U
+
+#define NUM_TYPES_OF_MEM	3U
+
+#define NUM_MODS_CORE_TILE	2U
+#define NUM_MODS_MEM_TILE	1U
+#define NUM_MODS_SHIMPL_TILE	1U
+
+#define NUM_UTIL_EVENTS		4U
+
+/*
+ * Number of resources per module
+ */
+#define AIEML_NUM_PERF_TILE_CORE_MOD		4U
+#define AIEML_NUM_USEREVENT_TILE_CORE_MOD	4U
+#define AIEML_NUM_TRACECONTROL_TILE_CORE_MOD	1U
+#define AIEML_NUM_PCEVENT_TILE_CORE_MOD		4U
+#define AIEML_NUM_SSSELECT_TILE_CORE_MOD	8U
+#define AIEML_NUM_BROADCAST_TILE_CORE_MOD	16U
+#define AIEML_NUM_COMBOEVENT_TILE_CORE_MOD	4U
+#define AIEML_NUM_GROUPEVENTS_TILE_CORE_MOD	9U
+
+#define AIEML_NUM_PERF_TILE_MEM_MOD		2U
+#define AIEML_NUM_USEREVENT_TILE_MEM_MOD	4U
+#define AIEML_NUM_TRACECONTROL_TILE_MEM_MOD	1U
+#define AIEML_NUM_PCEVENT_TILE_MEM_MOD		0U
+#define AIEML_NUM_SSSELECT_TILE_MEM_MOD		0U
+#define AIEML_NUM_BROADCAST_TILE_MEM_MOD	16U
+#define AIEML_NUM_COMBOEVENT_TILE_MEM_MOD	4U
+#define AIEML_NUM_GROUPEVENTS_TILE_MEM_MOD	8U
+
+#define AIEML_NUM_PERF_MEM_MOD			4U
+#define AIEML_NUM_USEREVENT_MEM_MOD		2U
+#define AIEML_NUM_TRACECONTROL_MEM_MOD		1U
+#define AIEML_NUM_PCEVENT_MEM_MOD		0U
+#define AIEML_NUM_SSSELECT_MEM_MOD		8U
+#define AIEML_NUM_BROADCAST_MEM_MOD		16U
+#define AIEML_NUM_COMBOEVENT_MEM_MOD		4U
+#define AIEML_NUM_GROUPEVENTS_MEM_MOD		9U
+
+#define AIEML_NUM_PERF_PL_MOD			2U
+#define AIEML_NUM_USEREVENT_PL_MOD		2U
+#define AIEML_NUM_TRACECONTROL_PL_MOD		1U
+#define AIEML_NUM_PCEVENT_PL_MOD		0U
+#define AIEML_NUM_SSSELECT_PL_MOD		8U
+#define AIEML_NUM_BROADCAST_PL_MOD		16U
+#define AIEML_NUM_COMBOEVENT_PL_MOD		4U
+#define AIEML_NUM_GROUPEVENTS_PL_MOD		6U
+
+/*
+ * Register offsets
+ */
+#define AIEML_SHIMNOC_AXIMM_REGOFF			0x0001e020U
+#define AIEML_SHIMNOC_BD0_0_REGOFF			0x0001d000U
+#define AIEML_SHIMNOC_BD15_7_REGOFF			0x0001d1fcU
+#define AIEML_SHIMNOC_L2INTR_MASK_REGOFF		0x00015000U
+#define AIEML_SHIMNOC_L2INTR_INTR_REGOFF		0x00015010U
+#define AIEML_SHIMNOC_LOCK_REGOFF			0x00014000U
+#define AIEML_SHIMNOC_LOCK_OVERFLOW_REGOFF		0x00014120U
+#define AIEML_SHIMNOC_LOCK_UNDERFLOW_REGOFF		0x00014128U
+#define AIEML_SHIMNOC_DMA_S2MM_STATUS_REGOFF		0x0001D220U
+#define AIEML_SHIMNOC_DMA_MM2S_STATUS_REGOFF		0x0001D228U
+
+#define AIEML_SHIMPL_BISRCACHE_CTRL_REGOFF		0x00036000U
+#define AIEML_SHIMPL_COLCLOCK_CTRL_REGOFF		0x000fff20U
+#define AIEML_SHIMPL_COLRESET_CTRL_REGOFF		0x000fff28U
+#define AIEML_SHIMPL_EVENT_BC0_REGOFF			0x00034010U
+#define AIEML_SHIMPL_EVENT_STATUS0_REGOFF		0x00034200U
+#define AIEML_SHIMPL_GROUP0_REGOFF			0x00034500U
+#define AIEML_SHIMPL_GROUPERROR_REGOFF			0x0003450cU
+#define AIEML_SHIMPL_L1INTR_MASK_A_REGOFF		0x00035000U
+#define AIEML_SHIMPL_L1INTR_BLOCK_NORTH_B_REGOFF	0x00035050U
+#define AIEML_SHIMPL_TILECTRL_REGOFF			0x00036030U
+#define AIEML_SHIMPL_MODCLOCK_CTRL_0_REGOFF		0x000fff00U
+#define AIEML_SHIMPL_MODCLOCK_CTRL_1_REGOFF		0x000fff04U
+#define AIEML_SHIMPL_MODRESET_CTRL_0_REGOFF		0x000fff10U
+#define AIEML_SHIMPL_MODRESET_CTRL_1_REGOFF		0x000fff14U
+
+#define AIEML_MEMORY_BD0_0_REGOFF			0x000A0000U
+#define AIEML_MEMORY_GROUP0_REGOFF			0x00094500U
+#define AIEML_MEMORY_GROUPERROR_REGOFF			0x00094518U
+#define AIEML_MEMORY_TILECTRL_REGOFF			0x00096030U
+#define AIEML_MEMORY_EVENT_BC0_REGOFF			0x00094010U
+#define AIEML_MEMORY_EVENT_STATUS0_REGOFF		0x00094200U
+#define AIEML_MEMORY_MEMCTRL_REGOFF			0x00096048U
+#define AIEML_MEMORY_MODCLOCKCTRL_REGOFF		0x000fff00U
+#define AIEML_MEMORY_MODRESETCTRL_REGOFF		0x000fff10U
+#define AIEML_MEMORY_LOCK_REGOFF			0x000C0000U
+#define AIEML_MEMORY_LOCK_OVERFLOW_REGOFF		0x000C0420U
+#define AIEML_MEMORY_LOCK_UNDERFLOW_REGOFF		0x000C0428U
+#define AIEML_MEMORY_DMA_S2MM_STATUS_REGOFF		0x000A0660U
+#define AIEML_MEMORY_DMA_MM2S_STATUS_REGOFF		0x000A0680U
+
+#define AIEML_TILE_COREMOD_AMLL0_PART1_REGOFF		0x00030000U
+#define AIEML_TILE_COREMOD_AMHH8_PART2_REGOFF		0x00030470U
+#define AIEML_TILE_COREMOD_GROUPERROR_REGOFF		0x00034510U
+#define AIEML_TILE_COREMOD_TILECTRL_REGOFF		0x00036030U
+#define AIEML_TILE_COREMOD_GROUP0_REGOFF		0x00034500U
+#define AIEML_TILE_COREMOD_EVENT_BC0_REGOFF		0x00034010U
+#define AIEML_TILE_COREMOD_EVENT_STATUS0_REGOFF		0x00034200U
+#define AIEML_TILE_COREMOD_MEMCTRL_REGOFF		0x00036070U
+#define AIEML_TILE_COREMOD_MODCLOCKCTRL_REGOFF		0x00060000U
+#define AIEML_TILE_COREMOD_MODRESETCTRL_REGOFF		0x00060010U
+#define AIEML_TILE_COREMOD_WL0_PART1_REGOFF		0x00030800U
+#define AIEML_TILE_COREMOD_WH11_PART2_REGOFF		0x00030af0U
+#define AIEML_TILE_COREMOD_R0_REGOFF			0x00030c00U
+#define AIEML_TILE_COREMOD_R31_REGOFF			0x00030df0U
+#define AIEML_TILE_COREMOD_CORE_STATUS_REGOFF		0x00032004U
+#define AIEML_TILE_COREMOD_CORE_PC_REGOFF		0x00031100U
+#define AIEML_TILE_COREMOD_CORE_SP_REGOFF		0x00031120U
+#define AIEML_TILE_COREMOD_CORE_LR_REGOFF		0x00031130U
+#define AIEML_TILE_MEMMOD_BD0_0_REGOFF			0x0001D000U
+#define AIEML_TILE_MEMMOD_GROUPERROR_REGOFF		0x00014514U
+#define AIEML_TILE_MEMMOD_GROUP0_REGOFF			0x00014500U
+#define AIEML_TILE_MEMMOD_EVENT_BC0_REGOFF		0x00014010U
+#define AIEML_TILE_MEMMOD_EVENT_STATUS0_REGOFF		0x00014200U
+#define AIEML_TILE_MEMMOD_MEMCTRL_REGOFF		0x00016010U
+#define AIEML_TILE_MEMMOD_LOCK_REGOFF			0x0001F000U
+#define AIEML_TILE_MEMMOD_LOCK_OVERFLOW_REGOFF		0x0001F120U
+#define AIEML_TILE_MEMMOD_LOCK_UNDERFLOW_REGOFF		0x0001F128U
+#define AIEML_TILE_MEMMOD_DMA_S2MM_STATUS_REGOFF	0x0001DF00U
+#define AIEML_TILE_MEMMOD_DMA_MM2S_STATUS_REGOFF	0x0001DF10U
+#define AIEML_TILE_COREMOD_PERFCTRL_REGOFF		0x00031500U
+#define AIEML_TILE_COREMOD_PERFCTRL_RESET_REGOFF	0x00031508U
+#define AIEML_TILE_COREMOD_PERFCNT0_REGOFF		0x00031520U
+#define AIEML_TILE_CORE_EVNTGEN_REGOFF			0x00034008U
+
+/*
+ * Register masks
+ */
+#define AIEML_SHIMPL_COLRESET_CTRL_MASK			GENMASK(1, 0)
+#define AIEML_SHIMPL_COLCLOCK_CTRL_MASK			GENMASK(1, 0)
+#define AIEML_TILE_PERFCTRL_CNT0_MASK			0x7F7FU
+#define AIEML_TILE_PERFCTRL_RESET_MASK			0x7FU
+#define AIEML_TILE_CORE_PERFCNT0_MASK			0xFFFFFFFFU
+#define AIEML_TILE_CORE_EVNTGEN_MASK			0x7F
+
+/* Macros to define size of a sysfs binary attribute */
+#define AIEML_PART_SYSFS_CORE_BINA_SIZE		0x4000		/* 16KB */
+#define AIEML_PART_SYSFS_LOCK_BINA_SIZE		0x28000		/* 160KB */
+#define AIEML_PART_SYSFS_ERROR_BINA_SIZE	0x4000		/* 16KB */
+#define AIEML_PART_SYSFS_DMA_BINA_SIZE		0xC800		/* 50KB */
+#define AIEML_PART_SYSFS_STATUS_BINA_SIZE	0x3c000		/* 240KB */
+
+static const struct aie_tile_regs aieml_kernel_regs[] = {
+	/* SHIM AXI MM Config */
+	{.attribute = AIE_TILE_TYPE_MASK_SHIMNOC << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_SHIMNOC_AXIMM_REGOFF,
+	 .eoff = AIEML_SHIMNOC_AXIMM_REGOFF,
+	},
+	/* SHIM DMA buffer descriptor address range */
+	{.attribute = AIE_TILE_TYPE_MASK_SHIMNOC << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_SHIMNOC_BD0_0_REGOFF,
+	 .eoff = AIEML_SHIMNOC_BD15_7_REGOFF,
+	},
+	/* SHIM 2nd level interrupt controller */
+	{.attribute = AIE_TILE_TYPE_MASK_SHIMNOC << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_SHIMNOC_L2INTR_MASK_REGOFF,
+	 .eoff = AIEML_SHIMNOC_L2INTR_INTR_REGOFF,
+	},
+	/* SHIM BISR cache control */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		      AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_SHIMPL_BISRCACHE_CTRL_REGOFF,
+	 .eoff = AIEML_SHIMPL_BISRCACHE_CTRL_REGOFF,
+	},
+	/* SHIM column clock control */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_SHIMPL_COLCLOCK_CTRL_REGOFF,
+	 .eoff = AIEML_SHIMPL_COLCLOCK_CTRL_REGOFF,
+	},
+	/* SHIM column reset control */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_SHIMPL_COLRESET_CTRL_REGOFF,
+	 .eoff = AIEML_SHIMPL_COLRESET_CTRL_REGOFF,
+	},
+	/* SHIM tile control */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_SHIMPL_TILECTRL_REGOFF,
+	 .eoff = AIEML_SHIMPL_TILECTRL_REGOFF,
+	},
+	/* SHIM group error enable */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		      AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_SHIMPL_GROUPERROR_REGOFF,
+	 .eoff = AIEML_SHIMPL_GROUPERROR_REGOFF,
+	},
+	/* SHIM 1st level interrupt controller */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_SHIMPL_L1INTR_MASK_A_REGOFF,
+	 .eoff = AIEML_SHIMPL_L1INTR_BLOCK_NORTH_B_REGOFF,
+	},
+	/* SHIM module clock control */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_SHIMPL_MODCLOCK_CTRL_0_REGOFF,
+	 .eoff = AIEML_SHIMPL_MODCLOCK_CTRL_1_REGOFF,
+	},
+	/* SHIM module reset control */
+	{.attribute = (AIE_TILE_TYPE_MASK_SHIMPL | AIE_TILE_TYPE_MASK_SHIMNOC) <<
+		AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_SHIMPL_MODRESET_CTRL_0_REGOFF,
+	 .eoff = AIEML_SHIMPL_MODRESET_CTRL_1_REGOFF,
+	},
+	/* MEMORY tile group error enable */
+	{.attribute = AIE_TILE_TYPE_MASK_MEMORY << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_MEMORY_GROUPERROR_REGOFF,
+	 .eoff = AIEML_MEMORY_GROUPERROR_REGOFF,
+	},
+	/* MEMORY mem tile control */
+	{.attribute = AIE_TILE_TYPE_MASK_MEMORY << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_MEMORY_TILECTRL_REGOFF,
+	 .eoff = AIEML_MEMORY_TILECTRL_REGOFF,
+	},
+	/* MEMORY tile mem control */
+	{.attribute = AIE_TILE_TYPE_MASK_MEMORY << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_MEMORY_MEMCTRL_REGOFF,
+	 .eoff = AIEML_MEMORY_MEMCTRL_REGOFF,
+	},
+	/* MEMORY tile module clock control */
+	{.attribute = AIE_TILE_TYPE_MASK_MEMORY << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_MEMORY_MODCLOCKCTRL_REGOFF,
+	 .eoff = AIEML_MEMORY_MODCLOCKCTRL_REGOFF,
+	},
+	/* MEMORY tile module reset control */
+	{.attribute = AIE_TILE_TYPE_MASK_MEMORY << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_MEMORY_MODRESETCTRL_REGOFF,
+	 .eoff = AIEML_MEMORY_MODRESETCTRL_REGOFF,
+	},
+	/* TILE core module group error enable */
+	{.attribute = AIE_TILE_TYPE_MASK_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_TILE_COREMOD_GROUPERROR_REGOFF,
+	 .eoff = AIEML_TILE_COREMOD_GROUPERROR_REGOFF,
+	},
+	/* TILE tile control */
+	{.attribute = AIE_TILE_TYPE_MASK_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_TILE_COREMOD_TILECTRL_REGOFF,
+	 .eoff = AIEML_TILE_COREMOD_TILECTRL_REGOFF,
+	},
+	/* TILE memory control */
+	{.attribute = AIE_TILE_TYPE_MASK_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_TILE_COREMOD_MEMCTRL_REGOFF,
+	 .eoff = AIEML_TILE_COREMOD_MEMCTRL_REGOFF,
+	},
+	/* TILE module clock control */
+	{.attribute = AIE_TILE_TYPE_MASK_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_TILE_COREMOD_MODCLOCKCTRL_REGOFF,
+	 .eoff = AIEML_TILE_COREMOD_MODCLOCKCTRL_REGOFF,
+	},
+	/* TILE module reset control */
+	{.attribute = AIE_TILE_TYPE_MASK_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_TILE_COREMOD_MODRESETCTRL_REGOFF,
+	 .eoff = AIEML_TILE_COREMOD_MODRESETCTRL_REGOFF,
+	},
+	/* TILE memory module group error enable */
+	{.attribute = AIE_TILE_TYPE_MASK_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_TILE_MEMMOD_GROUPERROR_REGOFF,
+	 .eoff = AIEML_TILE_MEMMOD_GROUPERROR_REGOFF,
+	},
+	/* TILE memory module mem control */
+	{.attribute = AIE_TILE_TYPE_MASK_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	 .soff = AIEML_TILE_MEMMOD_MEMCTRL_REGOFF,
+	 .eoff = AIEML_TILE_MEMMOD_MEMCTRL_REGOFF,
+	},
+};
+
+/* resource attributes for core tile type */
+static const
+struct aie_tile_rsc_attr aieml_core_tile_rscs_attr[AIE_RSCTYPE_MAX] = {
+	{
+		/* perf counter */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_PERF_TILE_MEM_MOD,},
+			{.num_rscs = AIEML_NUM_PERF_TILE_CORE_MOD,},
+		},
+	},
+	{
+		/* user event */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_USEREVENT_TILE_MEM_MOD,},
+			{.num_rscs = AIEML_NUM_USEREVENT_TILE_CORE_MOD,},
+		},
+	},
+	{
+		/* trace control */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_TRACECONTROL_TILE_MEM_MOD,},
+			{.num_rscs = AIEML_NUM_TRACECONTROL_TILE_CORE_MOD,},
+		},
+	},
+	{
+		/* pc event */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_PCEVENT_TILE_MEM_MOD,},
+			{.num_rscs = AIEML_NUM_PCEVENT_TILE_CORE_MOD,},
+		},
+	},
+	{
+		/* stream switch port select */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_SSSELECT_TILE_MEM_MOD,},
+			{.num_rscs = AIEML_NUM_SSSELECT_TILE_CORE_MOD,},
+		},
+	},
+	{
+		/* broadcast */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_BROADCAST_TILE_MEM_MOD,},
+			{.num_rscs = AIEML_NUM_BROADCAST_TILE_CORE_MOD,},
+		},
+	},
+	{
+		/* combo events */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_COMBOEVENT_TILE_MEM_MOD,},
+			{.num_rscs = AIEML_NUM_COMBOEVENT_TILE_CORE_MOD,},
+		},
+	},
+	{
+		/* group events */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_GROUPEVENTS_TILE_MEM_MOD,},
+			{.num_rscs = AIEML_NUM_GROUPEVENTS_TILE_CORE_MOD,},
+		},
+	},
+};
+
+/* resource attributes for mem tile type */
+static const
+struct aie_tile_rsc_attr aieml_mem_tile_rscs_attr[AIE_RSCTYPE_MAX] = {
+	{
+		/* perf counter */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_PERF_MEM_MOD,},
+		},
+	},
+	{
+		/* user event */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_USEREVENT_MEM_MOD,},
+		},
+	},
+	{
+		/* trace control */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_TRACECONTROL_MEM_MOD,},
+		},
+	},
+	{
+		/* pc event */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_PCEVENT_MEM_MOD,},
+		},
+	},
+	{
+		/* stream switch port select */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_SSSELECT_MEM_MOD,},
+		},
+	},
+	{
+		/* broadcast */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_BROADCAST_MEM_MOD,},
+		},
+	},
+	{
+		/* combo events */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_COMBOEVENT_MEM_MOD,},
+		},
+	},
+	{
+		/* group events */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_GROUPEVENTS_MEM_MOD,},
+		},
+	},
+};
+
+/* resource attributes for shim tile type */
+static const
+struct aie_tile_rsc_attr aieml_shimpl_tile_rscs_attr[AIE_RSCTYPE_MAX] = {
+	{
+		/* perf counter */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_PERF_PL_MOD,},
+		},
+	},
+	{
+		/* user event */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_USEREVENT_PL_MOD,},
+		},
+	},
+	{
+		/* trace control */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_TRACECONTROL_PL_MOD,},
+		},
+	},
+	{
+		/* pc event */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_PCEVENT_PL_MOD,},
+		},
+	},
+	{
+		/* stream switch port select */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_SSSELECT_PL_MOD,},
+		},
+	},
+	{
+		/* broadcast */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_BROADCAST_PL_MOD,},
+		},
+	},
+	{
+		/* combo events */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_COMBOEVENT_PL_MOD,},
+		},
+	},
+	{
+		/* group events */
+		.mod_attr = {
+			{.num_rscs = AIEML_NUM_GROUPEVENTS_PL_MOD,},
+		},
+	},
+};
+
+/* Events needed for core tile utilization */
+static const
+enum aie_events aieml_core_util_events[NUM_UTIL_EVENTS] = {
+		[AIE_EVENT_CORE_ACTIVE] = 28,
+		[AIE_EVENT_CORE_DISABLED] = 29,
+		[AIE_EVENT_CORE_USER_EVNT_0] = 124,
+		[AIE_EVENT_CORE_USER_EVNT_1] = 125,
+};
+
+/* modules types array of CORE tile */
+static const
+enum aie_module_type aieml_core_tile_module_types[NUM_MODS_CORE_TILE] = {
+	AIE_MEM_MOD,
+	AIE_CORE_MOD,
+};
+
+/* modules types array of MEM tile */
+static const
+enum aie_module_type aieml_mem_tile_module_types[NUM_MODS_MEM_TILE] = {
+	AIE_MEM_MOD,
+};
+
+/* modules types array of SHIM PL tile */
+static const
+enum aie_module_type aieml_shimpl_tile_module_types[NUM_MODS_SHIMPL_TILE] = {
+	AIE_PL_MOD,
+};
+
+static const struct aie_event_prop aieml_core_stream_error_prop[] = {
+	{
+		.event = 71U,
+		.event_str = "stream_switch_port_parity_error",
+	},
+	{
+		.event = 57U,
+		.event_str = "control_pkt_error",
+	},
+	{
+		.event = 56U,
+		.event_str = "stream_pkt_parity_error",
+	},
+};
+
+static const struct aie_event_prop aieml_core_inst_error_prop[] = {
+	{
+		.event = 59U,
+		.event_str = "instruction_decompression_error",
+	},
+	{
+		.event = 70U,
+		.event_str = "decompression_underflow",
+	},
+};
+
+static const struct aie_event_prop aieml_core_ecc_error_prop[] = {
+	{
+		.event = 64U,
+		.event_str = "pm_ecc_error_2-bit",
+	},
+	{
+		.event = 62U,
+		.event_str = "pm_ecc_error_scrub_2-bit",
+	},
+};
+
+static const struct aie_event_prop aieml_core_access_error_prop[] = {
+	{
+		.event = 55U,
+		.event_str = "pm_reg_access_failure",
+	},
+	{
+		.event = 60U,
+		.event_str = "dm_address_out_of_range",
+	},
+	{
+		.event = 65U,
+		.event_str = "pm_address_out_of_range",
+	},
+	{
+		.event = 66U,
+		.event_str = "dm_access_to_unavailable",
+	},
+};
+
+static const struct aie_event_prop aieml_core_lock_error_prop[] = {
+	{
+		.event = 67U,
+		.event_str = "lock_access_to_unavailable",
+	},
+	{
+		.event = 72U,
+		.event_str = "processor_bus_error",
+	},
+};
+
+static const struct aie_event_prop aieml_core_bus_error_prop[] = {
+	{
+		.event = 58U,
+		.event_str = "axi_mm_slave_error",
+	},
+};
+
+static const struct aie_event_prop aieml_mem_ecc_error_prop[] = {
+	{
+		.event = 88U,
+		.event_str = "dm_ecc_error_scrub_2-bit",
+	},
+	{
+		.event = 90U,
+		.event_str = "dm_ecc_error_2-bit",
+	},
+};
+
+static const struct aie_event_prop aieml_mem_parity_error_prop[] = {
+	{
+		.event = 96U,
+		.event_str = "dm_parity_error_bank_7",
+	},
+	{
+		.event = 95U,
+		.event_str = "dm_parity_error_bank_6",
+	},
+	{
+		.event = 94U,
+		.event_str = "dm_parity_error_bank_5",
+	},
+	{
+		.event = 93U,
+		.event_str = "dm_parity_error_bank_4",
+	},
+	{
+		.event = 92U,
+		.event_str = "dm_parity_error_bank_3",
+	},
+	{
+		.event = 91U,
+		.event_str = "dm_parity_error_bank_2",
+	},
+};
+
+static const struct aie_event_prop aieml_mem_dma_error_prop[] = {
+	{
+		.event = 100U,
+		.event_str = "dma_mm2s_1_error",
+	},
+	{
+		.event = 99U,
+		.event_str = "dma_mm2s_0_error",
+	},
+	{
+		.event = 98U,
+		.event_str = "dma_s2mm_1_error",
+	},
+	{
+		.event = 97U,
+		.event_str = "dma_s2mm_0_error",
+	},
+};
+
+static const struct aie_event_prop aieml_memtile_ecc_error_prop[] = {
+	{
+		.event = 132U,
+		.event_str = "dm_ecc_error_2-bit",
+	},
+	{
+		.event = 130U,
+		.event_str = "dm_ecc_error_scrub_2-bit",
+	},
+};
+
+static const struct aie_event_prop aieml_memtile_dma_error_prop[] = {
+	{
+		.event = 134U,
+		.event_str = "dma_mm2s_error",
+	},
+	{
+		.event = 133U,
+		.event_str = "dma_s2mm_error",
+	},
+};
+
+static const struct aie_event_prop aieml_memtile_stream_error_prop[] = {
+	{
+		.event = 137U,
+		.event_str = "control_pkt_error",
+	},
+	{
+		.event = 136U,
+		.event_str = "stream_pkt_parity_error",
+	},
+	{
+		.event = 135U,
+		.event_str = "stream_switch_port_parity_error",
+	},
+};
+
+static const struct aie_event_prop aieml_memtile_lock_error_prop[] = {
+	{
+		.event = 139U,
+		.event_str = "lock_error",
+	},
+};
+
+static const struct aie_event_prop aieml_memtile_bus_error_prop[] = {
+	{
+		.event = 58U,
+		.event_str = "axi_mm_slave_error",
+	},
+};
+
+static const struct aie_event_prop aieml_shim_bus_error_prop[] = {
+	{
+		.event = 71U,
+		.event_str = "axi_mm_byte_strobe_error",
+	},
+	{
+		.event = 70U,
+		.event_str = "axi_mm_unsecure_access_in_secure_mode",
+	},
+	{
+		.event = 69U,
+		.event_str = "axi_mm_unsupported_traffic",
+	},
+	{
+		.event = 68U,
+		.event_str = "axi_mm_slave_nsu_error",
+	},
+	{
+		.event = 67U,
+		.event_str = "axi_mm_decode_nsu_error",
+	},
+	{
+		.event = 64U,
+		.event_str = "axi_mm_slave_tile_error",
+	},
+};
+
+static const struct aie_event_prop aieml_shim_stream_error_prop[] = {
+	{
+		.event = 66U,
+		.event_str = "stream_switch_port_parity_error",
+	},
+	{
+		.event = 65U,
+		.event_str = "control_pkt_error",
+	},
+};
+
+static const struct aie_event_prop aieml_shim_dma_error_prop[] = {
+	{
+		.event = 73U,
+		.event_str = "dma_mm2s_error",
+	},
+	{
+		.event = 72U,
+		.event_str = "dma_s2mm_error",
+	},
+};
+
+static const struct aie_err_category aieml_core_err_category[] = {
+	{
+		/* AIE_ERROR_CATEGORY_STREAM */
+		.err_category = AIE_ERROR_CATEGORY_STREAM,
+		.num_events = ARRAY_SIZE(aieml_core_stream_error_prop),
+		.prop = aieml_core_stream_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_ACCESS */
+		.err_category = AIE_ERROR_CATEGORY_ACCESS,
+		.num_events = ARRAY_SIZE(aieml_core_access_error_prop),
+		.prop = aieml_core_access_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_BUS */
+		.err_category = AIE_ERROR_CATEGORY_BUS,
+		.num_events = ARRAY_SIZE(aieml_core_bus_error_prop),
+		.prop = aieml_core_bus_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_INSTRUCTION */
+		.err_category = AIE_ERROR_CATEGORY_INSTRUCTION,
+		.num_events = ARRAY_SIZE(aieml_core_inst_error_prop),
+		.prop = aieml_core_inst_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_ECC */
+		.err_category = AIE_ERROR_CATEGORY_ECC,
+		.num_events = ARRAY_SIZE(aieml_core_ecc_error_prop),
+		.prop = aieml_core_ecc_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_LOCK */
+		.err_category = AIE_ERROR_CATEGORY_LOCK,
+		.num_events = ARRAY_SIZE(aieml_core_lock_error_prop),
+		.prop = aieml_core_lock_error_prop,
+	},
+};
+
+static const struct aie_err_category aieml_mem_err_category[] = {
+	{
+		/* AIE_ERROR_CATEGORY_ECC */
+		.err_category = AIE_ERROR_CATEGORY_ECC,
+		.num_events = ARRAY_SIZE(aieml_mem_ecc_error_prop),
+		.prop = aieml_mem_ecc_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_MEM_PARITY */
+		.err_category = AIE_ERROR_CATEGORY_MEM_PARITY,
+		.num_events = ARRAY_SIZE(aieml_mem_parity_error_prop),
+		.prop = aieml_mem_parity_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_DMA */
+		.err_category = AIE_ERROR_CATEGORY_DMA,
+		.num_events = ARRAY_SIZE(aieml_mem_dma_error_prop),
+		.prop = aieml_mem_dma_error_prop,
+	},
+};
+
+static const struct aie_err_category aieml_memtile_err_category[] = {
+	{
+		/* AIE_ERROR_CATEGORY_ECC */
+		.err_category = AIE_ERROR_CATEGORY_ECC,
+		.num_events = ARRAY_SIZE(aieml_memtile_ecc_error_prop),
+		.prop = aieml_memtile_ecc_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_STREAM */
+		.err_category = AIE_ERROR_CATEGORY_STREAM,
+		.num_events = ARRAY_SIZE(aieml_memtile_stream_error_prop),
+		.prop = aieml_memtile_stream_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_DMA */
+		.err_category = AIE_ERROR_CATEGORY_DMA,
+		.num_events = ARRAY_SIZE(aieml_memtile_dma_error_prop),
+		.prop = aieml_memtile_dma_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_BUS */
+		.err_category = AIE_ERROR_CATEGORY_BUS,
+		.num_events = ARRAY_SIZE(aieml_memtile_bus_error_prop),
+		.prop = aieml_memtile_bus_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_LOCK */
+		.err_category = AIE_ERROR_CATEGORY_LOCK,
+		.num_events = ARRAY_SIZE(aieml_memtile_lock_error_prop),
+		.prop = aieml_memtile_lock_error_prop,
+	},
+};
+
+static const struct aie_err_category aieml_shim_err_category[] = {
+	{
+		/* AIE_ERROR_CATEGORY_BUS */
+		.err_category = AIE_ERROR_CATEGORY_BUS,
+		.num_events = ARRAY_SIZE(aieml_shim_bus_error_prop),
+		.prop = aieml_shim_bus_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_STREAM */
+		.err_category = AIE_ERROR_CATEGORY_STREAM,
+		.num_events = ARRAY_SIZE(aieml_shim_stream_error_prop),
+		.prop = aieml_shim_stream_error_prop,
+	},
+	{
+		/* AIE_ERROR_CATEGORY_DMA */
+		.err_category = AIE_ERROR_CATEGORY_DMA,
+		.num_events = ARRAY_SIZE(aieml_shim_dma_error_prop),
+		.prop = aieml_shim_dma_error_prop,
+	},
+};
+
+static const struct aie_error_attr aieml_core_error = {
+	.num_err_categories = ARRAY_SIZE(aieml_core_err_category),
+	.err_category = aieml_core_err_category,
+};
+
+static const struct aie_error_attr aieml_mem_error = {
+	.num_err_categories = ARRAY_SIZE(aieml_mem_err_category),
+	.err_category = aieml_mem_err_category,
+};
+
+static const struct aie_error_attr aieml_memtile_error = {
+	.num_err_categories = ARRAY_SIZE(aieml_memtile_err_category),
+	.err_category = aieml_memtile_err_category,
+};
+
+static const struct aie_error_attr aieml_shim_error = {
+	.num_err_categories = ARRAY_SIZE(aieml_shim_err_category),
+	.err_category = aieml_shim_err_category,
+};
+
+static const struct aie_tile_regs aieml_core_amxx_regs = {
+	.attribute = AIE_TILE_TYPE_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	.soff = AIEML_TILE_COREMOD_AMLL0_PART1_REGOFF,
+	.eoff = AIEML_TILE_COREMOD_AMHH8_PART2_REGOFF,
+};
+
+static const struct aie_tile_regs aieml_core_wx_regs = {
+	.attribute = AIE_TILE_TYPE_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	.soff = AIEML_TILE_COREMOD_WL0_PART1_REGOFF,
+	.eoff = AIEML_TILE_COREMOD_WH11_PART2_REGOFF,
+};
+
+static const struct aie_tile_regs aieml_core_32bit_regs = {
+	.attribute = AIE_TILE_TYPE_TILE << AIE_REGS_ATTR_TILE_TYPE_SHIFT,
+	.soff = AIEML_TILE_COREMOD_R0_REGOFF,
+	.eoff = AIEML_TILE_COREMOD_R31_REGOFF,
+};
+
+static const struct aie_core_regs_attr aieml_core_regs[] = {
+	{.core_regs = &aieml_core_amxx_regs,
+	 .width = 4,
+	},
+	{.core_regs = &aieml_core_wx_regs,
+	 .width = 4,
+	},
+	{.core_regs = &aieml_core_32bit_regs,
+	 .width = 1,
+	},
+};
+
+static const struct aie_single_reg_field aieml_col_rst = {
+	.mask = AIEML_SHIMPL_COLRESET_CTRL_MASK,
+	.regoff = AIEML_SHIMPL_COLRESET_CTRL_REGOFF,
+};
+
+static const struct aie_single_reg_field aieml_col_clkbuf = {
+	.mask = AIEML_SHIMPL_COLCLOCK_CTRL_MASK,
+	.regoff = AIEML_SHIMPL_COLCLOCK_CTRL_REGOFF,
+};
+
+static const struct aie_single_reg_field aieml_core_perfctrl = {
+	.mask = AIEML_TILE_PERFCTRL_CNT0_MASK,
+	.regoff = AIEML_TILE_COREMOD_PERFCTRL_REGOFF,
+};
+
+static const struct aie_single_reg_field aieml_core_perfctrl_reset = {
+	.mask = AIEML_TILE_PERFCTRL_RESET_MASK,
+	.regoff = AIEML_TILE_COREMOD_PERFCTRL_RESET_REGOFF,
+};
+
+static const struct aie_single_reg_field aieml_core_perfcnt = {
+	.mask = AIEML_TILE_CORE_PERFCNT0_MASK,
+	.regoff = AIEML_TILE_COREMOD_PERFCNT0_REGOFF,
+};
+
+static const struct aie_single_reg_field aieml_core_evntgen = {
+	.mask = AIEML_TILE_CORE_EVNTGEN_MASK,
+	.regoff = AIEML_TILE_CORE_EVNTGEN_REGOFF,
+};
+
+static const struct aie_single_reg_field aieml_core_sts = {
+	.mask = GENMASK(21, 0),
+	.regoff = AIEML_TILE_COREMOD_CORE_STATUS_REGOFF,
+};
+
+static const struct aie_single_reg_field aieml_core_pc = {
+	.mask = GENMASK(19, 0),
+	.regoff = AIEML_TILE_COREMOD_CORE_PC_REGOFF,
+};
+
+static const struct aie_single_reg_field aieml_core_lr = {
+	.mask = GENMASK(19, 0),
+	.regoff = AIEML_TILE_COREMOD_CORE_LR_REGOFF,
+};
+
+static const struct aie_single_reg_field aieml_core_sp = {
+	.mask = GENMASK(19, 0),
+	.regoff = AIEML_TILE_COREMOD_CORE_SP_REGOFF,
+};
+
+static const struct aie_bd_lock_attr aieml_tile_lockbd = {
+	.lock_acq_id = {
+		.mask = GENMASK(3, 0),
+		.regoff = 0x14U,
+	},
+	.lock_acq_val = {
+		.mask = GENMASK(11, 5),
+		.regoff = 0x14U,
+	},
+	.lock_acq_en = {
+		.mask = BIT(12),
+		.regoff = 0x14U,
+	},
+	.lock_rel_id = {
+		.mask = GENMASK(16, 13),
+		.regoff = 0x14U,
+	},
+	.lock_rel_val = {
+		.mask = GENMASK(24, 18),
+		.regoff = 0x14U,
+	},
+};
+
+static const struct aie_bd_lock_attr aieml_memtile_lockbd = {
+	.lock_acq_id = {
+		.mask = GENMASK(7, 0),
+		.regoff = 0x1CU,
+	},
+	.lock_acq_val = {
+		.mask = GENMASK(14, 8),
+		.regoff = 0x1CU,
+	},
+	.lock_acq_en = {
+		.mask = BIT(15),
+		.regoff = 0x1CU,
+	},
+	.lock_rel_id = {
+		.mask = GENMASK(23, 16),
+		.regoff = 0x1CU,
+	},
+	.lock_rel_val = {
+		.mask = GENMASK(30, 24),
+		.regoff = 0x1CU,
+	},
+};
+
+static const struct aie_bd_lock_attr aieml_shim_lockbd = {
+	.lock_acq_id = {
+		.mask = GENMASK(3, 0),
+		.regoff = 0x1CU,
+	},
+	.lock_acq_val = {
+		.mask = GENMASK(11, 5),
+		.regoff = 0x1CU,
+	},
+	.lock_acq_en = {
+		.mask = BIT(12),
+		.regoff = 0x1CU,
+	},
+	.lock_rel_id = {
+		.mask = GENMASK(16, 13),
+		.regoff = 0x1CU,
+	},
+	.lock_rel_val = {
+		.mask = GENMASK(24, 18),
+		.regoff = 0x1CU,
+	},
+};
+
+static const struct aie_bd_pkt_attr aieml_tile_pktbd = {
+	.pkt_en = {
+		.mask = BIT(30),
+		.regoff = 0x4U,
+	},
+	.pkt_type = {
+		.mask = GENMASK(18, 16),
+		.regoff = 0x4U,
+	},
+	.pkt_id = {
+		.mask = GENMASK(23, 19),
+		.regoff = 0x4U,
+	},
+};
+
+static const struct aie_bd_pkt_attr aieml_memtile_pktbd = {
+	.pkt_en = {
+		.mask = BIT(31),
+		.regoff = 0x0U,
+	},
+	.pkt_type = {
+		.mask = GENMASK(30, 28),
+		.regoff = 0x0U,
+	},
+	.pkt_id = {
+		.mask = GENMASK(27, 23),
+		.regoff = 0x0U,
+	},
+};
+
+static const struct aie_bd_pkt_attr aieml_shim_pktbd = {
+	.pkt_en = {
+		.mask = BIT(30),
+		.regoff = 0x8U,
+	},
+	.pkt_type = {
+		.mask = GENMASK(18, 16),
+		.regoff = 0x8U,
+	},
+	.pkt_id = {
+		.mask = GENMASK(23, 19),
+		.regoff = 0x8U,
+	},
+};
+
+static const struct aie_bd_axi_attr aieml_shim_axibd = {
+	.smid = {
+		.mask = GENMASK(31, 28),
+		.regoff = 0x14U,
+	},
+	.cache = {
+		.mask = GENMASK(27, 24),
+		.regoff = 0x14U,
+	},
+	.qos = {
+		.mask = GENMASK(23, 20),
+		.regoff = 0x14U,
+	},
+	.secure_en = {
+		.mask = BIT(30),
+		.regoff = 0xCU,
+	},
+	.burst_len = {
+		.mask = GENMASK(31, 30),
+		.regoff = 0x10U,
+	},
+};
+
+static const struct aie_bd_aieml_dim_attr aieml_tile_dimbd = {
+	.iter_curr = {
+		.mask = GENMASK(24, 19),
+		.regoff = 0x10U,
+	},
+	.iter = {
+		.wrap = {
+			.mask = GENMASK(18, 13),
+			.regoff = 0x10U,
+		},
+		.step_size = {
+			.mask = GENMASK(12, 0),
+			.regoff = 0x10U,
+		},
+	},
+	.dims = {
+		/* Dim 0 */
+		{
+			.wrap = {
+				.mask = GENMASK(20, 13),
+				.regoff = 0xCU,
+			},
+			.step_size = {
+				.mask = GENMASK(12, 0),
+				.regoff = 0x8U,
+			},
+		},
+		/* Dim 1 */
+		{
+			.wrap = {
+				.mask = GENMASK(28, 21),
+				.regoff = 0xCU,
+			},
+			.step_size = {
+				.mask = GENMASK(25, 13),
+				.regoff = 0x8U,
+			},
+		},
+		/* Dim 2 */
+		{
+			.step_size = {
+				.mask = GENMASK(12, 0),
+				.regoff = 0xCU,
+			},
+		},
+	},
+};
+
+static const struct aie_bd_aieml_dim_attr aieml_memtile_dimbd = {
+	.iter_curr = {
+		.mask = GENMASK(28, 23),
+		.regoff = 0x18U,
+	},
+	.iter = {
+		.wrap = {
+			.mask = GENMASK(22, 17),
+			.regoff = 0x18U,
+		},
+		.step_size = {
+			.mask = GENMASK(16, 0),
+			.regoff = 0x18U,
+		},
+	},
+	.dims = {
+		/* Dim 0 */
+		{
+			.wrap = {
+				.mask = GENMASK(26, 17),
+				.regoff = 0x8U,
+			},
+			.step_size = {
+				.mask = GENMASK(16, 0),
+				.regoff = 0x8U,
+			},
+		},
+		/* Dim 1 */
+		{
+			.wrap = {
+				.mask = GENMASK(26, 17),
+				.regoff = 0xCU,
+			},
+			.step_size = {
+				.mask = GENMASK(16, 0),
+				.regoff = 0xCU,
+			},
+		},
+		/* Dim 2 */
+		{
+			.wrap = {
+				.mask = GENMASK(26, 17),
+				.regoff = 0x10U,
+			},
+			.step_size = {
+				.mask = GENMASK(16, 0),
+				.regoff = 0x10U,
+			},
+		},
+		/* Dim 3 */
+		{
+			.step_size = {
+				.mask = GENMASK(16, 0),
+				.regoff = 0x14U,
+			},
+		},
+	},
+	.pads = {
+		/* Dim 0 */
+		{
+			.before = {
+				.mask = GENMASK(31, 26),
+				.regoff = 0x4U,
+			},
+			.after = {
+				.mask = GENMASK(22, 17),
+				.regoff = 0x14U,
+			},
+		},
+		/* Dim 1 */
+		{
+			.before = {
+				.mask = GENMASK(31, 27),
+				.regoff = 0xCU,
+			},
+			.after = {
+				.mask = GENMASK(27, 23),
+				.regoff = 0x14U,
+			},
+		},
+		/* Dim 2 */
+		{
+			.before = {
+				.mask = GENMASK(30, 27),
+				.regoff = 0x10U,
+			},
+			.after = {
+				.mask = GENMASK(31, 28),
+				.regoff = 0x14U,
+			},
+		},
+	},
+};
+
+static const struct aie_bd_aieml_dim_attr aieml_shim_dimbd = {
+	.iter_curr = {
+		.mask = GENMASK(31, 26),
+		.regoff = 0x18U,
+	},
+	.iter = {
+		.wrap = {
+			.mask = GENMASK(25, 20),
+			.regoff = 0x18U,
+		},
+		.step_size = {
+			.mask = GENMASK(19, 0),
+			.regoff = 0x18U,
+		},
+	},
+	.dims = {
+		/* Dim 0 */
+		{
+			.wrap = {
+				.mask = GENMASK(29, 20),
+				.regoff = 0xCU,
+			},
+			.step_size = {
+				.mask = GENMASK(19, 0),
+				.regoff = 0xCU,
+			},
+		},
+		/* Dim 1 */
+		{
+			.wrap = {
+				.mask = GENMASK(29, 20),
+				.regoff = 0x10U,
+			},
+			.step_size = {
+				.mask = GENMASK(19, 0),
+				.regoff = 0x10U,
+			},
+		},
+		/* Dim 2 */
+		{
+			.step_size = {
+				.mask = GENMASK(19, 0),
+				.regoff = 0x14U,
+			},
+		},
+	},
+};
+
+static const struct aie_bd_attr aieml_tilebd = {
+	.valid_bd = {
+		.mask = BIT(25),
+		.regoff = 0x14U,
+	},
+	.next_bd = {
+		.mask = GENMASK(30, 27),
+		.regoff = 0x14U,
+	},
+	.use_next = {
+		.mask = BIT(26),
+		.regoff = 0x14U,
+	},
+	.addr = {
+		.addr = {
+			.mask = GENMASK(27, 14),
+			.regoff = 0x0U,
+		},
+		.length = {
+			.mask = GENMASK(13, 0),
+			.regoff = 0x0U,
+		},
+	},
+	.compression_en = {
+		.mask = BIT(31),
+		.regoff = 0x4U,
+	},
+	.out_of_order_id = {
+		.mask = GENMASK(29, 24),
+		.regoff = 0x4U,
+	},
+	.tlast_suppress = {
+		.mask = BIT(31),
+		.regoff = 0x14U,
+	},
+	.lock = aieml_tile_lockbd,
+	.packet = aieml_tile_pktbd,
+	.aieml_dim = aieml_tile_dimbd,
+	.num_dims = 3,
+	.bd_idx_off = 0x20U,
+};
+
+static const struct aie_bd_attr aieml_memtilebd = {
+	.valid_bd = {
+		.mask = BIT(31),
+		.regoff = 0x1CU,
+	},
+	.next_bd = {
+		.mask = GENMASK(25, 20),
+		.regoff = 0x4U,
+	},
+	.use_next = {
+		.mask = BIT(19),
+		.regoff = 0x4U,
+	},
+	.addr = {
+		.addr = {
+			.mask = GENMASK(18, 0),
+			.regoff = 0x4U,
+		},
+		.length = {
+			.mask = GENMASK(16, 0),
+			.regoff = 0x0U,
+		},
+	},
+	.compression_en = {
+		.mask = BIT(31),
+		.regoff = 0x10U,
+	},
+	.out_of_order_id = {
+		.mask = GENMASK(22, 17),
+		.regoff = 0x0U,
+	},
+	.tlast_suppress = {
+		.mask = BIT(31),
+		.regoff = 0x8U,
+	},
+	.lock = aieml_memtile_lockbd,
+	.packet = aieml_memtile_pktbd,
+	.aieml_dim = aieml_memtile_dimbd,
+	.num_dims = 4,
+	.bd_idx_off = 0x20U,
+};
+
+static const struct aie_bd_attr aieml_shimbd = {
+	.valid_bd = {
+		.mask = BIT(25),
+		.regoff = 0x1CU,
+	},
+	.next_bd = {
+		.mask = GENMASK(30, 27),
+		.regoff = 0x1CU,
+	},
+	.use_next = {
+		.mask = BIT(26),
+		.regoff = 0x1CU,
+	},
+	.addr = {
+		.addr = {
+			.mask = GENMASK(31, 0),
+			.regoff = 0x4U,
+		},
+		.length = {
+			.mask = GENMASK(31, 0),
+			.regoff = 0x0U,
+		},
+	},
+	.addr_2 = {
+		.addr = {
+			.mask = GENMASK(15, 0),
+			.regoff = 0x8U,
+		},
+	},
+	.compression_en = {
+		.mask = BIT(31),
+		.regoff = 0x10U,
+	},
+	.out_of_order_id = {
+		.mask = GENMASK(29, 24),
+		.regoff = 0x8U,
+	},
+	.tlast_suppress = {
+		.mask = BIT(31),
+		.regoff = 0x1CU,
+	},
+	.lock = aieml_shim_lockbd,
+	.packet = aieml_shim_pktbd,
+	.axi = aieml_shim_axibd,
+	.aieml_dim = aieml_shim_dimbd,
+	.num_dims = 3,
+	.bd_idx_off = 0x20U,
+};
+
+static const struct aie_dma_attr aieml_shimdma = {
+	.laddr = {
+		.mask = 0xffffffffU,
+		.regoff = 0x4U,
+	},
+	.haddr = {
+		.mask = 0xffffU,
+		.regoff = 0x8U,
+	},
+	.buflen = {
+		.mask = 0xffffffffU,
+		.regoff = 0x0U,
+	},
+	.chansts = {
+		.mask = BIT(19),
+		.regoff = 0x4,
+	},
+	.qsize = {
+		.mask = GENMASK(22, 20),
+		.regoff = 0x0,
+	},
+	.qsts = {
+		.mask = BIT(18),
+		.regoff = 0x0,
+	},
+	.curbd = {
+		.mask = GENMASK(27, 24),
+		.regoff = 0x0,
+	},
+	.bd_regoff = AIEML_SHIMNOC_BD0_0_REGOFF,
+	.num_bds = 16,
+	.bd_len = 0x20U,
+	.num_mm2s_chan = 2U,
+	.num_s2mm_chan = 2U,
+	.mm2s_sts_regoff = AIEML_SHIMNOC_DMA_MM2S_STATUS_REGOFF,
+	.s2mm_sts_regoff = AIEML_SHIMNOC_DMA_S2MM_STATUS_REGOFF,
+};
+
+static const struct aie_dma_attr aieml_tiledma = {
+	.chansts = {
+		.mask = BIT(19),
+		.regoff = 0x4,
+	},
+	.qsize = {
+		.mask = GENMASK(22, 20),
+		.regoff = 0x0,
+	},
+	.qsts = {
+		.mask = BIT(18),
+		.regoff = 0x0,
+	},
+	.curbd = {
+		.mask = GENMASK(27, 24),
+		.regoff = 0x0,
+	},
+	.bd_regoff = AIEML_TILE_MEMMOD_BD0_0_REGOFF,
+	.num_bds = 16,
+	.bd_len = 0x18U,
+	.num_mm2s_chan = 2U,
+	.num_s2mm_chan = 2U,
+	.mm2s_sts_regoff = AIEML_TILE_MEMMOD_DMA_MM2S_STATUS_REGOFF,
+	.s2mm_sts_regoff = AIEML_TILE_MEMMOD_DMA_S2MM_STATUS_REGOFF,
+};
+
+static const struct aie_dma_attr aieml_memtiledma = {
+	.chansts = {
+		.mask = BIT(19),
+		.regoff = 0x4,
+	},
+	.qsize = {
+		.mask = GENMASK(22, 20),
+		.regoff = 0x0,
+	},
+	.qsts = {
+		.mask = BIT(18),
+		.regoff = 0x0,
+	},
+	.curbd = {
+		.mask = GENMASK(29, 24),
+		.regoff = 0x0,
+	},
+	.bd_regoff = AIEML_MEMORY_BD0_0_REGOFF,
+	.num_bds = 48,
+	.bd_len = 0x20U,
+	.num_mm2s_chan = 6U,
+	.num_s2mm_chan = 6U,
+	.mm2s_sts_regoff = AIEML_MEMORY_DMA_MM2S_STATUS_REGOFF,
+	.s2mm_sts_regoff = AIEML_MEMORY_DMA_S2MM_STATUS_REGOFF,
+};
+
+static const struct aie_lock_attr aieml_pl_lock = {
+	.sts = {
+		.mask = GENMASK(5, 0),
+		.regoff = 0x10,
+	},
+	.sts_regoff = AIEML_SHIMNOC_LOCK_REGOFF,
+	.num_locks = 16U,
+	.overflow = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0x4,
+	},
+	.overflow_regoff = AIEML_SHIMNOC_LOCK_OVERFLOW_REGOFF,
+	.underflow = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0x4,
+	},
+	.underflow_regoff = AIEML_SHIMNOC_LOCK_UNDERFLOW_REGOFF,
+};
+
+static const struct aie_lock_attr aieml_mem_lock = {
+	.sts = {
+		.mask = GENMASK(5, 0),
+		.regoff = 0x10,
+	},
+	.sts_regoff = AIEML_TILE_MEMMOD_LOCK_REGOFF,
+	.num_locks = 16U,
+	.overflow = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0x4,
+	},
+	.overflow_regoff = AIEML_TILE_MEMMOD_LOCK_OVERFLOW_REGOFF,
+	.underflow = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0x4,
+	},
+	.underflow_regoff = AIEML_TILE_MEMMOD_LOCK_UNDERFLOW_REGOFF,
+};
+
+static const struct aie_lock_attr aieml_memtile_lock = {
+	.sts = {
+		.mask = GENMASK(5, 0),
+		.regoff = 0x10,
+	},
+	.sts_regoff = AIEML_MEMORY_LOCK_REGOFF,
+	.num_locks = 64U,
+	.overflow = {
+		.mask = GENMASK(31, 0),
+		.regoff = 0x4,
+	},
+	.overflow_regoff = AIEML_MEMORY_LOCK_OVERFLOW_REGOFF,
+	.underflow = {
+		.mask = GENMASK(31, 0),
+		.regoff = 0x4,
+	},
+	.underflow_regoff = AIEML_MEMORY_LOCK_UNDERFLOW_REGOFF,
+};
+
+static const struct aie_event_attr aieml_pl_event = {
+	.bc_event = {
+		.mask = GENMASK(6, 0),
+		.regoff = 0U,
+	},
+	.group_error = {
+		.mask = GENMASK(11, 0),
+		.regoff = 0xcU,
+	},
+	.bc_regoff = AIEML_SHIMPL_EVENT_BC0_REGOFF,
+	.status_regoff = AIEML_SHIMPL_EVENT_STATUS0_REGOFF,
+	.group_regoff = AIEML_SHIMPL_GROUP0_REGOFF,
+	.base_error_event = 64U,
+	.num_broadcasts = 16U,
+	.base_bc_event = 110U,
+	.num_events = 128U,
+};
+
+static const struct aie_event_attr aieml_memtile_event = {
+	.bc_event = {
+		.mask = GENMASK(7, 0),
+		.regoff = 0U,
+	},
+	.group_error = {
+		.mask = GENMASK(11, 0),
+		.regoff = 0x18U,
+	},
+	.bc_regoff = AIEML_MEMORY_EVENT_BC0_REGOFF,
+	.status_regoff = AIEML_MEMORY_EVENT_STATUS0_REGOFF,
+	.group_regoff = AIEML_MEMORY_GROUP0_REGOFF,
+	.base_error_event = 129U,
+	.num_broadcasts = 16U,
+	.base_bc_event = 142U,
+	.num_events = 192U,
+};
+
+static const struct aie_event_attr aieml_mem_event = {
+	.bc_event = {
+		.mask = GENMASK(6, 0),
+		.regoff = 0U,
+	},
+	.group_error = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0x14U,
+	},
+	.bc_regoff = AIEML_TILE_MEMMOD_EVENT_BC0_REGOFF,
+	.status_regoff = AIEML_TILE_MEMMOD_EVENT_STATUS0_REGOFF,
+	.group_regoff = AIEML_TILE_MEMMOD_GROUP0_REGOFF,
+	.base_error_event = 87U,
+	.num_broadcasts = 16U,
+	.base_bc_event = 107U,
+	.num_events = 128U,
+};
+
+static const struct aie_event_attr aieml_core_event = {
+	.bc_event = {
+		.mask = GENMASK(6, 0),
+		.regoff = 0U,
+	},
+	.group_error = {
+		.mask = GENMASK(24, 0),
+		.regoff = 0x10U,
+	},
+	.bc_regoff = AIEML_TILE_COREMOD_EVENT_BC0_REGOFF,
+	.status_regoff = AIEML_TILE_COREMOD_EVENT_STATUS0_REGOFF,
+	.group_regoff = AIEML_TILE_COREMOD_GROUP0_REGOFF,
+	.base_error_event = 48U,
+	.num_broadcasts = 16U,
+	.base_bc_event = 107U,
+	.num_events = 128U,
+};
+
+static const struct aie_l1_intr_ctrl_attr aieml_l1_intr_ctrl = {
+	.swa_status = {
+		.mask = GENMASK(19, 0),
+		.regoff = 0xcU,
+	},
+	.swb_status = {
+		.mask = GENMASK(19, 0),
+		.regoff = 0x3cU,
+	},
+	.swa_event = {
+		.mask = GENMASK(6, 0),
+		.regoff = 0x14U,
+	},
+	.swb_event = {
+		.mask = GENMASK(6, 0),
+		.regoff = 0x44U,
+	},
+	.regoff = 0x35000U,
+	.event_lsb = 8,
+	.num_broadcasts = 0x14U,
+};
+
+static const struct aie_l2_intr_ctrl_attr aieml_l2_intr_ctrl = {
+	.mask = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0x0U,
+	},
+	.enable = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0x4U,
+	},
+	.disable = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0x8U,
+	},
+	.status = {
+		.mask = GENMASK(15, 0),
+		.regoff = 0xcU,
+	},
+	.regoff = 0x15000U,
+	.num_broadcasts = 0x10U,
+};
+
+static char *aieml_core_status_str[] = {
+	"enable",
+	"reset",
+	"south_memory_stall",
+	"west_memory_stall",
+	"north_memory_stall",
+	"east_memory_stall",
+	"south_lock_stall",
+	"west_lock_stall",
+	"north_lock_stall",
+	"east_lock_stall",
+	"stream_stall_ss0",
+	"",
+	"stream_stall_ms0",
+	"",
+	"cascade_stall_scd",
+	"cascade_stall_mcd",
+	"debug_halt",
+	"ecc_error_stall",
+	"ecc_scrubbing_stall",
+	"error_halt",
+	"core_done",
+	"core_processor_bus_stall",
+};
+
+static char *aieml_dma_chan_status_str[] = {
+	"idle",
+	"running",
+};
+
+static char *aieml_dma_qsts_str[] = {
+	"okay",
+	"overflow",
+};
+
+static const struct aie_dev_attr aieml_aperture_dev_attr[] = {
+	AIE_APERTURE_ATTR_RO(hardware_info),
+};
+
+static const struct aie_dev_attr aieml_tile_dev_attr[] = {
+	AIE_TILE_DEV_ATTR_RO(bd, AIE_TILE_TYPE_MASK_TILE |
+			     AIE_TILE_TYPE_MASK_MEMORY |
+			     AIE_TILE_TYPE_MASK_SHIMNOC),
+	AIE_TILE_DEV_ATTR_RO(core, AIE_TILE_TYPE_MASK_TILE),
+	AIE_TILE_DEV_ATTR_RO(dma, AIE_TILE_TYPE_MASK_TILE |
+			     AIE_TILE_TYPE_MASK_MEMORY |
+			     AIE_TILE_TYPE_MASK_SHIMNOC),
+	AIE_TILE_DEV_ATTR_RO(error, AIE_TILE_TYPE_MASK_TILE |
+			     AIE_TILE_TYPE_MASK_MEMORY |
+			     AIE_TILE_TYPE_MASK_SHIMNOC |
+			     AIE_TILE_TYPE_MASK_SHIMPL),
+	AIE_TILE_DEV_ATTR_RO(event, AIE_TILE_TYPE_MASK_TILE |
+			     AIE_TILE_TYPE_MASK_MEMORY |
+			     AIE_TILE_TYPE_MASK_SHIMNOC |
+			     AIE_TILE_TYPE_MASK_SHIMPL),
+	AIE_TILE_DEV_ATTR_RO(lock, AIE_TILE_TYPE_MASK_TILE |
+			     AIE_TILE_TYPE_MASK_MEMORY |
+			     AIE_TILE_TYPE_MASK_SHIMNOC),
+};
+
+static const struct aie_dev_attr aieml_part_dev_attr[] = {
+	AIE_PART_DEV_ATTR_RO(current_freq),
+	AIE_PART_DEV_ATTR_RO(error_stat),
+};
+
+static const struct aie_bin_attr aieml_part_bin_attr[] = {
+	AIE_PART_BIN_ATTR_RO(core, AIEML_PART_SYSFS_CORE_BINA_SIZE),
+	AIE_PART_BIN_ATTR_RO(lock, AIEML_PART_SYSFS_LOCK_BINA_SIZE),
+	AIE_PART_BIN_ATTR_RO(dma, AIEML_PART_SYSFS_DMA_BINA_SIZE),
+	AIE_PART_BIN_ATTR_RO(error, AIEML_PART_SYSFS_ERROR_BINA_SIZE),
+	AIE_PART_BIN_ATTR_RO(status, AIEML_PART_SYSFS_STATUS_BINA_SIZE),
+};
+
+static const struct aie_sysfs_attr aieml_aperture_sysfs_attr = {
+	.dev_attr = aieml_aperture_dev_attr,
+	.bin_attr = NULL,
+	.num_dev_attrs = ARRAY_SIZE(aieml_aperture_dev_attr),
+	.num_bin_attrs = 0U,
+};
+
+static const struct aie_sysfs_attr aieml_part_sysfs_attr = {
+	.dev_attr = aieml_part_dev_attr,
+	.bin_attr = aieml_part_bin_attr,
+	.num_dev_attrs = ARRAY_SIZE(aieml_part_dev_attr),
+	.num_bin_attrs = ARRAY_SIZE(aieml_part_bin_attr),
+};
+
+static const struct aie_sysfs_attr aieml_tile_sysfs_attr = {
+	.dev_attr = aieml_tile_dev_attr,
+	.bin_attr = NULL,
+	.num_dev_attrs = ARRAY_SIZE(aieml_tile_dev_attr),
+	.num_bin_attrs = 0U,
+};
+
+static u32 aieml_get_tile_type(struct aie_device *adev,
+			       struct aie_location *loc)
+{
+	u8 num_mem_rows = adev->ttype_attr[AIE_TILE_TYPE_MEMORY].num_rows;
+
+	if (loc->row > num_mem_rows)
+		return AIE_TILE_TYPE_TILE;
+	if (loc->row && loc->row <= num_mem_rows)
+		return AIE_TILE_TYPE_MEMORY;
+	if (loc->row == 0)
+		if ((loc->col % 4) < 2)
+			return AIE_TILE_TYPE_SHIMPL;
+
+	return AIE_TILE_TYPE_SHIMNOC;
+}
+
+static u32 aieml_get_lock_status(struct aie_partition *apart,
+				 struct aie_location *loc, u8 lock)
+{
+	const struct aie_lock_attr *attr;
+	u32 ttype, stsoff, regoff, value;
+
+	ttype = aieml_get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_TILE)
+		attr = &aieml_mem_lock;
+	else if (ttype == AIE_TILE_TYPE_MEMORY)
+		attr = &aieml_memtile_lock;
+	else
+		attr = &aieml_pl_lock;
+
+	stsoff = attr->sts.regoff * lock + attr->sts_regoff;
+	regoff = aie_cal_regoff(apart->adev, *loc, stsoff);
+	value = ioread32(apart->aperture->base + regoff);
+
+	return aie_get_reg_field(&attr->sts, value);
+}
+
+static u64 aieml_get_lock_overflow_status(struct aie_partition *apart,
+					  struct aie_location *loc)
+{
+	const struct aie_lock_attr *attr;
+	u32 ttype, stsoff, regoff;
+	u64 value = 0;
+
+	ttype = aieml_get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_TILE)
+		attr = &aieml_mem_lock;
+	else if (ttype == AIE_TILE_TYPE_MEMORY)
+		attr = &aieml_memtile_lock;
+	else
+		attr = &aieml_pl_lock;
+
+	if (ttype != AIE_TILE_TYPE_MEMORY) {
+		stsoff = attr->overflow_regoff;
+		regoff = aie_cal_regoff(apart->adev, *loc, stsoff);
+		value = ioread32(apart->aperture->base + regoff);
+		value = aie_get_reg_field(&attr->overflow, value);
+	} else {
+		stsoff = attr->overflow_regoff;
+		regoff = aie_cal_regoff(apart->adev, *loc, stsoff);
+		value = ioread32(apart->aperture->base + regoff);
+
+		stsoff = attr->overflow.regoff * 1U + attr->overflow_regoff;
+		regoff = aie_cal_regoff(apart->adev, *loc, stsoff);
+		value |= ((u64)ioread32(apart->aperture->base + regoff)) << 32;
+	}
+	return value;
+}
+
+static u64 aieml_get_lock_underflow_status(struct aie_partition *apart,
+					   struct aie_location *loc)
+{
+	const struct aie_lock_attr *attr;
+	u32 ttype, stsoff, regoff;
+	u64 value = 0;
+
+	ttype = aieml_get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_TILE)
+		attr = &aieml_mem_lock;
+	else if (ttype == AIE_TILE_TYPE_MEMORY)
+		attr = &aieml_memtile_lock;
+	else
+		attr = &aieml_pl_lock;
+
+	if (ttype != AIE_TILE_TYPE_MEMORY) {
+		stsoff = attr->underflow_regoff;
+		regoff = aie_cal_regoff(apart->adev, *loc, stsoff);
+		value = ioread32(apart->aperture->base + regoff);
+		value = aie_get_reg_field(&attr->underflow, value);
+	} else {
+		stsoff = attr->underflow_regoff;
+		regoff = aie_cal_regoff(apart->adev, *loc, stsoff);
+		value = ioread32(apart->aperture->base + regoff);
+
+		stsoff = attr->underflow.regoff * 1U + attr->underflow_regoff;
+		regoff = aie_cal_regoff(apart->adev, *loc, stsoff);
+		value |= ((u64)ioread32(apart->aperture->base + regoff)) << 32;
+	}
+	return value;
+}
+
+static ssize_t aieml_get_tile_sysfs_lock_status(struct aie_partition *apart,
+						struct aie_location *loc,
+						char *buffer, ssize_t size)
+{
+	unsigned long overflow, underflow;
+	u32 i, ttype, num_locks;
+	ssize_t len = 0;
+
+	ttype = aieml_get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_SHIMPL)
+		return len;
+
+	if (ttype == AIE_TILE_TYPE_TILE)
+		num_locks = aieml_mem_lock.num_locks;
+	else if (ttype == AIE_TILE_TYPE_MEMORY)
+		num_locks = aieml_memtile_lock.num_locks;
+	else
+		num_locks = aieml_pl_lock.num_locks;
+
+	if (!aie_part_check_clk_enable_loc(apart, loc)) {
+		for (i = 0; i < num_locks; i++) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%d: clock_gated\n", i);
+		}
+		return len;
+	}
+
+	overflow = aieml_get_lock_overflow_status(apart, loc);
+
+	underflow = aieml_get_lock_underflow_status(apart, loc);
+
+	for (i = 0; i < num_locks; i++) {
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d: %d",
+				 i, aieml_get_lock_status(apart, loc, i));
+
+		if (test_bit(num_locks, &overflow))
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "|overflow");
+
+		if (test_bit(num_locks, &underflow))
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "|underflow");
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	}
+
+	return len;
+}
+
+static ssize_t aieml_get_part_sysfs_lock_status(struct aie_partition *apart,
+						struct aie_location *loc,
+						char *buffer, ssize_t size)
+{
+	u32 i, ttype, num_locks;
+	ssize_t len = 0;
+
+	if (!aie_part_check_clk_enable_loc(apart, loc)) {
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "clock_gated");
+		return len;
+	}
+
+	ttype = aieml_get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_TILE)
+		num_locks = aieml_mem_lock.num_locks;
+	else if (ttype == AIE_TILE_TYPE_MEMORY)
+		num_locks = aieml_memtile_lock.num_locks;
+	else
+		num_locks = aieml_pl_lock.num_locks;
+
+	for (i = 0; i < num_locks; i++) {
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d",
+				 aieml_get_lock_status(apart, loc, i));
+
+		if (i < num_locks - 1)
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 DELIMITER_LEVEL0);
+	}
+
+	return len;
+}
+
+/*
+ * aieml_get_tile_bd_attr() - gets tile bd attribute for AIEML
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @attr: pointer of attribute to assign
+ */
+static void aieml_get_tile_bd_attr(struct aie_partition *apart,
+				   struct aie_location *loc,
+				   const struct aie_bd_attr **attr)
+{
+	u32 ttype;
+
+	ttype = aieml_get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_TILE)
+		*attr = &aieml_tilebd;
+	else if (ttype == AIE_TILE_TYPE_MEMORY)
+		*attr = &aieml_memtilebd;
+	else
+		*attr = &aieml_shimbd;
+}
+
+/*
+ * aieml_get_tile_dma_attr() - gets tile dma attribute for AIEML
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @attr: pointer of attribute to assign
+ */
+static void aieml_get_tile_dma_attr(struct aie_partition *apart,
+				    struct aie_location *loc,
+				    const struct aie_dma_attr **attr)
+{
+	u32 ttype;
+
+	ttype = aieml_get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_TILE)
+		*attr = &aieml_tiledma;
+	else if (ttype == AIE_TILE_TYPE_MEMORY)
+		*attr = &aieml_memtiledma;
+	else
+		*attr = &aieml_shimdma;
+}
+
+/**
+ * aieml_get_dma_s2mm_status() - reads the DMA stream to memory map status.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @chanid: DMA channel ID.
+ * @return: 32-bit register value.
+ */
+static u32 aieml_get_dma_s2mm_status(struct aie_partition *apart,
+				     struct aie_location *loc, u8 chanid)
+{
+	const struct aie_dma_attr *attr;
+	u32 stsoff, regoff;
+
+	aieml_get_tile_dma_attr(apart, loc, &attr);
+
+	stsoff = attr->s2mm_sts_regoff + chanid * attr->chansts.regoff;
+	regoff = aie_cal_regoff(apart->adev, *loc, stsoff);
+
+	return ioread32(apart->aperture->base + regoff);
+}
+
+/**
+ * aieml_get_dma_mm2s_status() - reads the DMA memory map to stream status.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @chanid: DMA channel ID.
+ * @return: 32-bit register value.
+ */
+static u32 aieml_get_dma_mm2s_status(struct aie_partition *apart,
+				     struct aie_location *loc, u8 chanid)
+{
+	const struct aie_dma_attr *attr;
+	u32 stsoff, regoff;
+
+	aieml_get_tile_dma_attr(apart, loc, &attr);
+
+	stsoff = attr->mm2s_sts_regoff + chanid * attr->chansts.regoff;
+	regoff = aie_cal_regoff(apart->adev, *loc, stsoff);
+
+	return ioread32(apart->aperture->base + regoff);
+}
+
+/**
+ * aieml_get_chan_status() - reads the DMA channel status from DMA status value.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @status: status value of DMA.
+ * @return: 8-bit status value.
+ */
+static u8 aieml_get_chan_status(struct aie_partition *apart,
+				struct aie_location *loc, u32 status)
+{
+	const struct aie_dma_attr *attr;
+
+	aieml_get_tile_dma_attr(apart, loc, &attr);
+
+	return aie_get_reg_field(&attr->chansts, status);
+}
+
+/**
+ * aieml_get_queue_size() - reads the DMA queue size from DMA status value.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @status: status value of DMA.
+ * @return: 8-bit value.
+ */
+static u8 aieml_get_queue_size(struct aie_partition *apart,
+			       struct aie_location *loc, u32 status)
+{
+	const struct aie_dma_attr *attr;
+
+	aieml_get_tile_dma_attr(apart, loc, &attr);
+
+	return aie_get_reg_field(&attr->qsize, status);
+}
+
+/**
+ * aieml_get_queue_status() - reads the DMA queue status from DMA status value.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @status: status value of DMA.
+ * @return: 8-bit status value.
+ */
+static u8 aieml_get_queue_status(struct aie_partition *apart,
+				 struct aie_location *loc, u32 status)
+{
+	const struct aie_dma_attr *attr;
+
+	aieml_get_tile_dma_attr(apart, loc, &attr);
+
+	return aie_get_reg_field(&attr->qsts, status);
+}
+
+/**
+ * aieml_get_current_bd() - reads the current buffer descriptor being processed
+ *			    by DMA channel from DMA status value.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @status: status value of DMA.
+ * @return: 8-bit buffer descriptor value.
+ */
+static u8 aieml_get_current_bd(struct aie_partition *apart,
+			       struct aie_location *loc, u32 status)
+{
+	const struct aie_dma_attr *attr;
+
+	aieml_get_tile_dma_attr(apart, loc, &attr);
+
+	return aie_get_reg_field(&attr->curbd, status);
+}
+
+/**
+ * aieml_get_part_sysfs_dma_status() - returns the status of DMA in string
+ *				       format with MM2S and S2MM type channel
+ *				       separated by a ',' symbol. Channels with
+ *				       a given type are separated by a '|'
+ *				       symbol.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @buffer: location to return DMA status string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+static ssize_t aieml_get_part_sysfs_dma_status(struct aie_partition *apart,
+					       struct aie_location *loc,
+					       char *buffer, ssize_t size)
+{
+	u32 i, ttype, num_s2mm_chan, num_mm2s_chan;
+	bool is_delimit_req = false;
+	ssize_t len = 0;
+
+	if (!aie_part_check_clk_enable_loc(apart, loc)) {
+		len += scnprintf(buffer, max(0L, size - len),
+				 "mm2s: clock_gated%ss2mm: clock_gated",
+				 DELIMITER_LEVEL1);
+		return len;
+	}
+
+	ttype = aieml_get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		num_mm2s_chan = aieml_tiledma.num_mm2s_chan;
+		num_s2mm_chan = aieml_tiledma.num_s2mm_chan;
+	} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+		num_mm2s_chan = aieml_memtiledma.num_mm2s_chan;
+		num_s2mm_chan = aieml_memtiledma.num_s2mm_chan;
+	} else {
+		num_mm2s_chan = aieml_shimdma.num_mm2s_chan;
+		num_s2mm_chan = aieml_shimdma.num_s2mm_chan;
+	}
+
+	/* MM2S */
+	len += scnprintf(&buffer[len], max(0L, size - len), "mm2s: ");
+	for (i = 0; i < num_mm2s_chan; i++) {
+		u32 status = aieml_get_dma_mm2s_status(apart, loc, i);
+		u32 value = aieml_get_chan_status(apart, loc, status);
+
+		if (is_delimit_req)
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 DELIMITER_LEVEL0);
+
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 aieml_dma_chan_status_str[value]);
+		is_delimit_req = true;
+	}
+
+	/* S2MM */
+	is_delimit_req = false;
+	len += scnprintf(&buffer[len], max(0L, size - len), "%ss2mm: ",
+			 DELIMITER_LEVEL1);
+	for (i = 0; i < num_s2mm_chan; i++) {
+		u32 status = aieml_get_dma_s2mm_status(apart, loc, i);
+		u32 value = aieml_get_chan_status(apart, loc, status);
+
+		if (is_delimit_req)
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 DELIMITER_LEVEL0);
+
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 aieml_dma_chan_status_str[value]);
+		is_delimit_req = true;
+	}
+
+	return len;
+}
+
+/**
+ * aieml_get_tile_sysfs_dma_status() - exports AI engine DMA channel status,
+ *				       queue size, queue status, and current
+ *				       buffer descriptor ID being processed by
+ *				       DMA channel to a tile level sysfs node.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA.
+ * @buffer: location to return DMA status string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+static ssize_t aieml_get_tile_sysfs_dma_status(struct aie_partition *apart,
+					       struct aie_location *loc,
+					       char *buffer, ssize_t size)
+{
+	u32 i, ttype, chan, mm2s[AIE_MAX_MM2S_CH], s2mm[AIE_MAX_S2MM_CH],
+	    num_s2mm_chan, num_mm2s_chan;
+	bool is_delimit_req = false;
+	ssize_t len = 0;
+
+	if (!aie_part_check_clk_enable_loc(apart, loc)) {
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "channel_status: mm2s: clock_gated%ss2mm: clock_gated\n",
+				 DELIMITER_LEVEL1);
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "queue_size: mm2s: clock_gated%ss2mm: clock_gated\n",
+				 DELIMITER_LEVEL1);
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "queue_status: mm2s: clock_gated%ss2mm: clock_gated\n",
+				 DELIMITER_LEVEL1);
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "current_bd: mm2s: clock_gated%ss2mm: clock_gated\n",
+				 DELIMITER_LEVEL1);
+		return len;
+	}
+
+	ttype = aieml_get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		num_mm2s_chan = aieml_tiledma.num_mm2s_chan;
+		num_s2mm_chan = aieml_tiledma.num_s2mm_chan;
+	} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+		num_mm2s_chan = aieml_memtiledma.num_mm2s_chan;
+		num_s2mm_chan = aieml_memtiledma.num_s2mm_chan;
+	} else {
+		num_mm2s_chan = aieml_shimdma.num_mm2s_chan;
+		num_s2mm_chan = aieml_shimdma.num_s2mm_chan;
+	}
+
+	len += scnprintf(&buffer[len], max(0L, size - len), "channel_status: ");
+	len += aieml_get_part_sysfs_dma_status(apart, loc, &buffer[len],
+					       max(0L, size - len));
+
+	for (i = 0; i < num_mm2s_chan; i++)
+		mm2s[i] = aieml_get_dma_mm2s_status(apart, loc, i);
+
+	for (i = 0; i < num_s2mm_chan; i++)
+		s2mm[i] = aieml_get_dma_s2mm_status(apart, loc, i);
+
+	/* Queue size */
+	len += scnprintf(&buffer[len], max(0L, size - len),
+			 "\nqueue_size: mm2s: ");
+
+	for (chan = 0; chan < num_mm2s_chan; chan++) {
+		if (is_delimit_req)
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					DELIMITER_LEVEL0);
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d",
+				 aieml_get_queue_size(apart, loc, mm2s[chan]));
+		is_delimit_req = true;
+	}
+
+	len += scnprintf(&buffer[len], max(0L, size - len), "%ss2mm: ",
+			 DELIMITER_LEVEL1);
+
+	is_delimit_req = false;
+	for (chan = 0; chan < num_s2mm_chan; chan++) {
+		if (is_delimit_req)
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					DELIMITER_LEVEL0);
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d",
+				 aieml_get_queue_size(apart, loc, s2mm[chan]));
+		is_delimit_req = true;
+	}
+
+	/* Queue status */
+	len += scnprintf(&buffer[len], max(0L, size - len),
+			 "\nqueue_status: mm2s: ");
+
+	is_delimit_req = false;
+	for (chan = 0; chan < num_mm2s_chan; chan++) {
+		u32 value = aieml_get_queue_status(apart, loc, mm2s[chan]);
+
+		if (is_delimit_req)
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					DELIMITER_LEVEL0);
+
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 aieml_dma_qsts_str[value]);
+		is_delimit_req = true;
+	}
+
+	len += scnprintf(&buffer[len], max(0L, size - len), "%ss2mm: ",
+			 DELIMITER_LEVEL1);
+
+	is_delimit_req = false;
+	for (chan = 0; chan < num_s2mm_chan; chan++) {
+		u32 value = aieml_get_queue_status(apart, loc, s2mm[chan]);
+
+		if (is_delimit_req)
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					DELIMITER_LEVEL0);
+
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 aieml_dma_qsts_str[value]);
+		is_delimit_req = true;
+	}
+
+	/* Current BD */
+	len += scnprintf(&buffer[len], max(0L, size - len),
+			 "\ncurrent_bd: mm2s: ");
+
+	is_delimit_req = false;
+	for (chan = 0; chan < num_mm2s_chan; chan++) {
+		if (is_delimit_req)
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					DELIMITER_LEVEL0);
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d",
+				 aieml_get_current_bd(apart, loc, mm2s[chan]));
+		is_delimit_req = true;
+	}
+
+	len += scnprintf(&buffer[len], max(0L, size - len), "%ss2mm: ",
+			 DELIMITER_LEVEL1);
+
+	is_delimit_req = false;
+	for (chan = 0; chan < num_s2mm_chan; chan++) {
+		if (is_delimit_req)
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					DELIMITER_LEVEL0);
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d",
+				 aieml_get_current_bd(apart, loc, s2mm[chan]));
+		is_delimit_req = true;
+	}
+
+	len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	return len;
+}
+
+/**
+ * aieml_get_tile_sysfs_bd_metadata() - exports AI engine DMA buffer descriptor
+ *					metadata for all buffer descriptors to
+ *					a tile level sysfs node.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine DMA buffer descriptors.
+ * @buffer: location to return DMA buffer descriptor metadata string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+static ssize_t aieml_get_tile_sysfs_bd_metadata(struct aie_partition *apart,
+						struct aie_location *loc,
+						char *buffer, ssize_t size)
+{
+	const struct aie_dma_attr *dma_attr;
+	const struct aie_bd_attr *bd_attr;
+	u32 enabled, ttype;
+	ssize_t len = 0;
+
+	aieml_get_tile_dma_attr(apart, loc, &dma_attr);
+	aieml_get_tile_bd_attr(apart, loc, &bd_attr);
+
+	ttype = aieml_get_tile_type(apart->adev, loc);
+	enabled = aie_part_check_clk_enable_loc(apart, loc);
+	for (u32 bd = 0; bd < dma_attr->num_bds; bd++) {
+		u32 bd_data[AIE_MAX_BD_SIZE];
+		u32 i, index, base_bdoff;
+		u64 value;
+
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "%d: ", bd);
+		if (!enabled) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "clock_gated\n");
+			continue;
+		}
+
+		base_bdoff = dma_attr->bd_regoff + (bd_attr->bd_idx_off * bd);
+		memset(bd_data, 0, sizeof(bd_data));
+		for (i = 0; i < dma_attr->bd_len / sizeof(u32); i++) {
+			u32 regoff;
+
+			regoff = aie_cal_regoff(apart->adev, *loc,
+						base_bdoff + (i * 4U));
+			bd_data[i] = ioread32(apart->aperture->base + regoff);
+		}
+
+		/* address and length */
+		index = bd_attr->addr.addr.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->addr.addr, bd_data[index]);
+		if (ttype == AIE_TILE_TYPE_SHIMNOC) {
+			u32 h_addr;
+
+			/* add high address */
+			h_addr = bd_data[bd_attr->addr_2.addr.regoff / sizeof(u32)];
+			h_addr = aie_get_reg_field(&bd_attr->addr_2.addr, h_addr);
+			value |= (u64)h_addr << 32;
+		}
+		len += scnprintf(&buffer[len], max(0L, size - len), "%llx%s",
+				 value, DELIMITER_LEVEL0);
+
+		index = bd_attr->addr.length.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->addr.length, bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+
+		/* locks */
+		index = bd_attr->lock.lock_acq_id.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->lock.lock_acq_id,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->lock.lock_acq_val,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->lock.lock_acq_en,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->lock.lock_rel_id,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->lock.lock_rel_val,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+
+		/* packet */
+		index = bd_attr->packet.pkt_en.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->packet.pkt_en,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->packet.pkt_id,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->packet.pkt_type,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+
+		/* control */
+		index = bd_attr->valid_bd.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->valid_bd, bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		index = bd_attr->use_next.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->use_next, bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		index = bd_attr->next_bd.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->next_bd, bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		index = bd_attr->tlast_suppress.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->tlast_suppress,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		index = bd_attr->out_of_order_id.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->out_of_order_id,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		if (ttype != AIE_TILE_TYPE_SHIMNOC) {
+			index = bd_attr->compression_en.regoff / sizeof(u32);
+			value = aie_get_reg_field(&bd_attr->compression_en,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+		}
+
+		/* Dimensions */
+		index = bd_attr->aieml_dim.iter_curr.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->aieml_dim.iter_curr,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->aieml_dim.iter.step_size,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+		value = aie_get_reg_field(&bd_attr->aieml_dim.iter.wrap,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld%s",
+				 value, DELIMITER_LEVEL0);
+
+		for (i = 0; i < bd_attr->num_dims - 1; i++) {
+			index = bd_attr->aieml_dim.dims[i].step_size.regoff / sizeof(u32);
+			value = aie_get_reg_field(&bd_attr->aieml_dim.dims[i].step_size,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+			index = bd_attr->aieml_dim.dims[i].wrap.regoff / sizeof(u32);
+			value = aie_get_reg_field(&bd_attr->aieml_dim.dims[i].wrap,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+			/* padding */
+			if (ttype == AIE_TILE_TYPE_MEMORY) {
+				index = bd_attr->aieml_dim.pads[i].before.regoff / sizeof(u32);
+				value = aie_get_reg_field(&bd_attr->aieml_dim.pads[i].before,
+							  bd_data[index]);
+				len += scnprintf(&buffer[len], max(0L, size - len),
+						 "%lld%s", value, DELIMITER_LEVEL0);
+				index = bd_attr->aieml_dim.pads[i].after.regoff / sizeof(u32);
+				value = aie_get_reg_field(&bd_attr->aieml_dim.pads[i].after,
+							  bd_data[index]);
+				len += scnprintf(&buffer[len], max(0L, size - len),
+						 "%lld%s", value, DELIMITER_LEVEL0);
+			}
+		}
+		index = bd_attr->aieml_dim.dims[i].step_size.regoff / sizeof(u32);
+		value = aie_get_reg_field(&bd_attr->aieml_dim.dims[i].step_size,
+					  bd_data[index]);
+		len += scnprintf(&buffer[len], max(0L, size - len), "%lld", value);
+
+		/* axi settings */
+		if (ttype == AIE_TILE_TYPE_SHIMNOC) {
+			index = bd_attr->axi.smid.regoff / sizeof(u32);
+			value = aie_get_reg_field(&bd_attr->axi.smid,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%s%lld%s", DELIMITER_LEVEL0, value,
+					 DELIMITER_LEVEL0);
+			value = aie_get_reg_field(&bd_attr->axi.cache,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+			value = aie_get_reg_field(&bd_attr->axi.qos,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+			index = bd_attr->axi.secure_en.regoff / sizeof(u32);
+			value = aie_get_reg_field(&bd_attr->axi.secure_en,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld%s", value, DELIMITER_LEVEL0);
+			index = bd_attr->axi.burst_len.regoff / sizeof(u32);
+			value = aie_get_reg_field(&bd_attr->axi.burst_len,
+						  bd_data[index]);
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%lld", value);
+		}
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	}
+
+	return len;
+}
+
+static u32 aieml_get_core_status(struct aie_partition *apart,
+				 struct aie_location *loc)
+{
+	u32 regoff, regvalue;
+
+	regoff = aie_cal_regoff(apart->adev, *loc, aieml_core_sts.regoff);
+	regvalue = ioread32(apart->aperture->base + regoff);
+
+	return aie_get_reg_field(&aieml_core_sts, regvalue);
+}
+
+static unsigned int aieml_get_mem_info(struct aie_device *adev,
+				       struct aie_range *range,
+				       struct aie_part_mem *pmem)
+{
+	unsigned int i;
+	u8 start_row, num_rows;
+
+	if (range->start.row + range->size.row <= 1) {
+		/* SHIM row only, no memories in this range */
+		return 0;
+	}
+
+	if (!pmem)
+		return NUM_TYPES_OF_MEM;
+
+	for (i = 0; i < NUM_TYPES_OF_MEM; i++) {
+		struct aie_mem *mem = &pmem[i].mem;
+
+		memcpy(&mem->range, range, sizeof(*range));
+	}
+
+	start_row = adev->ttype_attr[AIE_TILE_TYPE_TILE].start_row;
+	num_rows = adev->ttype_attr[AIE_TILE_TYPE_TILE].num_rows;
+	/* Setup tile data memory information */
+	pmem[0].mem.offset = 0;
+	pmem[0].mem.size = KBYTES(64);
+	pmem[0].mem.range.start.row = start_row;
+	pmem[0].mem.range.size.row = num_rows;
+
+	/* Setup program memory information */
+	pmem[1].mem.offset = 0x20000;
+	pmem[1].mem.size = KBYTES(16);
+	pmem[1].mem.range.start.row = start_row;
+	pmem[1].mem.range.size.row = num_rows;
+
+	start_row = adev->ttype_attr[AIE_TILE_TYPE_MEMORY].start_row;
+	num_rows = adev->ttype_attr[AIE_TILE_TYPE_MEMORY].num_rows;
+	/* Setup memory tile memory information */
+	pmem[2].mem.offset = 0;
+	pmem[2].mem.size = KBYTES(512);
+	pmem[2].mem.range.start.row = start_row;
+	pmem[2].mem.range.size.row = num_rows;
+
+	return NUM_TYPES_OF_MEM;
+}
+
+static int aieml_init_part_clk_state(struct aie_partition *apart)
+{
+	int ret, num_tiles;
+
+	num_tiles = apart->range.size.col * (apart->range.size.row - 1);
+
+	ret = aie_resource_initialize(&apart->cores_clk_state, num_tiles);
+	if (ret) {
+		dev_err(&apart->dev,
+			"failed to initialize tiles clock state resource.\n");
+		return ret;
+	}
+
+	ret = aie_resource_initialize(&apart->tiles_inuse, num_tiles);
+	if (ret)
+		dev_err(&apart->dev,
+			"failed to initialize tiles in use resource.\n");
+
+	return ret;
+}
+
+static int aieml_scan_part_clocks(struct aie_partition *apart)
+{
+	struct aie_device *adev = apart->adev;
+	struct aie_aperture *aperture = apart->aperture;
+	struct aie_range *range = &apart->range;
+	struct aie_location loc;
+
+	/* Clear the bitmap of cores and memories clock state */
+	aie_resource_put_region(&apart->cores_clk_state, 0,
+				apart->cores_clk_state.total);
+
+	/*
+	 * In aieml if clock buffer on shim tile is enabled, the clock for all
+	 * tiles in the same column is enabled.
+	 */
+
+	loc.row = 0;
+	for (loc.col = range->start.col;
+	     loc.col < range->start.col + range->size.col;
+	     loc.col++) {
+		void __iomem *va;
+		u32 val, nbitpos;
+
+		nbitpos = loc.col * (range->size.row - 1) + loc.row;
+
+		va = aperture->base +
+		     aie_cal_regoff(adev, loc,
+				    AIEML_SHIMPL_COLCLOCK_CTRL_REGOFF);
+		val = ioread32(va);
+
+		if (!(val & AIEML_SHIMPL_COLCLOCK_CTRL_MASK))
+			continue;
+
+		aie_resource_set(&apart->cores_clk_state, nbitpos,
+				 range->size.row - 1);
+	}
+	/*
+	 * Set the tiles in use bitmap.
+	 * In case of scanning, tiles which are powered on are considered as
+	 * tiles in use.
+	 */
+	bitmap_copy(apart->tiles_inuse.bitmap, apart->cores_clk_state.bitmap,
+		    apart->tiles_inuse.total);
+
+	return 0;
+}
+
+static int aieml_set_part_clocks(struct aie_partition *apart)
+{
+	struct aie_range *range = &apart->range;
+	u32 node_id = apart->adev->pm_node_id;
+	struct aie_location loc;
+	int ret, status;
+
+	for (loc.col = range->start.col;
+	     loc.col < range->start.col + range->size.col;
+	     loc.col++) {
+		u32 startbit, col_inuse = 0;
+
+		startbit = loc.col * (range->size.row - 1);
+
+		for (loc.row = range->start.row + 1;
+		     loc.row < range->start.row + range->size.row;
+		     loc.row++) {
+			u32 nbitpos = startbit + loc.row - 1;
+
+			if (aie_resource_testbit(&apart->tiles_inuse, nbitpos)) {
+				col_inuse = 1;
+				break;
+			}
+		}
+
+		if (col_inuse) {
+			ret = zynqmp_pm_aie_operation(node_id, loc.col,
+						      1,
+						      XILINX_AIE_OPS_ENB_COL_CLK_BUFF);
+			if (ret < 0) {
+				dev_err(&apart->dev,
+					"failed to enable clock for column: %d\n",
+					loc.col);
+				return ret;
+			}
+
+			status = aie_resource_set(&apart->tiles_inuse,
+						  startbit, apart->range.size.row - 1);
+			status = aie_resource_set(&apart->cores_clk_state,
+						  startbit, apart->range.size.row - 1);
+		} else {
+			ret = zynqmp_pm_aie_operation(node_id, loc.col,
+						      1,
+						      XILINX_AIE_OPS_DIS_COL_CLK_BUFF);
+			if (ret < 0) {
+				dev_err(&apart->dev,
+					"failed to disable clock for column: %d\n",
+					loc.col);
+				return ret;
+			}
+
+			status = aie_resource_clear(&apart->tiles_inuse,
+						    startbit, apart->range.size.row - 1);
+			status = aie_resource_clear(&apart->cores_clk_state,
+						    startbit, apart->range.size.row - 1);
+		}
+	}
+
+	return status;
+}
+
+static int aieml_part_clear_mems(struct aie_partition *apart)
+{
+	struct aie_range *range = &apart->range;
+	u32 node_id = apart->adev->pm_node_id;
+	int ret;
+
+	ret = zynqmp_pm_aie_operation(node_id, range->start.col,
+				      range->size.col,
+				      XILINX_AIE_OPS_ZEROISATION);
+	if (ret < 0)
+		dev_err(&apart->dev, "failed to clear memory for partition\n");
+
+	return ret;
+}
+
+/**
+ * aieml_set_tile_isolation() - Set isolation boundary of AI engile tile
+ * @apart: AI engine partition
+ * @loc: Location of tile
+ * @dir: Direction to block
+ * @return: return 0 if success negative value for failure.
+ *
+ * Possible direction values are:
+ *	- AIE_ISOLATE_EAST_MASK
+ *	- AIE_ISOLATE_NORTH_MASK
+ *	- AIE_ISOLATE_WEST_MASK
+ *	- AIE_ISOLATE_SOUTH_MASK
+ *	- AIE_ISOLATE_ALL_MASK
+ *	- or "OR" of multiple values
+ */
+static int aieml_set_tile_isolation(struct aie_partition *apart,
+				    struct aie_location *loc, u8 dir)
+{
+	struct aie_device *adev = apart->adev;
+	struct aie_aperture *aperture = apart->aperture;
+	void __iomem *va;
+	u32 ttype, val;
+
+	/* For AIEML device, dir input will match register mask */
+	val = (u32)dir;
+	ttype = aieml_get_tile_type(adev, loc);
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		va = aperture->base +
+		     aie_cal_regoff(adev, *loc,
+				    AIEML_TILE_COREMOD_TILECTRL_REGOFF);
+	} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+		va = aperture->base +
+		     aie_cal_regoff(adev, *loc, AIEML_MEMORY_TILECTRL_REGOFF);
+	} else {
+		va = aperture->base +
+		     aie_cal_regoff(adev, *loc, AIEML_SHIMPL_TILECTRL_REGOFF);
+	}
+	iowrite32(val, va);
+	return 0;
+}
+
+static const struct aie_tile_operations aieml_ops = {
+	.get_tile_type = aieml_get_tile_type,
+	.get_mem_info = aieml_get_mem_info,
+	.get_core_status = aieml_get_core_status,
+	.get_part_sysfs_lock_status = aieml_get_part_sysfs_lock_status,
+	.get_tile_sysfs_lock_status = aieml_get_tile_sysfs_lock_status,
+	.get_part_sysfs_dma_status = aieml_get_part_sysfs_dma_status,
+	.get_tile_sysfs_dma_status = aieml_get_tile_sysfs_dma_status,
+	.get_tile_sysfs_bd_metadata = aieml_get_tile_sysfs_bd_metadata,
+	.init_part_clk_state = aieml_init_part_clk_state,
+	.scan_part_clocks = aieml_scan_part_clocks,
+	.set_part_clocks = aieml_set_part_clocks,
+	.set_tile_isolation = aieml_set_tile_isolation,
+	.mem_clear = aieml_part_clear_mems,
+	.get_dma_s2mm_status = aieml_get_dma_s2mm_status,
+	.get_dma_mm2s_status = aieml_get_dma_mm2s_status,
+	.get_chan_status = aieml_get_chan_status,
+	.get_lock_status = aieml_get_lock_status,
+};
+
+/**
+ * aieml_device_init_rscs_attr() - initialize AI engine device resources
+ *				   attributes
+ * @adev: AI engine device
+ */
+static void aieml_device_init_rscs_attr(struct aie_device *adev)
+{
+	struct aie_tile_attr *tattr;
+
+	tattr = &adev->ttype_attr[AIE_TILE_TYPE_TILE];
+	tattr->num_mods = NUM_MODS_CORE_TILE;
+	tattr->rscs_attr = aieml_core_tile_rscs_attr;
+	tattr->mods = aieml_core_tile_module_types;
+
+	tattr = &adev->ttype_attr[AIE_TILE_TYPE_MEMORY];
+	tattr->num_mods = NUM_MODS_MEM_TILE;
+	tattr->rscs_attr = aieml_mem_tile_rscs_attr;
+	tattr->mods = aieml_mem_tile_module_types;
+
+	tattr = &adev->ttype_attr[AIE_TILE_TYPE_SHIMPL];
+	tattr->num_mods = NUM_MODS_SHIMPL_TILE;
+	tattr->rscs_attr = aieml_shimpl_tile_rscs_attr;
+	tattr->mods = aieml_shimpl_tile_module_types;
+
+	/*
+	 * For now, SHIMNOC is the same as SHIMPL as there is
+	 * no SHIMNOC specific resources managed by kernel
+	 * driver yet.
+	 */
+	tattr = &adev->ttype_attr[AIE_TILE_TYPE_SHIMNOC];
+	tattr->num_mods = NUM_MODS_SHIMPL_TILE;
+	tattr->rscs_attr = aieml_shimpl_tile_rscs_attr;
+	tattr->mods = aieml_shimpl_tile_module_types;
+}
+
+int aieml_device_init(struct aie_device *adev)
+{
+	adev->array_shift = AIEML_ARRAY_SHIFT;
+	adev->col_shift = AIEML_COL_SHIFT;
+	adev->row_shift = AIEML_ROW_SHIFT;
+	adev->ops = &aieml_ops;
+	adev->num_kernel_regs = ARRAY_SIZE(aieml_kernel_regs);
+	adev->kernel_regs = aieml_kernel_regs;
+	adev->num_core_regs = ARRAY_SIZE(aieml_core_regs);
+	adev->core_regs = aieml_core_regs;
+	adev->col_rst = &aieml_col_rst;
+	adev->col_clkbuf = &aieml_col_clkbuf;
+	adev->tile_bd = &aieml_tilebd;
+	adev->shim_bd = &aieml_shimbd;
+	adev->memtile_bd = &aieml_memtilebd;
+	adev->tile_dma = &aieml_tiledma;
+	adev->shim_dma = &aieml_shimdma;
+	adev->memtile_dma = &aieml_memtiledma;
+	adev->aperture_sysfs_attr = &aieml_aperture_sysfs_attr;
+	adev->part_sysfs_attr = &aieml_part_sysfs_attr;
+	adev->tile_sysfs_attr = &aieml_tile_sysfs_attr;
+	adev->core_status_str = aieml_core_status_str;
+	adev->core_pc = &aieml_core_pc;
+	adev->core_lr = &aieml_core_lr;
+	adev->core_sp = &aieml_core_sp;
+	adev->pl_events = &aieml_pl_event;
+	adev->memtile_events = &aieml_memtile_event;
+	adev->mem_events = &aieml_mem_event;
+	adev->mem_lock = &aieml_mem_lock;
+	adev->pl_lock = &aieml_pl_lock;
+	adev->memtile_lock = &aieml_memtile_lock;
+	adev->core_events = &aieml_core_event;
+	adev->core_errors = &aieml_core_error;
+	adev->mem_errors = &aieml_mem_error;
+	adev->memtile_errors = &aieml_memtile_error;
+	adev->shim_errors = &aieml_shim_error;
+	adev->l1_ctrl = &aieml_l1_intr_ctrl;
+	adev->l2_ctrl = &aieml_l2_intr_ctrl;
+	adev->core_perfctrl = &aieml_core_perfctrl;
+	adev->core_perfctrl_reset = &aieml_core_perfctrl_reset;
+	adev->core_perfcnt = &aieml_core_perfcnt;
+	adev->core_evntgen = &aieml_core_evntgen;
+	adev->core_util_events = aieml_core_util_events;
+
+	aieml_device_init_rscs_attr(adev);
+
+	return 0;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-aperture.c b/drivers/misc/xilinx-ai-engine/ai-engine-aperture.c
new file mode 100644
index 000000000..afcb214c2
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-aperture.c
@@ -0,0 +1,568 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine partition driver
+ *
+ * Copyright (C) 2022 Xilinx, Inc.
+ * Copyright (C) 2023 Advanced Micro Devices, Inc.
+ */
+
+#include <linux/bitmap.h>
+#include <linux/device.h>
+#include <linux/firmware/xlnx-zynqmp.h>
+#include <linux/firmware/xlnx-versal-error-events.h>
+#include <linux/firmware/xlnx-event-manager.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_device.h>
+#include <linux/of_irq.h>
+#include <uapi/linux/xlnx-ai-engine.h>
+
+#include "ai-engine-internal.h"
+
+/* AI engine SHIM DMA address width is 48bits */
+#define XAIE_DMA_BIT_MASK	48U
+
+/**
+ * aie_aperture_get_num_parts() - get number of AI engine partitions of aperture
+ * @aperture: AI engine aperture
+ * @return: number of partitions of this aperture
+ *
+ * This function returns the number of AI engine partitions of the aperture.
+ * It includes the number of partitions in use and the number of available
+ * of partitions. If no partitions are in use, the number of available
+ * partitions is 1. One available partition is the max contiguous available
+ * columns region. E.g. if there is only one partition in use starting from
+ * column 10 to 14 in the aperture. The number of all partitions of this
+ * aperture is 3. They are column 0 to 9, 10 to 14 and 15+. This function
+ * returns 3, for which columns for each partition, and whether they are in
+ * use will be returned by another function @aie_aperture_enquire_parts().
+ */
+unsigned int aie_aperture_get_num_parts(struct aie_aperture *aperture)
+{
+	struct aie_partition *apart;
+	int ret;
+	unsigned int rs, re, num_parts = 0;
+
+	ret = mutex_lock_interruptible(&aperture->mlock);
+	if (ret)
+		return ret;
+
+	list_for_each_entry(apart, &aperture->partitions, node) {
+		num_parts++;
+	}
+
+	for_each_clear_bitrange(rs, re, aperture->cols_res.bitmap,
+				(aperture->range.size.col - 1)) {
+		num_parts++;
+	}
+
+	mutex_unlock(&aperture->mlock);
+
+	return num_parts;
+}
+
+/**
+ * aie_aperture_enquire_parts() - get partitions information
+ * @aperture: AI engine aperture`
+ * @num_queries: number of queries entries to return
+ * @queries: return the partitions information
+ * @num_parts_left: number of partitions not filled
+ * @to_user: indicate it if information is required to user space
+ * @return: number of partitions have been filled in if succeeded,
+ *	    negative value for error.
+ *
+ * This function returns each columns information and if the partition is
+ * in use for each partition until the queries array is filled up. The
+ * @num_parts_left will contains the partitions whose information are not
+ * able to put into the queries due to the queries array is full.
+ * Internal function, will not validate the queries, and num_parts_left
+ * pointers. Caller should not pass invalid values.
+ */
+int aie_aperture_enquire_parts(struct aie_aperture *aperture,
+			       unsigned int num_queries,
+			       struct aie_range_args  *queries,
+			       int *num_parts_left, bool to_user)
+{
+	struct aie_partition *apart;
+	int ret;
+	unsigned int rs, re, num_queries_left;
+
+	*num_parts_left = 0;
+	num_queries_left = num_queries;
+	ret = mutex_lock_interruptible(&aperture->mlock);
+	if (ret)
+		return ret;
+
+	list_for_each_entry(apart, &aperture->partitions, node) {
+		struct aie_range_args query;
+
+		if (!num_queries_left) {
+			*num_parts_left += 1;
+			continue;
+		}
+		query.partition_id = apart->range.start.col <<
+				     AIE_PART_ID_START_COL_SHIFT;
+		query.partition_id += apart->range.size.col <<
+				      AIE_PART_ID_NUM_COLS_SHIFT;
+		query.range.start.col = apart->range.start.col;
+		query.range.size.col = apart->range.size.col;
+		query.range.start.row = apart->range.start.row;
+		query.range.size.row = apart->range.size.row;
+		query.status = apart->status;
+
+		if (to_user) {
+			if (copy_to_user((void __user *)queries, &query,
+					 sizeof(query))) {
+				mutex_unlock(&aperture->mlock);
+				return -EFAULT;
+			}
+		} else {
+			memcpy(queries, &query, sizeof(query));
+		}
+		queries++;
+		num_queries_left--;
+	}
+
+	for_each_clear_bitrange(rs, re, aperture->cols_res.bitmap,
+				(aperture->range.size.col - 1)) {
+		struct aie_range_args query;
+
+		if (!num_queries_left) {
+			*num_parts_left += 1;
+			continue;
+		}
+		query.partition_id = (rs & AIE_PART_ID_START_COL_MASK) <<
+				     AIE_PART_ID_START_COL_SHIFT;
+		query.partition_id += ((re - rs + 1) &
+				       AIE_PART_ID_NUM_COLS_MASK) <<
+				      AIE_PART_ID_NUM_COLS_SHIFT;
+		query.range.start.col = rs;
+		query.range.size.col = re - rs + 1;
+		query.range.start.row = aperture->range.start.row;
+		query.range.size.row = aperture->range.size.row;
+		query.status = 0;
+
+		if (to_user) {
+			if (copy_to_user((void __user *)queries, &query,
+					 sizeof(query))) {
+				mutex_unlock(&aperture->mlock);
+				return -EFAULT;
+			}
+		} else {
+			memcpy(queries, &query, sizeof(query));
+		}
+		queries++;
+		num_queries_left--;
+	}
+
+	mutex_unlock(&aperture->mlock);
+
+	return (num_queries - num_queries_left);
+}
+
+/**
+ * aie_aperture_request_part_from_id() - request AI engine partition from id
+ * @aperture: AI engine aperture
+ * @partition_id: partition id to check
+ * @return: partition pointer for success, and error pointer for failure
+ *
+ * The partition ID contains the start column and number of columns
+ * information for the partition.
+ */
+struct aie_partition *
+aie_aperture_request_part_from_id(struct aie_aperture *aperture,
+				  u32 partition_id)
+{
+	struct aie_partition *apart = NULL;
+	u32 in_partition_id = partition_id;
+	u8 start_col, num_cols;
+	int ret;
+
+	start_col = aie_part_id_get_start_col(partition_id);
+	num_cols = aie_part_id_get_num_cols(partition_id);
+	/*
+	 * TODO: this is for backward compatibility, once zocl update
+	 * to pass the expected partition id format, can remove the
+	 * num_cols.
+	 */
+	if (num_cols == 0) {
+		start_col = aperture->range.start.col;
+		num_cols = aperture->range.size.col;
+		partition_id = ((u32)start_col << AIE_PART_ID_START_COL_SHIFT) |
+			       ((u32)num_cols << AIE_PART_ID_NUM_COLS_SHIFT);
+	}
+
+	if (start_col < aperture->range.start.col ||
+	    num_cols > aperture->range.size.col ||
+	    (start_col + num_cols) >
+	    (aperture->range.start.col + aperture->range.size.col)) {
+		dev_err(&aperture->dev, "invalid partition %u: %u,%u.\n",
+			partition_id, start_col, num_cols);
+		return ERR_PTR(-EINVAL);
+	}
+
+	ret = mutex_lock_interruptible(&aperture->mlock);
+	if (ret)
+		return ERR_PTR(ret);
+
+	ret = aie_resource_get_region(&aperture->cols_res, start_col, num_cols);
+	if (ret != (u32)start_col) {
+		dev_err(&aperture->dev, "partition %u already requested.\n",
+			in_partition_id);
+		mutex_unlock(&aperture->mlock);
+		return ERR_PTR(-EINVAL);
+	}
+
+	apart = aie_create_partition(aperture, partition_id);
+	if (IS_ERR(apart)) {
+		dev_err(&aperture->dev, "failed to create partition %u.\n",
+			partition_id);
+		mutex_unlock(&aperture->mlock);
+		return apart;
+	}
+
+	list_add_tail(&apart->node, &aperture->partitions);
+
+	mutex_unlock(&aperture->mlock);
+
+	return apart;
+}
+
+/**
+ * aie_aperture_check_part_avail() - Check an AI engine partition availability
+ * @aperture: AI engine aperture
+ * @req: AI engine partition requesting arguments
+ * @return: if AI engine partition is available, in use or not valid for the
+ *	    aperture.
+ *
+ * This functions checks the specified partition availability in the aperture.
+ * This function is internal call, it will not valid the input pointers.
+ */
+int aie_aperture_check_part_avail(struct aie_aperture *aperture,
+				  struct aie_partition_req *req)
+{
+	unsigned int start_col, end_col, num_cols;
+
+	start_col = aie_part_id_get_start_col(req->partition_id);
+	num_cols = aie_part_id_get_num_cols(req->partition_id);
+	/*
+	 * TODO: this is for backward compatibility, once zocl update
+	 * to pass the expected partition id format, can remove the
+	 * num_cols.
+	 */
+	if (num_cols == 0) {
+		start_col = aperture->range.start.col;
+		num_cols = aperture->range.size.col;
+	}
+
+	end_col = start_col + num_cols - 1;
+
+	if (start_col < aperture->range.start.col ||
+	    end_col >= aperture->range.start.col + aperture->range.size.col) {
+		return XAIE_PART_STATUS_INVALID;
+	}
+
+	if (aie_resource_check_region(&aperture->cols_res, start_col,
+				      num_cols) < 0)
+		return XAIE_PART_STATUS_INUSE;
+
+	return XAIE_PART_STATUS_IDLE;
+}
+
+/**
+ * aie_aperture_release_device() - release an AI engine aperture instance
+ * @dev: AI engine aperture device
+ *
+ * It will be called by device driver core when no one holds a valid
+ * pointer to @dev anymore.
+ */
+static void aie_aperture_release_device(struct device *dev)
+{
+	struct aie_aperture *aperture = dev_get_drvdata(dev);
+
+	aie_resource_uninitialize(&aperture->cols_res);
+	kfree(aperture->l2_mask.val);
+	zynqmp_pm_release_node(aperture->node_id);
+	kfree(aperture);
+}
+
+/**
+ * aie_aperture_remove() - destroy AI engine aperture
+ * @aperture: AI engine aperture
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will remove AI engine aperture.
+ */
+int aie_aperture_remove(struct aie_aperture *aperture)
+{
+	struct list_head *node, *pos;
+	int ret;
+
+	ret = mutex_lock_interruptible(&aperture->mlock);
+	if (ret)
+		return ret;
+
+	list_for_each_safe(pos, node, &aperture->partitions) {
+		struct aie_partition *apart;
+
+		apart = list_entry(pos, struct aie_partition, node);
+		list_del(&apart->node);
+		aie_part_remove(apart);
+	}
+
+	if (aperture->adev->device_name == AIE_DEV_GEN_S100 ||
+	    aperture->adev->device_name == AIE_DEV_GEN_S200) {
+		xlnx_unregister_event(PM_NOTIFY_CB, XPM_NODETYPE_VERSAL_EVENT_ERROR_PMC_ERR1,
+				      XPM_VERSAL_EVENT_ERROR_MASK_AIE_CR,
+				      aie_interrupt_callback, aperture);
+	}
+	mutex_unlock(&aperture->mlock);
+
+	aie_aperture_sysfs_remove_entries(aperture);
+	of_node_clear_flag(aperture->dev.of_node, OF_POPULATED);
+	device_del(&aperture->dev);
+	put_device(&aperture->dev);
+
+	return 0;
+}
+
+/**
+ * aie_aperture_add_dev() - initialize and add AI engine aperture device
+ * @aperture: AI engine aperture
+ * @nc: AI engine aperture device node
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will initialize and add AI engine aperture device to Linux
+ * kernel device framework.
+ * TODO: This function should be moved back to of_aie_aperture_probe()
+ * implementation once v1.0 device node support is removed.
+ */
+int aie_aperture_add_dev(struct aie_aperture *aperture,
+			 struct device_node *nc)
+{
+	struct device *dev = &aperture->dev;
+
+	/* register device for aperture */
+	dev = &aperture->dev;
+	dev->class = aie_class;
+	dev->parent = &aperture->adev->dev;
+	dev->of_node = nc;
+	dev->driver_data = aperture;
+	dev_set_name(dev, "aieaperture_%u_%u", aperture->range.start.col,
+		     aperture->range.size.col);
+	/* We can now rely on the release function for cleanup */
+	dev->release = aie_aperture_release_device;
+
+	return device_register(&aperture->dev);
+}
+
+/**
+ * of_aie_aperture_probe() - probes AI engine aperture node
+ * @adev: AI engine device
+ * @nc: aperture device node
+ * @return: AI engine aperture pointer for success, error pointer for failure.
+ *
+ * This function will probe AI engine aperture node and will create an AI
+ * engine aperture instance for the node.
+ * It requires the caller to lock the @adev before calling this function.
+ */
+struct aie_aperture *
+of_aie_aperture_probe(struct aie_device *adev, struct device_node *nc)
+{
+	struct aie_aperture *aperture, *laperture;
+	struct device *dev;
+	struct aie_range *range;
+	u32 regs[2];
+	int ret;
+
+	aperture = kzalloc(sizeof(*aperture), GFP_KERNEL);
+	if (!aperture)
+		return ERR_PTR(-ENOMEM);
+
+	aperture->adev = adev;
+	INIT_LIST_HEAD(&aperture->partitions);
+	mutex_init(&aperture->mlock);
+
+	range = &aperture->range;
+	ret = of_property_read_u32_array(nc, "xlnx,columns", regs,
+					 ARRAY_SIZE(regs));
+	if (ret < 0) {
+		dev_err(&adev->dev,
+			"probe %pOF failed, no tiles range information.\n",
+			nc);
+		goto free_aperture;
+	}
+	range->start.col = regs[0] & aligned_byte_mask(1);
+	range->size.col = regs[1] & aligned_byte_mask(1);
+
+	/*
+	 * Row information is used to calculate the clock or other resources
+	 * bitmaps. It can be moved aie_device later.
+	 */
+	range->start.row = 0;
+	range->size.row = adev->ttype_attr[AIE_TILE_TYPE_SHIMPL].num_rows +
+			  adev->ttype_attr[AIE_TILE_TYPE_MEMORY].num_rows +
+			  adev->ttype_attr[AIE_TILE_TYPE_TILE].num_rows;
+
+	ret = of_property_read_u32_index(nc, "xlnx,node-id", 0,
+					 &aperture->node_id);
+	if (ret < 0) {
+		dev_err(&adev->dev,
+			"probe %pOF failed, no aperture node id.\n", nc);
+		goto free_aperture;
+	}
+
+	/* Validate the aperture */
+	list_for_each_entry(laperture, &adev->apertures, node) {
+		u32 start_col, end_col, check_start_col, check_end_col;
+
+		if (laperture->node_id == aperture->node_id) {
+			dev_err(&adev->dev,
+				"probe failed, aperture %u exists.\n",
+				aperture->node_id);
+			ret = -EINVAL;
+			goto free_aperture;
+		}
+
+		start_col = range->start.col;
+		end_col  = start_col + range->size.col - 1;
+		check_start_col = laperture->range.start.col;
+		check_end_col = check_start_col + laperture->range.size.col - 1;
+		if ((start_col >= check_start_col &&
+		     start_col <= check_end_col) ||
+		    (end_col >= check_start_col &&
+		     end_col <= check_end_col)) {
+			dev_err(&adev->dev,
+				"probe failed, aperture %u overlaps other aperture.\n",
+				aperture->node_id);
+			ret = -EINVAL;
+			goto free_aperture;
+		}
+	}
+
+	/* register device for aperture */
+	ret = aie_aperture_add_dev(aperture, nc);
+	if (ret) {
+		dev_err(&aperture->dev, "device_add failed: %d\n", ret);
+		goto free_aperture;
+	}
+	dev = &aperture->dev;
+
+	/*
+	 * Initialize columns resource map to remember which columns have been
+	 * assigned. Used for partition management.
+	 */
+	ret = aie_resource_initialize(&aperture->cols_res,
+				      aperture->range.size.col);
+	if (ret) {
+		dev_err(dev, "failed to initialize columns resource.\n");
+		goto put_aperture_dev;
+	}
+
+	ret = of_address_to_resource(nc, 0, &aperture->res);
+	if (ret < 0) {
+		dev_err(dev, "failed to get address from device node.\n");
+		goto put_aperture_dev;
+	}
+	aperture->base = devm_ioremap_resource(dev, &aperture->res);
+	if (!aperture->base) {
+		ret = -ENOMEM;
+		goto put_aperture_dev;
+	}
+
+	/* Get device node DMA setting */
+	dev->coherent_dma_mask = DMA_BIT_MASK(XAIE_DMA_BIT_MASK);
+	dev->dma_mask = &dev->coherent_dma_mask;
+	ret = of_dma_configure(&aperture->dev, nc, true);
+	if (ret)
+		dev_warn(&aperture->dev, "Failed to configure DMA.\n");
+
+	/* Get device-name from device tree */
+	ret = of_property_read_u32_index(nc, "xlnx,device-name", 0,
+					 &aperture->adev->device_name);
+	if (ret < 0) {
+		aperture->adev->device_name = AIE_DEV_GENERIC_DEVICE;
+		dev_info(&adev->dev,
+			 "probe %pOF failed, no device-name", nc);
+	}
+
+	/* Initialize interrupt */
+	if (aperture->adev->device_name == AIE_DEV_GEN_S100 ||
+	    aperture->adev->device_name == AIE_DEV_GEN_S200) {
+		INIT_WORK(&aperture->backtrack, aie_aperture_backtrack);
+		ret = aie_aperture_create_l2_mask(aperture);
+		if (ret) {
+			dev_err(dev, "failed to initialize l2 mask resource.\n");
+			goto put_aperture_dev;
+		}
+
+		ret = xlnx_register_event(PM_NOTIFY_CB, XPM_NODETYPE_VERSAL_EVENT_ERROR_PMC_ERR1,
+					  XPM_VERSAL_EVENT_ERROR_MASK_AIE_CR,
+					  false, aie_interrupt_callback, aperture);
+
+		if (ret) {
+			dev_err(dev, "Interrupt forwarding failed.\n");
+			goto put_aperture_dev;
+		}
+	} else {
+		ret = of_irq_get_byname(nc, "interrupt1");
+		if (ret < 0) {
+			dev_warn(&adev->dev, "no interrupt in device node.");
+		} else {
+			aperture->irq = ret;
+			INIT_WORK(&aperture->backtrack, aie_aperture_backtrack);
+
+			ret = aie_aperture_create_l2_mask(aperture);
+			if (ret) {
+				dev_err(dev, "failed to initialize l2 mask resource.\n");
+				goto put_aperture_dev;
+			}
+
+			ret = devm_request_threaded_irq(dev, aperture->irq, NULL,
+							aie_interrupt, IRQF_ONESHOT,
+							dev_name(dev), aperture);
+			if (ret) {
+				dev_err(dev, "Failed to request AIE IRQ.\n");
+				goto put_aperture_dev;
+			}
+		}
+	}
+
+	ret = zynqmp_pm_request_node(aperture->node_id,
+				     ZYNQMP_PM_CAPABILITY_ACCESS, 0,
+				     ZYNQMP_PM_REQUEST_ACK_BLOCKING);
+	if (ret < 0) {
+		dev_err(dev, "Unable to request node %d\n", aperture->node_id);
+		goto put_aperture_dev;
+	}
+
+	of_node_get(nc);
+
+	ret = aie_aperture_sysfs_create_entries(aperture);
+	if (ret) {
+		dev_err(dev, "Failed to create aperture sysfs: %d\n", ret);
+		goto put_aperture_dev;
+	}
+
+	dev_info(dev,
+		 "AI engine aperture %s, id 0x%x, cols(%u, %u) aie_tile_rows(%u, %u) memory_tile_rows(%u, %u) is probed successfully.\n",
+		 dev_name(dev), aperture->node_id,
+		 aperture->range.start.col, aperture->range.size.col,
+		 adev->ttype_attr[AIE_TILE_TYPE_TILE].start_row,
+		 adev->ttype_attr[AIE_TILE_TYPE_TILE].num_rows,
+		 adev->ttype_attr[AIE_TILE_TYPE_MEMORY].start_row,
+		 adev->ttype_attr[AIE_TILE_TYPE_MEMORY].num_rows);
+
+	return aperture;
+
+put_aperture_dev:
+	put_device(dev);
+	return ERR_PTR(ret);
+
+free_aperture:
+	kfree(aperture);
+	return ERR_PTR(ret);
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-clock.c b/drivers/misc/xilinx-ai-engine/ai-engine-clock.c
new file mode 100644
index 000000000..a6a36122b
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-clock.c
@@ -0,0 +1,527 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include "ai-engine-internal.h"
+#include <linux/export.h>
+#include <linux/firmware/xlnx-zynqmp.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/xlnx-ai-engine.h>
+
+/**
+ * aie_part_get_clk_state_bit() - return bit position of the clock state of a
+ *				  tile
+ * @apart: AI engine partition
+ * @loc: AI engine tile location
+ * @return: bit position for success, negative value for failure
+ */
+static int aie_part_get_clk_state_bit(struct aie_partition *apart,
+				      struct aie_location *loc)
+{
+	u32 ttype = apart->adev->ops->get_tile_type(apart->adev, loc);
+
+	if (ttype != AIE_TILE_TYPE_TILE && ttype != AIE_TILE_TYPE_MEMORY)
+		return -EINVAL;
+
+	return (loc->col - apart->range.start.col) *
+	       (apart->range.size.row - 1) + loc->row - 1;
+}
+
+/**
+ * aie_part_scan_clk_state() - scan the clock states of tiles of the AI engine
+ *			       partition
+ * @apart: AI engine partition
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function will scan the clock status of both the memory and core
+ * modules.
+ */
+int aie_part_scan_clk_state(struct aie_partition *apart)
+{
+	return apart->adev->ops->scan_part_clocks(apart);
+}
+
+/**
+ * aie_part_check_clk_enable_loc() - return if clock of a tile is enabled
+ * @apart: AI engine partition
+ * @loc: AI engine tile location
+ * @return: true for enabled, false for disabled
+ */
+bool aie_part_check_clk_enable_loc(struct aie_partition *apart,
+				   struct aie_location *loc)
+{
+	int bit;
+	u32 ttype = apart->adev->ops->get_tile_type(apart->adev, loc);
+
+	if (ttype != AIE_TILE_TYPE_TILE && ttype != AIE_TILE_TYPE_MEMORY)
+		return true;
+
+	bit = aie_part_get_clk_state_bit(apart, loc);
+	return aie_resource_testbit(&apart->cores_clk_state, bit);
+}
+
+/**
+ * aie_part_request_tiles() - request tiles from an AI engine partition.
+ * @apart: AI engine partition
+ * @num_tiles: number of tiles to request. If it is 0, it means all tiles
+ * @locs: the AI engine tiles locations array which will be requested
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function will enable clocks of the specified tiles.
+ */
+int aie_part_request_tiles(struct aie_partition *apart, int num_tiles,
+			   struct aie_location *locs)
+{
+	if (num_tiles == 0) {
+		aie_resource_set(&apart->tiles_inuse, 0,
+				 apart->tiles_inuse.total);
+	} else {
+		u32 n;
+
+		if (!locs)
+			return -EINVAL;
+
+		for (n = 0; n < num_tiles; n++) {
+			int bit = aie_part_get_clk_state_bit(apart, &locs[n]);
+
+			if (bit >= 0)
+				aie_resource_set(&apart->tiles_inuse, bit, 1);
+		}
+	}
+
+	return apart->adev->ops->set_part_clocks(apart);
+}
+
+/**
+ * aie_part_release_tiles() - release tiles from an AI engine partition.
+ * @apart: AI engine partition
+ * @num_tiles: number of tiles to release. If it is 0, it means all tiles
+ * @locs: the AI engine tiles locations array which will be released
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function will disable clocks of the specified tiles.
+ */
+int aie_part_release_tiles(struct aie_partition *apart, int num_tiles,
+			   struct aie_location *locs)
+{
+	if (num_tiles == 0) {
+		aie_resource_clear(&apart->tiles_inuse, 0,
+				   apart->tiles_inuse.total);
+	} else {
+		u32 n;
+
+		if (!locs)
+			return -EINVAL;
+
+		for (n = 0; n < num_tiles; n++) {
+			int bit = aie_part_get_clk_state_bit(apart, &locs[n]);
+
+			if (bit >= 0)
+				aie_resource_clear(&apart->tiles_inuse, bit, 1);
+		}
+	}
+
+	return apart->adev->ops->set_part_clocks(apart);
+}
+
+/**
+ * aie_part_request_tiles_from_user() - request tiles from an AI engine
+ *					partition from user
+ * @apart: AI engine partition
+ * @user_args: user AI engine request tiles argument
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function will request tiles from user request.
+ */
+int aie_part_request_tiles_from_user(struct aie_partition *apart,
+				     void __user *user_args)
+{
+	struct aie_tiles_array args;
+	struct aie_location *locs = NULL;
+	int ret;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	if (args.num_tiles) {
+		u32 i;
+
+		locs = kmalloc_array(args.num_tiles, sizeof(*locs),
+				     GFP_KERNEL);
+		if (!locs)
+			return -ENOMEM;
+
+		if (copy_from_user(locs, (void __user *)args.locs,
+				   args.num_tiles * sizeof(*locs))) {
+			kfree(locs);
+			return -EFAULT;
+		}
+
+		/* update the location to absolute location */
+		for (i = 0; i < args.num_tiles; i++) {
+			if (locs[i].col > apart->range.size.col ||
+			    locs[i].row > apart->range.size.row) {
+				dev_err(&apart->dev,
+					"failed to request tiles, invalid tile(%u,%u).\n",
+					locs[i].col, locs[i].row);
+				kfree(locs);
+				return -EINVAL;
+			}
+			locs[i].col += apart->range.start.col;
+			locs[i].row += apart->range.start.row;
+		}
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		kfree(locs);
+		return ret;
+	}
+
+	ret = aie_part_request_tiles(apart, args.num_tiles, locs);
+	mutex_unlock(&apart->mlock);
+
+	kfree(locs);
+	return ret;
+}
+
+/**
+ * aie_part_release_tiles_from_user() - release tiles from an AI engine
+ *					partition from user
+ * @apart: AI engine partition
+ * @user_args: user AI engine request tiles argument
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function will release tiles from user request.
+ */
+int aie_part_release_tiles_from_user(struct aie_partition *apart,
+				     void __user *user_args)
+{
+	struct aie_tiles_array args;
+	struct aie_location *locs = NULL;
+	int ret;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	if (args.num_tiles) {
+		int i;
+
+		locs = kmalloc_array(args.num_tiles, sizeof(*locs),
+				     GFP_KERNEL);
+		if (!locs)
+			return -ENOMEM;
+
+		if (copy_from_user(locs, (void __user *)args.locs,
+				   args.num_tiles * sizeof(*locs))) {
+			kfree(locs);
+			return -EFAULT;
+		}
+
+		/* update the location to absolute location */
+		for (i = 0; i < args.num_tiles; i++) {
+			if (locs[i].col > apart->range.size.col ||
+			    locs[i].row > apart->range.size.row) {
+				dev_err(&apart->dev,
+					"failed to release tiles, invalid tile(%u,%u).\n",
+					locs[i].col, locs[i].row);
+				kfree(locs);
+				return -EINVAL;
+			}
+			locs[i].col += apart->range.start.col;
+			locs[i].row += apart->range.start.row;
+		}
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		kfree(locs);
+		return ret;
+	}
+
+	ret = aie_part_release_tiles(apart, args.num_tiles, locs);
+	mutex_unlock(&apart->mlock);
+
+	kfree(locs);
+	return ret;
+}
+
+/**
+ * aie_part_set_column_clock_from_user() - enable/disable column clock register
+ *                                        from an AI engine partition from
+ *                                        user
+ * @apart: AI engine partition
+ * @user_args: user AI engine request tiles argument
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function will request tiles from user request.h
+ */
+int aie_part_set_column_clock_from_user(struct aie_partition *apart,
+					void __user *user_args)
+{
+	u32 part_end_col = apart->range.start.col + apart->range.size.col - 1;
+	u32 node_id = apart->adev->pm_node_id;
+	struct aie_column_args args;
+	struct aie_location locs;
+	int ret;
+	u32 c;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	if ((args.start_col + args.num_cols - 1) > part_end_col) {
+		dev_err(&apart->dev, "invalid start column/size column\n");
+		return -EINVAL;
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+
+	if (ret)
+		return ret;
+
+	if (args.enable) {
+		ret = zynqmp_pm_aie_operation(node_id, args.start_col,
+					      args.num_cols,
+					      XILINX_AIE_OPS_ENB_COL_CLK_BUFF);
+		if (ret < 0) {
+			dev_err(&apart->dev, "failed to enable clocks for partition\n");
+			goto exit;
+		}
+
+		for (c = (args.start_col + apart->range.start.col);
+				c < (args.start_col + args.num_cols); c++) {
+			int bit = aie_part_get_clk_state_bit(apart, &locs);
+
+			locs.col = c;
+			locs.row = 1;
+
+			if (bit >= 0)
+				aie_resource_set(&apart->tiles_inuse, bit, 1);
+		}
+
+		aie_resource_set(&apart->cores_clk_state, 0,
+				 apart->cores_clk_state.total);
+	} else {
+		ret = zynqmp_pm_aie_operation(node_id, args.start_col,
+					      args.num_cols,
+					      XILINX_AIE_OPS_DIS_COL_CLK_BUFF);
+		if (ret < 0) {
+			dev_err(&apart->dev, "failed to disable clocks for partition\n");
+			goto exit;
+		}
+
+		for (c = (args.start_col + apart->range.start.col);
+				c < (args.start_col + args.num_cols); c++) {
+			int bit = aie_part_get_clk_state_bit(apart, &locs);
+
+			locs.col = c;
+			locs.row = 1;
+
+			if (bit >= 0)
+				aie_resource_clear(&apart->tiles_inuse, bit, 1);
+		}
+
+		aie_resource_clear(&apart->cores_clk_state, 0,
+				   apart->cores_clk_state.total);
+	}
+
+exit:
+	mutex_unlock(&apart->mlock);
+	return ret;
+}
+
+/**
+ * aie_aperture_get_freq_req() - get current required frequency of aperture
+ * @aperture: AI engine aperture
+ * @return: required clock frequency of the aperture which is the largest
+ *	    required clock frequency of all partitions of the aperture. If
+ *	    return value is 0, it means no partition has specific frequency
+ *	    requirement.
+ */
+static unsigned long aie_aperture_get_freq_req(struct aie_aperture *aperture)
+{
+	struct aie_partition *apart;
+	unsigned long freq_req = 0;
+	int ret;
+
+	ret = mutex_lock_interruptible(&aperture->mlock);
+	if (ret)
+		return freq_req;
+
+	list_for_each_entry(apart, &aperture->partitions, node) {
+		if (apart->freq_req > freq_req)
+			freq_req = apart->freq_req;
+	}
+
+	mutex_unlock(&aperture->mlock);
+
+	return freq_req;
+}
+
+/**
+ * aie_part_set_freq() - set frequency requirement of an AI engine partition
+ *
+ * @apart: AI engine partition
+ * @freq: required frequency
+ * @return: 0 for success, negative value for failure
+ *
+ * This function sets frequency requirement for the partition.
+ * It will call aie_dev_set_freq() to check the frequency requirements
+ * of all partitions. it will send QoS EEMI request to request the max
+ * frequency of all the partitions.
+ */
+int aie_part_set_freq(struct aie_partition *apart, u64 freq)
+{
+	struct aie_device *adev = apart->adev;
+	struct aie_aperture *aperture = apart->aperture;
+	unsigned long clk_rate;
+	u64 temp_freq;
+	u32 boot_qos, current_qos, target_qos;
+	int ret;
+
+	clk_rate = clk_get_rate(adev->clk);
+	if (freq > (u64)clk_rate) {
+		dev_err(&apart->dev,
+			"Invalid frequency to set, larger than full frequency(%lu).\n",
+			clk_rate);
+		return -EINVAL;
+	}
+
+	temp_freq = apart->freq_req;
+	apart->freq_req = freq;
+
+	freq = aie_aperture_get_freq_req(aperture);
+	if (!freq)
+		return 0;
+
+	ret = zynqmp_pm_get_qos(aperture->node_id, &boot_qos, &current_qos);
+	if (ret < 0) {
+		dev_err(&apart->dev, "Failed to get clock divider value.\n");
+		return -EINVAL;
+	}
+
+	target_qos = (boot_qos * clk_rate) / freq;
+
+	/* The clock divisor value (QoS) is a 10-bit value */
+	if (target_qos > (BIT(10) - 1)) {
+		/*
+		 * Reset the logged partition frequency requirement to its
+		 * pervious value.
+		 */
+		apart->freq_req = temp_freq;
+		dev_err(&apart->dev, "Failed to set frequency requirement. Frequency value out-of bound.\n");
+		return -EINVAL;
+	}
+
+	ret = zynqmp_pm_set_requirement(aperture->node_id,
+					ZYNQMP_PM_CAPABILITY_ACCESS, target_qos,
+					ZYNQMP_PM_REQUEST_ACK_BLOCKING);
+	if (ret < 0) {
+		apart->freq_req = temp_freq;
+		dev_err(&apart->dev, "Failed to set frequency requirement.\n");
+	}
+
+	return ret;
+}
+
+/**
+ * aie_partition_set_freq_req() - set partition frequency requirement
+ *
+ * @dev: AI engine partition device
+ * @freq: required frequency
+ * @return: 0 for success, negative value for failure
+ *
+ * This function sets the minimum required frequency for the AI engine
+ * partition. If there are other partitions requiring a higher frequency in the
+ * system, AI engine device will be clocked at that value to satisfy frequency
+ * requirements of all partitions.
+ */
+int aie_partition_set_freq_req(struct device *dev, u64 freq)
+{
+	struct aie_partition *apart;
+
+	if (!dev)
+		return -EINVAL;
+
+	apart = dev_to_aiepart(dev);
+	return aie_part_set_freq(apart, freq);
+}
+EXPORT_SYMBOL_GPL(aie_partition_set_freq_req);
+
+/**
+ * aie_part_get_freq() - get running frequency of AI engine device.
+ *
+ * @apart: AI engine partition
+ * @freq: return running frequency
+ * @return: 0 for success, negative value for failure
+ *
+ * This function gets clock divider value with EEMI requests, and it gets the
+ * full clock frequency from common clock framework. And then it divides the
+ * full clock frequency by the divider value and returns the result.
+ */
+int aie_part_get_freq(struct aie_partition *apart, u64 *freq)
+{
+	unsigned long clk_rate;
+	struct aie_device *adev = apart->adev;
+	u32 boot_qos, current_qos;
+	int ret;
+
+	if (!freq)
+		return -EINVAL;
+
+	clk_rate = clk_get_rate(adev->clk);
+	ret = zynqmp_pm_get_qos(apart->aperture->node_id, &boot_qos,
+				&current_qos);
+	if (ret < 0) {
+		dev_err(&apart->dev, "Failed to get clock divider value.\n");
+		return ret;
+	}
+
+	*freq = (clk_rate * boot_qos) / current_qos;
+	return 0;
+}
+
+/**
+ * aie_partition_get_freq() - get partition running frequency
+ *
+ * @dev: AI engine partition device
+ * @freq: return running frequency
+ * @return: 0 for success, negative value for failure
+ */
+int aie_partition_get_freq(struct device *dev, u64 *freq)
+{
+	struct aie_partition *apart;
+
+	if (!dev)
+		return -EINVAL;
+
+	apart = dev_to_aiepart(dev);
+	return aie_part_get_freq(apart, freq);
+}
+EXPORT_SYMBOL_GPL(aie_partition_get_freq);
+
+/**
+ * aie_partition_get_freq_req() - get partition required frequency
+ *
+ * @dev: AI engine partition device
+ * @freq: return partition required frequency. 0 means partition doesn't
+ *	  have frequency requirement.
+ * @return: 0 for success, negative value for failure
+ */
+int aie_partition_get_freq_req(struct device *dev, u64 *freq)
+{
+	struct aie_partition *apart;
+
+	if (!dev)
+		return -EINVAL;
+
+	apart = dev_to_aiepart(dev);
+	*freq = apart->freq_req;
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(aie_partition_get_freq_req);
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-dev-v1_0.c b/drivers/misc/xilinx-ai-engine/ai-engine-dev-v1_0.c
new file mode 100644
index 000000000..06eaad423
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-dev-v1_0.c
@@ -0,0 +1,202 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include <linux/anon_inodes.h>
+#include <linux/cdev.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/idr.h>
+#include <linux/interrupt.h>
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/xlnx-ai-engine.h>
+
+#include "ai-engine-internal.h"
+
+/**
+ * xilinx_ai_engine_probe_v1() - probe device tree v1.0 AI engine device
+ * @pdev: AI engine platform device
+ * @return: 0 for success, negative value for failure
+ */
+int xilinx_ai_engine_probe_v1(struct platform_device *pdev)
+{
+	struct aie_device *adev;
+	struct aie_aperture *aperture;
+	struct aie_range *range;
+	struct device_node *nc;
+	struct resource *res;
+	u32 pm_reg[2], regs[4];
+	int ret;
+
+	dev_info(&pdev->dev, "probing xlnx,ai-engine-v1.0 device.\n");
+
+	adev = devm_kzalloc(&pdev->dev, sizeof(*adev), GFP_KERNEL);
+	if (!adev)
+		return -ENOMEM;
+	platform_set_drvdata(pdev, adev);
+	INIT_LIST_HEAD(&adev->apertures);
+	mutex_init(&adev->mlock);
+
+	/* Initialize AIE device specific instance. */
+	ret = aie_device_init(adev);
+	if (ret < 0) {
+		dev_err(&pdev->dev, "failed to initialize device instance.\n");
+		return ret;
+	}
+
+	/*
+	 * AI Engine platform management node ID is required for requesting
+	 * services from firmware driver.
+	 */
+	ret = of_property_read_u32_array(pdev->dev.of_node, "power-domains",
+					 pm_reg, ARRAY_SIZE(pm_reg));
+	if (ret < 0) {
+		dev_err(&pdev->dev,
+			"Failed to read power manangement information\n");
+		return ret;
+	}
+	adev->pm_node_id = pm_reg[1];
+
+	adev->clk = devm_clk_get(&pdev->dev, NULL);
+	if (!adev->clk) {
+		dev_err(&pdev->dev, "Failed to get device clock.\n");
+		return -EINVAL;
+	}
+
+	ret = xilinx_ai_engine_add_dev(adev, pdev);
+	if (ret) {
+		dev_err(&pdev->dev, "Failed to add AI engine device.\n");
+		return ret;
+	}
+
+	/* v1 support single aperture only */
+	nc = of_get_next_child(pdev->dev.of_node, NULL);
+	if (!nc) {
+		dev_err(&pdev->dev,
+			"device tree node v1.0, no child node.\n");
+		put_device(&pdev->dev);
+		return -EINVAL;
+	}
+
+	aperture = kzalloc(sizeof(*aperture), GFP_KERNEL);
+	if (!aperture) {
+		put_device(&pdev->dev);
+		return -ENOMEM;
+	}
+
+	aperture->adev = adev;
+	INIT_LIST_HEAD(&aperture->partitions);
+	mutex_init(&aperture->mlock);
+
+	ret = of_property_read_u32_array(nc, "reg", regs,
+					 ARRAY_SIZE(regs));
+	if (ret < 0) {
+		dev_err(&adev->dev,
+			"probe %pOF failed, no tiles range information.\n",
+			nc);
+		kfree(aperture);
+		put_device(&pdev->dev);
+		return ret;
+	}
+	range = &aperture->range;
+	range->start.col = regs[0] & aligned_byte_mask(1);
+	range->size.col = regs[2] & aligned_byte_mask(1);
+	range->start.row = 0;
+	range->size.row = adev->ttype_attr[AIE_TILE_TYPE_SHIMPL].num_rows +
+			  adev->ttype_attr[AIE_TILE_TYPE_TILE].num_rows;
+
+	/* register device for aperture */
+	ret = aie_aperture_add_dev(aperture, nc);
+	if (ret) {
+		dev_err(&pdev->dev,
+			"failed to add AI engine aperture device\n");
+		kfree(aperture);
+		put_device(&pdev->dev);
+		return ret;
+	}
+
+	/**
+	 * Initialize columns resource map to remember which columns have been
+	 * assigned. Used for partition management.
+	 */
+	ret = aie_resource_initialize(&aperture->cols_res,
+				      aperture->range.size.col);
+	if (ret) {
+		dev_err(&pdev->dev, "failed to initialize columns resource.\n");
+		put_device(&pdev->dev);
+		return ret;
+	}
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(&pdev->dev, "No memory resource.\n");
+		put_device(&aperture->dev);
+		put_device(&pdev->dev);
+		return -EINVAL;
+	}
+	/* resource information will be used by ready only register mmap */
+	memcpy(&aperture->res, res, sizeof(*res));
+	aperture->base = devm_ioremap_resource(&aperture->dev, &aperture->res);
+	if (IS_ERR(aperture->base)) {
+		dev_err(&pdev->dev, "no io memory resource.\n");
+		ret = PTR_ERR(aperture->base);
+		put_device(&aperture->dev);
+		put_device(&pdev->dev);
+		return ret;
+	}
+
+	/* Get device node DMA setting */
+	aperture->dev.coherent_dma_mask = DMA_BIT_MASK(48);
+	aperture->dev.dma_mask = &aperture->dev.coherent_dma_mask;
+	ret = of_dma_configure(&aperture->dev, nc, true);
+	if (ret)
+		dev_warn(&aperture->dev, "Failed to configure DMA.\n");
+
+	INIT_WORK(&aperture->backtrack, aie_aperture_backtrack);
+	ret = aie_aperture_create_l2_mask(aperture);
+	if (ret) {
+		dev_err(&aperture->dev,
+			"failed to initialize l2 mask resource.\n");
+		put_device(&aperture->dev);
+		put_device(&pdev->dev);
+		return ret;
+	}
+
+	ret = platform_get_irq_byname(pdev, "interrupt1");
+	if (ret < 0) {
+		put_device(&aperture->dev);
+		put_device(&pdev->dev);
+		return ret;
+	}
+	aperture->irq = ret;
+
+	ret = devm_request_threaded_irq(&aperture->dev, aperture->irq, NULL,
+					aie_interrupt, IRQF_ONESHOT,
+					dev_name(&aperture->dev), aperture);
+	if (ret) {
+		dev_err(&pdev->dev, "Failed to request AIE IRQ.\n");
+		put_device(&aperture->dev);
+		put_device(&pdev->dev);
+		return ret;
+	}
+
+	of_node_get(nc);
+
+	list_add_tail(&aperture->node, &adev->apertures);
+
+	dev_info(&pdev->dev, "ai-enginee-v1.0 device node is probed.\n");
+	return 0;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-dev.c b/drivers/misc/xilinx-ai-engine/ai-engine-dev.c
new file mode 100644
index 000000000..dea4dba1e
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-dev.c
@@ -0,0 +1,844 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include <linux/anon_inodes.h>
+#include <linux/cdev.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/idr.h>
+#include <linux/interrupt.h>
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/mutex.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/xlnx-ai-engine.h>
+#include <uapi/linux/xlnx-ai-engine.h>
+
+#include "ai-engine-internal.h"
+
+#define AIE_DEV_MAX			(MINORMASK + 1)
+
+static dev_t aie_major;
+struct class *aie_class;
+
+static DEFINE_IDA(aie_device_ida);
+static DEFINE_IDA(aie_minor_ida);
+
+/**
+ * aie_partition_fd() - returns a file descriptor for the given AI engine
+ *			partition device
+ * @apart: AI engine partition
+ * @return: file descriptor of the AI engine partition for success,
+ *	    negative value for failure.
+ *
+ * This function allocate a file descriptor for the AI engine partition
+ * export file.
+ */
+static int aie_partition_fd(struct aie_partition *apart)
+{
+	int ret;
+
+	ret = get_unused_fd_flags(O_CLOEXEC);
+	if (ret < 0) {
+		dev_err(&apart->dev,
+			"Failed to get fd for partition %u.\n",
+			apart->partition_id);
+		return ret;
+	}
+	fd_install(ret, apart->filep);
+
+	return ret;
+}
+
+/**
+ * aie_enquire_partitions() - get AI engine partitions information
+ * @adev: AI engine device
+ * @query: data struct to store the partition information
+ * @return: 0 for success, and negative value for failure.
+ */
+static int aie_enquire_partitions(struct aie_device *adev,
+				  struct aie_partition_query *query)
+{
+	struct aie_aperture *aperture;
+	struct aie_range_args *query_parts;
+	u32 part_cnt, parts_filled;
+	int ret;
+
+	if (!query->partitions) {
+		part_cnt = 0;
+
+		/*
+		 * If partitions information buffer is NULL.
+		 * It is to get the number of partitions.
+		 */
+		ret = mutex_lock_interruptible(&adev->mlock);
+		if (ret)
+			return ret;
+
+		list_for_each_entry(aperture, &adev->apertures, node) {
+			part_cnt += aie_aperture_get_num_parts(aperture);
+		}
+		mutex_unlock(&adev->mlock);
+
+		query->partition_cnt = part_cnt;
+
+		return 0;
+	}
+
+	part_cnt = query->partition_cnt;
+	if (!part_cnt)
+		return 0;
+	parts_filled = 0;
+	query_parts = query->partitions;
+
+	ret = mutex_lock_interruptible(&adev->mlock);
+	if (ret)
+		return ret;
+
+	list_for_each_entry(aperture, &adev->apertures, node) {
+		int lparts_filled, num_parts_left;
+
+		lparts_filled = aie_aperture_enquire_parts(aperture,
+							   part_cnt,
+							   query_parts,
+							   &num_parts_left,
+							   true);
+		if (lparts_filled < 0) {
+			dev_err(&adev->dev,
+				"failed to enquire partitions.\n");
+			mutex_unlock(&adev->mlock);
+			return lparts_filled;
+		}
+		parts_filled += lparts_filled;
+		query_parts += lparts_filled;
+		/*
+		 * input partitions enquires buffers are less than the number
+		 * of partitions.
+		 * TODO: ioctl arguments can be updated to include how many
+		 * number of partitions information not yet filled.
+		 */
+		if (num_parts_left)
+			break;
+	}
+	mutex_unlock(&adev->mlock);
+	query->partition_cnt = parts_filled;
+
+	return 0;
+}
+
+/**
+ * aie_request_part_from_id() - request AI engine partition from id
+ * @adev: AI engine device
+ * @partition_id: partition id to check
+ * @return: partition pointer for success, and NULL for failure
+ *
+ * The partition ID contains the start column and number of columns
+ * information for the partition.
+ * This function expect the caller to lock mlock of @adev.
+ */
+static struct aie_partition *aie_request_part_from_id(struct aie_device *adev,
+						      u32 partition_id)
+{
+	struct aie_aperture *aperture;
+
+	list_for_each_entry(aperture, &adev->apertures, node) {
+		struct aie_partition *apart;
+
+		apart = aie_aperture_request_part_from_id(aperture,
+							  partition_id);
+		if (apart)
+			return apart;
+	}
+
+	return ERR_PTR(-EINVAL);
+}
+
+/**
+ * aie_partition_get() - Request the specified AI engine partition
+ *
+ * @apart: AI engine partition
+ * @req: AI engine partition request information which includes image UID.
+ *	 flag to indicate if the partition will cleanup or not when releasing
+ *	 the partition.
+ * @return: 0 for success, and negative value for failure.
+ *
+ * This function will check if the specified partition can be requested, it
+ * will check if the partition has been loaded with an image, if no, as long
+ * as it is not in use, the partition request will be granted. If there is
+ * image loaded, it will check if the given UID from @req matches the image UID
+ * loaded on the partition, if they match, the partition request will be
+ * granted. A file will be created for the requested partition.
+ */
+static int aie_partition_get(struct aie_partition *apart,
+			     struct aie_partition_req *req)
+{
+	struct file *filep;
+	int ret;
+
+	(void)req;
+
+	if (apart->status == XAIE_PART_STATUS_INUSE) {
+		dev_err(&apart->dev,
+			"request partition %u failed, partition in use.\n",
+			apart->partition_id);
+		return -EBUSY;
+	}
+	/*
+	 * TODO:
+	 * 1. It will check image UID too to see if the user matches what's
+	 *    loaded in the AI engine partition. And check the meta data to see
+	 *    which resources used by application.
+	 */
+
+	/* Get a file for the partition */
+	filep = anon_inode_getfile(dev_name(&apart->dev), &aie_part_fops,
+				   apart, O_RDWR);
+	if (IS_ERR(filep)) {
+		dev_err(&apart->dev,
+			"Failed to request partition %u, failed to get file.\n",
+			apart->partition_id);
+		return PTR_ERR(filep);
+	}
+
+	filep->f_mode |= (FMODE_LSEEK | FMODE_PREAD | FMODE_PWRITE);
+	apart->filep = filep;
+
+	apart->cntrflag = req->flag;
+
+	/* open AI engine partition instance to get it ready for use */
+	ret = aie_part_open(apart, (void *)req->meta_data);
+	if (ret) {
+		dev_err(&apart->dev, "Failed to open partition %u instance.\n",
+			apart->partition_id);
+		fput(filep);
+		return ret;
+	}
+
+	return 0;
+}
+
+/**
+ * aie_partition_request_from_adev() - request AI engine partition from AI
+ *				       engine device
+ * @adev: AI engine device
+ * @req: partition request, includes the requested AI engine information
+ *	 such as partition node ID and the UID of the image which is used
+ *	 to describe the partition to request.
+ * @return: pointer to the AI engine partition for success, and negative
+ *	    value for failure.
+ *
+ * This function finds a defined partition which matches the specified
+ * partition id, and request it.
+ */
+static struct aie_partition *
+aie_partition_request_from_adev(struct aie_device *adev,
+				struct aie_partition_req *req)
+{
+	struct aie_partition *apart;
+	int ret;
+
+	ret = mutex_lock_interruptible(&adev->mlock);
+	if (ret)
+		return ERR_PTR(ret);
+
+	apart = aie_request_part_from_id(adev, req->partition_id);
+	if (IS_ERR(apart)) {
+		dev_err(&adev->dev,
+			"request partition %u failed, not exist.\n",
+			req->partition_id);
+		mutex_unlock(&adev->mlock);
+		return ERR_PTR(-EINVAL);
+	}
+	mutex_unlock(&adev->mlock);
+
+	ret = aie_partition_get(apart, req);
+
+	if (ret)
+		apart = ERR_PTR(ret);
+	return apart;
+}
+
+static long xilinx_ai_engine_ioctl(struct file *filp, unsigned int cmd,
+				   unsigned long arg)
+{
+	struct inode *inode = file_inode(filp);
+	struct aie_device *adev = cdev_to_aiedev(inode->i_cdev);
+	void __user *argp = (void __user *)arg;
+	int ret;
+
+	switch (cmd) {
+	case AIE_ENQUIRE_PART_IOCTL:
+	{
+		struct aie_partition_query query;
+		struct aie_partition_query  __user *uquery_ptr = argp;
+
+		if (copy_from_user(&query, uquery_ptr, sizeof(query)))
+			return -EFAULT;
+		ret = aie_enquire_partitions(adev, &query);
+		if (ret < 0)
+			return ret;
+		if (copy_to_user((void __user *)&uquery_ptr->partition_cnt,
+				 &query.partition_cnt,
+				 sizeof(query.partition_cnt)))
+			return -EFAULT;
+		break;
+	}
+	case AIE_REQUEST_PART_IOCTL:
+	{
+		struct aie_partition_req req;
+		struct aie_partition *apart;
+
+		if (copy_from_user(&req, argp, sizeof(req)))
+			return -EFAULT;
+
+		apart = aie_partition_request_from_adev(adev, &req);
+		if (IS_ERR(apart))
+			return PTR_ERR(apart);
+
+		/* Allocate fd */
+		ret = aie_partition_fd(apart);
+		if (ret < 0) {
+			fput(apart->filep);
+			break;
+		}
+		break;
+	}
+	default:
+		dev_err(&adev->dev, "Invalid ioctl command %u.\n", cmd);
+		ret = -EINVAL;
+		break;
+	}
+
+	return ret;
+}
+
+static const struct file_operations aie_device_fops = {
+	.owner		= THIS_MODULE,
+	.unlocked_ioctl	= xilinx_ai_engine_ioctl,
+};
+
+static void xilinx_ai_engine_release_device(struct device *dev)
+{
+	struct aie_device *adev = dev_to_aiedev(dev);
+
+	ida_simple_remove(&aie_device_ida, dev->id);
+	ida_simple_remove(&aie_minor_ida, MINOR(dev->devt));
+	cdev_del(&adev->cdev);
+}
+
+/**
+ * of_xilinx_ai_engine_aperture_probe() - probes AI engine aperture nodes
+ * @adev: AI engine device
+ *
+ * This function will probe children AI engine apertures nodes and create
+ * an AI engine aperture instance for each node.
+ */
+void of_xilinx_ai_engine_aperture_probe(struct aie_device *adev)
+{
+	struct device_node *nc;
+
+	for_each_available_child_of_node(adev->dev.of_node, nc) {
+		struct aie_aperture *aperture;
+		int ret;
+
+		if (of_node_test_and_set_flag(nc, OF_POPULATED))
+			continue;
+
+		ret = mutex_lock_interruptible(&adev->mlock);
+		if (ret)
+			return;
+
+		aperture = of_aie_aperture_probe(adev, nc);
+		if (IS_ERR(aperture)) {
+			dev_err(&adev->dev,
+				"Failed to probe AI engine aperture for %pOF\n",
+				nc);
+			/* try to probe the next node */
+			continue;
+		}
+		list_add_tail(&aperture->node, &adev->apertures);
+
+		mutex_unlock(&adev->mlock);
+	}
+}
+
+/**
+ * xilinx_ai_engine_add_dev() - initialize and add AI engine device
+ * @adev: AI engine device
+ * @pdev: AI engine platform device
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will initialize and add AI engine device to Linux kernel
+ * device framework.
+ * TODO: This function should be moved back to xilinx_ai_engine_probe()
+ * implementation once v1.0 device node support is removed.
+ */
+int xilinx_ai_engine_add_dev(struct aie_device *adev,
+			     struct platform_device *pdev)
+{
+	struct device *dev;
+	int ret;
+
+	dev = &adev->dev;
+	device_initialize(dev);
+	dev->class = aie_class;
+	dev->parent = &pdev->dev;
+	dev->of_node = pdev->dev.of_node;
+
+	ret = ida_simple_get(&aie_minor_ida, 0, AIE_DEV_MAX, GFP_KERNEL);
+	if (ret < 0)
+		return ret;
+	dev->devt = MKDEV(MAJOR(aie_major), ret);
+	ret = ida_simple_get(&aie_device_ida, 0, 0, GFP_KERNEL);
+	if (ret < 0) {
+		ida_simple_remove(&aie_minor_ida, MINOR(dev->devt));
+		return ret;
+	}
+	dev->id = ret;
+	dev_set_name(&adev->dev, "aie%d", dev->id);
+
+	cdev_init(&adev->cdev, &aie_device_fops);
+	adev->cdev.owner = THIS_MODULE;
+	ret = cdev_add(&adev->cdev, dev->devt, 1);
+	if (ret) {
+		ida_simple_remove(&aie_device_ida, dev->id);
+		ida_simple_remove(&aie_minor_ida, MINOR(dev->devt));
+		return ret;
+	}
+	/* We can now rely on the release function for cleanup */
+	dev->release = xilinx_ai_engine_release_device;
+
+	ret = device_add(dev);
+	if (ret) {
+		dev_err(&pdev->dev, "device_add failed: %d\n", ret);
+		put_device(dev);
+		return ret;
+	}
+
+	return 0;
+}
+
+static int xilinx_ai_engine_probe(struct platform_device *pdev)
+{
+	struct aie_device *adev;
+	u32 pm_reg[2];
+	int ret;
+	u8 regs_u8[2];
+	u8 aie_gen;
+
+	adev = devm_kzalloc(&pdev->dev, sizeof(*adev), GFP_KERNEL);
+	if (!adev)
+		return -ENOMEM;
+	platform_set_drvdata(pdev, adev);
+	INIT_LIST_HEAD(&adev->apertures);
+	mutex_init(&adev->mlock);
+
+	/* check if device node is v1.0 or not */
+	ret = of_device_is_compatible(pdev->dev.of_node, "xlnx,ai-engine-v1.0");
+	if (ret)
+		return xilinx_ai_engine_probe_v1(pdev);
+
+	ret = of_property_read_u8(pdev->dev.of_node, "xlnx,aie-gen", &aie_gen);
+	if (ret < 0) {
+		dev_warn(&pdev->dev,
+			 "no aie dev generation information in device tree\n");
+		return ret;
+	}
+
+	ret = of_property_read_u8_array(pdev->dev.of_node, "xlnx,shim-rows",
+					regs_u8, ARRAY_SIZE(regs_u8));
+	if (ret < 0) {
+		dev_warn(&pdev->dev,
+			 "no SHIM rows information in device tree\n");
+		return ret;
+	}
+	adev->ttype_attr[AIE_TILE_TYPE_SHIMPL].start_row = regs_u8[0];
+	adev->ttype_attr[AIE_TILE_TYPE_SHIMPL].num_rows = regs_u8[1];
+	adev->ttype_attr[AIE_TILE_TYPE_SHIMNOC].start_row = regs_u8[0];
+	adev->ttype_attr[AIE_TILE_TYPE_SHIMNOC].num_rows = regs_u8[1];
+
+	ret = of_property_read_u8_array(pdev->dev.of_node, "xlnx,core-rows",
+					regs_u8, ARRAY_SIZE(regs_u8));
+	if (ret < 0) {
+		dev_err(&pdev->dev,
+			"Failed to read core rows information\n");
+		return ret;
+	}
+	adev->ttype_attr[AIE_TILE_TYPE_TILE].start_row = regs_u8[0];
+	adev->ttype_attr[AIE_TILE_TYPE_TILE].num_rows = regs_u8[1];
+
+	ret = of_property_read_u8_array(pdev->dev.of_node, "xlnx,mem-rows",
+					regs_u8, ARRAY_SIZE(regs_u8));
+	if (ret < 0) {
+		dev_err(&pdev->dev,
+			"Failed to read mem rows information\n");
+		return ret;
+	}
+
+	adev->ttype_attr[AIE_TILE_TYPE_MEMORY].start_row = regs_u8[0];
+	adev->ttype_attr[AIE_TILE_TYPE_MEMORY].num_rows = regs_u8[1];
+
+	adev->dev_gen = aie_gen;
+	if (aie_gen == AIE_DEVICE_GEN_AIE) {
+		ret = aie_device_init(adev);
+	} else if (aie_gen == AIE_DEVICE_GEN_AIEML) {
+		ret = aieml_device_init(adev);
+	} else {
+		dev_err(&pdev->dev, "Invalid device generation\n");
+		return -EINVAL;
+	}
+	if (ret < 0) {
+		dev_err(&pdev->dev, "failed to initialize device instance.\n");
+		return ret;
+	}
+
+	/*
+	 * AI Engine platform management node ID is required for requesting
+	 * services from firmware driver.
+	 */
+	ret = of_property_read_u32_array(pdev->dev.of_node, "power-domains",
+					 pm_reg, ARRAY_SIZE(pm_reg));
+	if (ret < 0) {
+		dev_err(&pdev->dev,
+			"Failed to read power manangement information\n");
+		return ret;
+	}
+	adev->pm_node_id = pm_reg[1];
+
+	adev->clk = devm_clk_get(&pdev->dev, "aclk0");
+	if (!adev->clk) {
+		dev_err(&pdev->dev, "Failed to get device clock.\n");
+		return -EINVAL;
+	}
+
+	ret = xilinx_ai_engine_add_dev(adev, pdev);
+	if (ret) {
+		dev_err(&pdev->dev, "Failed to add ai engine device.\n");
+		return ret;
+	}
+
+	of_xilinx_ai_engine_aperture_probe(adev);
+	dev_info(&pdev->dev,
+		 "Xilinx AI Engine device %s probed. Device generation: %u. Clock frequency: %ldHz.\n",
+		 dev_name(&pdev->dev), aie_gen, clk_get_rate(adev->clk));
+
+	return 0;
+}
+
+static int xilinx_ai_engine_remove(struct platform_device *pdev)
+{
+	struct aie_device *adev = platform_get_drvdata(pdev);
+	struct list_head *node, *pos;
+
+	list_for_each_safe(pos, node, &adev->apertures) {
+		struct aie_aperture *aperture;
+		int ret;
+
+		aperture = list_entry(pos, struct aie_aperture, node);
+		ret = aie_aperture_remove(aperture);
+		if (ret)
+			return ret;
+	}
+
+	device_del(&adev->dev);
+	put_device(&adev->dev);
+
+	return 0;
+}
+
+static const struct of_device_id xilinx_ai_engine_of_match[] = {
+	{ .compatible = "xlnx,ai-engine-v2.0", },
+	{ .compatible = "xlnx,ai-engine-v1.0", },
+	{ /* end of table */ },
+};
+MODULE_DEVICE_TABLE(of, xilinx_ai_engine_of_match);
+
+static struct platform_driver xilinx_ai_engine_driver = {
+	.probe			= xilinx_ai_engine_probe,
+	.remove			= xilinx_ai_engine_remove,
+	.driver			= {
+		.name		= "xilinx-ai-engine",
+		.of_match_table	= xilinx_ai_engine_of_match,
+	},
+};
+
+/**
+ * of_ai_engine_class_find() - find AI engine device with device node
+ * @np: device node
+ * @return: AI engine device pointer if found, NULL if it is not found
+ *
+ * This function checks every AI engine device of the aie_class, returns the
+ * one whose of_node matches the input of node.
+ */
+struct aie_device *of_ai_engine_class_find(struct device_node *np)
+{
+	struct device *dev;
+
+	dev = class_find_device(aie_class, NULL, np, device_match_of_node);
+	if (!dev)
+		return NULL;
+
+	return dev_to_aiedev(dev);
+}
+
+/**
+ * aie_partition_is_available() - Check if an AI engine partition is available
+ * @req: AI engine partition requesting arguments
+ * @return: true if the AI engine partition is not in use, otherwise, false
+ *
+ * This function looks up the AI engine class devices to find the AI engine
+ * partition whose partition ID matches the given partition ID in @req. If
+ * the partition can be found, if will check if the partition is in use.
+ *
+ * In case the AI engine release function is called from kernel context, the
+ * release() will be scheduled when the AI engine partition reference count is
+ * reduced to 0 instead of get called synchronously, and thus, this is a helper
+ * function for another kernel module to check if the partitions is released
+ * after calling release function from kernel context
+ *
+ * However, if closing the partition is from user context, it will not return
+ * until the release is complete when there is no reference to the AI engine
+ * partition file. In this case, user doesn't need to call this function to
+ * check if the partition is released.
+ */
+bool aie_partition_is_available(struct aie_partition_req *req)
+{
+	struct device *dev;
+	struct class_dev_iter iter;
+
+	if (!req)
+		return false;
+
+	class_dev_iter_init(&iter, aie_class, NULL, NULL);
+	while ((dev = class_dev_iter_next(&iter))) {
+		struct aie_aperture *aperture;
+		int ret;
+
+		if (strncmp(dev_name(dev), "aieaperture",
+			    strlen("aieaperture")))
+			continue;
+
+		aperture = dev_get_drvdata(dev);
+		ret = aie_aperture_check_part_avail(aperture, req);
+		if (ret == XAIE_PART_STATUS_INUSE) {
+			class_dev_iter_exit(&iter);
+			return false;
+		} else if (ret == XAIE_PART_STATUS_IDLE) {
+			class_dev_iter_exit(&iter);
+			return true;
+		}
+
+		continue;
+	}
+	class_dev_iter_exit(&iter);
+
+	return false;
+}
+EXPORT_SYMBOL_GPL(aie_partition_is_available);
+
+/**
+ * aie_partition_request() - Request an AI engine partition
+ * @req: AI engine partition requesting arguments
+ * @return: pointer to the AI engine partition device, error value for failure.
+ *
+ * This function looks up the AI engine class devices to find the AI engine
+ * partition whose partition ID matches the given partition ID in @req. If
+ * the partition can be found, it will try to request it. It will get a file
+ * for the requested AI engine partition. User can only use the AI engine
+ * partition after it is successfully requested.
+ */
+struct device *aie_partition_request(struct aie_partition_req *req)
+{
+	struct aie_partition *apart = NULL;
+	struct device *dev;
+	struct class_dev_iter iter;
+	int ret;
+
+	if (!req)
+		return ERR_PTR(-EINVAL);
+
+	class_dev_iter_init(&iter, aie_class, NULL, NULL);
+	while ((dev = class_dev_iter_next(&iter))) {
+		struct aie_aperture *aperture;
+		int ret;
+
+		if (strncmp(dev_name(dev), "aieaperture",
+			    strlen("aieaperture")))
+			continue;
+
+		aperture = dev_get_drvdata(dev);
+		ret = aie_aperture_check_part_avail(aperture, req);
+		if (ret == XAIE_PART_STATUS_INVALID) {
+			continue;
+		} else if (ret == XAIE_PART_STATUS_INUSE) {
+			class_dev_iter_exit(&iter);
+			dev_err(&aperture->dev,
+				"failed to request partition %u: in use.\n",
+				req->partition_id);
+			return ERR_PTR(-EBUSY);
+		}
+
+		class_dev_iter_exit(&iter);
+
+		apart = aie_aperture_request_part_from_id(aperture,
+							  req->partition_id);
+		if (IS_ERR(apart))
+			return ERR_PTR(PTR_ERR(apart));
+		break;
+	}
+
+	if (!apart) {
+		pr_err("failed to request partition %u: invalid partition.\n",
+		       req->partition_id);
+		return ERR_PTR(-EINVAL);
+	}
+
+	ret = aie_partition_get(apart, req);
+	if (ret) {
+		if (mutex_lock_interruptible(&apart->aperture->mlock))
+			return ERR_PTR(ret);
+
+		list_del(&apart->node);
+		aie_part_remove(apart);
+		mutex_unlock(&apart->aperture->mlock);
+	}
+
+	return &apart->dev;
+}
+EXPORT_SYMBOL_GPL(aie_partition_request);
+
+/**
+ * aie_partition_get_fd() - get AI engine partition file descriptor
+ * @dev: AI engine partition device pointer
+ * @return: file descriptor for the AI engine partition for success, and
+ *	    negative value for failure.
+ *
+ * This function allocate a file descriptor for the AI engine requested
+ * partition, and increase the reference count to the AI engine partition file.
+ */
+int aie_partition_get_fd(struct device *dev)
+{
+	struct aie_partition *apart;
+	int ret;
+
+	if (!dev)
+		return -EINVAL;
+
+	apart = dev_to_aiepart(dev);
+
+	ret = aie_partition_fd(apart);
+	if (ret < 0)
+		return ret;
+
+	get_file(apart->filep);
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aie_partition_get_fd);
+
+/**
+ * aie_partition_release() - Recrease refcount of the AI engine partition
+ * @dev: AI engine partition device
+ */
+void aie_partition_release(struct device *dev)
+{
+	struct aie_partition *apart;
+
+	if (WARN_ON(!dev))
+		return;
+
+	apart = dev_to_aiepart(dev);
+	fput(apart->filep);
+}
+EXPORT_SYMBOL_GPL(aie_partition_release);
+
+/**
+ * aie_partition_reset() - Reset AI engine partition
+ * @dev: AI engine partition device
+ * @return: 0 for success, negative value for failure
+ */
+int aie_partition_reset(struct device *dev)
+{
+	struct aie_partition *apart;
+
+	if (WARN_ON(!dev))
+		return -EINVAL;
+
+	apart = dev_to_aiepart(dev);
+	return aie_part_reset(apart);
+}
+EXPORT_SYMBOL_GPL(aie_partition_reset);
+
+/**
+ * aie_partition_post_reinit() - Indicate AI engine partition driver the
+ *				 partition has been re-initialized.
+ * @dev: AI engine partition device
+ * @return: 0 for success, negative value for failure
+ *
+ * This function is called after the AI engine partition is reconfigured with
+ * PDI outside the AI engine driver.
+ */
+int aie_partition_post_reinit(struct device *dev)
+{
+	struct aie_partition *apart;
+
+	if (WARN_ON(!dev))
+		return -EINVAL;
+
+	apart = dev_to_aiepart(dev);
+	return aie_part_post_reinit(apart);
+}
+EXPORT_SYMBOL_GPL(aie_partition_post_reinit);
+
+static int __init xilinx_ai_engine_init(void)
+{
+	int ret;
+
+	ret = alloc_chrdev_region(&aie_major, 0, AIE_DEV_MAX, "aie");
+	if (ret < 0) {
+		pr_err("aie: failed to allocate aie region\n");
+		return ret;
+	}
+
+	aie_class = class_create(THIS_MODULE, "aie");
+	if (IS_ERR(aie_class)) {
+		pr_err("failed to create aie class\n");
+		unregister_chrdev_region(aie_major, AIE_DEV_MAX);
+		return PTR_ERR(aie_class);
+	}
+
+	ret = aie_overlay_register_notifier();
+	if (ret) {
+		pr_err("aie: failed to register device tree overlay notifier.\n");
+		return ret;
+	}
+
+	platform_driver_register(&xilinx_ai_engine_driver);
+
+	return 0;
+}
+postcore_initcall(xilinx_ai_engine_init);
+
+static void __exit xilinx_ai_engine_exit(void)
+{
+	aie_overlay_unregister_notifier();
+	platform_driver_unregister(&xilinx_ai_engine_driver);
+	class_destroy(aie_class);
+	unregister_chrdev_region(aie_major, AIE_DEV_MAX);
+}
+module_exit(xilinx_ai_engine_exit);
+
+MODULE_AUTHOR("Xilinx, Inc.");
+MODULE_LICENSE("GPL v2");
+MODULE_IMPORT_NS(DMA_BUF);
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-dma.c b/drivers/misc/xilinx-ai-engine/ai-engine-dma.c
new file mode 100644
index 000000000..34254672b
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-dma.c
@@ -0,0 +1,702 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine driver DMA implementation
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include "ai-engine-internal.h"
+#include <linux/dma-buf.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/refcount.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/uaccess.h>
+
+/**
+ * struct aie_dmabuf - AI engine dmabuf information
+ * @attach: dmabuf attachment pointer
+ * @sgt: scatter/gather table
+ * @refs: refcount of the attached aie_dmabuf
+ * @node: list node
+ */
+struct aie_dmabuf {
+	struct dma_buf_attachment *attach;
+	struct sg_table *sgt;
+	refcount_t refs;
+	struct list_head node;
+};
+
+/**
+ * aie_part_find_dmabuf() - find a attached dmabuf
+ * @apart: AI engine partition
+ * @dmabuf: pointer to dmabuf
+ * @return: pointer to AI engine dmabuf struct of the found dmabuf, if dmabuf
+ *	    is not found, returns NULL.
+ *
+ * This function scans all the attached dmabufs to see the input dmabuf is
+ * in the list. if it is attached, return the corresponding struct aie_dmabuf
+ * pointer.
+ */
+static struct aie_dmabuf *
+aie_part_find_dmabuf(struct aie_partition *apart, struct dma_buf *dmabuf)
+{
+	struct aie_dmabuf *adbuf;
+
+	list_for_each_entry(adbuf, &apart->dbufs, node) {
+		if (dmabuf == adbuf->attach->dmabuf)
+			return adbuf;
+	}
+
+	return NULL;
+}
+
+/**
+ * aie_part_find_dmabuf_from_file() - find a attached dmabuf from file
+ * @apart: AI engine partition
+ * @file: file which belongs to a dmabuf
+ * @return: pointer to AI engine dmabuf struct of the found dmabuf, if dmabuf
+ *	    is not found, returns NULL.
+ *
+ * This function scans all the attached dmabufs of the AI engine partition,
+ * it checks the file with the attached dmabufs, if it founds a match, it
+ * returns the aie_dmabuf pointer.
+ */
+static struct aie_dmabuf *
+aie_part_find_dmabuf_from_file(struct aie_partition *apart,
+			       const struct file *file)
+{
+	struct aie_dmabuf *adbuf;
+
+	list_for_each_entry(adbuf, &apart->dbufs, node) {
+		if (file == adbuf->attach->dmabuf->file)
+			return adbuf;
+	}
+
+	return NULL;
+}
+
+/**
+ * aie_part_get_dmabuf_da() -  get DMA address from the va
+ * @apart: AI engine partition
+ * @va: virtual address
+ * @len: memory length
+ * @return: dma address of the specified va, or 0 if va is not valid
+ *
+ * This function returns DMA address if the has been mapped to a dmabuf which
+ * has been attached to the AI engine partition.
+ */
+static dma_addr_t aie_part_get_dmabuf_da(struct aie_partition *apart,
+					 void *va, size_t len)
+{
+	struct vm_area_struct *vma;
+	struct aie_dmabuf *adbuf;
+	unsigned long va_start, va_off;
+
+	va_start = (unsigned long)((uintptr_t)va);
+	if (!current->mm) {
+		dev_err(&apart->dev,
+			"failed to get dma address from va, no process mm.\n");
+		return 0;
+	}
+
+	vma = find_vma(current->mm, va_start);
+	if (!vma) {
+		dev_err(&apart->dev, "failed to find vma for %p, 0x%zx.\n",
+			va, len);
+		return 0;
+	}
+
+	adbuf = aie_part_find_dmabuf_from_file(apart, vma->vm_file);
+	if (!adbuf) {
+		dev_err(&apart->dev,
+			"failed to get dma address for %p, no dma buf is found.\n",
+			va);
+		return 0;
+	}
+
+	va_off = va_start - vma->vm_start;
+	/*
+	 * As we only support continuous DMA memory which is guaranteed from
+	 * dmabuf attachment, we will compared with the size of the dmabuf only
+	 */
+	if (va_off + len >= adbuf->attach->dmabuf->size) {
+		dev_err(&apart->dev,
+			"failed to get dma address for %p, 0x%zx.\n", va, len);
+		return 0;
+	}
+
+	return sg_dma_address(adbuf->sgt->sgl) + va_off;
+}
+
+/**
+ * aie_part_get_dmabuf_da_from_off() - get DMA address from offset to a dmabuf
+ * @apart: AI engine partition
+ * @dmabuf_fd: dmabuf file descriptor
+ * @off: offset to the start of a dmabuf
+ * @len: memory length
+ * @return: dma address, or 0 if @off or @len is invalid, or if @dmabuf_fd is
+ *	    not attached.
+ *
+ * This function returns DMA address if has been mapped to a dmabuf which has
+ * been attached to the AI engine partition.
+ */
+static dma_addr_t
+aie_part_get_dmabuf_da_from_off(struct aie_partition *apart, int dmabuf_fd,
+				u64 off, size_t len)
+{
+	struct dma_buf *dbuf = dma_buf_get(dmabuf_fd);
+	struct aie_dmabuf *adbuf;
+
+	if (IS_ERR(dbuf)) {
+		dev_err(&apart->dev,
+			"failed to get dma address, not able to get dmabuf from %d.\n",
+			dmabuf_fd);
+		return 0;
+	}
+
+	adbuf = aie_part_find_dmabuf(apart, dbuf);
+	dma_buf_put(dbuf);
+	if (!adbuf) {
+		dev_err(&apart->dev,
+			"failed to get dma address, dmabuf %d not attached.\n",
+			dmabuf_fd);
+		return 0;
+	}
+
+	if (off >= dbuf->size || off + len >= dbuf->size) {
+		dev_err(&apart->dev,
+			"failed to get dma address from buf %d, off=0x%llx, len=0x%zx.\n",
+			dmabuf_fd, off, len);
+		return 0;
+	}
+
+	return sg_dma_address(adbuf->sgt->sgl) + off;
+}
+
+/**
+ * aie_part_set_shimdma_bd() - Set the buffer descriptor to AI engine partition
+ *			       hardware
+ * @apart: AI engine partition
+ * @loc: AI engine tile location relative in partition
+ * @bd_id: buffer descriptor ID
+ * @bd: pointer buffer descriptor content
+ * @return: 0 for success, negative value for failure
+ *
+ * This function sets the specified buffer descriptor content to the
+ * specified buffer descriptor in the specified AI engine SHIM NOC tile.
+ */
+static int aie_part_set_shimdma_bd(struct aie_partition *apart,
+				   struct aie_location loc, u32 bd_id, u32 *bd)
+{
+	struct aie_aperture *aperture = apart->aperture;
+	const struct aie_dma_attr *shim_dma = apart->adev->shim_dma;
+	struct aie_location loc_adjust;
+	u32 i, regoff, intile_regoff;
+
+	intile_regoff = shim_dma->bd_regoff + shim_dma->bd_len * bd_id;
+	loc_adjust.col = loc.col + apart->range.start.col;
+	loc_adjust.row = loc.row + apart->range.start.row;
+	regoff = aie_aperture_cal_regoff(aperture, loc_adjust, intile_regoff);
+
+	for (i = 0; i < shim_dma->bd_len / (sizeof(*bd));
+	     i++, regoff += sizeof(*bd))
+		iowrite32(bd[i], aperture->base + regoff);
+	return 0;
+}
+
+/**
+ * aie_part_validate_bdloc() - Validate SHIM DMA buffer descriptor location
+ * @apart: AI engine partition
+ * @loc: tile location
+ * @bd_id: buffer descriptor id
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function validate the SHIM DMA buffer descriptor base address.
+ */
+static int aie_part_validate_bdloc(struct aie_partition *apart,
+				   struct aie_location loc, u32 bd_id)
+{
+	const struct aie_dma_attr *shim_dma = apart->adev->shim_dma;
+	struct aie_location loc_adjust;
+	u32 ttype;
+
+	loc_adjust.col = loc.col + apart->range.start.col;
+	loc_adjust.row = loc.row + apart->range.start.row;
+
+	if (aie_validate_location(apart, loc) < 0) {
+		dev_err(&apart->dev,
+			"invalid loc (%u,%u) in (%u,%u).\n",
+			loc.col, loc.row,
+			apart->range.size.col, apart->range.size.row);
+		return -EINVAL;
+	}
+
+	ttype = apart->adev->ops->get_tile_type(apart->adev, &loc_adjust);
+	if (ttype != AIE_TILE_TYPE_SHIMNOC) {
+		dev_err(&apart->dev,
+			"failed to set bd, (%u,%u) is not SHIM NOC\n",
+			loc.col, loc.row);
+		return -EINVAL;
+	}
+
+	if (bd_id >= shim_dma->num_bds) {
+		dev_err(&apart->dev,
+			"invalid SHIM DMA bd id: %u.\n", bd_id);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_attach_dmabuf() - Attach dmabuf to an AI engine
+ * @apart: AI engine partition
+ * @dbuf: pointer to the DMA buffer to attach
+ * @return: pointer to AI engine dmabuf structure for success, or error value
+ *	    for failure
+ *
+ * This function attaches a dmabuf to the specified AI engine partition.
+ */
+static struct aie_dmabuf *aie_part_attach_dmabuf(struct aie_partition *apart,
+						 struct dma_buf *dbuf)
+{
+	struct aie_dmabuf *adbuf;
+	struct dma_buf_attachment *attach;
+	struct sg_table *sgt;
+
+	attach = dma_buf_attach(dbuf, &apart->dev);
+	if (IS_ERR(attach)) {
+		dev_err(&apart->dev, "failed to attach dmabuf\n");
+		return ERR_CAST(attach);
+	}
+
+	sgt = dma_buf_map_attachment(attach, DMA_BIDIRECTIONAL);
+	if (IS_ERR(sgt)) {
+		dev_err(&apart->dev, "failed to map dmabuf attachment\n");
+		dma_buf_detach(dbuf, attach);
+		return ERR_CAST(sgt);
+	}
+
+	if (sgt->nents != 1) {
+		dma_addr_t next_sg_addr = sg_dma_address(sgt->sgl);
+		struct scatterlist *s;
+		unsigned int i;
+
+		for_each_sg(sgt->sgl, s, sgt->nents, i) {
+			if (sg_dma_address(s) != next_sg_addr) {
+				dev_err(&apart->dev,
+					"dmabuf not contiguous\n");
+				dma_buf_unmap_attachment(attach, sgt,
+							 attach->dir);
+				dma_buf_detach(dbuf, attach);
+				return ERR_PTR(-EINVAL);
+			}
+
+			next_sg_addr = sg_dma_address(s) + sg_dma_len(s);
+		}
+	}
+
+	adbuf = kmem_cache_alloc(apart->dbufs_cache, GFP_KERNEL);
+	if (!adbuf) {
+		dma_buf_unmap_attachment(attach, sgt, attach->dir);
+		dma_buf_detach(dbuf, attach);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	adbuf->attach = attach;
+	/*
+	 * dmabuf attachment doesn't always include the sgt, store it in
+	 * AI engine dma buf structure.
+	 */
+	adbuf->sgt = sgt;
+
+	refcount_set(&adbuf->refs, 1);
+
+	list_add(&adbuf->node, &apart->dbufs);
+	return adbuf;
+}
+
+/**
+ * aie_part_dmabuf_attach_get() - Get reference to an dmabuf attachment
+ * @adbuf: AI engine partition attached dmabuf
+ *
+ * This call will increase the reference count by 1
+ */
+static void aie_part_dmabuf_attach_get(struct aie_dmabuf *adbuf)
+{
+	refcount_inc(&adbuf->refs);
+}
+
+/**
+ * aie_part_dmabuf_attach_put() - Put reference to an dmabuf attachment
+ * @adbuf: AI engine partition attached dmabuf
+ *
+ * This call will decrease the reference count by 1. If the refcount reaches
+ * 0, it will detach the dmabuf.
+ */
+static void aie_part_dmabuf_attach_put(struct aie_dmabuf *adbuf)
+{
+	struct dma_buf *dbuf;
+	struct aie_partition *apart;
+
+	if (!refcount_dec_and_test(&adbuf->refs))
+		return;
+
+	apart = dev_to_aiepart(adbuf->attach->dev);
+	dbuf = adbuf->attach->dmabuf;
+	dma_buf_unmap_attachment(adbuf->attach, adbuf->sgt, adbuf->attach->dir);
+	dma_buf_detach(dbuf, adbuf->attach);
+	dma_buf_put(dbuf);
+	list_del(&adbuf->node);
+	kmem_cache_free(apart->dbufs_cache, adbuf);
+}
+
+/**
+ * aie_part_release_dmabufs() - detach all the attached dmabufs from partition
+ * @apart: AI engine partition
+ */
+void aie_part_release_dmabufs(struct aie_partition *apart)
+{
+	struct aie_dmabuf *adbuf, *tmpadbuf;
+
+	list_for_each_entry_safe(adbuf, tmpadbuf, &apart->dbufs, node) {
+		struct dma_buf *dbuf = adbuf->attach->dmabuf;
+
+		dma_buf_unmap_attachment(adbuf->attach, adbuf->sgt,
+					 adbuf->attach->dir);
+		dma_buf_detach(dbuf, adbuf->attach);
+		dma_buf_put(dbuf);
+		list_del(&adbuf->node);
+		kmem_cache_free(apart->dbufs_cache, adbuf);
+	}
+}
+
+/**
+ * aie_part_attach_dmabuf_req() - Handle attaching dmabuf to an AI engine
+ *				  partition request
+ * @apart: AI engine partition
+ * @user_args: user AI engine dmabuf argument
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function attaches a dmabuf to the specified AI engine partition and map
+ * the attachment. It checks if the dmabuf is already attached, if it is not
+ * attached, attach it. It returns the number of entries of the attachment to
+ * the AI engine dmabuf user argument. If user wants to know the sg list, it
+ * can use AI engine get sg ioctl.
+ */
+long aie_part_attach_dmabuf_req(struct aie_partition *apart,
+				void __user *user_args)
+{
+	struct aie_dmabuf *adbuf;
+	struct dma_buf *dbuf;
+	long ret;
+	int dmabuf_fd = (int)(uintptr_t)user_args;
+
+	dbuf = dma_buf_get(dmabuf_fd);
+	if (IS_ERR(dbuf)) {
+		dev_err(&apart->dev, "failed to get dmabuf from %d.\n",
+			dmabuf_fd);
+		return PTR_ERR(dbuf);
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dma_buf_put(dbuf);
+		return ret;
+	}
+
+	adbuf = aie_part_find_dmabuf(apart, dbuf);
+	if (!adbuf)
+		adbuf = aie_part_attach_dmabuf(apart, dbuf);
+	else
+		aie_part_dmabuf_attach_get(adbuf);
+
+	mutex_unlock(&apart->mlock);
+
+	if (IS_ERR(adbuf)) {
+		dev_err(&apart->dev, "failed to attach dmabuf\n");
+		dma_buf_put(dbuf);
+		return PTR_ERR(adbuf);
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_detach_dmabuf_req() - Handle detaching dmabuf from an AI engine
+ *				  partition request
+ * @apart: AI engine partition
+ * @user_args: user AI engine dmabuf argument
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function unmaps and detaches a dmabuf from the specified AI engine
+ * partition.
+ */
+long aie_part_detach_dmabuf_req(struct aie_partition *apart,
+				void __user *user_args)
+{
+	int dmabuf_fd;
+	struct dma_buf *dbuf;
+	struct aie_dmabuf *adbuf;
+	int ret;
+
+	dmabuf_fd = (int)(uintptr_t)user_args;
+
+	dbuf = dma_buf_get(dmabuf_fd);
+	if (IS_ERR(dbuf)) {
+		dev_err(&apart->dev, "failed to get dmabuf %d.\n", dmabuf_fd);
+		return PTR_ERR(dbuf);
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dma_buf_put(dbuf);
+		return ret;
+	}
+
+	adbuf = aie_part_find_dmabuf(apart, dbuf);
+	dma_buf_put(dbuf);
+	if (!adbuf) {
+		dev_err(&apart->dev, "failed to find dmabuf %d.\n", dmabuf_fd);
+		mutex_unlock(&apart->mlock);
+		return -EINVAL;
+	}
+
+	aie_part_dmabuf_attach_put(adbuf);
+
+	mutex_unlock(&apart->mlock);
+
+	return 0;
+}
+
+/**
+ * aie_part_set_bd() - Set AI engine SHIM DMA buffer descriptor
+ * @apart: AI engine partition
+ * @args: user AI engine dmabuf argument
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function set the user specified buffer descriptor into the SHIM DMA
+ * buffer descriptor.
+ */
+long aie_part_set_bd(struct aie_partition *apart, struct aie_dma_bd_args *args)
+{
+	struct aie_device *adev = apart->adev;
+	const struct aie_dma_attr *shim_dma = adev->shim_dma;
+	u32 *bd, *tmpbd, buf_len, laddr, haddr, regval;
+	dma_addr_t addr;
+	int ret;
+
+	ret = aie_part_validate_bdloc(apart, args->loc, args->bd_id);
+	if (ret) {
+		dev_err(&apart->dev, "invalid SHIM DMA BD reg address.\n");
+		return -EINVAL;
+	}
+
+	bd = memdup_user((void __user *)args->bd, shim_dma->bd_len);
+	if (IS_ERR(bd))
+		return PTR_ERR(bd);
+
+	regval = bd[shim_dma->buflen.regoff / sizeof(u32)];
+	buf_len = aie_get_reg_field(&shim_dma->buflen, regval);
+	if (!buf_len) {
+		dev_err(&apart->dev, "no buf length from shim dma bd.\n");
+		kfree(bd);
+		return -EINVAL;
+	}
+
+	/* Get device address from virtual address */
+	addr = aie_part_get_dmabuf_da(apart, (void *)(uintptr_t)args->data_va,
+				      buf_len);
+	if (!addr) {
+		dev_err(&apart->dev, "invalid buffer 0x%llx, 0x%x.\n",
+			args->data_va, buf_len);
+		kfree(bd);
+		return -EINVAL;
+	}
+
+	/* Set low 32bit address */
+	laddr = lower_32_bits(addr);
+	tmpbd = (u32 *)((char *)bd + shim_dma->laddr.regoff);
+	*tmpbd &= ~shim_dma->laddr.mask;
+	*tmpbd |= aie_get_field_val(&shim_dma->laddr, laddr);
+
+	/* Set high 32bit address */
+	haddr = upper_32_bits(addr);
+	tmpbd = (u32 *)((char *)bd + shim_dma->haddr.regoff);
+	*tmpbd &= ~shim_dma->haddr.mask;
+	*tmpbd |= aie_get_field_val(&shim_dma->haddr, haddr);
+
+	ret = aie_part_set_shimdma_bd(apart, args->loc, args->bd_id, bd);
+	if (ret)
+		dev_err(&apart->dev, "failed to set to shim dma bd.\n");
+
+	kfree(bd);
+	return ret;
+}
+
+/**
+ * aie_part_set_bd_from_user() - Set AI engine SHIM DMA buffer descriptor
+ * @apart: AI engine partition
+ * @user_args: user AI engine dmabuf argument
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function set the user specified buffer descriptor into the SHIM DMA
+ * buffer descriptor.
+ */
+long aie_part_set_bd_from_user(struct aie_partition *apart, void __user *user_args)
+{
+	struct aie_dma_bd_args args;
+	int ret;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	ret = aie_part_set_bd(apart, &args);
+
+	mutex_unlock(&apart->mlock);
+	return ret;
+}
+
+/**
+ * aie_part_set_dmabuf_bd() - Set AI engine SHIM DMA dmabuf buffer descriptor
+ * @apart: AI engine partition
+ * @args: user AI engine dmabuf argument
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function set the user specified buffer descriptor into the SHIM DMA
+ * buffer descriptor. The buffer descriptor contained in the @user_args has the
+ * offset to the start of the buffer descriptor.
+ */
+long aie_part_set_dmabuf_bd(struct aie_partition *apart,
+			    struct aie_dmabuf_bd_args *args)
+{
+	struct aie_device *adev = apart->adev;
+	const struct aie_dma_attr *shim_dma = adev->shim_dma;
+	u32 *bd, *tmpbd, len, laddr, haddr, regval;
+	u64 off;
+	dma_addr_t addr;
+	int ret;
+
+	ret = aie_part_validate_bdloc(apart, args->loc, args->bd_id);
+	if (ret) {
+		dev_err(&apart->dev, "invalid SHIM DMA BD reg address.\n");
+		return -EINVAL;
+	}
+
+	bd = memdup_user((void __user *)args->bd, shim_dma->bd_len);
+	if (IS_ERR(bd))
+		return PTR_ERR(bd);
+
+	regval = bd[shim_dma->buflen.regoff / sizeof(u32)];
+	len = aie_get_reg_field(&shim_dma->buflen, regval);
+	if (!len) {
+		dev_err(&apart->dev, "no buf length from shim dma bd.\n");
+		kfree(bd);
+		return -EINVAL;
+	}
+
+	/* Get low 32bit address offset */
+	tmpbd = (u32 *)((char *)bd + shim_dma->laddr.regoff);
+	laddr = *tmpbd & shim_dma->laddr.mask;
+	/* Get high 32bit address offset */
+	tmpbd = (u32 *)((char *)bd + shim_dma->haddr.regoff);
+	haddr = *tmpbd & shim_dma->haddr.mask;
+	off = laddr | ((u64)haddr << 32);
+
+	/* Get device address from offset */
+	addr = aie_part_get_dmabuf_da_from_off(apart, args->buf_fd, off, len);
+	if (!addr) {
+		dev_err(&apart->dev, "invalid buffer 0x%llx, 0x%x.\n",
+			off, len);
+		kfree(bd);
+		return -EINVAL;
+	}
+
+	/* Set low 32bit address */
+	laddr = lower_32_bits(addr);
+	tmpbd = (u32 *)((char *)bd + shim_dma->laddr.regoff);
+	*tmpbd &= ~shim_dma->laddr.mask;
+	*tmpbd |= aie_get_field_val(&shim_dma->laddr, laddr);
+
+	/* Set high 32bit address */
+	haddr = upper_32_bits(addr);
+	tmpbd = (u32 *)((char *)bd + shim_dma->haddr.regoff);
+	*tmpbd &= ~shim_dma->haddr.mask;
+	*tmpbd |= aie_get_field_val(&shim_dma->haddr, haddr);
+
+	ret = aie_part_set_shimdma_bd(apart, args->loc, args->bd_id, bd);
+	if (ret)
+		dev_err(&apart->dev, "failed to set to shim dma bd.\n");
+
+	kfree(bd);
+	return ret;
+}
+
+/**
+ * aie_part_set_dmabuf_bd_from_user() - Set AI engine SHIM DMA dmabuf buffer descriptor
+ * @apart: AI engine partition
+ * @user_args: user AI engine dmabuf argument
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function set the user specified buffer descriptor into the SHIM DMA
+ * buffer descriptor. The buffer descriptor contained in the @user_args has the
+ * offset to the start of the buffer descriptor.
+ */
+long aie_part_set_dmabuf_bd_from_user(struct aie_partition *apart,
+			    void __user *user_args)
+{
+	struct aie_dmabuf_bd_args args;
+	int ret;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	ret = aie_part_set_dmabuf_bd(apart, &args);
+
+	mutex_unlock(&apart->mlock);
+	return ret;
+}
+
+/**
+ * aie_part_prealloc_dbufs_cache() - Preallocate dmabuf descriptors memory
+ *
+ * @apart: AI engine partition
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function preallocate memories to save dmabuf descriptors. When dmabuf
+ * is attached to the partition at runtime, it can get the descriptor memory
+ * from this preallocated memory pool.
+ */
+int aie_part_prealloc_dbufs_cache(struct aie_partition *apart)
+{
+	struct kmem_cache *dbufs_cache;
+	char name[64];
+
+	sprintf(name, "%s_dbufs", dev_name(&apart->dev));
+	dbufs_cache = kmem_cache_create(name, sizeof(struct aie_dmabuf),
+					0, 0, NULL);
+	if (!dbufs_cache)
+		return -ENOMEM;
+
+	apart->dbufs_cache = dbufs_cache;
+
+	return 0;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-internal.h b/drivers/misc/xilinx-ai-engine/ai-engine-internal.h
new file mode 100644
index 000000000..f5a4bf2ee
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-internal.h
@@ -0,0 +1,1511 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Xilinx AI Engine driver internal header
+ *
+ * Copyright (C) 2020 - 2021 Xilinx, Inc.
+ */
+
+#ifndef AIE_INTERNAL_H
+#define AIE_INTERNAL_H
+
+#include <linux/bitfield.h>
+#include <linux/bitmap.h>
+#include <linux/bits.h>
+#include <linux/cdev.h>
+#include <linux/clk.h>
+#include <linux/device.h>
+#include <linux/dma-buf.h>
+#include <linux/file.h>
+#include <linux/io.h>
+#include <linux/interrupt.h>
+#include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/slab.h>
+#include <uapi/linux/xlnx-ai-engine.h>
+
+#define AIE_DEVICE_GEN_AIE	1U
+#define AIE_DEVICE_GEN_AIEML	2U
+
+/*
+ * Macros for AI engine tile type bitmasks
+ */
+enum aie_tile_type {
+	AIE_TILE_TYPE_TILE,
+	AIE_TILE_TYPE_SHIMPL,
+	AIE_TILE_TYPE_SHIMNOC,
+	AIE_TILE_TYPE_MEMORY,
+	AIE_TILE_TYPE_MAX
+};
+
+#define AIE_TILE_TYPE_MASK_TILE		BIT(AIE_TILE_TYPE_TILE)
+#define AIE_TILE_TYPE_MASK_SHIMPL	BIT(AIE_TILE_TYPE_SHIMPL)
+/* SHIM NOC tile includes SHIM PL and SHIM NOC modules */
+#define AIE_TILE_TYPE_MASK_SHIMNOC	BIT(AIE_TILE_TYPE_SHIMNOC)
+#define AIE_TILE_TYPE_MASK_MEMORY	BIT(AIE_TILE_TYPE_MEMORY)
+
+#define AIE_ISOLATE_EAST_MASK		BIT(3)
+#define AIE_ISOLATE_NORTH_MASK		BIT(2)
+#define AIE_ISOLATE_WEST_MASK		BIT(1)
+#define AIE_ISOLATE_SOUTH_MASK		BIT(0)
+#define AIE_ISOLATE_ALL_MASK		GENMASK(3, 0)
+
+/*
+ * Macros for attribute property of AI engine registers accessed by kernel
+ * 0 - 7 bits: tile type bits
+ * 8 - 15 bits: permission bits. If it is 1, it allows write from userspace
+ */
+#define AIE_REGS_ATTR_TILE_TYPE_SHIFT	0U
+#define AIE_REGS_ATTR_PERM_SHIFT	8U
+#define AIE_REGS_ATTR_TILE_TYPE_MASK	GENMASK(AIE_REGS_ATTR_PERM_SHIFT - 1, \
+						AIE_REGS_ATTR_TILE_TYPE_SHIFT)
+#define AIE_REGS_ATTR_PERM_MASK		GENMASK(15, \
+						AIE_REGS_ATTR_PERM_SHIFT)
+
+#define KBYTES(n)	((n) * 1024)
+
+#define AIE_NPI_ERROR_ID		BIT(1)
+
+/* Macros relevant to interrupts */
+#define AIE_INTR_L2_CTRL_MASK_WIDTH	32U
+
+/* Max number of modules per tile */
+#define AIE_MAX_MODS_PER_TILE		2U
+
+/* AIE core registers step size */
+#define AIE_CORE_REGS_STEP		0x10
+
+/* Number of event status registers */
+#define AIE_NUM_EVENT_STS_CORETILE	4U
+#define AIE_NUM_EVENT_STS_MEMTILE	6U
+#define AIE_NUM_EVENT_STS_SHIMTILE	4U
+
+/* Number of DMA channels */
+#define AIE_MAX_MM2S_CH		6U
+#define AIE_MAX_S2MM_CH		6U
+
+/* Max size of DMA buffer descriptors */
+#define AIE_MAX_BD_SIZE		8U
+
+/* Program memory offset and size index */
+#define AIE_PM_MEM_OFFSET_IDX	1U
+
+/*
+ * Macros of AI engine module type index of a tile type
+ * e.g.
+ * id 0 of CORE tile is memory module, and 1 is core module
+ * id 0 of MEM tile is memory module
+ * id 0 of SHIM tile is pl module, and 1 is noc module
+ */
+#define AIE_TILE_MOD_START		AIE_MEM_MOD
+#define AIE_MOD_ID(T, M)		((M) - AIE_##T ## _MOD_START)
+#define AIE_TILE_MEM_MOD_ID		AIE_MOD_ID(TILE, AIE_MEM_MOD)
+#define AIE_TILE_CORE_MOD_ID		AIE_MOD_ID(TILE, AIE_CORE_MOD)
+#define AIE_MEMORY_MOD_START		AIE_MEM_MOD
+#define AIE_MEMORY_MEM_MOD_ID		AIE_MOD_ID(MEMORY, AIE_MEM_MOD)
+#define AIE_SHIMPL_MOD_START		AIE_PL_MOD
+#define AIE_SHIMNOC_MOD_START		AIE_PL_MOD
+#define AIE_SHIM_PL_MOD_ID		AIE_MOD_ID(SHIMPL, AIE_PL_MOD)
+#define AIE_SHIM_NOC_MOD_ID		AIE_MOD_ID(SHIMNOC, AIE_NOC_MOD)
+
+/* String delimiter to format sysfs data */
+#define DELIMITER_LEVEL0 "|"
+#define DELIMITER_LEVEL1 ", "
+#define DELIMITER_LEVEL2 "; "
+
+/* Helper macros to dynamically create sysfs device attribute */
+#define AIE_APERTURE_ATTR_RO(_name) {				\
+	.name		= __stringify(_name),			\
+	.mode		= 0444,					\
+	.show		= aie_aperture_show_##_name,		\
+}
+
+#define AIE_PART_DEV_ATTR_RO(_name) {				\
+	.name		= __stringify(_name),			\
+	.mode		= 0444,					\
+	.show		= aie_part_show_##_name,		\
+}
+
+#define AIE_PART_DEV_ATTR_WO(_name) {				\
+	.name		= __stringify(_name),			\
+	.mode		= 0200,					\
+	.store		= aie_part_store_##_name,		\
+}
+
+#define AIE_PART_DEV_ATTR_RW(_name) {				\
+	.name		= __stringify(_name),			\
+	.mode		= 0644,					\
+	.show		= aie_part_show_##_name,		\
+	.store		= aie_part_store_##_name,		\
+}
+
+#define AIE_TILE_DEV_ATTR_RO(_name, _ttype) {			\
+	.name		= __stringify(_name),			\
+	.mode		= 0444,					\
+	.tile_type	= _ttype,				\
+	.show		= aie_tile_show_##_name,		\
+}
+
+#define AIE_TILE_DEV_ATTR_WO(_name, _ttype) {			\
+	.name		= __stringify(_name),			\
+	.mode		= 0200,					\
+	.tile_type	= _ttype,				\
+	.store		= aie_tile_store_##_name,		\
+}
+
+#define AIE_TILE_DEV_ATTR_RW(_name, _ttype) {			\
+	.name		= __stringify(_name),			\
+	.mode		= 0644,					\
+	.tile_type	= _ttype,				\
+	.show		= aie_tile_show_##_name,		\
+	.store		= aie_tile_store_##_name,		\
+}
+
+#define AIE_PART_BIN_ATTR_RO(_name, _size) {			\
+	.name		= __stringify(_name),			\
+	.mode		= 0444,					\
+	.size		= _size,				\
+	.read		= aie_sysfs_read_handler,		\
+	.read_callback	= aie_part_read_cb_##_name,		\
+}
+
+#define AIE_PART_BIN_ATTR_WO(_name, _size) {			\
+	.name		= __stringify(_name),			\
+	.mode		= 0200,					\
+	.size		= _size,				\
+	.write		= aie_part_write_handler,		\
+	.write_callback	= aie_part_write_cb_##_name,		\
+}
+
+#define AIE_PART_BIN_ATTR_RW(_name, _size) {			\
+	.name		= __stringify(_name),			\
+	.mode		= 0644					\
+	.size		= _size,				\
+	.read		= aie_sysfs_read_handler,		\
+	.write		= aie_part_write_handler,		\
+	.read_callback	= aie_part_read_cb_##_name,		\
+	.write_callback	= aie_part_write_cb_##_name,		\
+}
+
+#define AIE_TILE_BIN_ATTR_RO(_name, _size, _ttype) {		\
+	.name		= __stringify(_name),			\
+	.mode		= 0444,					\
+	.size		= _size,				\
+	.tile_type	= _ttype,				\
+	.read		= aie_sysfs_read_handler,		\
+	.read_callback	= aie_tile_read_cb_##_name,		\
+}
+
+#define AIE_TILE_BIN_ATTR_WO(_name, _size, _ttype) {		\
+	.name		= __stringify(_name),			\
+	.mode		= 0200,					\
+	.size		= _size,				\
+	.tile_type	= _ttype,				\
+	.write		= aie_tile_write_handler,		\
+	.write_callback	= aie_tile_write_cb_##_name,		\
+}
+
+#define AIE_TILE_BIN_ATTR_RW(_name, _size, _ttype) {		\
+	.name		= __stringify(_name),			\
+	.mode		= 0644,					\
+	.size		= _size,				\
+	.tile_type	= _ttype,				\
+	.read		= aie_sysfs_read_handler,		\
+	.write		= aie_tile_write_handler,		\
+	.read_callback	= aie_tile_read_cb_##_name,		\
+	.write_callback	= aie_tile_write_cb_##_name,		\
+}
+
+#define AIE_CORE_NUM_CYCLE		2U
+#define AIE_CORE_NUM_PERFCNT_PER_REG	2U
+#define AIE_CORE_PERFCNT_EVNT_BITS	8U
+#define AIE_CORE_PERFCNT_CTRL_IDX	4U
+
+/*
+ * enum aie_shim_switch_type - identifies different switches in shim tile.
+ */
+enum aie_shim_switch_type {
+	AIE_SHIM_SWITCH_A,
+	AIE_SHIM_SWITCH_B
+};
+
+/*
+ * enum for SSIT devices
+ */
+enum aie_device_type {
+	AIE_DEV_GENERIC_DEVICE,
+	AIE_DEV_GEN_S100 = 100,
+	AIE_DEV_GEN_S200 = 200
+};
+
+/*
+ * enum for kernel utilization cycle.
+ */
+enum aie_kernel_utilization {
+	AIE_CORE_ACTIVE_CYCLE,
+	AIE_CORE_TOTAL_CYCLE
+};
+
+/*
+ * enum for events for kernel utilization
+ */
+enum aie_events {
+	AIE_EVENT_CORE_ACTIVE,
+	AIE_EVENT_CORE_DISABLED,
+	AIE_EVENT_CORE_USER_EVNT_0,
+	AIE_EVENT_CORE_USER_EVNT_1,
+};
+
+/**
+ * struct aie_tile_regs - contiguous range of AI engine register
+ *			  within an AI engine tile
+ * @soff: start offset of the range
+ * @eoff: end offset of the range
+ * @attribute: registers attribute. It uses AIE_REGS_ATTR_* macros defined
+ *	       above.
+ */
+struct aie_tile_regs {
+	size_t soff;
+	size_t eoff;
+	u32 attribute;
+};
+
+/**
+ * struct aie_single_reg_field - AI engine single field register attribute
+ * @mask: field mask
+ * @regoff: register offset of the field
+ */
+struct aie_single_reg_field {
+	u32 mask;
+	u32 regoff;
+};
+
+struct aie_device;
+struct aie_partition;
+
+/**
+ * struct aie_part_mem - AI engine partition memory information structure
+ * @apart: AI engine partition
+ * @dbuf: dmabuf pointer associated with the memory
+ * @mem: memory information of a type of memory
+ * @size: size of the total memories in the partition
+ *
+ * This structure is to keep the information of a type of memory in a
+ * partition. The memory information will be stored in @mem property.
+ * The following information will be keep:
+ *  * memory start address offset within a tile
+ *  * memory size
+ *  * what tiles contain this type of memory
+ */
+struct aie_part_mem {
+	struct aie_partition *apart;
+	struct dma_buf *dbuf;
+	struct aie_mem mem;
+	size_t size;
+};
+
+/**
+ * struct aie_bd_addr_attr - AI engine buffer descriptor address attributes
+ * @addr: address field attributes
+ * @length: length field attributes
+ */
+struct aie_bd_addr_attr {
+	struct aie_single_reg_field addr;
+	struct aie_single_reg_field length;
+};
+
+/**
+ * struct aie_bd_lock_attr - AI engine buffer descriptor lock attributes
+ * @lock_acq_id: lock acquire id field attributes
+ * @lock_acq_val: lock acquire value field attributes
+ * @lock_acq_en: lock acquire enable field attributes
+ * @lock_acq_val_en: lock acquire value enable field attributes
+ * @lock_rel_id: lock release id field attributes
+ * @lock_rel_val: lock release value field attributes
+ * @lock_rel_en: lock release enable field attributes
+ * @lock_rel_val_en: lock release value enable field attributes
+ */
+struct aie_bd_lock_attr {
+	struct aie_single_reg_field lock_acq_id;
+	struct aie_single_reg_field lock_acq_val;
+	struct aie_single_reg_field lock_acq_en;
+	struct aie_single_reg_field lock_acq_val_en;
+	struct aie_single_reg_field lock_rel_id;
+	struct aie_single_reg_field lock_rel_val;
+	struct aie_single_reg_field lock_rel_en;
+	struct aie_single_reg_field lock_rel_val_en;
+};
+
+/**
+ * struct aie_bd_pkt_attr - AI engine buffer descriptor packet attributes
+ * @pkt_en: packet enable field attributes
+ * @pkt_type: packet type field attributes
+ * @pkt_id: packet id field attributes
+ */
+struct aie_bd_pkt_attr {
+	struct aie_single_reg_field pkt_en;
+	struct aie_single_reg_field pkt_type;
+	struct aie_single_reg_field pkt_id;
+};
+
+/**
+ * struct aie_bd_axi_attr - AI engine buffer descriptor AXI attributes
+ * @smid: smid field attributes
+ * @cache: AxCache field attributes
+ * @qos: Axi QoS field attributes
+ * @secure_en: Axi Secure access field attributes
+ * @burst_len: Axi bursth length field attributes
+ */
+struct aie_bd_axi_attr {
+	struct aie_single_reg_field smid;
+	struct aie_single_reg_field cache;
+	struct aie_single_reg_field qos;
+	struct aie_single_reg_field secure_en;
+	struct aie_single_reg_field burst_len;
+};
+
+/**
+ * struct aie_bd_aie_dim_attr - AI engine buffer descriptor dimension
+ *				attributes for aie
+ * @x_incr: x increment field attributes
+ * @x_wrap: x wrap field attributes
+ * @x_off: x offset field attributes
+ * @y_incr: y increment field attributes
+ * @y_wrap: y wrap field attributes
+ * @y_off: y offset field attributes
+ */
+struct aie_bd_aie_dim_attr {
+	struct aie_single_reg_field x_incr;
+	struct aie_single_reg_field x_wrap;
+	struct aie_single_reg_field x_off;
+	struct aie_single_reg_field y_incr;
+	struct aie_single_reg_field y_wrap;
+	struct aie_single_reg_field y_off;
+};
+
+/**
+ * struct aie_bd_multi_dim_attr - AI engine buffer descriptor dimension
+ *				  attributes
+ * @wrap: wrap field attributes
+ * @step_size: step size field attributes
+ */
+struct aie_bd_multi_dim_attr {
+	struct aie_single_reg_field wrap;
+	struct aie_single_reg_field step_size;
+};
+
+/**
+ * struct aie_bd_pad_attr - AI engine buffer descriptor padding attributes
+ * @before: before padding attributes
+ * @after: after padding attributes
+ */
+struct aie_bd_pad_attr {
+	struct aie_single_reg_field before;
+	struct aie_single_reg_field after;
+};
+
+/**
+ * struct aie_bd_aieml_dim_attr - AI engine buffer descriptor dimension
+ *				  attributes for aieml
+ * @iter_curr: iteration current field attributes
+ * @iter: iteration field attributes
+ * @dims: dimension field attributes, supports up to 4 dimensions
+ * @pads: padding field attributes
+ */
+struct aie_bd_aieml_dim_attr {
+	struct aie_single_reg_field iter_curr;
+	struct aie_bd_multi_dim_attr iter;
+	struct aie_bd_multi_dim_attr dims[4U];
+	struct aie_bd_pad_attr pads[3U];
+};
+
+/**
+ * struct aie_bd_attr - AI engine DMA attributes structure
+ * @valid_bd: buffer descriptor valid bd field attributes
+ * @next_bd: buffer descriptor next bd field attributes
+ * @use_next: buffer descriptor use next bd field attributes
+ * @addr: buffer descriptor address attributes
+ * @addr_2: buffer descriptor address attributes of second address
+ * @lock: buffer descriptor lock attributes
+ * @lock_2: buffer descriptor lock attributes of second lock
+ * @packet: buffer descriptor packet attributes
+ * @axi: buffer descriptor AXI attributes
+ * @aie_dim: buffer descriptor dimension attributes for aie dma
+ * @aieml_dim: buffer descriptor dimension attributes for aieml dma
+ * @buf_sel: buffer descriptor buffer selection field attributes
+ * @curr_ptr: buffer descriptor current pointer field attributes
+ * @interleave_en: buffer descriptor interleave enable field attributes
+ * @interleave_cnt: buffer descriptor interleave count field attributes
+ * @double_buff_en: buffer descriptor double buffer enable field attributes
+ * @fifo_mode: buffer descriptor fifo mode field attributes
+ * @compression_en: buffer descriptor compression enable field attributes
+ * @out_of_order_id: buffer descriptor out of order bd id field attributes
+ * @tlast_suppress: buffer descriptor tlast suppress field attributes
+ * @num_dims: number of dimensions for tile buffer descriptor
+ * @bd_idx_off: buffer descriptor index offset in bytes
+ */
+struct aie_bd_attr {
+	struct aie_single_reg_field valid_bd;
+	struct aie_single_reg_field next_bd;
+	struct aie_single_reg_field use_next;
+	struct aie_bd_addr_attr addr;
+	struct aie_bd_addr_attr addr_2;
+	struct aie_bd_lock_attr lock;
+	struct aie_bd_lock_attr lock_2;
+	struct aie_bd_pkt_attr packet;
+	struct aie_bd_axi_attr axi;
+	union {
+		struct aie_bd_aie_dim_attr aie_dim;
+		struct aie_bd_aieml_dim_attr aieml_dim;
+	};
+	struct aie_single_reg_field buf_sel;
+	struct aie_single_reg_field curr_ptr;
+	struct aie_single_reg_field interleave_en;
+	struct aie_single_reg_field interleave_cnt;
+	struct aie_single_reg_field double_buff_en;
+	struct aie_single_reg_field fifo_mode;
+	struct aie_single_reg_field compression_en;
+	struct aie_single_reg_field out_of_order_id;
+	struct aie_single_reg_field tlast_suppress;
+	u32 num_dims;
+	u32 bd_idx_off;
+};
+
+/**
+ * struct aie_dma_attr - AI engine DMA attributes structure
+ * @laddr: low address field attributes
+ * @haddr: high address field attributes
+ * @buflen: buffer length field attributes
+ * @sts: FSM status field attributes
+ * @chansts: channel status field attributes
+ * @stall: queue stall status field attributes
+ * @qsize: queue size field attributes
+ * @curbd: current buffer descriptor field attributes
+ * @qsts: queue status field attributes
+ * @fifo_cnt: FIFO counter field attributes
+ * @bd_regoff: SHIM DMA buffer descriptors register offset
+ * @mm2s_sts_regoff: MM2S status register offset
+ * @s2mm_sts_regoff: S2MM status register offset
+ * @fifo_cnt_regoff: FIFO counter register offset
+ * @num_mm2s_chan: number of MM2S channels
+ * @num_s2mm_chan: number of S2MM channels
+ * @num_bds: number of buffer descriptors
+ * @bd_len: length of a buffer descriptor in bytes
+ */
+struct aie_dma_attr {
+	struct aie_single_reg_field laddr;
+	struct aie_single_reg_field haddr;
+	struct aie_single_reg_field buflen;
+	struct aie_single_reg_field sts;
+	struct aie_single_reg_field chansts;
+	struct aie_single_reg_field stall;
+	struct aie_single_reg_field qsize;
+	struct aie_single_reg_field curbd;
+	struct aie_single_reg_field qsts;
+	struct aie_single_reg_field fifo_cnt;
+	u32 bd_regoff;
+	u32 mm2s_sts_regoff;
+	u32 s2mm_sts_regoff;
+	u32 fifo_cnt_regoff;
+	u32 num_mm2s_chan;
+	u32 num_s2mm_chan;
+	u32 num_bds;
+	u32 bd_len;
+};
+
+/**
+ * struct aie_core_regs_attr - AI engine core register attributes structure
+ * @core_regs: core registers
+ * @width: number of 32 bit words
+ */
+struct aie_core_regs_attr {
+	const struct aie_tile_regs *core_regs;
+	u32 width;
+};
+
+struct aie_aperture;
+/**
+ * struct aie_tile_operations - AI engine device operations
+ * @get_tile_type: get type of tile based on tile operation
+ * @get_mem_info: get different types of memories information
+ * @get_core_status: get the status of AIE core.
+ * @get_part_sysfs_lock_status: get partition lock status for sysfs.
+ * @get_tile_sysfs_lock_status: get tile lock status for sysfs.
+ * @get_part_sysfs_dma_status: get partition dma status for sysfs.
+ * @get_tile_sysfs_dma_status: get tile dma status for sysfs.
+ * @get_tile_sysfs_bd_metadata: get tile bd metadata for sysfs.
+ * @init_part_clk_state: initialize clock states software structure which is a
+ *			 bitmap for the AI engine partition. The clock states
+ *			 structure is the structure used to keep track of if
+ *			 the modules in the AI engine partition are gated.
+ * @scan_part_clocks: scan partition modules to check whether the modules are
+ *		      clock gated or not, and update the soft clock states
+ *		      structure. It is required to be called when the partition
+ *		      is requested so that the driver knows which modules are
+ *		      clock gated when the partition is requested. This function
+ *		      expects the caller to apply partition lock before calling
+ *		      this function.
+ * @set_part_clocks: set partition modules clocks gate registers based on the
+ *		     partition clock states bitmap. This function expects the
+ *		     caller to apply partition lock before calling this
+ *		     function. The caller function will need to set the bitmap
+ *		     on which tiles are required to be clocked on.
+ * @set_tile_isolation: set tile isolation boundary for input direction.
+ * @mem_clear: clear data memory banks of the partition.
+ * @get_dma_s2mm_status: get dma s2mm status
+ * @get_dma_mm2s_status: get dma mm2s status
+ * @get_chan_status: get dma channel status
+ * @get_lock_status: get tile, shimdma and memtile lock status
+ *
+ * Different AI engine device version has its own device
+ * operation.
+ */
+struct aie_tile_operations {
+	u32 (*get_tile_type)(struct aie_device *adev, struct aie_location *loc);
+	unsigned int (*get_mem_info)(struct aie_device *adev,
+				     struct aie_range *range,
+				     struct aie_part_mem *pmem);
+	u32 (*get_core_status)(struct aie_partition *apart,
+			       struct aie_location *loc);
+	ssize_t (*get_part_sysfs_lock_status)(struct aie_partition *apart,
+					      struct aie_location *loc,
+					      char *buffer, ssize_t size);
+	ssize_t (*get_tile_sysfs_lock_status)(struct aie_partition *apart,
+					      struct aie_location *loc,
+					      char *buffer, ssize_t size);
+	ssize_t (*get_part_sysfs_dma_status)(struct aie_partition *apart,
+					     struct aie_location *loc,
+					     char *buffer, ssize_t size);
+	ssize_t (*get_tile_sysfs_dma_status)(struct aie_partition *apart,
+					     struct aie_location *loc,
+					     char *buffer, ssize_t size);
+	ssize_t (*get_tile_sysfs_bd_metadata)(struct aie_partition *apart,
+					      struct aie_location *loc,
+					      char *buffer, ssize_t size);
+	int (*init_part_clk_state)(struct aie_partition *apart);
+	int (*scan_part_clocks)(struct aie_partition *apart);
+	int (*set_part_clocks)(struct aie_partition *apart);
+	int (*set_tile_isolation)(struct aie_partition *apart,
+				  struct aie_location *loc, u8 dir);
+	int (*mem_clear)(struct aie_partition *apart);
+	u32 (*get_dma_s2mm_status)(struct aie_partition *apart,
+				   struct aie_location *loc,
+				   u8 chanid);
+	u32 (*get_dma_mm2s_status)(struct aie_partition *apart,
+				   struct aie_location *loc,
+				   u8 chanid);
+	u8 (*get_chan_status)(struct aie_partition *apart,
+			      struct aie_location *loc,
+			      u32 status);
+	u32 (*get_lock_status)(struct aie_partition *apart,
+			       struct aie_location *loc,
+			       u8 lock);
+};
+
+/**
+ * struct aie_resource - AI engine resource structure
+ * @bitmap: resource bitmap
+ * @total: total number of resource
+ */
+struct aie_resource {
+	unsigned long *bitmap;
+	u32 total;
+};
+
+/**
+ * struct aie_event_attr - AI Engine event attributes structure.
+ * @bc_event: broadcast event attribute to capture event mask value and
+ *	      register offset from @bc_regoff.
+ * @group_error: group error attribute to capture error group mask value and
+ *		 register offset value from @group_regoff.
+ * @bc_regoff: base broadcast register offset.
+ * @status_regoff: base status register offset.
+ * @group_regoff: base group error register offset.
+ * @base_error_event: event ID of first error event in a group error.
+ * @num_broadcasts: total number of broadcast events.
+ * @base_bc_event: broadcast 0 vent ID
+ * @num_events: total number of events.
+ */
+struct aie_event_attr {
+	struct aie_single_reg_field bc_event;
+	struct aie_single_reg_field group_error;
+	u32 bc_regoff;
+	u32 status_regoff;
+	u32 group_regoff;
+	u32 base_error_event;
+	u32 num_broadcasts;
+	u32 base_bc_event;
+	u32 num_events;
+};
+
+/**
+ * struct aie_l1_intr_ctrl_attr - AI engine level 1 interrupt controller
+ *				  attributes structure.
+ * @mask: level 1 interrupt controller mask attribute.
+ * @swa_status: switch A level 1 interrupt controller status attribute.
+ * @swb_status: switch A level 1 interrupt controller status attribute.
+ * @swa_event: switch A level 1 interrupt controller event attribute.
+ * @swb_event: switch A level 1 interrupt controller event attribute.
+ * @regoff: base level 1 interrupt controller register offset.
+ * @event_lsb: lsb of IRQ event within IRQ event switch register.
+ * @num_broadcasts: total number of broadcast signals to level 1 interrupt
+ *		    controller.
+ */
+struct aie_l1_intr_ctrl_attr {
+	struct aie_single_reg_field swa_status;
+	struct aie_single_reg_field swb_status;
+	struct aie_single_reg_field swa_event;
+	struct aie_single_reg_field swb_event;
+	u32 regoff;
+	u32 event_lsb;
+	u32 num_broadcasts;
+};
+
+/**
+ * struct aie_l2_intr_ctrl_attr - AI engine level 2 interrupt controller
+ *				  attributes structure.
+ * @mask: level 2 interrupt controller mask attribute.
+ * @enable: level 2 interrupt controller enable attribute.
+ * @disable: level 2 interrupt controller disable attribute.
+ * @status: level 2 interrupt controller status attribute.
+ * @regoff: level 2 interrupt controller register offset.
+ * @num_broadcasts: total number of broadcast signals to level 2 interrupt
+ *		    controller.
+ */
+struct aie_l2_intr_ctrl_attr {
+	struct aie_single_reg_field mask;
+	struct aie_single_reg_field enable;
+	struct aie_single_reg_field disable;
+	struct aie_single_reg_field status;
+	u32 regoff;
+	u32 num_broadcasts;
+};
+
+/**
+ * struct aie_error_cb - AI engine error callback struct.
+ * @cb: pointer to callback function.
+ * @priv: data to be passed to the callback function.
+ */
+struct aie_error_cb {
+	void (*cb)(void *priv);
+	void *priv;
+};
+
+/**
+ * struct aie_event_prop - AI engine event property.
+ * @event: error event ID.
+ * @event_str: error string.
+ */
+struct aie_event_prop {
+	u32 event;
+	char *event_str;
+};
+
+/**
+ * struct aie_err_category - AI engine errors category.
+ * @err_category: category of error.
+ * @num_events: number of event IDs in a category.
+ * @prop: pointer to an array event properties.
+ */
+struct aie_err_category {
+	u32 err_category;
+	u32 num_events;
+	const struct aie_event_prop *prop;
+};
+
+/**
+ * struct aie_error_attr - AI engine error attribute.
+ * @num_err_categories: number of possible error categories valid for a given
+ *			module.
+ * @err_category: pointer to an array of error categories.
+ */
+struct aie_error_attr {
+	u32 num_err_categories;
+	const struct aie_err_category *err_category;
+};
+
+/**
+ * struct aie_rsc_stat - AI engine hardware resource status bitmap of a
+ *			 resource of a module type of a tile type of an AI
+ *			 engine partition
+ * @rbits: runtime allocated resource bitmap
+ * @sbits: static resource bitmap for resources allocated at compilation
+ *	   time
+ */
+struct aie_rsc_stat {
+	struct aie_resource rbits;
+	struct aie_resource sbits;
+};
+
+/**
+ * struct aie_mod_rscs - AI engine hardware resource status bitmaps of
+ *			 a module type of a tile type of an AI engine
+ *			 partition.
+ * @rscs_stat: resource status bitmaps
+ */
+struct aie_mod_rscs {
+	struct aie_rsc_stat *rscs_stat;
+};
+
+/**
+ * struct aie_tile_rscs - AI engine hardware resource status bitmaps of all
+ *			  resources of a tile type of a partition.
+ * @mod_rscs: array of pointers of AI engine resources. Each element is an
+ *	      array of hardware resources of different modules of a particular
+ *	      resource type of a tile type.
+ *	      e.g. if the tile type is TILE. The rscs are an arrary of
+ *	      resources bitmap of all the defined AI engine resources types of
+ *	      TILE type. e.g. the element of AIE_RSCTYPE_PERF. It is an array
+ *	      of perfcounter resources bitmaps for both core module and memory
+ *	      module of TILE type of an AI engine partition.
+ */
+struct aie_tile_rscs {
+	struct aie_mod_rscs *mod_rscs[AIE_RSCTYPE_MAX];
+};
+
+/**
+ * struct aie_mod_rsc_attr - AI engine resource attribute of a module
+ * @num_rscs: number of resource
+ */
+struct aie_mod_rsc_attr {
+	u8 num_rscs;
+};
+
+/**
+ * struct aie_tile_rsc_attr - AI engine resource attributes
+ * @mod_attr: array of resource attribute different modules of a tile type of
+ *	      a particular resource type.
+ */
+struct aie_tile_rsc_attr {
+	struct aie_mod_rsc_attr mod_attr[AIE_MAX_MODS_PER_TILE];
+};
+
+/**
+ * struct aie_lock_attr - AI engine lock attributes
+ * @sts: lock status field attributes
+ * @sts_regoff: lock status register offset
+ * @num_locks: number of locks
+ * @overflow: overflow status field attributes
+ * @overflow_regoff: overflow status register offset
+ * @underflow: underflow status field attributes
+ * @underflow_regoff: underflowstatus register offset
+ */
+struct aie_lock_attr {
+	struct aie_single_reg_field sts;
+	u32 sts_regoff;
+	u32 num_locks;
+	struct aie_single_reg_field overflow;
+	u32 overflow_regoff;
+	struct aie_single_reg_field underflow;
+	u32 underflow_regoff;
+};
+
+/**
+ * struct aie_tile_attr - AI engine device tile type attributes
+ * @start_row: start row
+ * @num_rows: number of rows
+ * @num_mods: number of modules of this tile type
+ * @mods: array of module types of this tile type
+ * @rscs_attr: resources attributes array. Each element is an array of
+ *	       attributes of a resource type of a tile type.
+ */
+struct aie_tile_attr {
+	u8 start_row;
+	u8 num_rows;
+	u8 num_mods;
+	const enum aie_module_type *mods;
+	const struct aie_tile_rsc_attr *rscs_attr;
+};
+
+/**
+ * struct aie_dev_attr - device attribute properties for AI Engine sysfs nodes.
+ * @name: name of the device attribute
+ * @mode: permissions associated
+ * @tile_type: tile type(s) attribute is valid for. use AIE_TILE_TYPE_MASK_*.
+ * @show: read function handler
+ * @store: write function handler
+ */
+struct aie_dev_attr {
+	const char *name;
+	umode_t mode;
+	u32 tile_type;
+	ssize_t (*show)(struct device *dev, struct device_attribute *attr,
+			char *buf);
+	ssize_t (*store)(struct device *dev, struct device_attribute *attr,
+			 const char *buf, size_t count);
+};
+
+/**
+ * struct aie_sysfs_prop - private data passed to the sysfs read/write handler.
+ * @data: buffer to export sysfs data
+ * @size: size of data exported
+ * @max_size: max size of data that could be exported
+ * @read_callback: callback to fetch data from on read
+ * @write_callback: callback to send data to on write
+ */
+struct aie_sysfs_prop {
+	char *data;
+	ssize_t size;
+	ssize_t max_size;
+	ssize_t (*read_callback)(struct kobject *kobj, char *buffer,
+				 ssize_t size);
+	ssize_t (*write_callback)(struct kobject *kobj, char *buffer,
+				  ssize_t size);
+};
+
+/**
+ * struct aie_bin_attr - binary attribute properties for AI Engine sysfs nodes
+ * @name: name of the binary attribute
+ * @mode: permissions associated
+ * @size: size of the buffer to be allocated
+ * @tile_type: tile type(s) attribute is valid for. use AIE_TILE_TYPE_MASK_*.
+ * @read: read handler
+ * @write: write handler
+ * @read_callback: callback to fetch data from on read
+ * @write_callback:  callback to send data to on write
+ */
+struct aie_bin_attr {
+	const char *name;
+	umode_t mode;
+	ssize_t size;
+	u32 tile_type;
+	ssize_t (*read)(struct file *filp, struct kobject *kobj,
+			struct bin_attribute *attr, char *buf, loff_t offset,
+			size_t max_size);
+	ssize_t (*write)(struct file *filp, struct kobject *kobj,
+			 struct bin_attribute *attr, char *buf, loff_t offset,
+			 size_t max_size);
+	ssize_t (*read_callback)(struct kobject *kobj, char *buffer,
+				 ssize_t size);
+	ssize_t (*write_callback)(struct kobject *kobj, char *buffer,
+				  ssize_t size);
+};
+
+/**
+ * struct aie_sysfs_attr - captures all sysfs attributes defined at
+ *			   partition or tile level.
+ * @dev_attr: pointer to array of device attributes
+ * @bin_attr: pointer to array of binary attributes
+ * @num_dev_attrs: number of device attributes
+ * @num_bin_attrs: number of binary attributes
+ */
+struct aie_sysfs_attr {
+	const struct aie_dev_attr *dev_attr;
+	const struct aie_bin_attr *bin_attr;
+	u32 num_dev_attrs;
+	u32 num_bin_attrs;
+};
+
+/**
+ * struct aie_tile - AI engine tile structure
+ * @loc: tile co-ordinates
+ * @apart: parent partition the tile belongs to
+ * @dev: device for the AI engine tile device
+ * @attr_grp: attribute group
+ */
+struct aie_tile {
+	struct aie_location loc;
+	struct aie_partition *apart;
+	struct device dev;
+	struct attribute_group *attr_grp;
+};
+
+/**
+ * struct aie_utilization_timer - AI engine user space pinned region
+ * @timer: timer structure to set expiry and callback.
+ * @apart: AI engine partition.
+ * @util: array to capture core utilization.
+ */
+struct aie_utilization_timer {
+	struct timer_list timer;
+	struct aie_partition *apart;
+	struct aie_occupancy *util;
+};
+
+/**
+ * struct aie_device - AI engine device structure
+ * @apertures: list of apertures
+ * @cdev: cdev for the AI engine
+ * @dev: device for the AI engine device
+ * @mlock: protection for AI engine device operations
+ * @clk: AI enigne device clock
+ * @kernel_regs: array of kernel only registers
+ * @core_regs: array of core registers
+ * @ops: tile operations
+ * @col_rst: column reset attribute
+ * @col_clkbuf: column clock buffer attribute
+ * @shim_bd: SHIM DMA buffer descriptor attribute
+ * @tile_bd: tile DMA buffer descriptor attribute
+ * @memtile_bd: MEM tile DMA buffer descriptor attribute
+ * @shim_dma: SHIM DMA attribute
+ * @tile_dma: tile DMA attribute
+ * @memtile_dma: MEM tile DMA attribute
+ * @pl_events: pl module event attribute
+ * @memtile_events: memory tile event attribute
+ * @mem_events: memory module event attribute
+ * @core_events: core module event attribute
+ * @mem_lock: mem lock attribute
+ * @pl_lock: Shim tile lock attribute
+ * @memtile_lock: Mem Tile lock attribute
+ * @l1_ctrl: level 1 interrupt controller attribute
+ * @l2_ctrl: level 2 interrupt controller attribute
+ * @core_errors: core module error attribute
+ * @mem_errors: memory module error attribute
+ * @memtile_errors: memory tile error attribute
+ * @shim_errors: shim tile error attribute
+ * @core_perfctrl: core module performance control attribute
+ * @core_perfctrl_reset: core module performance control reset attribute
+ * @core_perfcnt: core module performance counter attribute
+ * @core_evntgen: core module event generate attribute
+ * @util_timer: utilization timer
+ * @perfinst: performance instance
+ * @core_util_events: core module events to capture active and total cycle
+ * @array_shift: array address shift
+ * @col_shift: column address shift
+ * @row_shift: row address shift
+ * @dev_gen: aie hardware device generation
+ * @cols_res: AI engine columns resources to indicate
+ *	      while columns are occupied by partitions.
+ * @num_kernel_regs: number of kernel only registers range
+ * @num_core_regs: number of core registers range
+ * @pm_node_id: AI Engine platform management node ID
+ * @clock_id: AI Engine clock ID
+ * @device_name: identify ssit device id
+ * @ttype_attr: tile type attributes
+ * @aperture_sysfs_attr: aperture level sysfs attributes
+ * @part_sysfs_attr: partition level sysfs attributes
+ * @tile_sysfs_attr: tile level sysfs attributes
+ * @core_status_str: core status in string format
+ * @core_pc: program counter attribute
+ * @core_lr: link register attribute
+ * @core_sp: stack pointer attribute
+ * @dma_status_str: DMA channel status in string format
+ * @queue_status_str: DMA queue status in string format
+ */
+struct aie_device {
+	struct list_head apertures;
+	struct cdev cdev;
+	struct device dev;
+	struct mutex mlock; /* protection for AI engine apertures */
+	struct clk *clk;
+	const struct aie_tile_regs *kernel_regs;
+	const struct aie_core_regs_attr *core_regs;
+	const struct aie_tile_operations *ops;
+	const struct aie_single_reg_field *col_rst;
+	const struct aie_single_reg_field *col_clkbuf;
+	const struct aie_bd_attr *shim_bd;
+	const struct aie_bd_attr *tile_bd;
+	const struct aie_bd_attr *memtile_bd;
+	const struct aie_dma_attr *shim_dma;
+	const struct aie_dma_attr *tile_dma;
+	const struct aie_dma_attr *memtile_dma;
+	const struct aie_event_attr *pl_events;
+	const struct aie_event_attr *memtile_events;
+	const struct aie_event_attr *mem_events;
+	const struct aie_event_attr *core_events;
+	const struct aie_lock_attr *mem_lock;
+	const struct aie_lock_attr *memtile_lock;
+	const struct aie_lock_attr *pl_lock;
+	const struct aie_l1_intr_ctrl_attr *l1_ctrl;
+	const struct aie_l2_intr_ctrl_attr *l2_ctrl;
+	const struct aie_error_attr *core_errors;
+	const struct aie_error_attr *mem_errors;
+	const struct aie_error_attr *memtile_errors;
+	const struct aie_error_attr *shim_errors;
+	const struct aie_single_reg_field *core_perfctrl;
+	const struct aie_single_reg_field *core_perfctrl_reset;
+	const struct aie_single_reg_field *core_perfcnt;
+	const struct aie_single_reg_field *core_evntgen;
+	const enum aie_events *core_util_events;
+	struct aie_utilization_timer util_timer;
+	struct aie_perfinst_args perfinst;
+	u32 array_shift;
+	u32 col_shift;
+	u32 row_shift;
+	u32 dev_gen;
+	u32 num_kernel_regs;
+	u32 num_core_regs;
+	u32 pm_node_id;
+	u32 clock_id;
+	u32 device_name;
+	struct aie_tile_attr ttype_attr[AIE_TILE_TYPE_MAX];
+	const struct aie_sysfs_attr *aperture_sysfs_attr;
+	const struct aie_sysfs_attr *part_sysfs_attr;
+	const struct aie_sysfs_attr *tile_sysfs_attr;
+	char **core_status_str;
+	const struct aie_single_reg_field *core_pc;
+	const struct aie_single_reg_field *core_lr;
+	const struct aie_single_reg_field *core_sp;
+};
+
+struct aie_l2_mask {
+	u32 *val;
+	int count;
+};
+
+/**
+ * struct aie_aperture - AI engine aperture structure
+ * @node: list node
+ * @partitions: list of partitions of this aperture
+ * @adev: pointer to AI device instance
+ * @mlock: protection for AI engine aperture operations
+ * @base: AI engine aperture base virtual address
+ * @res: memory resource of AI engine aperture
+ * @dev: device of aperture
+ * @cols_res: AI engine columns resources to indicate
+ *	      while columns are occupied by partitions.
+ * @node_id: AI engine aperture node id which is to identify
+ *	     the aperture in the system in firmware
+ * @irq: Linux IRQ number
+ * @range: range of aperture
+ * @backtrack: workqueue to backtrack interrupt
+ * @l2_mask: level 2 interrupt controller mask bitmap
+ * @attr_grp: attribute group for sysfs
+ */
+struct aie_aperture {
+	struct list_head node;
+	struct list_head partitions;
+	struct aie_device *adev;
+	struct mutex mlock; /* protection for AI engine aperture operations */
+	void __iomem *base;
+	struct resource res;
+	struct device dev;
+	struct aie_resource cols_res;
+	u32 node_id;
+	int irq;
+	struct aie_range range;
+	struct work_struct backtrack;
+	struct aie_l2_mask l2_mask;
+	struct attribute_group *attr_grp;
+};
+
+/**
+ * struct aie_partition - AI engine partition structure
+ * @node: list node
+ * @dbufs: dmabufs list
+ * @aperture: pointer to AI engine aperture
+ * @adev: pointer to AI device instance
+ * @filep: pointer to file for refcount on the users of the partition
+ * @pmems: pointer to partition memories types
+ * @dbufs_cache: memory management object for preallocated dmabuf descriptors
+ * @trscs: resources bitmaps for each tile
+ * @freq_req: required frequency
+ * @range: range of partition
+ * @mlock: protection for AI engine partition operations
+ * @dev: device for the AI engine partition
+ * @atiles: pointer to an array of AIE tile structure.
+ * @cores_clk_state: bitmap to indicate the power state of core modules
+ * @tiles_inuse: bitmap to indicate if a tile is in use
+ * @error_cb: error callback
+ * @core_event_status: core module event bitmap
+ * @mem_event_status: memory module event bitmap
+ * @pl_event_status: pl module event bitmap
+ * @attr_grp: attribute group
+ * @partition_id: partition id. Partition ID is the identifier
+ *		  of the AI engine partition in the system.
+ * @status: indicate if the partition is in use
+ * @cntrflag: partition control flag. e.g. whether to reset columns when
+ *	      the partition is released
+ * @error_to_report: indicates if there are errors pending to be reported to
+ *		     the application. This value is set to true if errors are
+ *		     found during backtracking, and error interrupt was
+ *		     received when partition was not requested yet.
+ */
+struct aie_partition {
+	struct list_head node;
+	struct list_head dbufs;
+	struct aie_aperture *aperture;
+	struct aie_device *adev;
+	struct file *filep;
+	struct aie_part_mem *pmems;
+	struct kmem_cache *dbufs_cache;
+	struct aie_tile_rscs trscs[AIE_TILE_TYPE_MAX];
+	u64 freq_req;
+	struct aie_range range;
+	struct mutex mlock; /* protection for AI engine partition operations */
+	struct device dev;
+	struct aie_tile *atiles;
+	struct aie_resource cores_clk_state;
+	struct aie_resource tiles_inuse;
+	struct aie_error_cb error_cb;
+	struct aie_resource core_event_status;
+	struct aie_resource mem_event_status;
+	struct aie_resource pl_event_status;
+	struct attribute_group *attr_grp;
+	u32 partition_id;
+	u32 status;
+	u32 cntrflag;
+	u8 error_to_report;
+};
+
+/**
+ * struct aie_part_pinned_region - AI engine user space pinned region
+ * @user_addr: user space address
+ * @len: length of the user space buffer in bytes
+ * @npages: number of pages of the user space buffer
+ * @pages: array to receive pointers to the pages pinned.
+ *	   should be at least npages long
+ */
+struct aie_part_pinned_region {
+	u64 user_addr;
+	u64 len;
+	struct page **pages;
+	int npages;
+};
+
+extern struct class *aie_class;
+extern const struct file_operations aie_part_fops;
+
+#define cdev_to_aiedev(i_cdev) container_of((i_cdev), struct aie_device, cdev)
+#define dev_to_aiedev(_dev) container_of((_dev), struct aie_device, dev)
+#define dev_to_aieaperture(_dev) container_of((_dev), struct aie_aperture, dev)
+#define dev_to_aiepart(_dev) container_of((_dev), struct aie_partition, dev)
+#define dev_to_aietile(_dev) container_of((_dev), struct aie_tile, dev)
+
+#define aie_col_mask(adev) ({ \
+	struct aie_device *_adev = (adev); \
+	GENMASK_ULL(_adev->array_shift - 1, _adev->col_shift);  \
+	})
+
+#define aie_row_mask(adev) ({ \
+	struct aie_device *_adev = (adev); \
+	GENMASK_ULL(_adev->col_shift - 1, _adev->row_shift);  \
+	})
+
+#define aie_tile_reg_mask(adev) ({ \
+	struct aie_device *_adev = (adev); \
+	GENMASK_ULL(_adev->row_shift - 1, 0);  \
+	})
+
+/*
+ * Need to define field get, as AI engine shift mask is not constant.
+ * Cannot use FIELD_GET()
+ */
+#define aie_tile_reg_field_get(mask, shift, regoff) ( \
+	((regoff) & (mask)) >> (shift))
+
+#define aie_cal_tile_reg(adev, regoff) ( \
+	aie_tile_reg_field_get(aie_tile_reg_mask(adev), 0, regoff))
+
+/**
+ * aie_get_field_val() - calculate value of an AI engine register field
+ * @field: a field in a register
+ * @val: value of the field
+ * @return: value of a register field
+ */
+static inline u32 aie_get_field_val(const struct aie_single_reg_field *field,
+				    u32 val)
+{
+	long long mask = (long long)field->mask & 0x00000000ffffffff;
+
+	return (val << __bf_shf(mask)) & field->mask;
+}
+
+/**
+ * aie_get_reg_field() - get value from a field from a register valuer
+ * @field: a field in a register
+ * @regval: register value
+ * @return: value of a register field
+ */
+static inline u32 aie_get_reg_field(const struct aie_single_reg_field *field,
+				    u32 regval)
+{
+	long long mask64 = (long long)field->mask & 0x00000000ffffffff;
+
+	return (regval & field->mask) >> __bf_shf(mask64);
+}
+
+/**
+ * aie_cal_regoff() - calculate register offset to the whole AI engine
+ *		      device start address
+ * @adev: AI engine device
+ * @loc: AI engine tile location
+ * @regoff_intile: register offset within a tile
+ * @return: register offset to the whole AI engine device start address
+ */
+static inline u32 aie_cal_regoff(struct aie_device *adev,
+				 struct aie_location loc, u32 regoff_intile)
+{
+	return regoff_intile + (loc.col << adev->col_shift) +
+	       (loc.row << adev->row_shift);
+}
+
+/**
+ * aie_aperture_cal_regoff() - calculate register offset to the whole AI engine
+ *                             device start address
+ * @aperture: AI aperture
+ * @loc: AI engine tile location
+ * @regoff_intile: register offset within a tile
+ * @return: register offset to the whole AI engine device start address
+ */
+static inline u32 aie_aperture_cal_regoff(struct aie_aperture *aperture,
+					  struct aie_location loc,
+					  u32 regoff_intile)
+{
+	struct aie_device *adev = aperture->adev;
+
+	return regoff_intile + ((loc.col - aperture->range.start.col) <<
+				adev->col_shift) + (loc.row << adev->row_shift);
+}
+
+/**
+ * aie_validate_location() - validate tile location within an AI engine
+ *			     partition
+ * @apart: AI engine partition
+ * @loc: AI engine tile location relative in partition
+ * @return: return 0 if it is valid, negative value for errors.
+ *
+ * This function checks if the AI engine location is within the AI engine
+ * partition.
+ */
+static inline int aie_validate_location(struct aie_partition *apart,
+					struct aie_location loc)
+{
+	if (loc.col >= apart->range.size.col ||
+	    loc.row >= apart->range.size.row)
+		return -EINVAL;
+
+	return 0;
+}
+
+/**
+ * aie_resource_or_get_valueul() - get unsigned long value of specified
+ *				   number of bits starting from specified
+ *				   start bit of a resource bitmap
+ *
+ * @res: pointer to AI engine resource
+ * @sbit: start bit for OR operation
+ * @nbits: number of bits to OR
+ * @return: or result of @nbits of two bitmaps starting from @sbit
+ *
+ * OR @nbits of two resource bitmaps starting from @sbit
+ */
+static inline
+unsigned long aie_resource_or_get_valueul(struct aie_resource *res,
+					  u32 sbit, u32 nbits)
+{
+	const size_t i = BIT_WORD(sbit);
+	unsigned long bits;
+
+	bits = res->bitmap[i];
+	bits >>= (sbit % BITS_PER_LONG);
+	bits |= BITMAP_FIRST_WORD_MASK(nbits);
+
+	return bits;
+}
+
+int aie_resource_initialize(struct aie_resource *res, int count);
+void aie_resource_uninitialize(struct aie_resource *res);
+int aie_resource_check_region(struct aie_resource *res, u32 start,
+			      u32 count);
+int aie_resource_get_region(struct aie_resource *res, u32 start,
+			    u32 count);
+void aie_resource_put_region(struct aie_resource *res, int start, u32 count);
+int aie_resource_set(struct aie_resource *res, u32 start, u32 count);
+int aie_resource_cpy_from_arr32(struct aie_resource *res, u32 start,
+				const u32 *src, u32 nbits);
+int aie_resource_cpy_to_arr32(struct aie_resource *res, u32 start, u32 *dst,
+			      u32 nbits);
+int aie_resource_clear(struct aie_resource *res, u32 start, u32 count);
+int aie_resource_clear_all(struct aie_resource *res);
+bool aie_resource_testbit(struct aie_resource *res, u32 bit);
+int aie_resource_check_common_avail(struct aie_resource *res0,
+				    struct aie_resource *res1,
+				    u32 sbit, u32 nbits);
+int aie_resource_get_common_avail(struct aie_resource *res0,
+				  struct aie_resource *res1,
+				  u32 sbit, u32 nbits, u32 total,
+				  struct aie_rsc *rscs);
+int aie_resource_check_pattern_region(struct aie_resource *res,
+				      u32 start, u32 end, u32 count);
+int aie_resource_check_common_pattern_region(struct aie_resource *res0,
+					     struct aie_resource *res1,
+					     u32 sbit, u32 nbits, u32 total);
+int aie_resource_get_common_pattern_region(struct aie_resource *res0,
+					   struct aie_resource *res1,
+					   u32 sbit, u32 nbits, u32 total,
+					   struct aie_rsc *rscs);
+
+const struct file_operations *aie_part_get_fops(void);
+u8 aie_part_in_use(struct aie_partition *apart);
+struct aie_partition *aie_get_partition_from_id(struct aie_device *adev,
+						u32 partition_id);
+void of_xilinx_ai_engine_aperture_probe(struct aie_device *adev);
+struct aie_device *of_ai_engine_class_find(struct device_node *np);
+int xilinx_ai_engine_add_dev(struct aie_device *adev,
+			     struct platform_device *pdev);
+int xilinx_ai_engine_probe_v1(struct platform_device *pdev);
+
+void aie_part_remove(struct aie_partition *apart);
+int aie_part_clear_context(struct aie_partition *apart);
+int aie_part_clean(struct aie_partition *apart);
+int aie_part_open(struct aie_partition *apart, void *rsc_metadata);
+int aie_part_initialize(struct aie_partition *apart, void __user *user_args);
+int aie_part_teardown(struct aie_partition *apart);
+
+int aie_mem_get_info(struct aie_partition *apart, unsigned long arg);
+
+long aie_part_attach_dmabuf_req(struct aie_partition *apart,
+				void __user *user_args);
+long aie_part_detach_dmabuf_req(struct aie_partition *apart,
+				void __user *user_args);
+long aie_part_set_bd_from_user(struct aie_partition *apart,
+					void __user *user_args);
+long aie_part_set_bd(struct aie_partition *apart,
+					struct aie_dma_bd_args *args);
+long aie_part_set_dmabuf_bd_from_user(struct aie_partition *apart,
+			    void __user *user_args);
+long aie_part_set_dmabuf_bd(struct aie_partition *apart,
+					struct aie_dmabuf_bd_args *args);
+void aie_part_release_dmabufs(struct aie_partition *apart);
+int aie_part_prealloc_dbufs_cache(struct aie_partition *apart);
+
+int aie_part_scan_clk_state(struct aie_partition *apart);
+bool aie_part_check_clk_enable_loc(struct aie_partition *apart,
+				   struct aie_location *loc);
+int aie_part_set_freq(struct aie_partition *apart, u64 freq);
+int aie_part_get_freq(struct aie_partition *apart, u64 *freq);
+
+int aie_part_request_tiles(struct aie_partition *apart, int num_tiles,
+			   struct aie_location *locs);
+int aie_part_release_tiles(struct aie_partition *apart, int num_tiles,
+			   struct aie_location *locs);
+int aie_part_request_tiles_from_user(struct aie_partition *apart,
+				     void __user *user_args);
+int aie_part_release_tiles_from_user(struct aie_partition *apart,
+				     void __user *user_args);
+int aie_device_init(struct aie_device *adev);
+int aieml_device_init(struct aie_device *adev);
+
+bool aie_part_has_mem_mmapped(struct aie_partition *apart);
+bool aie_part_has_regs_mmapped(struct aie_partition *apart);
+
+int aie_part_get_tile_rows(struct aie_partition *apart,
+			   enum aie_tile_type ttype);
+
+int aie_part_reset(struct aie_partition *apart);
+int aie_part_post_reinit(struct aie_partition *apart);
+int aie_part_init_isolation(struct aie_partition *apart);
+struct aie_partition *aie_create_partition(struct aie_aperture *aperture,
+					   u32 partition_id);
+
+void aie_aperture_backtrack(struct work_struct *work);
+irqreturn_t aie_interrupt(int irq, void *data);
+void aie_interrupt_callback(const u32 *payload, void *data);
+int aie_aperture_create_l2_mask(struct aie_aperture *aperture);
+bool aie_part_has_error(struct aie_partition *apart);
+void aie_part_clear_cached_events(struct aie_partition *apart);
+int aie_part_set_intr_rscs(struct aie_partition *apart);
+
+struct aie_aperture *
+of_aie_aperture_probe(struct aie_device *adev, struct device_node *nc);
+int aie_aperture_remove(struct aie_aperture *aperture);
+int aie_aperture_check_part_avail(struct aie_aperture *aperture,
+				  struct aie_partition_req *req);
+struct aie_partition *
+aie_aperture_request_part_from_id(struct aie_aperture *aperture,
+				  u32 partition_id);
+int aie_aperture_enquire_parts(struct aie_aperture *aperture,
+			       unsigned int num_queries,
+			       struct aie_range_args  *queries,
+			       int *num_parts_left, bool to_user);
+unsigned int aie_aperture_get_num_parts(struct aie_aperture *aperture);
+int aie_aperture_add_dev(struct aie_aperture *aperture,
+			 struct device_node *nc);
+
+int aie_part_rscmgr_init(struct aie_partition *apart);
+void aie_part_rscmgr_finish(struct aie_partition *apart);
+void aie_part_rscmgr_reset(struct aie_partition *apart);
+long aie_part_rscmgr_rsc_req(struct aie_partition *apart,
+			     void __user *user_args);
+long aie_part_rscmgr_rsc_release(struct aie_partition *apart,
+				 void __user *user_args);
+long aie_part_rscmgr_rsc_free(struct aie_partition *apart,
+			      void __user *user_args);
+long aie_part_rscmgr_rsc_req_specific(struct aie_partition *apart,
+				      void __user *user_args);
+long aie_part_rscmgr_rsc_check_avail(struct aie_partition *apart,
+				     void __user *user_args);
+long aie_part_rscmgr_get_broadcast(struct aie_partition *apart,
+				   void __user *user_args);
+int aie_part_rscmgr_set_static(struct aie_partition *apart, void *meta);
+int aie_part_rscmgr_set_tile_broadcast(struct aie_partition *apart,
+				       struct aie_location loc,
+				       enum aie_module_type mod, uint32_t id);
+
+int aie_aperture_sysfs_create_entries(struct aie_aperture *aperture);
+void aie_aperture_sysfs_remove_entries(struct aie_aperture *aperture);
+int aie_part_sysfs_create_entries(struct aie_partition *apart);
+void aie_part_sysfs_remove_entries(struct aie_partition *apart);
+int aie_tile_sysfs_create_entries(struct aie_tile *atile);
+void aie_tile_sysfs_remove_entries(struct aie_tile *atile);
+ssize_t aie_sysfs_read_handler(struct file *filp, struct kobject *kobj,
+			       struct bin_attribute *attr, char *buf,
+			       loff_t offset, size_t max_size);
+
+ssize_t aie_sysfs_get_core_status(struct aie_partition *apart,
+				  struct aie_location *loc, char *buffer,
+				  ssize_t size);
+ssize_t aie_tile_show_core(struct device *dev, struct device_attribute *attr,
+			   char *buffer);
+ssize_t aie_part_read_cb_core(struct kobject *kobj, char *buffer, ssize_t size);
+ssize_t aie_sysfs_get_dma_status(struct aie_partition *apart,
+				 struct aie_location *loc, char *buffer,
+				 ssize_t size);
+ssize_t aie_tile_show_bd(struct device *dev, struct device_attribute *attr,
+			 char *buffer);
+ssize_t aie_tile_show_dma(struct device *dev, struct device_attribute *attr,
+			  char *buffer);
+ssize_t aie_part_read_cb_dma(struct kobject *kobj, char *buffer, ssize_t size);
+ssize_t aie_tile_show_lock(struct device *dev, struct device_attribute *attr,
+			   char *buffer);
+ssize_t aie_part_read_cb_lock(struct kobject *kobj, char *buffer, ssize_t size);
+ssize_t aie_sysfs_get_lock_status(struct aie_partition *apart,
+				  struct aie_location *loc, char *buffer,
+				  ssize_t size);
+u32 aie_get_module_error_count(struct aie_partition *apart,
+			       struct aie_location loc,
+			       enum aie_module_type module,
+			       const struct aie_error_attr *err_attr);
+bool aie_check_tile_error(struct aie_partition *apart, struct aie_location loc);
+bool aie_check_error_bitmap(struct aie_partition *apart,
+			    struct aie_location loc,
+			    enum aie_module_type module, u8 event);
+u32 aie_get_error_count(struct aie_partition *apart);
+ssize_t aie_sysfs_get_errors(struct aie_partition *apart,
+			     struct aie_location *loc, char *buffer,
+			     ssize_t size);
+ssize_t aie_tile_show_error(struct device *dev, struct device_attribute *attr,
+			    char *buffer);
+ssize_t aie_aperture_show_hardware_info(struct device *dev,
+					struct device_attribute *attr,
+					char *buffer);
+ssize_t aie_part_show_error_stat(struct device *dev,
+				 struct device_attribute *attr, char *buffer);
+ssize_t aie_part_show_current_freq(struct device *dev,
+				   struct device_attribute *attr, char *buffer);
+ssize_t aie_part_read_cb_error(struct kobject *kobj, char *buffer,
+			       ssize_t size);
+ssize_t aie_tile_show_event(struct device *dev, struct device_attribute *attr,
+			    char *buffer);
+void aie_read_event_status(struct aie_partition *apart,
+			   struct aie_location *loc,
+			   enum aie_module_type module, u32 *reg);
+ssize_t aie_part_read_cb_status(struct kobject *kobj, char *buffer,
+				ssize_t size);
+long aie_part_rscmgr_get_statistics(struct aie_partition *apart,
+				    void __user *user_args);
+int  aie_part_set_column_clock_from_user(struct aie_partition *apart,
+					 void __user *user_args);
+
+int aie_overlay_register_notifier(void);
+void aie_overlay_unregister_notifier(void);
+u32 aie_get_core_pc(struct aie_partition *apart,
+		    struct aie_location *loc);
+u32 aie_get_core_lr(struct aie_partition *apart,
+		    struct aie_location *loc);
+u32 aie_get_core_sp(struct aie_partition *apart,
+		    struct aie_location *loc);
+
+#endif /* AIE_INTERNAL_H */
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-interrupt.c b/drivers/misc/xilinx-ai-engine/ai-engine-interrupt.c
new file mode 100644
index 000000000..a190b9ea1
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-interrupt.c
@@ -0,0 +1,1520 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+#include <linux/bitmap.h>
+#include <linux/firmware/xlnx-zynqmp.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/slab.h>
+#include <linux/workqueue.h>
+
+#include "ai-engine-internal.h"
+#include "linux/xlnx-ai-engine.h"
+
+#define AIE_ARRAY_TILE_ERROR_BC_ID		0U
+#define AIE_SHIM_TILE_ERROR_IRQ_ID		16U
+#define AIE_SHIM_INTR_BC_MAX			5U
+#define AIE_L2_MASK_REG_BITS			32U
+
+/**
+ * aie_get_broadcast_event() - get event ID being broadcast on given
+ *			       broadcast line.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @module: module type.
+ * @bc_id: broadcast ID.
+ * @return: event ID.
+ */
+static u8 aie_get_broadcast_event(struct aie_partition *apart,
+				  struct aie_location *loc,
+				  enum aie_module_type module, u8 bc_id)
+{
+	const struct aie_event_attr *event_mod;
+	u32 bcoff, regoff;
+	u32 ttype;
+
+	ttype = apart->adev->ops->get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		if (module == AIE_CORE_MOD)
+			event_mod = apart->adev->core_events;
+		else
+			event_mod = apart->adev->mem_events;
+	} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+		event_mod = apart->adev->memtile_events;
+	} else {
+		event_mod = apart->adev->pl_events;
+	}
+
+	bcoff = event_mod->bc_regoff + event_mod->bc_event.regoff + bc_id * 4U;
+	regoff = aie_aperture_cal_regoff(apart->aperture, *loc, bcoff);
+	return ioread32(apart->aperture->base + regoff);
+}
+
+/**
+ * aie_read_event_status() - get the status of event status registers.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @module: module type.
+ * @reg: array to store event status register values.
+ */
+void aie_read_event_status(struct aie_partition *apart,
+			   struct aie_location *loc,
+			   enum aie_module_type module, u32 *reg)
+{
+	const struct aie_event_attr *event_mod;
+	u8 offset;
+	u32 ttype;
+
+	ttype = apart->adev->ops->get_tile_type(apart->adev, loc);
+
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		if (module == AIE_CORE_MOD)
+			event_mod = apart->adev->core_events;
+		else
+			event_mod = apart->adev->mem_events;
+	} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+		event_mod = apart->adev->memtile_events;
+	} else {
+		event_mod = apart->adev->pl_events;
+	}
+
+	for (offset = 0; offset < (event_mod->num_events / 32); offset++) {
+		u32 status_off = event_mod->status_regoff + offset * 4U;
+		u32 regoff = aie_aperture_cal_regoff(apart->aperture, *loc,
+						     status_off);
+
+		reg[offset] = ioread32(apart->aperture->base + regoff);
+	}
+}
+
+/**
+ * aie_clear_event_status() - clears the status of event.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @module: module type.
+ * @event: event ID.
+ */
+static void aie_clear_event_status(struct aie_partition *apart,
+				   struct aie_location *loc,
+				   enum aie_module_type module, u8 event)
+{
+	const struct aie_event_attr *event_mod;
+	u32 status_off, regoff;
+	u32 ttype;
+
+	ttype = apart->adev->ops->get_tile_type(apart->adev, loc);
+
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		if (module == AIE_CORE_MOD)
+			event_mod = apart->adev->core_events;
+		else
+			event_mod = apart->adev->mem_events;
+	} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+		event_mod = apart->adev->memtile_events;
+	} else {
+		event_mod = apart->adev->pl_events;
+	}
+
+	if (event >= event_mod->num_events)
+		return;
+
+	status_off = event_mod->status_regoff + (event / 32) * 4U;
+	regoff = aie_aperture_cal_regoff(apart->aperture, *loc, status_off);
+	iowrite32(BIT(event % 32), apart->aperture->base + regoff);
+}
+
+/**
+ * aie_check_group_errors_enabled() - get error events enabled in group error.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @module: module type.
+ * @return: bitmap of enabled error events.
+ */
+static u32 aie_check_group_errors_enabled(struct aie_partition *apart,
+					  struct aie_location *loc,
+					  enum aie_module_type module)
+{
+	const struct aie_event_attr *event_mod;
+	u32 groff, regoff;
+	u32 ttype;
+
+	ttype = apart->adev->ops->get_tile_type(apart->adev, loc);
+
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		if (module == AIE_CORE_MOD)
+			event_mod = apart->adev->core_events;
+		else
+			event_mod = apart->adev->mem_events;
+	} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+		event_mod = apart->adev->memtile_events;
+	} else {
+		event_mod = apart->adev->pl_events;
+	}
+
+	groff = event_mod->group_regoff + event_mod->group_error.regoff;
+	regoff = aie_aperture_cal_regoff(apart->aperture, *loc, groff);
+	return ioread32(apart->aperture->base + regoff);
+}
+
+/**
+ * aie_set_error_event() - enable/disable error events in group error.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @module: module type.
+ * @bitmap: error event to enable/disable in group errors.
+ */
+static void aie_set_error_event(struct aie_partition *apart,
+				struct aie_location *loc,
+				enum aie_module_type module, u32 bitmap)
+{
+	const struct aie_event_attr *event_mod;
+	u32 groff, regoff;
+	u32 ttype;
+
+	ttype = apart->adev->ops->get_tile_type(apart->adev, loc);
+
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		if (module == AIE_CORE_MOD)
+			event_mod = apart->adev->core_events;
+		else
+			event_mod = apart->adev->mem_events;
+	} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+		event_mod = apart->adev->memtile_events;
+	} else {
+		event_mod = apart->adev->pl_events;
+	}
+
+	groff = event_mod->group_regoff + event_mod->group_error.regoff;
+	regoff = aie_aperture_cal_regoff(apart->aperture, *loc, groff);
+	iowrite32(bitmap, apart->aperture->base + regoff);
+}
+
+/**
+ * aie_get_error_event() - map group error status bit to actual error
+ *			   event number.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @module: module type.
+ * @index: event index within group errors.
+ * @return: true event ID.
+ */
+static u32 aie_get_error_event(struct aie_partition *apart,
+			       struct aie_location *loc,
+			       enum aie_module_type module, u8 index)
+{
+	const struct aie_event_attr *event_mod;
+	u32 ttype;
+
+	ttype = apart->adev->ops->get_tile_type(apart->adev, loc);
+
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		if (module == AIE_CORE_MOD)
+			event_mod = apart->adev->core_events;
+		else
+			event_mod = apart->adev->mem_events;
+	} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+		event_mod = apart->adev->memtile_events;
+	} else {
+		event_mod = apart->adev->pl_events;
+	}
+
+	return event_mod->base_error_event + index;
+}
+
+/**
+ * aie_get_bc_event() - get the broadcast event ID.
+ * @apart: AIE partition pointer.
+ * @ttype: tile type.
+ * @module: module type.
+ * @bc_id: broadcast line ID.
+ * @return: broadcast event ID.
+ */
+static u32 aie_get_bc_event(struct aie_partition *apart, u32 ttype,
+			    enum aie_module_type module, u8 bc_id)
+{
+	const struct aie_event_attr *event_mod;
+
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		if (module == AIE_CORE_MOD)
+			event_mod = apart->adev->core_events;
+		else
+			event_mod = apart->adev->mem_events;
+	} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+		event_mod = apart->adev->memtile_events;
+	} else {
+		event_mod = apart->adev->pl_events;
+	}
+
+	if (!event_mod)
+		return 0;
+
+	return event_mod->base_bc_event + bc_id;
+}
+
+/**
+ * aie_get_l1_event() - get event ID being broadcast on level 1 IRQ.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @sw: switch type.
+ * @irq_id: IRQ event ID to be read.
+ * @return: true event ID.
+ */
+static u8 aie_get_l1_event(struct aie_partition *apart,
+			   struct aie_location *loc,
+			   enum aie_shim_switch_type sw, u8 irq_id)
+{
+	const struct aie_l1_intr_ctrl_attr *intr_ctrl = apart->adev->l1_ctrl;
+	u32 l1off, l1mask, regoff, reg_value;
+
+	if (sw == AIE_SHIM_SWITCH_A) {
+		l1off = intr_ctrl->regoff + intr_ctrl->swa_event.regoff;
+		l1mask = intr_ctrl->swa_event.mask;
+	} else {
+		l1off = intr_ctrl->regoff + intr_ctrl->swb_event.regoff;
+		l1mask = intr_ctrl->swb_event.mask;
+	}
+
+	regoff = aie_aperture_cal_regoff(apart->aperture, *loc, l1off);
+	reg_value = ioread32(apart->aperture->base + regoff);
+	reg_value &= l1mask << (irq_id * intr_ctrl->event_lsb);
+	reg_value >>= (irq_id * intr_ctrl->event_lsb);
+	return reg_value;
+}
+
+/**
+ * aie_clear_l1_intr() - clear level 1 interrupt controller status.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @sw: switch type.
+ * @irq_id: IRQ ID to be cleared.
+ */
+static void aie_clear_l1_intr(struct aie_partition *apart,
+			      struct aie_location *loc,
+			      enum aie_shim_switch_type sw, u8 irq_id)
+{
+	const struct aie_l1_intr_ctrl_attr *intr_ctrl = apart->adev->l1_ctrl;
+	u32 l1off, regoff;
+
+	if (sw == AIE_SHIM_SWITCH_A)
+		l1off = intr_ctrl->regoff + intr_ctrl->swa_status.regoff;
+	else
+		l1off = intr_ctrl->regoff + intr_ctrl->swb_status.regoff;
+
+	regoff = aie_aperture_cal_regoff(apart->aperture, *loc, l1off);
+	iowrite32(BIT(irq_id), apart->aperture->base + regoff);
+}
+
+/**
+ * aie_get_l1_status() - get level 1 interrupt controller status value.
+ * @apart: AIE partition pointer.
+ * @loc: pointer to tile location.
+ * @sw: switch type.
+ * @return: status value.
+ */
+static u32 aie_get_l1_status(struct aie_partition *apart,
+			     struct aie_location *loc,
+			     enum aie_shim_switch_type sw)
+{
+	const struct aie_l1_intr_ctrl_attr *intr_ctrl = apart->adev->l1_ctrl;
+	u32 l1off, regoff;
+
+	if (sw == AIE_SHIM_SWITCH_A)
+		l1off = intr_ctrl->regoff + intr_ctrl->swa_status.regoff;
+	else
+		l1off = intr_ctrl->regoff + intr_ctrl->swb_status.regoff;
+
+	regoff = aie_aperture_cal_regoff(apart->aperture, *loc, l1off);
+	return ioread32(apart->aperture->base + regoff);
+}
+
+/**
+ * aie_aperture_clear_l2_intr() - clear level 2 interrupt controller status.
+ * @aperture: AIE aperture pointer.
+ * @loc: pointer to tile location.
+ * @bitmap_irq: IRQ bitmap. IRQ lines corresponding to set bits will be
+ *		cleared.
+ */
+static void aie_aperture_clear_l2_intr(struct aie_aperture *aperture,
+				       struct aie_location *loc, u32 bitmap_irq)
+{
+	const struct aie_l2_intr_ctrl_attr *intr_ctrl = aperture->adev->l2_ctrl;
+	u32 l2off = intr_ctrl->regoff + intr_ctrl->status.regoff;
+	u32 regoff = aie_aperture_cal_regoff(aperture, *loc, l2off);
+
+	iowrite32(bitmap_irq, aperture->base + regoff);
+}
+
+/**
+ * aie_aperture_get_l2_status() - get level 2 interrupt controller status value.
+ * @aperture: AIE aperture pointer.
+ * @loc: pointer to tile location.
+ * @return: status value.
+ */
+static u32 aie_aperture_get_l2_status(struct aie_aperture *aperture,
+				      struct aie_location *loc)
+{
+	const struct aie_l2_intr_ctrl_attr *intr_ctrl = aperture->adev->l2_ctrl;
+	u32 l2off = intr_ctrl->regoff + intr_ctrl->status.regoff;
+	u32 regoff = aie_aperture_cal_regoff(aperture, *loc, l2off);
+
+	return ioread32(aperture->base + regoff);
+}
+
+/**
+ * aie_aperture_get_l2_mask() - get level 2 interrupt controller mask value.
+ * @aperture: AIE aperture pointer.
+ * @loc: pointer to tile location.
+ * @return: mask value.
+ */
+static u32 aie_aperture_get_l2_mask(struct aie_aperture *aperture,
+				    struct aie_location *loc)
+{
+	const struct aie_l2_intr_ctrl_attr *intr_ctrl = aperture->adev->l2_ctrl;
+	u32 l2off = intr_ctrl->regoff + intr_ctrl->mask.regoff;
+	u32 regoff = aie_aperture_cal_regoff(aperture, *loc, l2off);
+
+	return ioread32(aperture->base + regoff);
+}
+
+/**
+ * aie_aperture_enable_l2_ctrl() - enable interrupts to level 2 interrupt
+ *				   controller.
+ * @aperture: AIE aperture pointer.
+ * @loc: pointer to tile location.
+ * @bit_map: bitmap of broadcast lines to enable.
+ */
+static void aie_aperture_enable_l2_ctrl(struct aie_aperture *aperture,
+					struct aie_location *loc, u32 bit_map)
+{
+	const struct aie_l2_intr_ctrl_attr *intr_ctrl = aperture->adev->l2_ctrl;
+	u32 l2off = intr_ctrl->regoff + intr_ctrl->enable.regoff;
+	u32 regoff = aie_aperture_cal_regoff(aperture, *loc, l2off);
+
+	bit_map &= intr_ctrl->enable.mask;
+	iowrite32(bit_map, aperture->base + regoff);
+}
+
+/**
+ * aie_aperture_disable_l2_ctrl() - disable interrupts to level 2 interrupt
+ *				    controller.
+ * @aperture: AIE aperture pointer.
+ * @loc: pointer to tile location.
+ * @bit_map: bitmap of broadcast lines to disable.
+ */
+static void aie_aperture_disable_l2_ctrl(struct aie_aperture *aperture,
+					 struct aie_location *loc, u32 bit_map)
+{
+	const struct aie_l2_intr_ctrl_attr *intr_ctrl = aperture->adev->l2_ctrl;
+	u32 l2off = intr_ctrl->regoff + intr_ctrl->disable.regoff;
+	u32 regoff = aie_aperture_cal_regoff(aperture, *loc, l2off);
+
+	bit_map &= intr_ctrl->disable.mask;
+	iowrite32(bit_map, aperture->base + regoff);
+}
+
+/**
+ * aie_part_set_event_bitmap() - set the status of event in local event
+ *				 bitmap.
+ * @apart: AIE partition pointer.
+ * @loc: tile location.
+ * @module: module type.
+ * @event: event ID to be logged.
+ */
+static void aie_part_set_event_bitmap(struct aie_partition *apart,
+				      struct aie_location loc,
+				      enum aie_module_type module, u8 event)
+{
+	u8 row, col, mod_num_events;
+	struct aie_resource *event_sts;
+	u32 offset;
+
+	if (module == AIE_CORE_MOD) {
+		event_sts = &apart->core_event_status;
+		mod_num_events = apart->adev->core_events->num_events;
+		row = loc.row - apart->range.start.row - 1;
+	} else if (module == AIE_MEM_MOD) {
+		event_sts = &apart->mem_event_status;
+		mod_num_events = apart->adev->mem_events->num_events;
+		row = loc.row - apart->range.start.row - 1;
+	} else {
+		event_sts = &apart->pl_event_status;
+		mod_num_events = apart->adev->pl_events->num_events;
+		row = loc.row;
+	}
+
+	col = loc.col - apart->range.start.col;
+
+	offset = (col + row * apart->range.size.col) * mod_num_events + event;
+	aie_resource_set(event_sts, offset, 1);
+}
+
+/**
+ * aie_check_error_bitmap() - check the status of event in local event bitmap.
+ * @apart: AIE partition pointer.
+ * @loc: tile location.
+ * @module: module type.
+ * @event: event ID to check.
+ * @return: true if event has happened, else false.
+ */
+bool aie_check_error_bitmap(struct aie_partition *apart,
+			    struct aie_location loc,
+			    enum aie_module_type module, u8 event)
+{
+	struct aie_resource *event_sts;
+	u32 offset;
+	u8 row, col, mod_num_events;
+
+	if (module == AIE_CORE_MOD) {
+		event_sts = &apart->core_event_status;
+		mod_num_events = apart->adev->core_events->num_events;
+		row = loc.row - apart->range.start.row - 1;
+	} else if (module == AIE_MEM_MOD) {
+		event_sts = &apart->mem_event_status;
+		mod_num_events = apart->adev->mem_events->num_events;
+		row = loc.row - apart->range.start.row - 1;
+	} else {
+		event_sts = &apart->pl_event_status;
+		mod_num_events = apart->adev->pl_events->num_events;
+		row = loc.row;
+	}
+
+	col = loc.col - apart->range.start.col;
+
+	offset = (col + row * apart->range.size.col) * mod_num_events + event;
+	return aie_resource_testbit(event_sts, offset);
+}
+
+/**
+ * aie_tile_backtrack() - if error was asserted on a broadcast line in
+ *			  the given array tile,
+ *				* disable the error from the group errors
+ *				* record the error event in local bitmap
+ * @apart: AIE partition pointer.
+ * @loc: tile location.
+ * @module: module type.
+ * @sw: switch type.
+ * @bc_id: broadcast ID.
+ * @status: tile status register.
+ * @return: true if error was asserted, else return false.
+ */
+static bool aie_tile_backtrack(struct aie_partition *apart,
+			       struct aie_location loc,
+			       enum aie_module_type module,
+			       enum aie_shim_switch_type sw, u8 bc_id,
+			       u32 *status)
+{
+	unsigned long grenabled;
+	u8 n, grevent, eevent;
+	bool ret = false;
+
+	if (module == AIE_PL_MOD)
+		grevent = aie_get_l1_event(apart, &loc, sw, bc_id);
+	else
+		grevent = aie_get_broadcast_event(apart, &loc, module, bc_id);
+
+	aie_read_event_status(apart, &loc, module, status);
+
+	if (!(status[grevent / 32] & BIT(grevent % 32)))
+		return ret;
+
+	grenabled = aie_check_group_errors_enabled(apart, &loc, module);
+	for_each_set_bit(n, &grenabled, 32) {
+		eevent = aie_get_error_event(apart, &loc, module, n);
+		if (!(status[eevent / 32] & BIT(eevent % 32)))
+			continue;
+		grenabled &= ~BIT(n);
+		aie_part_set_event_bitmap(apart, loc, module, eevent);
+		ret = true;
+
+		dev_err_ratelimited(&apart->adev->dev,
+				    "Asserted tile error event %d at col %d row %d\n",
+				    eevent, loc.col, loc.row);
+	}
+	aie_set_error_event(apart, &loc, module, grenabled);
+
+	return ret;
+}
+
+/**
+ * aie_map_l2_to_l1() - map the status bit set in level 2 interrupt controller
+ *		        to a level 1 interrupt controller.
+ * @apart: AIE partition pointer.
+ * @set_pos: position of level 2 set bit.
+ * @l2_col: level 2 interrupt controller column ID.
+ * @l1_col: pointer to return corresponding level 1 column ID.
+ * @sw: pointer to return the level 1 interrupt controller switch ID.
+ *
+ * This API implementation is tightly coupled with the level 2 to level 1
+ * static mapping created when AIE application CDOs are generated.
+ */
+static void aie_map_l2_to_l1(struct aie_partition *apart, u32 set_pos,
+			     u32 l2_col, u32 *l1_col,
+			     enum aie_shim_switch_type *sw)
+{
+	if (l2_col + 3 >= apart->range.start.col + apart->range.size.col) {
+		*l1_col = l2_col + (set_pos % 6) / 2;
+		*sw = (set_pos % 6) % 2;
+	} else if (l2_col % 2 == 0) {
+		/* set bit position could be 0 - 5 */
+		*l1_col = l2_col - (2 - (set_pos % 6) / 2);
+		*sw = (set_pos % 6) % 2;
+	} else {
+		/* set bit position could be 0 - 1 */
+		*l1_col = l2_col;
+		*sw = set_pos;
+	}
+}
+
+/**
+ * aie_l1_backtrack() - backtrack AIE array tiles or shim tile based on
+ *			the level 2 status bit set.
+ * @apart: AIE partition pointer.
+ * @loc: tile location of level 2 interrupt controller.
+ * @set_pos: set bit position in level 2 controller status.
+ * @return: true if error was asserted, else return false.
+ */
+static bool aie_l1_backtrack(struct aie_partition *apart,
+			     struct aie_location loc, u32 set_pos)
+{
+	u32 mem_srow, mem_erow, aie_srow, aie_erow;
+	enum aie_shim_switch_type sw;
+	struct aie_location l1_ctrl;
+	enum aie_module_type module;
+	bool ret = false;
+	u32 bc_event;
+	u32 status;
+
+	mem_srow = apart->adev->ttype_attr[AIE_TILE_TYPE_MEMORY].start_row;
+	mem_erow = mem_srow +
+		   apart->adev->ttype_attr[AIE_TILE_TYPE_MEMORY].num_rows;
+	aie_srow = apart->adev->ttype_attr[AIE_TILE_TYPE_TILE].start_row;
+	aie_erow = aie_srow +
+		   apart->adev->ttype_attr[AIE_TILE_TYPE_TILE].num_rows;
+
+	/*
+	 * Based on the set status bit find which level 1 interrupt
+	 * controller has generated an interrupt
+	 */
+	l1_ctrl.row = 0;
+	aie_map_l2_to_l1(apart, set_pos, loc.col, &l1_ctrl.col, &sw);
+	module = (sw == AIE_SHIM_SWITCH_A) ? AIE_CORE_MOD : AIE_MEM_MOD;
+	loc = l1_ctrl;
+
+	/*
+	 * This should not be the case if the routing is generated based on
+	 * the partition. In case, the routing is generated with different
+	 * partition which is not likely, if doesn't have this checking, it
+	 * can access the tiles outside the partition.
+	 */
+	if (l1_ctrl.col >= (apart->range.start.col + apart->range.size.col))
+		return false;
+
+	status = aie_get_l1_status(apart, &l1_ctrl, sw);
+
+	if (status & BIT(AIE_SHIM_TILE_ERROR_IRQ_ID)) {
+		u32 status[AIE_NUM_EVENT_STS_SHIMTILE] = {0};
+
+		aie_clear_l1_intr(apart, &l1_ctrl, sw,
+				  AIE_SHIM_TILE_ERROR_IRQ_ID);
+		if (aie_tile_backtrack(apart, l1_ctrl, AIE_PL_MOD, sw,
+				       AIE_SHIM_TILE_ERROR_IRQ_ID, status))
+			ret = true;
+	}
+
+	if (!(status & BIT(AIE_ARRAY_TILE_ERROR_BC_ID)))
+		return ret;
+
+	aie_clear_l1_intr(apart, &l1_ctrl, sw, AIE_ARRAY_TILE_ERROR_BC_ID);
+
+	if (sw != AIE_SHIM_SWITCH_A)
+		goto backtrack_aie_tile;
+
+	bc_event = aie_get_bc_event(apart, AIE_TILE_TYPE_MEMORY, AIE_MEM_MOD,
+				    AIE_ARRAY_TILE_ERROR_BC_ID);
+	for (loc.row = mem_srow; loc.row < mem_erow; loc.row++) {
+		u32 status[AIE_NUM_EVENT_STS_MEMTILE] = {0};
+
+		if (!aie_part_check_clk_enable_loc(apart, &loc))
+			continue;
+		ret |= aie_tile_backtrack(apart, loc, module, sw,
+					  AIE_ARRAY_TILE_ERROR_BC_ID, status);
+		aie_clear_event_status(apart, &loc, AIE_MEM_MOD, bc_event);
+	}
+
+backtrack_aie_tile:
+	bc_event = aie_get_bc_event(apart, AIE_TILE_TYPE_TILE, module,
+				    AIE_ARRAY_TILE_ERROR_BC_ID);
+	for (loc.row = aie_srow; loc.row < aie_erow; loc.row++) {
+		u32 status[AIE_NUM_EVENT_STS_CORETILE] = {0};
+
+		if (!aie_part_check_clk_enable_loc(apart, &loc))
+			continue;
+		ret |= aie_tile_backtrack(apart, loc, module, sw,
+					  AIE_ARRAY_TILE_ERROR_BC_ID, status);
+		if (!(status[bc_event / 32] & BIT(bc_event % 32)))
+			break;
+		aie_clear_event_status(apart, &loc, module, bc_event);
+	}
+
+	return ret;
+}
+
+/**
+ * aie_range_get_num_nocs() - get number of shim NOC tiles of AI enigne range
+ * @range: AI engine tiles range pointer
+ * @aperture: AI engine aperture pointer
+ * @l2_mask_off: return l2 mask start offset of the range
+ * @return: number of shim NOC tiles of the AI engine partition.
+ */
+static u32 aie_range_get_num_nocs(const struct aie_range *range,
+				  const struct aie_aperture *aperture,
+				  u32 *l2_mask_off)
+{
+	struct aie_location loc;
+	struct aie_device *adev = aperture->adev;
+	u32 num_nocs = 0;
+
+	for (loc.col = range->start.col, loc.row = 0;
+	     loc.col < range->start.col + range->size.col; loc.col++) {
+		u32 ttype;
+
+		ttype = adev->ops->get_tile_type(adev, &loc);
+		if (ttype != AIE_TILE_TYPE_SHIMNOC)
+			continue;
+		num_nocs++;
+	}
+
+	if (num_nocs && l2_mask_off) {
+		*l2_mask_off = 0;
+		for (loc.col = aperture->range.start.col, loc.row = 0;
+		     loc.col < range->start.col; loc.col++) {
+			u32 ttype;
+
+			ttype = adev->ops->get_tile_type(adev, &loc);
+			if (ttype != AIE_TILE_TYPE_SHIMNOC)
+				continue;
+			*l2_mask_off += 1;
+		}
+	}
+
+	return num_nocs;
+}
+
+/**
+ * aie_l2_backtrack() - iterate through each level 2 interrupt controller
+ *			in a given partition and backtrack its
+ *			corresponding level 1 interrupt controller.
+ * @apart: AIE partition pointer
+ */
+static void aie_l2_backtrack(struct aie_partition *apart)
+{
+	struct aie_aperture *aperture = apart->aperture;
+	u32 *aperture_l2_mask = aperture->l2_mask.val;
+	struct aie_location loc;
+	u32 l2_mask_index = 0;
+	u32 n, ttype, num_nocs;
+	int ret;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dev_err_ratelimited(&apart->dev,
+				    "Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return;
+	}
+
+	num_nocs = aie_range_get_num_nocs(&apart->range, aperture,
+					  &l2_mask_index);
+	if (!num_nocs) {
+		mutex_unlock(&apart->mlock);
+		return;
+	}
+
+	for (loc.col = apart->range.start.col, loc.row = 0;
+	     loc.col < apart->range.start.col + apart->range.size.col;
+	     loc.col++) {
+		unsigned long l2_mask;
+
+		ttype = apart->adev->ops->get_tile_type(apart->adev, &loc);
+		if (ttype != AIE_TILE_TYPE_SHIMNOC)
+			continue;
+		if (l2_mask_index >= aperture->l2_mask.count)
+			break;
+
+		l2_mask = aperture_l2_mask[l2_mask_index];
+		for_each_set_bit(n, &l2_mask,
+				 apart->adev->l2_ctrl->num_broadcasts) {
+			if (aie_l1_backtrack(apart, loc, n))
+				apart->error_to_report = 1;
+		}
+		aperture_l2_mask[l2_mask_index] = 0;
+		l2_mask_index++;
+		aie_aperture_enable_l2_ctrl(aperture, &loc, l2_mask);
+	}
+
+	mutex_unlock(&apart->mlock);
+
+	/*
+	 * If error was asserted or there are errors pending to be reported to
+	 * the application, then invoke callback.
+	 */
+	if (apart->error_cb.cb && apart->error_to_report) {
+		apart->error_to_report = 0;
+		apart->error_cb.cb(apart->error_cb.priv);
+	}
+}
+
+/**
+ * aie_part_backtrack() - backtrack a individual.
+ * @apart: AIE partition pointer.
+ */
+static void aie_part_backtrack(struct aie_partition *apart)
+{
+	aie_l2_backtrack(apart);
+}
+
+/**
+ * aie_aperture_backtrack() - backtrack each partition to find the source of
+ *			      error interrupt.
+ * @work: pointer to the work structure.
+ *
+ * This task will re-enable IRQ after errors in all partitions has been
+ * serviced.
+ */
+void aie_aperture_backtrack(struct work_struct *work)
+{
+	struct aie_aperture *aperture;
+	struct aie_partition *apart;
+	int ret;
+
+	aperture = container_of(work, struct aie_aperture, backtrack);
+
+	ret = mutex_lock_interruptible(&aperture->mlock);
+	if (ret) {
+		dev_err_ratelimited(&aperture->dev,
+				    "Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return;
+	}
+
+	list_for_each_entry(apart, &aperture->partitions, node) {
+		/*
+		 * If partition isn't requested yet, then only record the
+		 * occurrence of error interrupt. Such errors can only be
+		 * backtracked when the tiles in-use are known. Based on the
+		 * error_to_report value a task is scheduled in the workqueue
+		 * to backtrack this error interrupt when partition is
+		 * requested.
+		 */
+		if (!apart->status)
+			continue;
+		aie_part_backtrack(apart);
+	}
+
+	mutex_unlock(&aperture->mlock);
+}
+
+/**
+ * aie_interrupt() - interrupt handler for AIE.
+ * @irq: Interrupt number.
+ * @data: AI engine aperture structure.
+ * @return: IRQ_HANDLED.
+ *
+ * This thread function disables level 2 interrupt controllers and schedules a
+ * task in workqueue to backtrack the source of error interrupt. Disabled
+ * interrupts are re-enabled after successful completion of bottom half.
+ */
+irqreturn_t aie_interrupt(int irq, void *data)
+{
+	struct aie_aperture *aperture = data;
+	struct aie_device *adev = aperture->adev;
+	struct aie_location loc;
+	bool sched_work = false;
+	u32 *aperture_l2_mask = aperture->l2_mask.val;
+	int l2_mask_count = aperture->l2_mask.count;
+	int l2_mask_index = 0;
+
+	for (loc.col = aperture->range.start.col, loc.row = 0;
+	     loc.col < aperture->range.start.col + aperture->range.size.col;
+	     loc.col++) {
+		u32 ttype, l2_status, l2_mask;
+
+		ttype = adev->ops->get_tile_type(adev, &loc);
+		if (ttype != AIE_TILE_TYPE_SHIMNOC)
+			continue;
+
+		if (l2_mask_index >= l2_mask_count)
+			break;
+
+		l2_mask = aie_aperture_get_l2_mask(aperture, &loc);
+		if (l2_mask) {
+			aperture_l2_mask[l2_mask_index] = l2_mask;
+			aie_aperture_disable_l2_ctrl(aperture, &loc, l2_mask);
+		}
+
+		l2_status = aie_aperture_get_l2_status(aperture, &loc);
+		if (l2_status) {
+			aie_aperture_clear_l2_intr(aperture, &loc,
+						   l2_status);
+			sched_work = true;
+		} else {
+			aie_aperture_enable_l2_ctrl(aperture, &loc,
+						    l2_mask);
+		}
+		l2_mask_index++;
+	}
+
+	if (sched_work)
+		schedule_work(&aperture->backtrack);
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * aie_interrupt_callback() - S100/S200 callback.
+ * @payload: payload data.
+ * @data: AI engine aperture structure.
+ *
+ * This function calls aie_interrupt to disables level 2 interrupt controllers
+ * and schedules a task in workqueue to backtrack the source of error interrupt.
+ * Disabled interrupts are re-enabled after successful completion of bottom half.
+ */
+void aie_interrupt_callback(const u32 *payload, void *data)
+{
+	aie_interrupt(0, data);
+}
+
+/**
+ * aie_part_has_error() - check if AI engine partition has errors raised
+ * @apart: AIE partition pointer
+ * @return: true if AI engine partition has errors, false otherwise.
+ *
+ * This function checkes the aperture struct @l2_mask field, this field will
+ * be set when there is error interrupt generated from the SHIM NOC, and it
+ * will be cleared in the partition errors backtrack. And thus, if it is set
+ * it means there is error raised from the partition and backtrack is not done
+ * yet.
+ * It will request the aperture lock. Caller needs to make sure aperture lock
+ * is released before calling this function.
+ */
+bool aie_part_has_error(struct aie_partition *apart)
+{
+	struct aie_aperture *aperture = apart->aperture;
+	int ret;
+	u32 *l2_mask = aperture->l2_mask.val;
+	int i;
+
+	/*
+	 * TODO: errors backtracking is not support for AIEML. for now, return
+	 * false based on device generation.
+	 */
+	if (apart->adev->dev_gen == AIE_DEVICE_GEN_AIEML) {
+		dev_dbg(&apart->dev, "Skipping error backtracking.\n");
+		return false;
+	}
+
+	ret = mutex_lock_interruptible(&aperture->mlock);
+	if (ret) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return false;
+	}
+
+	for (i = 0; i < aperture->l2_mask.count; i++) {
+		if (l2_mask[i]) {
+			ret = true;
+			break;
+		}
+	}
+
+	mutex_unlock(&aperture->mlock);
+	return ret;
+}
+
+/**
+ * aie_aperture_create_l2_mask() - create bitmaps to record mask and status
+ *				     values for level 2 interrupt controllers.
+ * @aperture: AI engine aperture
+ * @return: 0 for success, and negative value for failure.
+ */
+int aie_aperture_create_l2_mask(struct aie_aperture *aperture)
+{
+	u32 num_nocs;
+
+	num_nocs = aie_range_get_num_nocs(&aperture->range, aperture, NULL);
+	if (!num_nocs)
+		return 0;
+
+	aperture->l2_mask.val = kcalloc(num_nocs, sizeof(*aperture->l2_mask.val),
+					GFP_KERNEL);
+	aperture->l2_mask.count = num_nocs;
+	if (!aperture->l2_mask.val)
+		return -ENOMEM;
+	return 0;
+}
+
+/**
+ * aie_get_module_error_count() - get the total count of errors in a module
+ *				  from local bitmap.
+ * @apart: AIE partition pointer.
+ * @loc: tile location.
+ * @module: module type.
+ * @err_attr: error attribute for given module type.
+ * @return: total number of errors found.
+ */
+u32 aie_get_module_error_count(struct aie_partition *apart,
+			       struct aie_location loc,
+			       enum aie_module_type module,
+			       const struct aie_error_attr *err_attr)
+{
+	u32 count = 0;
+	u8 i, j;
+
+	for (i = 0; i < err_attr->num_err_categories; i++) {
+		for (j = 0; j < err_attr->err_category[i].num_events; j++) {
+			u8 event = err_attr->err_category[i].prop[j].event;
+
+			if (aie_check_error_bitmap(apart, loc, module, event))
+				count++;
+		}
+	}
+	return count;
+}
+
+/**
+ * aie_check_module_error() - check if a given module has an active error.
+ * @apart: AIE partition pointer.
+ * @loc: tile location.
+ * @module: module type.
+ * @err_attr: error attribute for given module type.
+ * @return: true if tile has active errors.
+ */
+static bool aie_check_module_error(struct aie_partition *apart,
+				   struct aie_location loc,
+				   enum aie_module_type module,
+				   const struct aie_error_attr *err_attr)
+{
+	u8 i, j;
+
+	for (i = 0; i < err_attr->num_err_categories; i++) {
+		for (j = 0; j < err_attr->err_category[i].num_events; j++) {
+			u8 event = err_attr->err_category[i].prop[j].event;
+
+			if (aie_check_error_bitmap(apart, loc, module, event))
+				return true;
+		}
+	}
+	return false;
+}
+
+/**
+ * aie_check_tile_error() - check if a given tile location has an active error.
+ * @apart: AIE partition pointer.
+ * @loc: tile location.
+ * @return: true if tile has active errors.
+ */
+bool aie_check_tile_error(struct aie_partition *apart, struct aie_location loc)
+{
+	const struct aie_error_attr *core_errs = apart->adev->core_errors;
+	const struct aie_error_attr *mem_errs = apart->adev->mem_errors;
+	const struct aie_error_attr *shim_errs = apart->adev->shim_errors;
+	u32 ttype = apart->adev->ops->get_tile_type(apart->adev, &loc);
+
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		if (aie_check_module_error(apart, loc, AIE_CORE_MOD, core_errs))
+			return true;
+
+		if (aie_check_module_error(apart, loc, AIE_MEM_MOD, mem_errs))
+			return true;
+	} else {
+		if (aie_check_module_error(apart, loc, AIE_PL_MOD, shim_errs))
+			return true;
+	}
+	return false;
+}
+
+/**
+ * aie_get_error_count() - get the total count of errors in a partition from
+ *			   local bitmap.
+ * @apart: AIE partition pointer.
+ * @return: total number of errors found.
+ */
+u32 aie_get_error_count(struct aie_partition *apart)
+{
+	const struct aie_error_attr *core_errs = apart->adev->core_errors;
+	const struct aie_error_attr *mem_errs = apart->adev->mem_errors;
+	const struct aie_error_attr *memtile_errs = apart->adev->memtile_errors;
+	const struct aie_error_attr *shim_errs = apart->adev->shim_errors;
+	struct aie_location loc;
+	u32 ttype, num = 0;
+
+	for (loc.col = apart->range.start.col;
+	     loc.col < apart->range.start.col + apart->range.size.col;
+	     loc.col++) {
+		for (loc.row = apart->range.start.row;
+		     loc.row < apart->range.size.row; loc.row++) {
+			ttype = apart->adev->ops->get_tile_type(apart->adev,
+								&loc);
+			if (ttype == AIE_TILE_TYPE_TILE) {
+				num += aie_get_module_error_count(apart, loc,
+								  AIE_CORE_MOD,
+								  core_errs);
+				num += aie_get_module_error_count(apart, loc,
+								  AIE_MEM_MOD,
+								  mem_errs);
+			} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+				num += aie_get_module_error_count(apart, loc,
+								  AIE_MEM_MOD,
+								  memtile_errs);
+			} else {
+				num += aie_get_module_error_count(apart, loc,
+								  AIE_PL_MOD,
+								  shim_errs);
+			}
+		}
+	}
+
+	return num;
+}
+
+/**
+ * aie_get_errors_from_bitmap() - get status of errors from local bitmap.
+ * @apart: AIE partition pointer.
+ * @loc: tile location.
+ * @module: module type.
+ * @err_attr: error attribute for given module type.
+ * @aie_err: pointer to array of error structure.
+ * @return: total number of errors found.
+ *
+ * This function parses local bitmaps and initializes @aie_err structures with
+ * the tile location of error event, module type and, its event ID.
+ */
+static u32 aie_get_errors_from_bitmap(struct aie_partition *apart,
+				      struct aie_location loc,
+				      enum aie_module_type module,
+				      const struct aie_error_attr *err_attr,
+				      struct aie_error *aie_err)
+{
+	u8 i, j;
+	u32 num_err = 0;
+
+	for (i = 0; i < err_attr->num_err_categories; i++) {
+		const struct aie_err_category *category;
+
+		category = &err_attr->err_category[i];
+		for (j = 0; j < category->num_events; j++) {
+			u8 event = category->prop[j].event;
+
+			if (!aie_check_error_bitmap(apart, loc, module, event))
+				continue;
+
+			aie_err[num_err].loc.col = loc.col;
+			aie_err[num_err].loc.row = loc.row;
+			aie_err[num_err].module = module;
+			aie_err[num_err].error_id = event;
+			aie_err[num_err].category = category->err_category;
+			num_err++;
+		}
+	}
+	return num_err;
+}
+
+/**
+ * aie_get_module_errors() - get errors for a given module type
+ *			     in a partition.
+ * @apart: AIE partition pointer.
+ * @module: module type.
+ * @aie_err: pointer to array of error structure.
+ * @return: total number of errors found.
+ *
+ * This function parses local bitmaps and initializes @aie_err structures.
+ */
+static u32 aie_get_module_errors(struct aie_partition *apart,
+				 enum aie_module_type module,
+				 struct aie_error *aie_err)
+{
+	const struct aie_error_attr *err_attr;
+	struct aie_location loc;
+	u32 srow, erow, scol, ecol, num_err = 0;
+
+	if (module == AIE_CORE_MOD) {
+		err_attr = apart->adev->core_errors;
+		srow = apart->range.start.row + 1;
+		erow = srow + apart->range.size.row - 1;
+	} else if (module == AIE_MEM_MOD) {
+		err_attr = apart->adev->mem_errors;
+		srow = apart->range.start.row + 1;
+		erow = srow + apart->range.size.row - 1;
+	} else {
+		err_attr = apart->adev->shim_errors;
+		srow = 0;
+		erow = 0;
+	}
+
+	scol = apart->range.start.col;
+	ecol = apart->range.start.col + apart->range.size.col - 1;
+
+	for (loc.col = scol; loc.col <= ecol; loc.col++) {
+		for (loc.row = srow; loc.row <= erow; loc.row++) {
+			num_err +=
+				aie_get_errors_from_bitmap(apart, loc,
+							   module, err_attr,
+							   &aie_err[num_err]);
+		}
+	}
+	return num_err;
+}
+
+/**
+ * aie_part_clear_cached_events() - clear cached events in a partition.
+ * @apart: AIE partition pointer
+ *
+ * This function will clear the cached events in a partition.
+ */
+void aie_part_clear_cached_events(struct aie_partition *apart)
+{
+	aie_resource_clear_all(&apart->core_event_status);
+	aie_resource_clear_all(&apart->mem_event_status);
+	aie_resource_clear_all(&apart->pl_event_status);
+}
+
+/**
+ * aie_part_set_intr_rscs() - set broadcast resources used by interrupt
+ * @apart: AIE partition pointer
+ * @return: 0 for sueccess, and negative value for failure
+ *
+ * This function reserves interrupt broadcast channels resources.
+ */
+int aie_part_set_intr_rscs(struct aie_partition *apart)
+{
+	u32 c, r;
+	int ret;
+
+	for (c = 0; c < apart->range.size.col; c++) {
+		u32 b;
+		struct aie_location l = {
+			.col = apart->range.start.col + c,
+			.row = 0,
+		};
+
+		/* reserved broadcast channels 0 - 5 for SHIM */
+		for (b = 0; b <= AIE_SHIM_INTR_BC_MAX; b++) {
+			ret = aie_part_rscmgr_set_tile_broadcast(apart, l,
+								 AIE_PL_MOD,
+								 b);
+			if (ret)
+				return ret;
+		}
+
+		for (r = 1; r < apart->range.size.row; r++) {
+			struct aie_device *adev = apart->adev;
+			struct aie_tile_attr *tattr;
+			u32 m, ttype;
+
+			b = AIE_ARRAY_TILE_ERROR_BC_ID;
+			l.row = apart->range.start.row + r;
+			ttype = adev->ops->get_tile_type(apart->adev, &l);
+
+			if (WARN_ON(ttype >= AIE_TILE_TYPE_MAX))
+				return -EINVAL;
+
+			tattr = &adev->ttype_attr[ttype];
+			for (m = 0; m < tattr->num_mods; m++) {
+				enum aie_module_type mod = tattr->mods[m];
+
+				ret = aie_part_rscmgr_set_tile_broadcast(apart,
+									 l, mod,
+									 b);
+				if (ret)
+					return ret;
+			}
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * aie_register_error_notification() - register a callback for error
+ *				       notification.
+ * @dev: AIE partition device.
+ * @cb: pointer to a function that accepts pointer to private data as an
+ *	argument. callbacks are called in the bottom half without locks.
+ * @priv: private data to be passed to the callback.
+ * @return: 0 for success, and negative value for failure.
+ */
+int aie_register_error_notification(struct device *dev,
+				    void (*cb)(void *priv), void *priv)
+{
+	struct aie_partition *apart;
+	int ret;
+
+	if (!cb || !dev)
+		return -EINVAL;
+
+	apart = container_of(dev, struct aie_partition, dev);
+	if (apart->adev->dev_gen == AIE_DEVICE_GEN_AIEML) {
+		dev_err(&apart->dev,
+			"error handling is not supported for aieml device\n");
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return ret;
+	}
+
+	if (apart->error_cb.cb) {
+		dev_err(&apart->dev,
+			"Error callback already registered. Unregister the existing callback to register a new one.\n");
+		ret = -EINVAL;
+		goto exit;
+	}
+
+	apart->error_cb.cb = cb;
+	apart->error_cb.priv = priv;
+
+	/*
+	 * Errors during configuration are logged even for the partitions
+	 * which are not requested. Such errors must be reported back to the
+	 * application when a valid callback is registered.
+	 */
+	if (apart->error_to_report) {
+		mutex_unlock(&apart->mlock);
+		schedule_work(&apart->aperture->backtrack);
+		return ret;
+	}
+
+exit:
+	mutex_unlock(&apart->mlock);
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aie_register_error_notification);
+
+/**
+ * aie_unregister_error_notification() - Unregister the callback for error
+ *					 notification.
+ * @dev: AIE partition device.
+ * @return: 0 for success, and negative value for failure.
+ */
+int aie_unregister_error_notification(struct device *dev)
+{
+	struct aie_partition *apart;
+	int ret;
+
+	if (!dev)
+		return -EINVAL;
+
+	apart = container_of(dev, struct aie_partition, dev);
+	if (apart->adev->dev_gen == AIE_DEVICE_GEN_AIEML) {
+		dev_err(&apart->dev,
+			"error handling is not supported for aieml device\n");
+		return -EINVAL;
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return ret;
+	}
+
+	apart->error_cb.cb = NULL;
+	apart->error_cb.priv = NULL;
+
+	mutex_unlock(&apart->mlock);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(aie_unregister_error_notification);
+
+/**
+ * aie_get_errors() - get errors that has happened.
+ * @dev: AIE partition device.
+ * @return: struct pointer of type aie_errors.
+ *
+ * This API allocates and initializes data structures by parsing local
+ * bitmaps. Allocated data structure must be deallocated using
+ * aie_free_errors() function.
+ */
+struct aie_errors *aie_get_errors(struct device *dev)
+{
+	struct aie_partition *apart;
+	struct aie_errors *aie_errs;
+	struct aie_error *error;
+	u32 num_errs, count = 0;
+	int ret;
+
+	if (!dev)
+		return ERR_PTR(-EINVAL);
+
+	apart = container_of(dev, struct aie_partition, dev);
+	if (apart->adev->dev_gen == AIE_DEVICE_GEN_AIEML) {
+		dev_err(&apart->dev,
+			"error handling is not supported for aieml device\n");
+		return NULL;
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return ERR_PTR(ret);
+	}
+
+	num_errs = aie_get_error_count(apart);
+	if (!num_errs) {
+		mutex_unlock(&apart->mlock);
+		return NULL;
+	}
+
+	aie_errs = kzalloc(sizeof(*aie_errs), GFP_KERNEL);
+	if (!aie_errs) {
+		mutex_unlock(&apart->mlock);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	error = kcalloc(num_errs, sizeof(*error), GFP_KERNEL);
+	if (!error) {
+		kfree(aie_errs);
+		mutex_unlock(&apart->mlock);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	count += aie_get_module_errors(apart, AIE_MEM_MOD, &error[count]);
+	count += aie_get_module_errors(apart, AIE_CORE_MOD, &error[count]);
+	count += aie_get_module_errors(apart, AIE_PL_MOD, &error[count]);
+
+	aie_errs->dev = dev;
+	aie_errs->errors = error;
+	aie_errs->num_err = count;
+
+	mutex_unlock(&apart->mlock);
+	return aie_errs;
+}
+EXPORT_SYMBOL_GPL(aie_get_errors);
+
+/**
+ * aie_get_error_categories() - Get the error categories. Error information
+ *				returned by aie_get_errors() could be
+ *				abstracted by classifying errors into various
+ *				categories. All DMA channel error are
+ *				classified as AIE_ERROR_CATEGORY_DMA, program
+ *				and data memory ECC errors are classified as
+ *				AIE_ERROR_CATEGORY_ECC, and so on.
+ * @aie_errs: AIE errors structure.
+ * @return: Bitmap of category of error events.
+ */
+u32 aie_get_error_categories(struct aie_errors *aie_errs)
+{
+	u32 e, ret = 0;
+
+	if (!aie_errs || !aie_errs->errors)
+		return 0;
+
+	for (e = 0; e < aie_errs->num_err; e++) {
+		struct aie_error *error = &aie_errs->errors[e];
+
+		ret |= BIT(error->category);
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(aie_get_error_categories);
+
+/**
+ * aie_get_error_string() - get error string corresponding an error.
+ * @aie_errs: pointer to an array of error structure.
+ * @aie_err: AIE error.
+ * @return: string corresponding to @aie_err.
+ */
+const char *aie_get_error_string(struct aie_errors *aie_errs,
+				 struct aie_error *aie_err)
+{
+	struct aie_partition *apart;
+	const struct aie_error_attr *err_attr;
+	u8 i, j;
+	int ret;
+
+	if (!aie_errs || !aie_errs->dev || !aie_err)
+		return ERR_PTR(-EINVAL);
+
+	apart = container_of(aie_errs->dev, struct aie_partition, dev);
+	if (apart->adev->dev_gen == AIE_DEVICE_GEN_AIEML) {
+		dev_err(&apart->dev,
+			"error handling is not supported for aieml device\n");
+		return NULL;
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return ERR_PTR(ret);
+	}
+
+	if (aie_err->module == AIE_CORE_MOD)
+		err_attr = apart->adev->core_errors;
+	else if (aie_err->module == AIE_MEM_MOD)
+		err_attr = apart->adev->mem_errors;
+	else
+		err_attr = apart->adev->shim_errors;
+
+	for (i = 0; i < err_attr->num_err_categories; i++) {
+		for (j = 0; j < err_attr->err_category[i].num_events; j++) {
+			u8 event = err_attr->err_category[i].prop[j].event;
+
+			if (event != aie_err->error_id)
+				continue;
+
+			mutex_unlock(&apart->mlock);
+			return err_attr->err_category[i].prop[j].event_str;
+		}
+	}
+
+	mutex_unlock(&apart->mlock);
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(aie_get_error_string);
+
+/**
+ * aie_flush_errors() - Flush all pending errors.
+ * @dev: AIE partition device.
+ * @return: 0 for success, and negative value for failure.
+ *
+ * This function backtracks a given partition, updates local event status
+ * bitmaps and, invokes the registered callback function for any error event.
+ */
+int aie_flush_errors(struct device *dev)
+{
+	struct aie_partition *apart;
+
+	if (!dev)
+		return -EINVAL;
+
+	apart = container_of(dev, struct aie_partition, dev);
+	if (apart->adev->dev_gen == AIE_DEVICE_GEN_AIEML) {
+		dev_err(&apart->dev,
+			"error handling is not supported for aieml device\n");
+		return -EINVAL;
+	}
+	aie_part_backtrack(apart);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(aie_flush_errors);
+
+/**
+ * aie_free_errors() - Free allocated AIE error structure.
+ * @aie_errs: AIE error structure.
+ */
+void aie_free_errors(struct aie_errors *aie_errs)
+{
+	if (!aie_errs || !aie_errs->errors)
+		return;
+
+	kfree(aie_errs->errors);
+	kfree(aie_errs);
+}
+EXPORT_SYMBOL_GPL(aie_free_errors);
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-mem.c b/drivers/misc/xilinx-ai-engine/ai-engine-mem.c
new file mode 100644
index 000000000..3ed6b6714
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-mem.c
@@ -0,0 +1,303 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device memory implementation
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include <linux/dma-buf.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/uaccess.h>
+#include <uapi/linux/xlnx-ai-engine.h>
+
+#include "ai-engine-internal.h"
+
+#define aie_cal_reg_goffset(adev, loc, regoff) ({ \
+	struct aie_device *_adev = (adev); \
+	struct aie_location *_loc = &(loc); \
+	(_loc->col << _adev->col_shift) + \
+	(_loc->row << _adev->row_shift) + (regoff); \
+	})
+
+#define aie_cal_reg_pa(aperture, rloc, regoff) ({ \
+	struct aie_aperture *__aperture = (aperture); \
+	__aperture->res.start + aie_cal_reg_goffset(__aperture->adev, rloc, \
+						    regoff); \
+	})
+
+static struct sg_table *
+aie_mem_map_dma_buf(struct dma_buf_attachment *attachment,
+		    enum dma_data_direction direction)
+{
+	/*
+	 * TODO: It is mandatory by DMA buf operation. It is used return
+	 * scatterlist table of an attachment. We don't have the implementation
+	 * for now. And thus it has empty implementation.
+	 */
+	(void)attachment;
+	(void)direction;
+	dev_warn(attachment->dev,
+		 "AI engine memory map dma buf is not implemented.\n");
+	return NULL;
+}
+
+static void aie_mem_unmap_dma_buf(struct dma_buf_attachment *attachment,
+				  struct sg_table *table,
+				  enum dma_data_direction direction)
+{
+	/*
+	 * TODO: It is mandatory by DMA buf operation. It is used deallocate
+	 * scatterlist table of an attachment. We don't have the implementation
+	 * for now. And thus it has empty implementation.
+	 */
+	(void)attachment;
+	(void)table;
+	(void)direction;
+	dev_warn(attachment->dev,
+		 "AI engine memory unmap dma buf is not implemented.\n");
+}
+
+static int aie_mem_mmap(struct dma_buf *dmabuf, struct vm_area_struct *vma)
+{
+	struct aie_part_mem *pmem = dmabuf->priv;
+	struct aie_mem *mem = &pmem->mem;
+	struct aie_partition *apart = pmem->apart;
+	struct aie_aperture *aperture = apart->aperture;
+	struct aie_location rloc;
+	unsigned long addr = vma->vm_start;
+	unsigned long offset = vma->vm_pgoff * PAGE_SIZE, moffset = 0;
+	unsigned long remainder = vma->vm_end - addr;
+	size_t msize = mem->size;
+	u32 rstart_col = mem->range.start.col - aperture->range.start.col;
+
+	if (remainder + offset > pmem->size)
+		return -EINVAL;
+
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+	for (rloc.col = rstart_col;
+	     rloc.col < rstart_col + mem->range.size.col; rloc.col++) {
+		for (rloc.row = mem->range.start.row;
+		     rloc.row < mem->range.start.row + mem->range.size.row;
+		     rloc.row++) {
+			unsigned long toffset, len;
+			phys_addr_t mempa;
+			int ret;
+
+			remainder = vma->vm_end - addr;
+			if (!remainder)
+				return 0;
+
+			if (moffset + msize < offset) {
+				moffset += msize;
+				continue;
+			}
+			/*
+			 * calculate offset within the tile memory.
+			 * offset is the offset to vma->start.
+			 * moffset is the tile memory start offset to
+			 * vma->start.
+			 */
+			toffset = offset - moffset;
+			len = msize - toffset;
+			if (len > remainder)
+				len = remainder;
+			mempa = aie_cal_reg_pa(apart->aperture, rloc,
+					       toffset + mem->offset);
+
+			ret = remap_pfn_range(vma, addr, mempa >> PAGE_SHIFT,
+					      len, vma->vm_page_prot);
+			if (ret) {
+				dev_err(&apart->dev,
+					"failed to mmap (%u,%u)memory, remap failed, 0x%pa, 0x%lx.\n",
+					(rloc.col + aperture->range.start.col),
+					rloc.row, &mempa, len);
+				return ret;
+			}
+			addr += len;
+			offset += len;
+			moffset += msize;
+		}
+	}
+	return 0;
+}
+
+static void aie_mem_dmabuf_release(struct dma_buf *dmabuf)
+{
+	struct aie_part_mem *pmem = dmabuf->priv;
+
+	pmem->dbuf = NULL;
+}
+
+static const struct dma_buf_ops aie_mem_dma_buf_ops = {
+	.map_dma_buf = aie_mem_map_dma_buf,
+	.unmap_dma_buf = aie_mem_unmap_dma_buf,
+	.mmap = aie_mem_mmap,
+	.release = aie_mem_dmabuf_release,
+};
+
+/**
+ * aie_mem_create_dmabuf() - creates DMA buffer for AI engine partition
+ *			     memories
+ * @apart: AI engine partition
+ * @pmem: pointer to the partition memory information
+ * @mem: pointer to where it store the memory information and DMA buf file
+ *	 descriptor for user.
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will create DMA buffer for the AI engine partition memory
+ * and will store the DMA buffer file descriptor and memory information in
+ * @mem.
+ */
+static int aie_mem_create_dmabuf(struct aie_partition *apart,
+				 struct aie_part_mem *pmem,
+				 struct aie_mem *mem)
+{
+	struct dma_buf *dmabuf;
+	int ret;
+
+	if (!PAGE_ALIGNED(pmem->mem.size)) {
+		dev_warn(&apart->dev,
+			 "no dmabuf for mem(0x%zx, 0x%zx), not aligned with page size.\n",
+			 pmem->mem.offset, pmem->mem.size);
+		return -EINVAL;
+	}
+
+	dmabuf = pmem->dbuf;
+	if (!dmabuf) {
+		DEFINE_DMA_BUF_EXPORT_INFO(exp_info);
+
+		exp_info.ops = &aie_mem_dma_buf_ops;
+		exp_info.size = pmem->size;
+		exp_info.flags = O_RDWR;
+		exp_info.priv = pmem;
+
+		dmabuf = dma_buf_export(&exp_info);
+		if (IS_ERR(dmabuf))
+			return PTR_ERR(dmabuf);
+
+		pmem->dbuf = dmabuf;
+	}
+
+	ret = dma_buf_fd(dmabuf, O_CLOEXEC);
+	if (ret < 0) {
+		dev_err(&apart->dev,
+			"dmabuf creation failed, failed to get fd.\n");
+		return ret;
+	}
+	memcpy(mem, &pmem->mem, sizeof(*mem));
+	mem->fd = ret;
+
+	return 0;
+}
+
+/**
+ * aie_mem_get_info() - get AI engine memories information
+ * @apart: AI engine partition
+ * @arg: argument from user to enquire AI engine partition memory information
+ * @return: 0 for success, and negative value for failure
+ *
+ * This function will get the memories information for the specified AI engine
+ * partition. It will create DMA buf file descriptors for the memories and
+ * return the DMA buf file descriptors to users.
+ * It will create a DMA buffer per type of memories.
+ * e.g. There will be a DMA buffer for all the tile program memories in the
+ * partition, and another DMA buffer for all the tile data memories in the
+ * partition.
+ * User can first pass num_mems as 0 in the @arg to enquire for how many types
+ * of memories in this AI engine partition. And then, user can allocate memory
+ * to keep the information for different types of memories, and then use the
+ * same enqury with non-zero num_mems and none NULL pointer to ask for the
+ * details of the information of all the types of memories in the AI engine
+ * partition.
+ */
+int aie_mem_get_info(struct aie_partition *apart, unsigned long arg)
+{
+	struct aie_mem_args margs;
+	struct aie_mem *mems;
+	unsigned int num_mems, i;
+	int ret;
+
+	if (copy_from_user(&margs, (void __user *)arg, sizeof(margs)))
+		return -EFAULT;
+
+	num_mems = apart->adev->ops->get_mem_info(apart->adev, &apart->range,
+						  NULL);
+	if (num_mems <= 0)
+		return -EINVAL;
+
+	if (!margs.num_mems) {
+		struct aie_mem_args __user *umargs_ptr = (void __user *)arg;
+
+		/* This enquiry is to get the number of types of memories. */
+		if (copy_to_user((void __user *)&umargs_ptr->num_mems,
+				 &num_mems, sizeof(num_mems)))
+			return -EFAULT;
+		return 0;
+	}
+
+	if (num_mems != margs.num_mems) {
+		dev_err(&apart->dev,
+			"failed to get mem info, invalid num of mems %d,%d.\n",
+			num_mems, margs.num_mems);
+		return -EINVAL;
+	}
+	if (!margs.mems) {
+		dev_err(&apart->dev,
+			"failed to get mem info, mems pointer is NULL.\n");
+		return -EINVAL;
+	}
+
+	mems = kcalloc(num_mems, sizeof(*mems), GFP_KERNEL);
+	if (!mems)
+		return -ENOMEM;
+
+	/*
+	 * Create DMA buffer for the memories.
+	 * Each type of memory in the partition has its own DMA buf.
+	 */
+	for (i = 0; i < num_mems; i++) {
+		ret = aie_mem_create_dmabuf(apart, &apart->pmems[i], &mems[i]);
+		if (ret)
+			break;
+	}
+	if (!ret) {
+		if (copy_to_user((void __user *)margs.mems, mems,
+				 num_mems * sizeof(mems[0])))
+			ret = -EFAULT;
+	}
+
+	if (ret) {
+		for (i = 0; i < num_mems; i++) {
+			if (mems[i].fd)
+				put_unused_fd(mems[i].fd);
+		}
+	}
+
+	kfree(mems);
+	return ret;
+}
+
+/**
+ * aie_part_has_mem_mmapped() - check if memories in the partition are mapped
+ * @apart: AI engine partition
+ * @return: return true if there are memories mmaped, false otherwise.
+ *
+ * This function checks if there are memories in the partition mmapped in the
+ * partition.
+ */
+bool aie_part_has_mem_mmapped(struct aie_partition *apart)
+{
+	unsigned int num_mems, i;
+
+	num_mems = apart->adev->ops->get_mem_info(apart->adev, &apart->range,
+			NULL);
+	if (!num_mems)
+		return false;
+
+	for (i = 0; i < num_mems; i++) {
+		if (apart->pmems[i].dbuf)
+			return true;
+	}
+	return false;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-overlay.c b/drivers/misc/xilinx-ai-engine/ai-engine-overlay.c
new file mode 100644
index 000000000..03f022ec7
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-overlay.c
@@ -0,0 +1,124 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/xlnx-ai-engine.h>
+#include <uapi/linux/xlnx-ai-engine.h>
+
+#include "ai-engine-internal.h"
+
+/**
+ * of_aie_notify_pre_remove() - pre-remove overlay notification
+ *
+ * @adev: AI engine device
+ * @nd: overlay notification data
+ * @return: returns 0 for success, and error code for failure.
+ *
+ * Called when an overlay targeted to an AI engine device is about to be
+ * removed. Remove AI engine apertures specified in the device tree overlay to
+ * the target AI engine device.
+ */
+static int of_aie_notify_pre_remove(struct aie_device *adev,
+				    struct of_overlay_notify_data *nd)
+{
+	struct device_node *nc;
+
+	for_each_available_child_of_node(nd->overlay, nc) {
+		struct list_head *node, *pos;
+		int ret;
+
+		if (!of_node_test_and_set_flag(nc, OF_POPULATED))
+			continue;
+
+		ret = mutex_lock_interruptible(&adev->mlock);
+		if (ret)
+			return ret;
+
+		list_for_each_safe(pos, node, &adev->apertures) {
+			struct aie_aperture *aperture;
+
+			aperture = list_entry(pos, struct aie_aperture, node);
+			ret = aie_aperture_remove(aperture);
+			if (ret) {
+				mutex_unlock(&adev->mlock);
+				return ret;
+			}
+		}
+
+		mutex_unlock(&adev->mlock);
+	}
+
+	return 0;
+}
+
+/**
+ * of_aie_notify() - AI engine notifier for dynamic DT changes
+ * @nb: notifier block
+ * @action: notifier action
+ * @arg: reconfig data
+ * @return: NOTIFY_OK for success, error code for failure
+ *
+ * This notifier handles AI engine device node overlay.
+ */
+static int of_aie_notify(struct notifier_block *nb, unsigned long action,
+			 void *arg)
+{
+	struct of_overlay_notify_data *nd = arg;
+	struct aie_device *adev;
+	int ret = 0;
+
+	switch (action) {
+	case OF_OVERLAY_POST_APPLY:
+		adev = of_ai_engine_class_find(nd->target);
+		if (!adev) {
+			pr_debug("%s OF_OVERLAY_POST_APPLY\n", __func__);
+			return NOTIFY_OK;
+		}
+
+		of_xilinx_ai_engine_aperture_probe(adev);
+		break;
+	case OF_OVERLAY_PRE_REMOVE:
+		adev = of_ai_engine_class_find(nd->target);
+		if (!adev) {
+			pr_debug("%s OF_OVERLAY_POST_APPLY\n", __func__);
+			return NOTIFY_OK;
+		}
+
+		ret = of_aie_notify_pre_remove(adev, nd);
+		break;
+	default:
+		return NOTIFY_OK;
+	}
+
+	if (ret)
+		return notifier_from_errno(ret);
+
+	return NOTIFY_OK;
+}
+
+static struct notifier_block aie_of_nb = {
+	.notifier_call = of_aie_notify,
+};
+
+/**
+ * aie_overlay_register_notifier() - register AI engine device tree overlay
+ *				     notifier
+ * @return: 0 for success, error code for failure
+ */
+int aie_overlay_register_notifier(void)
+{
+	return of_overlay_notifier_register(&aie_of_nb);
+}
+
+/**
+ * aie_overlay_unregister_notifier() - unregister AI engine device tree overlay
+ *				       notifier
+ */
+void aie_overlay_unregister_notifier(void)
+{
+	of_overlay_notifier_unregister(&aie_of_nb);
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-part.c b/drivers/misc/xilinx-ai-engine/ai-engine-part.c
new file mode 100644
index 000000000..03fc951fc
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-part.c
@@ -0,0 +1,1388 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine partition driver
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include <linux/cdev.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/dma-map-ops.h>
+#include <linux/fs.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/mm.h>
+#include <linux/mman.h>
+#include <linux/mmu_context.h>
+#include <linux/mutex.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/uio.h>
+#include <linux/timer.h>
+#include <linux/time.h>
+#include <uapi/linux/xlnx-ai-engine.h>
+#include <asm/siginfo.h>
+
+#include "ai-engine-internal.h"
+
+#define AIE_CORE_STS_ENABLE_MASK 0x1U
+
+/**
+ * aie_cal_loc() - calculate tile location from register offset to the AI
+ *		   engine device
+ * @adev: AI engine device
+ * @loc: memory pointer to restore returning location information
+ * @regoff: tile internal register offset
+ *
+ * This function returns the tile location.
+ */
+static void aie_cal_loc(struct aie_device *adev,
+			struct aie_location *loc, u64 regoff)
+{
+	loc->col = (u32)aie_tile_reg_field_get(aie_col_mask(adev),
+					       adev->col_shift, regoff);
+	loc->row = (u32)aie_tile_reg_field_get(aie_row_mask(adev),
+					       adev->row_shift, regoff);
+}
+
+/**
+ * aie_part_reg_validation() - validate AI engine partition register access
+ * @apart: AI engine partition
+ * @offset: AI engine register offset relative in partition.
+ * @len: len of data to write/read
+ * @is_write: is the access to write to register
+ * @return: 0 for success, or negative value for failure.
+ *
+ * This function validate if the register to access is within the AI engine
+ * partition. If it is write access, if the register is writable by user.
+ */
+static int aie_part_reg_validation(struct aie_partition *apart, size_t offset,
+				   size_t len, u8 is_write)
+{
+	struct aie_device *adev;
+	u32 regend32, ttype;
+	u64 regoff, regend64;
+	struct aie_location loc, aloc;
+	unsigned int i, num_mems;
+	struct aie_part_mem *pmem = apart->pmems;
+
+	adev = apart->adev;
+	if (offset % sizeof(u32)) {
+		dev_err(&apart->dev,
+			"Invalid reg off(0x%zx), not 32bit aligned.\n",
+			offset);
+		return -EINVAL;
+	}
+
+	if (len % sizeof(u32)) {
+		dev_err(&apart->dev, "Invalid reg operation len %zu.\n", len);
+		return -EINVAL;
+	}
+
+	regoff = aie_cal_tile_reg(adev, offset);
+	regend64 = regoff + len - 1;
+	if (regend64 >= BIT_ULL(adev->row_shift)) {
+		dev_err(&apart->dev,
+			"Invalid reg operation len %zu.\n", len);
+		return -EINVAL;
+	}
+
+	aie_cal_loc(adev, &loc, offset);
+	if (aie_validate_location(apart, loc)) {
+		dev_err(&apart->dev,
+			"Invalid (%d,%d) out of part(%d,%d)\n",
+			loc.col, loc.row,
+			apart->range.size.col, apart->range.size.row);
+		return -EINVAL;
+	}
+
+	/*
+	 * We check if a tile is gated before trying to access the tile.
+	 * As we mmap() the registers as read only to enable faster status
+	 * enquiry, and mmap() memories as write/read to faster memory access,
+	 * user can still access the clock gated tiles from userspace by
+	 * accessing the mmapped space.
+	 * Accessing the gated tiles can cause decode error. With PDI flow,
+	 * the PDI sets up the SHIM NOC AXI MM to only generate AI engine error
+	 * even instead of generating the NSU error. but for non PDI flow, as
+	 * the AXI MM register are protected register, until we have EEMI API
+	 * to update the AXI MM register, access the gated tiles can cause NSU
+	 * errors.
+	 * TODO: To solve this, we need to either request EEMI to configure
+	 * AXI MM or split the mmapped space into tiles based lists.
+	 */
+	aloc.col = loc.col + apart->range.start.col;
+	aloc.row = loc.row;
+	if (!aie_part_check_clk_enable_loc(apart, &aloc)) {
+		dev_err(&apart->dev,
+			"Tile(%u,%d) is gated.\n", loc.col, loc.row);
+		return -EINVAL;
+	}
+
+	num_mems = apart->adev->ops->get_mem_info(apart->adev,
+						  &apart->range, NULL);
+	for (i = 0; i < num_mems; i++) {
+		if (i == AIE_PM_MEM_OFFSET_IDX)
+			continue;
+		if (pmem[i].mem.range.start.row <= aloc.row &&
+		    (pmem[i].mem.range.start.row +
+		     pmem[i].mem.range.size.row) > aloc.row) {
+			if (pmem[i].mem.offset <= regoff &&
+			    ((pmem[i].mem.offset + pmem[i].mem.size)
+			      >= regoff)) {
+				if ((pmem[i].mem.offset + pmem[i].mem.size)
+				     < regend64) {
+					dev_err(&apart->dev,
+						"address 0x%zx, 0x%zx not accessible.\n",
+						offset, len);
+					return -EINVAL;
+				}
+			} else if (pmem[i].mem.offset > regoff &&
+				   (pmem[i].mem.offset <= regend64 &&
+				    ((pmem[i].mem.offset + pmem[i].mem.size)
+				     >= regend64))) {
+				dev_err(&apart->dev,
+					"address 0x%zx, 0x%zx not accessible.\n",
+					offset, len);
+				return -EINVAL;
+			}
+		}
+	}
+
+	if (!is_write)
+		return 0;
+
+	regend32 = lower_32_bits(regend64);
+	ttype = adev->ops->get_tile_type(adev, &loc);
+	for (i = 0; i < adev->num_kernel_regs; i++) {
+		const struct aie_tile_regs *regs;
+		u32 rttype, writable;
+
+		regs = &adev->kernel_regs[i];
+		rttype = (regs->attribute & AIE_REGS_ATTR_TILE_TYPE_MASK) >>
+			 AIE_REGS_ATTR_TILE_TYPE_SHIFT;
+		writable = (regs->attribute & AIE_REGS_ATTR_PERM_MASK) >>
+			   AIE_REGS_ATTR_PERM_SHIFT;
+		if (!(BIT(ttype) & rttype))
+			continue;
+		if ((regoff >= regs->soff && regoff <= regs->eoff) ||
+		    (regend32 >= regs->soff && regend32 <= regs->eoff)) {
+			if (!writable) {
+				dev_err(&apart->dev,
+					"reg 0x%zx,0x%zx not writable.\n",
+					offset, len);
+				return -EINVAL;
+			}
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_write_register() - AI engine partition write register
+ * @apart: AI engine partition
+ * @offset: AI engine register offset
+ * @len: len of data to write
+ * @data: data to write
+ * @mask: mask, if it is non 0, it is mask write.
+ * @return: number of bytes write for success, or negative value for failure.
+ *
+ * This function writes data to the specified registers.
+ * If the mask is non 0, it is mask write.
+ */
+static int aie_part_write_register(struct aie_partition *apart, size_t offset,
+				   size_t len, void *data, u32 mask)
+{
+	struct aie_aperture *aperture = apart->aperture;
+	u32 i;
+	int ret;
+	void __iomem *va;
+
+	if (mask && len > sizeof(u32)) {
+		/* For mask write, only allow 32bit. */
+		dev_err(&apart->dev,
+			"failed mask write, len is more that 32bit.\n");
+		return -EINVAL;
+	}
+
+	/* offset is expected to be relative to the start of the partition */
+	ret = aie_part_reg_validation(apart, offset, len, 1);
+	if (ret < 0) {
+		dev_err(&apart->dev, "failed to write to 0x%zx,0x%zx.\n",
+			offset, len);
+		return ret;
+	}
+
+	offset += aie_aperture_cal_regoff(aperture, apart->range.start, 0);
+	va = aperture->base + offset;
+	if (!mask) {
+		/*
+		 * TODO: use the burst mode to improve performance when len
+		 * is more than 4. Additional checks have to be made to ensure
+		 * the destination address is 128 bit aligned when burst mode
+		 * is used.
+		 */
+		for (i = 0; i < len; i = i + 4)
+			iowrite32(*((u32 *)(data + i)), va + i);
+	} else {
+		u32 val = ioread32(va);
+
+		val &= ~mask;
+		val |= *((u32 *)data) & mask;
+		iowrite32(val, va);
+	}
+
+	return (int)len;
+}
+
+/**
+ * aie_part_read_register() - AI engine partition read register
+ * @apart: AI engine partition
+ * @offset: AI engine register offset
+ * @len: len of data to read
+ * @data: pointer to the memory to store the read data
+ * @return: number of bytes read for success, or negative value for failure.
+ *
+ * This function reads data from the specified registers.
+ */
+static int aie_part_read_register(struct aie_partition *apart, size_t offset,
+				  size_t len, void *data)
+{
+	struct aie_aperture *aperture = apart->aperture;
+	void __iomem *va;
+	int ret;
+
+	/* offset is expected to be relative to the start of the partition */
+	ret = aie_part_reg_validation(apart, offset, len, 0);
+	if (ret) {
+		dev_err(&apart->dev, "Invalid read request 0x%zx,0x%zx.\n",
+			offset, len);
+		return -EINVAL;
+	}
+
+	offset += aie_aperture_cal_regoff(aperture, apart->range.start, 0);
+	va = aperture->base + offset;
+	if (len == 4)
+		*((u32 *)data) = ioread32(va);
+	else
+		memcpy_fromio(data, va, len);
+
+	return (int)len;
+}
+
+/**
+ * aie_part_block_set() - AI Engine partition block set registers
+ * @apart: AI engine partition
+ * @args: regestier access arguments
+ * @return: 0 for success, and negative value for failure
+ */
+static int aie_part_block_set(struct aie_partition *apart,
+			      struct aie_reg_args *args)
+{
+	u32 i;
+	int ret;
+
+	for (i = 0; i < args->len; i++) {
+		size_t offset = (size_t)args->offset;
+
+		ret = aie_part_write_register(apart, offset + i * 4,
+					      sizeof(args->val), &args->val,
+					      args->mask);
+		if (ret < 0)
+			return ret;
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_pin_user_region() - pin user pages for access
+ * @apart: AI engine partition
+ * @region: user space region to pin. Includes virtual address and size of the
+ *	    user space buffer.
+ * @return: 0 for success, and negative value for failure.
+ *
+ * This function pins all the pages of a user space buffer.
+ */
+static int aie_part_pin_user_region(struct aie_partition *apart,
+				    struct aie_part_pinned_region *region)
+{
+	int ret, npages;
+	unsigned long first, last;
+	struct page **pages;
+
+	first = (region->user_addr & PAGE_MASK) >> PAGE_SHIFT;
+	last = ((region->user_addr + region->len - 1) & PAGE_MASK) >>
+		PAGE_SHIFT;
+	npages = last - first + 1;
+
+	pages = kcalloc(npages, sizeof(struct page *), GFP_KERNEL);
+	if (!pages)
+		return -ENOMEM;
+
+	ret = pin_user_pages_fast(region->user_addr, npages, 0, pages);
+	if (ret < 0) {
+		kfree(pages);
+		dev_err(&apart->dev, "Unable to pin user pages\n");
+		return ret;
+	} else if (ret != npages) {
+		unpin_user_pages(pages, ret);
+		kfree(pages);
+		dev_err(&apart->dev, "Unable to pin all user pages\n");
+		return -EFAULT;
+	}
+
+	region->pages = pages;
+	region->npages = npages;
+
+	return 0;
+}
+
+/**
+ * aie_part_unpin_user_region() - unpin user pages
+ * @region: user space region to unpin.
+ *
+ * This function unpins all the pages of a user space buffer. User region passed
+ * to this api must be pinned using aie_part_pin_user_region()
+ */
+static void aie_part_unpin_user_region(struct aie_part_pinned_region *region)
+{
+	unpin_user_pages(region->pages, region->npages);
+	kfree(region->pages);
+}
+
+/**
+ * aie_part_access_regs() - AI engine partition registers access
+ * @apart: AI engine partition
+ * @num_reqs: number of access requests
+ * @reqs: array of registers access
+ * @return: 0 for success, and negative value for failure.
+ *
+ * This function executes AI engine partition register access requests.
+ */
+static int aie_part_access_regs(struct aie_partition *apart, u32 num_reqs,
+				struct aie_reg_args *reqs)
+{
+	u32 i;
+
+	for (i = 0; i < num_reqs; i++) {
+		struct aie_reg_args *args = &reqs[i];
+		int ret;
+
+		switch (args->op) {
+		case AIE_REG_WRITE:
+		{
+			ret = aie_part_write_register(apart,
+						      (size_t)args->offset,
+						      sizeof(args->val),
+						      &args->val, args->mask);
+			break;
+		}
+		case AIE_REG_BLOCKWRITE:
+		{
+			struct aie_part_pinned_region region;
+
+			region.user_addr = args->dataptr;
+			region.len = args->len * sizeof(u32);
+			ret = aie_part_pin_user_region(apart, &region);
+			if (ret)
+				break;
+
+			ret = aie_part_write_register(apart,
+						      (size_t)args->offset,
+						      sizeof(u32) * args->len,
+						      (void *)args->dataptr,
+						      args->mask);
+			aie_part_unpin_user_region(&region);
+			break;
+		}
+		case AIE_REG_BLOCKSET:
+		{
+			ret = aie_part_block_set(apart, args);
+			break;
+		}
+		case AIE_CONFIG_SHIMDMA_BD:
+		{
+			struct aie_part_pinned_region data_region;
+
+			data_region.user_addr = args->dataptr;
+			data_region.len = sizeof(struct aie_dmabuf_bd_args);
+			ret = aie_part_pin_user_region(apart, &data_region);
+			if (ret)
+				break;
+
+			ret =  aie_part_set_bd(apart,
+				(struct aie_dma_bd_args *)args->dataptr);
+			aie_part_unpin_user_region(&data_region);
+			break;
+		}
+		case AIE_CONFIG_SHIMDMA_DMABUF_BD:
+		{
+			struct aie_part_pinned_region data_region;
+
+			data_region.user_addr = args->dataptr;
+			data_region.len = sizeof(struct aie_dmabuf_bd_args);
+			ret = aie_part_pin_user_region(apart, &data_region);
+			if (ret)
+				break;
+
+			ret =  aie_part_set_dmabuf_bd(apart,
+				(struct aie_dmabuf_bd_args *)args->dataptr);
+			aie_part_unpin_user_region(&data_region);
+			break;
+		}
+		default:
+			dev_err(&apart->dev,
+				"Invalid register command type: %u.\n",
+				args->op);
+			return -EINVAL;
+		}
+
+		if (ret < 0) {
+			dev_err(&apart->dev, "reg op %u failed: 0x%llx.\n",
+				args->op, args->offset);
+			return ret;
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_execute_transaction_from_user() - AI engine configure registers
+ * @apart: AI engine partition
+ * @user_args: arguments passed by user.
+ * @return: 0 for success, and negative value for failure.
+ *
+ * This function executes AI engine register access requests that are part of a
+ * buffer that is populated and passed by user.
+ */
+static int aie_part_execute_transaction_from_user(struct aie_partition *apart,
+						  void __user *user_args)
+{
+	long ret;
+	struct aie_txn_inst txn_inst;
+	struct aie_part_pinned_region region;
+
+	if (copy_from_user(&txn_inst, user_args, sizeof(txn_inst)))
+		return -EFAULT;
+
+	region.user_addr = txn_inst.cmdsptr;
+	region.len = txn_inst.num_cmds * sizeof(struct aie_reg_args);
+	ret = aie_part_pin_user_region(apart, &region);
+	if (ret)
+		return ret;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		aie_part_unpin_user_region(&region);
+		return ret;
+	}
+
+	ret = aie_part_access_regs(apart, txn_inst.num_cmds,
+				   (struct aie_reg_args *)region.user_addr);
+
+	mutex_unlock(&apart->mlock);
+
+	aie_part_unpin_user_region(&region);
+	return ret;
+}
+
+/**
+ * aie_part_create_event_bitmap() - create event bitmap for all modules in a
+ *				    given partition.
+ * @apart: AI engine partition
+ * @return: 0 for success, and negative value for failure.
+ */
+static int aie_part_create_event_bitmap(struct aie_partition *apart)
+{
+	struct aie_range range = apart->range;
+	u32 bitmap_sz;
+	u32 num_aie_module = range.size.col * (range.size.row - 1);
+	int ret;
+
+	/*
+	 * TODO: resource manager, events is not supported for AIEML device and
+	 * the users are not expected to call any function as of now.
+	 */
+	if (apart->adev->dev_gen == AIE_DEVICE_GEN_AIEML) {
+		dev_dbg(&apart->dev, "Skipping event bitmap allocation.\n");
+		return 0;
+	}
+	bitmap_sz = num_aie_module * apart->adev->core_events->num_events;
+	ret = aie_resource_initialize(&apart->core_event_status, bitmap_sz);
+	if (ret) {
+		dev_err(&apart->dev,
+			"failed to initialize event status resource.\n");
+		return -ENOMEM;
+	}
+
+	bitmap_sz = num_aie_module * apart->adev->mem_events->num_events;
+	ret = aie_resource_initialize(&apart->mem_event_status, bitmap_sz);
+	if (ret) {
+		dev_err(&apart->dev,
+			"failed to initialize event status resource.\n");
+		return -ENOMEM;
+	}
+
+	bitmap_sz = range.size.col * apart->adev->pl_events->num_events;
+	ret = aie_resource_initialize(&apart->pl_event_status, bitmap_sz);
+	if (ret) {
+		dev_err(&apart->dev,
+			"failed to initialize event status resource.\n");
+		return -ENOMEM;
+	}
+	return 0;
+}
+
+/**
+ * aie_part_release_event_bitmap() - Deallocates event bitmap for all modules
+ *				     in a given partition.
+ * @apart: AI engine partition
+ * @return: 0 for success, and negative value for failure.
+ */
+static void aie_part_release_event_bitmap(struct aie_partition *apart)
+{
+	/* TODO: remove check once resource manager is enabled for AIEML. */
+	if (apart->adev->dev_gen == AIE_DEVICE_GEN_AIEML)
+		return;
+
+	aie_resource_uninitialize(&apart->core_event_status);
+	aie_resource_uninitialize(&apart->mem_event_status);
+	aie_resource_uninitialize(&apart->pl_event_status);
+}
+
+static int aie_part_release(struct inode *inode, struct file *filp)
+{
+	struct aie_partition *apart = filp->private_data;
+	int ret;
+
+	/* some reset bits in NPI are global, we need to lock adev */
+	ret = mutex_lock_interruptible(&apart->adev->mlock);
+	if (ret)
+		return ret;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	aie_part_release_dmabufs(apart);
+	/* aie_part_clean() will do hardware reset */
+	aie_part_clean(apart);
+	mutex_unlock(&apart->adev->mlock);
+
+	apart->error_cb.cb = NULL;
+	apart->error_cb.priv = NULL;
+	apart->status = 0;
+	apart->error_to_report = 0;
+
+	aie_part_clear_cached_events(apart);
+
+	aie_part_rscmgr_reset(apart);
+
+	mutex_unlock(&apart->mlock);
+	aie_part_remove(apart);
+
+	return 0;
+}
+
+static ssize_t aie_part_write_iter(struct kiocb *iocb, struct iov_iter *from)
+{
+	struct file *filp = iocb->ki_filp;
+	struct aie_partition *apart = filp->private_data;
+	size_t len = iov_iter_count(from);
+	loff_t offset = iocb->ki_pos;
+	void *buf;
+	int ret;
+
+	buf = kzalloc(len, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+	if (!copy_from_iter_full(buf, len, from)) {
+		kfree(buf);
+		return -EFAULT;
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		kfree(buf);
+		return ret;
+	}
+
+	ret = aie_part_write_register(apart, (size_t)offset, len, buf, 0);
+	mutex_unlock(&apart->mlock);
+	kfree(buf);
+
+	return ret;
+}
+
+static ssize_t aie_part_read_iter(struct kiocb *iocb, struct iov_iter *to)
+{
+	struct file *filp = iocb->ki_filp;
+	struct aie_partition *apart = filp->private_data;
+	size_t len = iov_iter_count(to);
+	loff_t offset = iocb->ki_pos;
+	void *buf;
+	int ret;
+
+	buf = kzalloc(len, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		kfree(buf);
+		return ret;
+	}
+
+	ret = aie_part_read_register(apart, (size_t)offset, len, buf);
+	mutex_unlock(&apart->mlock);
+	if (ret > 0) {
+		if (copy_to_iter(buf, ret, to) != len) {
+			dev_err(&apart->dev, "Failed to copy to read iter.\n");
+			ret = -EFAULT;
+		}
+	}
+	kfree(buf);
+
+	return ret;
+}
+
+static const struct vm_operations_struct aie_part_physical_vm_ops = {
+#ifdef CONFIG_HAVE_IOREMAP_PROT
+	.access = generic_access_phys,
+#endif
+};
+
+static int aie_part_mmap(struct file *fp, struct vm_area_struct *vma)
+{
+	struct aie_partition *apart = fp->private_data;
+	struct aie_device *adev = apart->adev;
+	unsigned long offset = vma->vm_pgoff * PAGE_SIZE;
+	phys_addr_t addr;
+	size_t size;
+
+	if (vma->vm_end < vma->vm_start)
+		return -EINVAL;
+	/* Only allow userspace directly read registers */
+	if (vma->vm_flags & VM_WRITE) {
+		dev_err(&apart->dev, "%s: do not support writable mmap.\n",
+			__func__);
+		return -EINVAL;
+	}
+	vma->vm_private_data = apart;
+	vma->vm_ops = &aie_part_physical_vm_ops;
+	size = apart->range.size.col << adev->col_shift;
+	if ((vma->vm_end - vma->vm_start) > (size - offset)) {
+		dev_err(&apart->dev,
+			"%s: size exceed.\n", __func__);
+		return -EINVAL;
+	}
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+	/* Calculate the partition address */
+	addr = apart->aperture->res.start;
+	addr += (phys_addr_t)apart->range.start.col << adev->col_shift;
+	addr += (phys_addr_t)apart->range.start.row << adev->row_shift;
+	addr += offset;
+	return remap_pfn_range(vma,
+			       vma->vm_start,
+			       addr >> PAGE_SHIFT,
+			       vma->vm_end - vma->vm_start,
+			       vma->vm_page_prot);
+}
+
+/**
+ * aie_part_capture_utilization() - callback to capture core utilization.
+ * @time: Timer structure.
+ * @return: None.
+ */
+static void aie_part_capture_utilization(struct timer_list *time)
+{
+	struct aie_utilization_timer *util_timer = from_timer(util_timer, time,
+							timer);
+	struct aie_perfinst_args *perfinst = &util_timer->apart->adev->perfinst;
+	struct aie_partition *apart = util_timer->apart;
+	struct aie_occupancy *util = util_timer->util;
+	struct aie_device *adev = apart->adev;
+	struct kernel_siginfo signal_info;
+	struct task_struct *task;
+	u32 val, value;
+	size_t offset;
+	int ret;
+
+	/*
+	 * Captures the active cycle and the total cycle values from respective
+	 * performance counters.
+	 */
+	for (int index = 0; index < perfinst->util_size; index++) {
+		for (int cycle = 0; cycle < AIE_CORE_NUM_CYCLE; cycle++) {
+			offset = adev->core_perfcnt->regoff
+				+ util[index].perfcnt[cycle] * 4;
+
+			offset = (size_t)aie_cal_regoff(adev,
+					util[index].loc, (u32)offset);
+			value = 0;
+			ret = aie_part_read_register(apart, offset, sizeof(u32),
+						     &value);
+			if (cycle == AIE_CORE_ACTIVE_CYCLE) {
+				util[index].active_cycle = value;
+
+				/*
+				 * Stops the capture of total cycle. As to stop
+				 * active cycle capture Disable event has to be
+				 * generated which might affect the application
+				 * running, active cycle value is read before
+				 * stopping total cycle.
+				 */
+				offset = (size_t)aie_cal_regoff(adev,
+								util[index].loc,
+								(u32)adev->core_evntgen->regoff);
+				val = adev->core_util_events[AIE_EVENT_CORE_USER_EVNT_1];
+				ret = aie_part_write_register(apart, offset,
+							      sizeof(val), &val,
+							      adev->core_evntgen->mask);
+			} else if (cycle == AIE_CORE_TOTAL_CYCLE) {
+				util[index].total_cycle = value;
+			}
+		}
+	}
+
+	/*
+	 * Checks if the pid is still valid to raise the signal.
+	 */
+	task = pid_task(find_vpid(perfinst->task->pid), PIDTYPE_PID);
+	if (!task) {
+		dev_err(&apart->dev, "Capture interval exceeded application duration. Capture reporting failed!");
+		kfree(util);
+		return;
+	}
+
+	ret = copy_to_user((void __user *)perfinst->util, util,
+			   perfinst->util_size *
+			   sizeof(struct aie_occupancy));
+	if (ret)
+		dev_err(&apart->dev, "Copying memory failed: %d!\n", ret);
+
+	kfree(util);
+
+	signal_info.si_signo = SIGPERFUTIL;
+	signal_info.si_code = SI_QUEUE;
+	signal_info.si_int = 1;
+
+	if (send_sig_info(SIGPERFUTIL, &signal_info, perfinst->task) < 0)
+		dev_err(&apart->dev, "Failed to send signal!\n");
+}
+
+/**
+ * aie_part_performance_utilization() - initializes the performance control
+ *					 register and schedules callback to
+ *					 capture core utilization.
+ * @apart: AI engine partition
+ * @return: Number of tiles when scanning the array on success, 0 for success
+ *		when setting up the performance counter registers,
+ *		and negative value for failure.
+ */
+static __u32 aie_part_performance_utilization(struct aie_partition *apart)
+{
+	struct aie_perfinst_args *perfinst = &apart->adev->perfinst;
+	u32 startbit, bit, mask, shift, val, reset_val, reset_mask;
+	unsigned long status, util_jiffies;
+	struct aie_occupancy *util;
+	struct aie_device *adev;
+	struct aie_location loc;
+	size_t offset;
+	int ret;
+
+	loc.col = perfinst->range.start.col;
+	util = kmalloc_array(perfinst->range.size.col * perfinst->range.size.row,
+			     sizeof(struct aie_occupancy), GFP_KERNEL);
+	if (!util)
+		return -ENOMEM;
+
+	if (copy_from_user(util, (void __user *)perfinst->util,
+			   sizeof(struct aie_occupancy) *
+			   perfinst->range.size.col *
+			   perfinst->range.size.row)) {
+		dev_err(&apart->dev, "Copying memory failed!\n");
+		kfree(util);
+		return -EFAULT;
+	}
+
+	/*
+	 * Scanning the partition to store all the enabled and in use core tile
+	 * location. Returns number of elements in the array.
+	 */
+	if (perfinst->util_size == 0) {
+		perfinst->task = NULL;
+		for (; loc.col < perfinst->range.size.col; loc.col++) {
+			startbit = loc.col + (apart->range.size.row - 1);
+			for (loc.row = 0; (loc.row < apart->range.size.row) &&
+			     (perfinst->util_size <
+			     (perfinst->range.size.col
+			     * perfinst->range.size.row));
+			     loc.row++) {
+				if (apart->adev->ops->get_tile_type(apart->adev,
+								    &loc) ==
+								    AIE_TILE_TYPE_TILE) {
+					status = apart->adev->ops->get_core_status(apart,
+								&loc);
+					bit = startbit + loc.row - 1;
+					if ((status & AIE_CORE_STS_ENABLE_MASK) &&
+					    aie_resource_testbit(&apart->tiles_inuse,
+								 bit)) {
+						util[perfinst->util_size].loc =
+							loc;
+						perfinst->util_size++;
+					}
+				}
+			}
+		}
+		ret = perfinst->util_size;
+		if (copy_to_user((void __user *)perfinst->util, util,
+				 sizeof(struct aie_occupancy) *
+				 perfinst->util_size)) {
+			dev_err(&apart->dev, "Copying memory failed!\n");
+			kfree(util);
+			return -EFAULT;
+		}
+		kfree(util);
+
+	} else {
+		perfinst->task = get_current();
+		adev = apart->adev;
+
+		/*
+		 * Performance counter start, stop and reset event setup.
+		 */
+		for (int index = 0; index < perfinst->util_size; index++) {
+			reset_val = 0;
+			reset_mask = 0;
+			for (int cycle = 0; cycle < AIE_CORE_NUM_CYCLE; cycle++) {
+				if (cycle == AIE_CORE_ACTIVE_CYCLE) {
+					val = adev->core_util_events[AIE_EVENT_CORE_ACTIVE] |
+					       (adev->core_util_events[AIE_EVENT_CORE_DISABLED] <<
+						AIE_CORE_PERFCNT_EVNT_BITS);
+				} else if (cycle == AIE_CORE_TOTAL_CYCLE) {
+					val = adev->core_util_events[AIE_EVENT_CORE_USER_EVNT_0] |
+					     (adev->core_util_events[AIE_EVENT_CORE_USER_EVNT_1] <<
+					      AIE_CORE_PERFCNT_EVNT_BITS);
+				}
+				reset_val |= adev->core_util_events[AIE_EVENT_CORE_USER_EVNT_0]
+					<< (util[index].perfcnt[cycle]
+						* AIE_CORE_PERFCNT_EVNT_BITS);
+				reset_mask |= adev->core_perfctrl_reset->mask
+					<< (util[index].perfcnt[cycle]
+						* AIE_CORE_PERFCNT_EVNT_BITS);
+						offset = adev->core_perfctrl->regoff
+					 + (util[index].perfcnt[cycle] /
+						AIE_CORE_NUM_PERFCNT_PER_REG) *
+						AIE_CORE_PERFCNT_CTRL_IDX;
+				offset = (size_t)aie_cal_regoff(adev,
+						util[index].loc, (u32)offset);
+				shift = AIE_CORE_PERFCNT_EVNT_BITS *
+					AIE_CORE_NUM_PERFCNT_PER_REG *
+					(util[index].perfcnt[cycle] %
+					 AIE_CORE_NUM_PERFCNT_PER_REG);
+				val <<= shift;
+				mask = adev->core_perfctrl->mask << shift;
+				ret = aie_part_write_register(apart, offset,
+							      sizeof(val),
+							      &val, mask);
+			}
+				/*
+				 * Reset performance counter setup
+				 */
+				offset = adev->core_perfctrl_reset->regoff;
+				offset = (size_t)aie_cal_regoff(adev,
+						util[index].loc, (u32)offset);
+				ret = aie_part_write_register(apart, offset,
+							      sizeof(reset_val),
+							      &reset_val,
+							      reset_mask);
+		}
+
+		/*
+		 * Setting up timer structure to schedule capture callback.
+		 */
+		util_jiffies = msecs_to_jiffies(perfinst->time_interval_ms);
+		adev->util_timer.timer.function = &aie_part_capture_utilization;
+		adev->util_timer.util = util;
+		adev->util_timer.apart = apart;
+
+		for (int index = 0; index < perfinst->util_size; index++) {
+			offset = (size_t)aie_cal_regoff(adev, util[index].loc,
+					(u32)adev->core_evntgen->regoff);
+			val = adev->core_util_events[AIE_EVENT_CORE_USER_EVNT_0];
+			ret = aie_part_write_register(apart, offset, sizeof(val),
+						      &val,
+						      adev->core_evntgen->mask);
+		}
+
+		/*
+		 * Timer schedules the callback to capture core utilization
+		 * over the user defined time interval.
+		 */
+		adev->util_timer.timer.expires = jiffies + util_jiffies;
+		add_timer(&adev->util_timer.timer);
+
+		ret = 0;
+	}
+
+	return ret;
+}
+
+static long aie_part_ioctl(struct file *fp, unsigned int cmd, unsigned long arg)
+{
+	struct aie_partition *apart = fp->private_data;
+	void __user *argp = (void __user *)arg;
+	long ret;
+
+	switch (cmd) {
+	case AIE_PARTITION_INIT_IOCTL:
+		return aie_part_initialize(apart, argp);
+	case AIE_PARTITION_TEAR_IOCTL:
+		return aie_part_teardown(apart);
+	case AIE_PARTITION_CLR_CONTEXT_IOCTL:
+		return aie_part_clear_context(apart);
+	case AIE_REG_IOCTL:
+	{
+		struct aie_reg_args raccess;
+
+		if (copy_from_user(&raccess, argp, sizeof(raccess)))
+			return -EFAULT;
+
+		ret = mutex_lock_interruptible(&apart->mlock);
+		if (ret)
+			return ret;
+
+		ret = aie_part_access_regs(apart, 1, &raccess);
+		mutex_unlock(&apart->mlock);
+		break;
+	}
+	case AIE_GET_MEM_IOCTL:
+		return aie_mem_get_info(apart, arg);
+	case AIE_ATTACH_DMABUF_IOCTL:
+		return aie_part_attach_dmabuf_req(apart, argp);
+	case AIE_DETACH_DMABUF_IOCTL:
+		return aie_part_detach_dmabuf_req(apart, argp);
+	case AIE_SET_SHIMDMA_BD_IOCTL:
+		return aie_part_set_bd_from_user(apart, argp);
+	case AIE_SET_SHIMDMA_DMABUF_BD_IOCTL:
+		return aie_part_set_dmabuf_bd_from_user(apart, argp);
+	case AIE_REQUEST_TILES_IOCTL:
+		return aie_part_request_tiles_from_user(apart, argp);
+	case AIE_RELEASE_TILES_IOCTL:
+		return aie_part_release_tiles_from_user(apart, argp);
+	case AIE_TRANSACTION_IOCTL:
+		return aie_part_execute_transaction_from_user(apart, argp);
+	case AIE_RSC_REQ_IOCTL:
+		return aie_part_rscmgr_rsc_req(apart, argp);
+	case AIE_RSC_REQ_SPECIFIC_IOCTL:
+		return aie_part_rscmgr_rsc_req_specific(apart, argp);
+	case AIE_RSC_RELEASE_IOCTL:
+		return aie_part_rscmgr_rsc_release(apart, argp);
+	case AIE_RSC_FREE_IOCTL:
+		return aie_part_rscmgr_rsc_free(apart, argp);
+	case AIE_RSC_CHECK_AVAIL_IOCTL:
+		return aie_part_rscmgr_rsc_check_avail(apart, argp);
+	case AIE_RSC_GET_COMMON_BROADCAST_IOCTL:
+		return aie_part_rscmgr_get_broadcast(apart, argp);
+	case AIE_RSC_GET_STAT_IOCTL:
+		return aie_part_rscmgr_get_statistics(apart, argp);
+	case AIE_SET_COLUMN_CLOCK_IOCTL:
+		return aie_part_set_column_clock_from_user(apart, argp);
+	case AIE_PERFORMANCE_UTILIZATION_IOCTL:
+	{
+		if (copy_from_user(&apart->adev->perfinst, argp,
+				   sizeof(struct aie_perfinst_args)))
+			return -EFAULT;
+
+		return aie_part_performance_utilization(apart);
+	}
+	default:
+		dev_err(&apart->dev, "Invalid/Unsupported ioctl command %u.\n",
+			cmd);
+		ret = -EINVAL;
+		break;
+	}
+
+	return ret;
+}
+
+const struct file_operations aie_part_fops = {
+	.owner		= THIS_MODULE,
+	.release	= aie_part_release,
+	.read_iter	= aie_part_read_iter,
+	.write_iter	= aie_part_write_iter,
+	.mmap		= aie_part_mmap,
+	.unlocked_ioctl	= aie_part_ioctl,
+};
+
+/**
+ * aie_part_open() - open the AI engine partition instance to get it ready to
+ *		     be used.
+ * @apart: AI engine partition instance pointer
+ * @rsc_metadata: pointer to static resource metadata
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will make the AI engine partition instance ready to use. It
+ * should be called when the partition is requested.
+ */
+int aie_part_open(struct aie_partition *apart, void *rsc_metadata)
+{
+	int ret;
+
+	/* scan to setup the initial clock state for tiles */
+	ret = aie_part_scan_clk_state(apart);
+	if (ret)
+		return ret;
+
+	/* Sets bitmaps of statically allocated resources */
+	if (rsc_metadata) {
+		ret = aie_part_rscmgr_set_static(apart,
+						 rsc_metadata);
+		if (ret)
+			return ret;
+	}
+
+	/* preallocate memory pool for storing dmabuf descriptors */
+	ret =  aie_part_prealloc_dbufs_cache(apart);
+	if (ret)
+		return ret;
+
+	/* check if there is any errors reported for the partition */
+	if (aie_part_has_error(apart))
+		schedule_work(&apart->aperture->backtrack);
+
+	apart->status = XAIE_PART_STATUS_INUSE;
+
+	return 0;
+}
+
+/**
+ * aie_tile_release_device() - release an AI engine tile instance
+ * @dev: AI engine tile device
+ *
+ * It will be called by device driver core when no one holds a valid
+ * pointer to @dev anymore.
+ */
+static void aie_tile_release_device(struct device *dev)
+{
+	(void)dev;
+}
+
+/**
+ * aie_part_release_device() - release an AI engine partition instance
+ * @dev: AI engine partition device
+ *
+ * It will be called by device driver core when no one holds a valid
+ * pointer to @dev anymore.
+ */
+static void aie_part_release_device(struct device *dev)
+{
+	struct aie_partition *apart = dev_to_aiepart(dev);
+	struct aie_aperture *aperture = apart->aperture;
+	int ret;
+
+	ret = mutex_lock_interruptible(&aperture->mlock);
+	if (ret) {
+		dev_warn(&apart->dev,
+			 "getting adev->mlock is interrupted by signal\n");
+	}
+
+	aie_resource_put_region(&aperture->cols_res, apart->range.start.col,
+				apart->range.size.col);
+	aie_part_release_event_bitmap(apart);
+	list_del(&apart->node);
+	mutex_unlock(&aperture->mlock);
+	aie_resource_uninitialize(&apart->cores_clk_state);
+	aie_resource_uninitialize(&apart->tiles_inuse);
+	aie_part_rscmgr_finish(apart);
+	/* Check and set frequency requirement for aperture */
+	aie_part_set_freq(apart, 0);
+}
+
+/**
+ * aie_part_create_mems_info() - creates array to store the AI engine partition
+ *				 different memories types information
+ * @apart: AI engine partition
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will create array to store the information of different
+ * memories types in the partition. This array is stored in @apart->pmems.
+ */
+static int aie_part_create_mems_info(struct aie_partition *apart)
+{
+	unsigned int i, num_mems;
+
+	num_mems = apart->adev->ops->get_mem_info(apart->adev, &apart->range,
+						  NULL);
+	if (!num_mems)
+		return 0;
+
+	apart->pmems = devm_kcalloc(&apart->dev, num_mems,
+				    sizeof(struct aie_part_mem),
+				    GFP_KERNEL);
+	if (!apart->pmems)
+		return -ENOMEM;
+
+	apart->adev->ops->get_mem_info(apart->adev, &apart->range,
+				       apart->pmems);
+	for (i = 0; i < num_mems; i++) {
+		struct aie_mem *mem = &apart->pmems[i].mem;
+
+		apart->pmems[i].apart = apart;
+		apart->pmems[i].size = mem->size *
+				       mem->range.size.col *
+				       mem->range.size.row;
+	}
+	return 0;
+}
+
+/**
+ * aie_create_tiles() - create AI engine tile devices
+ * @apart: AI engine partition
+ * @return: 0 for success, error code on failure
+ *
+ * This function creates AI engine child tile devices for a given partition.
+ */
+static int aie_create_tiles(struct aie_partition *apart)
+{
+	struct aie_tile *atile;
+	u32 row, col, numtiles;
+	int ret = 0;
+
+	numtiles = apart->range.size.col * apart->range.size.row;
+	atile = devm_kzalloc(&apart->dev, numtiles * sizeof(struct aie_tile),
+			     GFP_KERNEL);
+	if (!atile)
+		return -ENOMEM;
+
+	apart->atiles = atile;
+	for (col = 0; col < apart->range.size.col; col++) {
+		for (row = 0; row < apart->range.size.row; row++) {
+			struct device *tdev = &atile->dev;
+			char tdevname[10];
+
+			atile->apart = apart;
+			atile->loc.col = apart->range.start.col + col;
+			atile->loc.row = apart->range.start.row + row;
+			device_initialize(tdev);
+			tdev->parent = &apart->dev;
+			dev_set_drvdata(tdev, atile);
+			snprintf(tdevname, sizeof(tdevname) - 1, "%d_%d",
+				 apart->range.start.col + col,
+				 apart->range.start.row + row);
+			dev_set_name(tdev, tdevname);
+			tdev->release = aie_tile_release_device;
+			ret = device_add(tdev);
+			if (ret) {
+				dev_err(tdev, "tile device_add failed: %d\n",
+					ret);
+				put_device(tdev);
+				return ret;
+			}
+			ret = aie_tile_sysfs_create_entries(atile);
+			if (ret) {
+				dev_err(tdev, "failed to create tile sysfs: %d\n",
+					ret);
+				device_del(tdev);
+				put_device(tdev);
+				return ret;
+			}
+			atile++;
+		}
+	}
+	return ret;
+}
+
+/**
+ * aie_create_partition() - create AI engine partition instance
+ * @aperture: AI engine aperture
+ * @partition_id: AI engine partition ID which contains partition range
+ *		  information such as start column and number of columns
+ * @return: created AI engine partition pointer for success, and PTR_ERR
+ *	    for failure.
+ *
+ * This function creates an AI engine partition instance.
+ * It creates AI engine partition, the AI engine partition device and
+ * the AI engine partition character device.
+ */
+struct aie_partition *aie_create_partition(struct aie_aperture *aperture,
+					   u32 partition_id)
+{
+	struct aie_partition *apart;
+	struct device *dev;
+	int ret;
+
+	apart = devm_kzalloc(&aperture->dev, sizeof(*apart), GFP_KERNEL);
+	if (!apart)
+		return ERR_PTR(-ENOMEM);
+
+	apart->aperture = aperture;
+	apart->adev = aperture->adev;
+	apart->partition_id = partition_id;
+	INIT_LIST_HEAD(&apart->dbufs);
+	mutex_init(&apart->mlock);
+	apart->range.start.col = aie_part_id_get_start_col(partition_id);
+	apart->range.size.col = aie_part_id_get_num_cols(partition_id);
+	apart->range.start.row = aperture->range.start.row;
+	apart->range.size.row = aperture->range.size.row;
+
+	/* Create AI engine partition device */
+	dev = &apart->dev;
+	dev->parent = &aperture->dev;
+	dev->class = aie_class;
+	dev_set_drvdata(dev, apart);
+	dev_set_name(dev, "aiepart_%d_%d", apart->range.start.col,
+		     apart->range.size.col);
+	/* We can now rely on the release function for cleanup */
+	dev->release = aie_part_release_device;
+	ret = device_register(dev);
+	if (ret) {
+		dev_err(dev, "device_add failed: %d\n", ret);
+		put_device(dev);
+		return ERR_PTR(ret);
+	}
+
+	/* Set up the DMA mask */
+	set_dma_ops(dev, get_dma_ops(&aperture->dev));
+	ret = dma_coerce_mask_and_coherent(dev, dma_get_mask(&aperture->dev));
+	if (ret) {
+		dev_warn(dev,
+			 "Failed to set DMA mask %llx. Trying to continue... %x\n",
+			 dma_get_mask(&aperture->dev), ret);
+	}
+
+	/* Create AI Engine tile devices */
+	ret = aie_create_tiles(apart);
+	if (ret) {
+		dev_err(dev, "Failed to create tile devices.\n");
+		put_device(dev);
+		return ERR_PTR(ret);
+	}
+
+	/*
+	 * Create array to keep the information of the different types of tile
+	 * memories information of the AI engine partition.
+	 */
+	ret = aie_part_create_mems_info(apart);
+	if (ret) {
+		put_device(dev);
+		return ERR_PTR(ret);
+	}
+
+	ret = apart->adev->ops->init_part_clk_state(apart);
+	if (ret) {
+		put_device(dev);
+		return ERR_PTR(ret);
+	}
+
+	/*
+	 * Create bitmap to record event status for each module in a
+	 * partition
+	 */
+	ret = aie_part_create_event_bitmap(apart);
+	if (ret < 0) {
+		dev_err(&apart->dev, "Failed to allocate event bitmap.\n");
+		put_device(dev);
+		return ERR_PTR(ret);
+	}
+
+	ret = aie_part_rscmgr_init(apart);
+	if (ret < 0) {
+		dev_err(&apart->dev,
+			"Failed to initialize resources bitmaps.\n");
+		put_device(dev);
+		return ERR_PTR(ret);
+	}
+
+	ret = aie_part_sysfs_create_entries(apart);
+	if (ret) {
+		dev_err(&apart->dev, "Failed to create partition sysfs.\n");
+		put_device(dev);
+		return ERR_PTR(ret);
+	}
+
+	dev_dbg(dev, "created AIE partition device.\n");
+
+	return apart;
+}
+
+/**
+ * aie_tile_remove() - remove AI engine tile device.
+ * @atile: AI engine tile.
+ *
+ * This function will remove AI engine tile device.
+ */
+static void aie_tile_remove(struct aie_tile *atile)
+{
+	aie_tile_sysfs_remove_entries(atile);
+	device_del(&atile->dev);
+	put_device(&atile->dev);
+}
+
+/**
+ * aie_part_remove() - destroy AI engine partition
+ * @apart: AI engine partition
+ *
+ * This function will remove AI engine partition.
+ */
+void aie_part_remove(struct aie_partition *apart)
+{
+	struct aie_tile *atile = apart->atiles;
+	u32 index;
+
+	for (index = 0; index < apart->range.size.col * apart->range.size.row;
+	     index++, atile++)
+		aie_tile_remove(atile);
+
+	aie_part_sysfs_remove_entries(apart);
+
+	device_del(&apart->dev);
+	put_device(&apart->dev);
+}
+
+/**
+ * aie_part_has_regs_mmapped() - check if registers in the partition are mapped.
+ * @apart: AI engine partition
+ * @return: return true if there are registers mmaped, false otherwise.
+ *
+ * This function checks if there are registerss in the partition mmapped in the
+ * partition.
+ */
+bool aie_part_has_regs_mmapped(struct aie_partition *apart)
+{
+	struct address_space *mapping;
+
+	mapping = apart->filep->f_inode->i_mapping;
+	return mapping_mapped(mapping);
+}
+
+/**
+ * aie_part_get_tile_rows - helper function to get the number of rows of a
+ *			    tile type.
+ *
+ * @apart: AI engine partition
+ * @ttype: tile type
+ * @return: number of rows of a tile type
+ */
+int aie_part_get_tile_rows(struct aie_partition *apart,
+			   enum aie_tile_type ttype)
+{
+	struct aie_tile_attr *tattr = &apart->adev->ttype_attr[ttype];
+
+	/*
+	 * TODO: number of rows information of the AI engine device
+	 * should get from device tree.
+	 */
+	if (tattr->num_rows != 0xFF)
+		return tattr->num_rows;
+	else
+		return (apart->range.size.row - tattr->start_row);
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-res.c b/drivers/misc/xilinx-ai-engine/ai-engine-res.c
new file mode 100644
index 000000000..501f79b91
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-res.c
@@ -0,0 +1,466 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include <linux/bitmap.h>
+
+#include "ai-engine-internal.h"
+
+/**
+ * aie_resource_initialize() - initialize AI engine resource
+ * @res: pointer to AI engine resource
+ * @count: total number of element of this resource
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function will initialize the data structure for the
+ * resource.
+ */
+int aie_resource_initialize(struct aie_resource *res, int count)
+{
+	if (!res || !count)
+		return -EINVAL;
+	res->bitmap = bitmap_zalloc(count, GFP_KERNEL);
+	if (!res->bitmap)
+		return -ENOMEM;
+	res->total = count;
+
+	return 0;
+}
+
+/**
+ * aie_resource_uninitialize() - uninitialize AI engine resource
+ * @res: pointer to AI engine resource
+ *
+ * This function will release the AI engine resource data members.
+ */
+void aie_resource_uninitialize(struct aie_resource *res)
+{
+	res->total = 0;
+	if (res->bitmap)
+		bitmap_free(res->bitmap);
+}
+
+/**
+ * aie_resource_check_region() - check availability of requested resource
+ * @res: pointer to AI engine resource to check
+ * @start: start index of the required resource, it will only be used if
+ *	   @continuous is 1. It will check the available resource starting from
+ *	   @start
+ * @count: number of requested element
+ * @return: start resource id if the requested number of resources are available
+ *	    It will return negative value of errors.
+ *
+ * This function will check the availability. It will return start resource id
+ * if the requested number of resources are available.
+ */
+int aie_resource_check_region(struct aie_resource *res,
+			      u32 start, u32 count)
+{
+	unsigned long id;
+
+	if (!res || !res->bitmap || !count)
+		return -EINVAL;
+	id = bitmap_find_next_zero_area(res->bitmap, res->total, start,
+					count, 0);
+	if (id >= res->total)
+		return -ERANGE;
+
+	return (int)id;
+}
+
+/**
+ * aie_resource_get_region() - get requested AI engine resource
+ * @res: pointer to AI engine resource to check
+ * @count: number of requested element
+ * @start: start index of the required resource
+ * @return: start resource id for success, and negative value for failure.
+ *
+ * This function check if the requested AI engine resource is available.
+ * If it is available, mark it used and return the start resource id.
+ */
+int aie_resource_get_region(struct aie_resource *res, u32 start, u32 count)
+{
+	unsigned long off;
+
+	if (!res || !res->bitmap || !count)
+		return -EINVAL;
+	off = bitmap_find_next_zero_area(res->bitmap, res->total, start,
+					 count, 0);
+	if (off >= res->total) {
+		pr_err("Failed to get available AI engine resource.\n");
+		return -ERANGE;
+	}
+	bitmap_set(res->bitmap, off, count);
+
+	return (int)off;
+}
+
+/**
+ * aie_resource_put_region() - release requested AI engine resource
+ * @res: pointer to AI engine resource to check
+ * @start: start index of the resource to release
+ * @count: number of elements to release
+ *
+ * This function release the requested AI engine resource.
+ */
+void aie_resource_put_region(struct aie_resource *res, int start, u32 count)
+{
+	if (!res || !count)
+		return;
+	bitmap_clear(res->bitmap, start, count);
+}
+
+/**
+ * aie_resource_set() - set the AI engine resource bits
+ * @res: pointer to AI engine resource
+ * @start: start bit to set
+ * @count: number of bits to set
+ * @return: 0 for success and negative value for failure
+ *
+ * This function sets the specified number bits in the resource.
+ */
+int aie_resource_set(struct aie_resource *res, u32 start, u32 count)
+{
+	if (!res || !res->bitmap || !count || start + count > res->total)
+		return -EINVAL;
+
+	bitmap_set(res->bitmap, start, count);
+	return 0;
+}
+
+/**
+ * aie_resource_cpy_from_arr32() - copies nbits from u32[] to bitmap.
+ * @res: pointer to AI engine resource
+ * @start: start bit in bitmap
+ * @src: source buffer
+ * @nbits: number of bits to copy from u32[]
+ * @return: 0 for success and negative value for failure
+ */
+int aie_resource_cpy_from_arr32(struct aie_resource *res, u32 start,
+				const u32 *src, u32 nbits)
+{
+	if (!res || !res->bitmap || !nbits || start + nbits  > res->total ||
+	    !src)
+		return -EINVAL;
+
+	bitmap_from_arr32(res->bitmap + BIT_WORD(start), src, nbits);
+	return 0;
+}
+
+/**
+ * aie_resource_cpy_to_arr32() - copies nbits to u32[] from bitmap.
+ * @res: pointer to AI engine resource
+ * @start: start bit in bitmap
+ * @dst: destination buffer
+ * @nbits: number of bits to copy to u32[]
+ * @return: 0 for success and negative value for failure
+ */
+int aie_resource_cpy_to_arr32(struct aie_resource *res, u32 start, u32 *dst,
+			      u32 nbits)
+{
+	if (!res || !res->bitmap || !nbits || start + nbits  > res->total ||
+	    !dst)
+		return -EINVAL;
+
+	bitmap_to_arr32(dst, res->bitmap + BIT_WORD(start), nbits);
+	return 0;
+}
+
+/**
+ * aie_resource_clear() - clear the AI engine resource bits
+ * @res: pointer to AI engine resource
+ * @start: start bit to set
+ * @count: number of bits to clear
+ * @return: 0 for success and negative value for failure
+ *
+ * This function clears the specified number bits in the resource.
+ */
+int aie_resource_clear(struct aie_resource *res, u32 start, u32 count)
+{
+	if (!res || !res->bitmap || !count || start + count > res->total)
+		return -EINVAL;
+
+	bitmap_clear(res->bitmap, start, count);
+	return 0;
+}
+
+/**
+ * aie_resource_clear_all() - clear all the AI engine resource bits
+ * @res: pointer to AI engine resource
+ * @return: 0 for success and negative value for failure
+ *
+ * This function clears all the bits in the resource.
+ */
+int aie_resource_clear_all(struct aie_resource *res)
+{
+	if (!res || !res->bitmap)
+		return -EINVAL;
+
+	bitmap_clear(res->bitmap, 0, res->total);
+	return 0;
+}
+
+/**
+ * aie_resource_testbit() - test if a bit is set in a AI engine resource
+ * @res: pointer to AI engine resource
+ * @bit: bit to check
+ * @return: true for set, false for not set
+ */
+bool aie_resource_testbit(struct aie_resource *res, u32 bit)
+{
+	if (!res || !res->bitmap || bit >= res->total)
+		return false;
+
+	/* Locate the unsigned long the required bit belongs to */
+	return test_bit(bit, res->bitmap);
+}
+
+/**
+ * aie_resource_check_common_avail() - check common available bits
+ *				       of two resources table
+ * @res0: pointer to AI engine resource0
+ * @res1: pointer to AI engine resource1
+ * @sbit: start bit to check
+ * @nbits: number of bits to check
+ * @return: number of common bits, or negative value for failure
+ */
+int aie_resource_check_common_avail(struct aie_resource *res0,
+				    struct aie_resource *res1,
+				    u32 sbit, u32 nbits)
+{
+	u32 ebit, avails;
+
+	if (!nbits || !res0 || !res1 || !res0->bitmap || !res1->bitmap ||
+	    (sbit + nbits) > res0->total || (sbit + nbits) > res1->total)
+		return -EINVAL;
+
+	ebit = sbit + nbits - 1;
+	avails = 0;
+	while (sbit <= ebit) {
+		unsigned long  *bitmap0, *bitmap1, tbits;
+		u32 tlbit, lbit = sbit % BITS_PER_LONG;
+		u32 lnbits = ebit - sbit + 1;
+
+		if (lnbits + lbit > BITS_PER_LONG)
+			lnbits = BITS_PER_LONG - lbit;
+
+		bitmap0 = &res0->bitmap[sbit / BITS_PER_LONG];
+		bitmap1 = &res1->bitmap[sbit / BITS_PER_LONG];
+		bitmap_or(&tbits, bitmap0, bitmap1, BITS_PER_LONG);
+		tlbit = lbit;
+		while (tlbit < lbit + lnbits) {
+			u32 b = bitmap_find_next_zero_area(&tbits,
+							   BITS_PER_LONG, tlbit,
+							   1, 0);
+			if (b >= lbit + lnbits)
+				break;
+			avails++;
+			tlbit = b + 1;
+		}
+		sbit += lnbits;
+	};
+
+	return avails;
+}
+
+/**
+ * aie_resource_get_common_avail() - get common available bits
+ *				     of two resources table
+ * @rres: pointer to AI engine runtime resource, runtime resource bitmap will
+ *	  be updated if the required resources are available.
+ * @sres: pointer to AI engine static resource, static resource bitmap will
+ *	  not be updated even if the required resources are available.
+ * @sbit: start bit to check
+ * @nbits: number of bits to get
+ * @total: total number of bits to check
+ * @rscs: resources array to return for resources ids
+ * @return: number of allocated bits for success, negative value for failure
+ */
+int aie_resource_get_common_avail(struct aie_resource *rres,
+				  struct aie_resource *sres,
+				  u32 sbit, u32 nbits, u32 total,
+				  struct aie_rsc *rscs)
+{
+	u32 ebit, tsbit, tnbits;
+
+	if (!nbits || !rres || !sres || !rres->bitmap || !sres->bitmap ||
+	    nbits > total || (sbit + total) > rres->total ||
+	    (sbit + total) > sres->total)
+		return -EINVAL;
+
+	ebit = sbit + total - 1;
+	tsbit = sbit;
+	tnbits = 0;
+	while (tsbit <= ebit && tnbits != nbits) {
+		unsigned long *rbitmap, *sbitmap, tbits;
+		u32 tlbit, lbit = tsbit % BITS_PER_LONG;
+		u32 lnbits = ebit - tsbit + 1;
+
+		if (lnbits + lbit > BITS_PER_LONG)
+			lnbits = BITS_PER_LONG - lbit;
+
+		rbitmap = &rres->bitmap[sbit / BITS_PER_LONG];
+		sbitmap = &sres->bitmap[sbit / BITS_PER_LONG];
+		bitmap_or(&tbits, rbitmap, sbitmap, BITS_PER_LONG);
+		tlbit = lbit;
+		while (tlbit < lbit + lnbits && tnbits != nbits) {
+			u32 b = bitmap_find_next_zero_area(&tbits,
+							   BITS_PER_LONG, tlbit,
+							   1, 0);
+			if (b >= lbit + lnbits)
+				break;
+			rscs[tnbits].id = tsbit - sbit + b - lbit;
+			tnbits++;
+			tlbit = b + 1;
+		}
+		tsbit += lnbits;
+	};
+
+	if (tnbits != nbits)
+		return -EINVAL;
+
+	while (tnbits--)
+		aie_resource_set(rres, sbit + rscs[tnbits].id, 1);
+
+	return nbits;
+}
+
+/**
+ * aie_resource_check_pattern_region() - check availability of requested
+ *					 contiguous resources of a pattern
+ * @res: pointer to AI engine resource to check
+ * @start: start index of the required resource
+ *	   It will check a continuous block of available resource starting from
+ *	   @start.
+ * @end: end index to check
+ * @count: number of requested element
+ * @return: start resource id if the requested number of resources are available
+ *	    It will return negative value of errors.
+ *
+ * This function will check the availability. It will return start resource id
+ * if the requested number of resources are available.
+ * The contiguous resources of a pattern is e.g.
+ * @count is 0, the resources starting from @start needs to be 0,1; or 2,3 and
+ * beyond
+ */
+int aie_resource_check_pattern_region(struct aie_resource *res,
+				      u32 start, u32 end, u32 count)
+{
+	unsigned long id;
+	u32 lstart;
+
+	if (!res || !res->bitmap || !count)
+		return -EINVAL;
+	lstart = start;
+	while (lstart < end) {
+		id = bitmap_find_next_zero_area(res->bitmap, res->total, lstart,
+						count, 0);
+		if (id + count > end + 1)
+			return -ERANGE;
+		else if (!((id - lstart) % count))
+			return (int)id;
+
+		lstart += count;
+	}
+
+	return -ERANGE;
+}
+
+/**
+ * aie_resource_check_common_pattern_region() - check common available region
+ *						of two resources table
+ * @res0: pointer to AI engine resource0
+ * @res1: pointer to AI engine resource1
+ * @sbit: start bit to check
+ * @nbits: number of bits to check
+ * @total: total number of bits to check
+ * @return: start bit of common region if it is found, negative value for
+ *	    failure
+ */
+int aie_resource_check_common_pattern_region(struct aie_resource *res0,
+					     struct aie_resource *res1,
+					     u32 sbit, u32 nbits, u32 total)
+{
+	int sbit0, sbit1;
+
+	if (!nbits || !res0 || !res1 || !res0->bitmap || !res1->bitmap ||
+	    nbits > total || (sbit + total) > res0->total ||
+	    (sbit + total) > res1->total)
+		return -EINVAL;
+
+	sbit0 = aie_resource_check_pattern_region(res0, sbit,
+						  sbit + total - 1, nbits);
+	if (sbit0 < 0)
+		return sbit0;
+
+	if ((u32)sbit0 + nbits > sbit + total)
+		return -EINVAL;
+
+	sbit1 = aie_resource_check_pattern_region(res1, sbit0,
+						  sbit0 + nbits - 1, nbits);
+	if (sbit1 != sbit0)
+		return -EINVAL;
+
+	return sbit1;
+}
+
+/**
+ * aie_resource_get_common_pattern_region() - get common available region
+ *					      of two resources table
+ * @res0: pointer to AI engine resource0
+ * @res1: pointer to AI engine resource1
+ * @sbit: start bit to check
+ * @nbits: number of bits to get
+ * @total: total number of bits to check
+ * @rscs: resources array to return for resources ids
+ * @return: start bit of the common region if it is found, negative value for
+ *	    failure
+ *
+ * The common pattern region is a contiguous block of resources which needs
+ * to be very number of @nbits.
+ * e.g. if nbits is 2, the offset to the start bit @sbit of returned resources
+ * needs to be: 0,1; 2,3 ...
+ */
+int aie_resource_get_common_pattern_region(struct aie_resource *res0,
+					   struct aie_resource *res1,
+					   u32 sbit, u32 nbits, u32 total,
+					   struct aie_rsc *rscs)
+{
+	int rsbit, ret;
+
+	rsbit = aie_resource_check_common_pattern_region(res0, res1, sbit,
+							 nbits, total);
+	if (rsbit < 0)
+		return rsbit;
+
+	ret = aie_resource_get_region(res0, rsbit, nbits);
+	if (ret < 0)
+		return ret;
+
+	if (ret != rsbit) {
+		aie_resource_put_region(res0, ret, nbits);
+		return -EINVAL;
+	}
+
+	ret = aie_resource_get_region(res1, rsbit, nbits);
+	if (ret < 0)
+		return ret;
+
+	if (ret != rsbit) {
+		aie_resource_put_region(res0, rsbit, nbits);
+		aie_resource_put_region(res1, ret, nbits);
+		return -EINVAL;
+	}
+
+	if (rscs) {
+		u32 i;
+
+		for (i = 0; i < nbits; i++, rscs++)
+			rscs->id = rsbit - sbit + i;
+	}
+
+	return rsbit;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-reset.c b/drivers/misc/xilinx-ai-engine/ai-engine-reset.c
new file mode 100644
index 000000000..abf2d437d
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-reset.c
@@ -0,0 +1,552 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver resets implementation
+ *
+ * Copyright (C) 2020 Xilinx, Inc.
+ */
+
+#include <linux/bitfield.h>
+#include <linux/firmware/xlnx-zynqmp.h>
+#include <linux/io.h>
+
+#include "ai-engine-internal.h"
+
+/**
+ * aie_part_clear_core_regs_of_tile() - clear registers of aie core
+ * @apart: AI engine partition
+ * @loc: location of aie tile to clear
+ */
+static void aie_part_clear_core_regs_of_tile(struct aie_partition *apart,
+					     struct aie_location loc)
+{
+	struct aie_device *adev = apart->adev;
+	struct aie_aperture *aperture = apart->aperture;
+	const struct aie_core_regs_attr *regs = adev->core_regs;
+	u32 i;
+
+	for (i = 0; i < adev->num_core_regs; i++) {
+		u32 j, soff, eoff, reg;
+
+		soff = aie_cal_regoff(adev, loc, regs[i].core_regs->soff);
+		eoff = aie_cal_regoff(adev, loc, regs[i].core_regs->eoff);
+
+		for (reg = soff; reg <= eoff; reg += AIE_CORE_REGS_STEP) {
+			for (j = 0; j < regs[i].width; j++)
+				iowrite32(0, aperture->base + reg + j * 4);
+		}
+	}
+}
+
+/**
+ * aie_part_clear_core_regs - clear registers of aie core of a partition
+ * @apart: AI engine partition
+ */
+static void aie_part_clear_core_regs(struct aie_partition *apart)
+{
+	struct aie_range *range = &apart->range;
+	u32 c, r;
+
+	/* clear core registers for each tile in the partition */
+	for (c = range->start.col; c < range->start.col + range->size.col;
+			c++) {
+		for (r = range->start.row;
+				r < range->start.row + range->size.row; r++) {
+			struct aie_location loc;
+			u32 ttype;
+
+			loc.row = r;
+			loc.col = c;
+			ttype = apart->adev->ops->get_tile_type(apart->adev,
+								&loc);
+			if (ttype == AIE_TILE_TYPE_TILE &&
+			    aie_part_check_clk_enable_loc(apart, &loc))
+				aie_part_clear_core_regs_of_tile(apart, loc);
+		}
+	}
+}
+
+/**
+ * aie_part_clear_data_mem() - clear data memory of every tile in a partition
+ * @apart: AI engine partition
+ * @return: 0 for success and negative value for failure
+ */
+static int aie_part_clear_data_mem(struct aie_partition *apart)
+{
+	struct aie_device *adev = apart->adev;
+	struct aie_part_mem *pmems = apart->pmems;
+	struct aie_mem *mem;
+	struct aie_range *range;
+	u32 num_mems, c, r;
+
+	/* Check if memory object is present */
+	num_mems = adev->ops->get_mem_info(adev, &apart->range, NULL);
+	if (!num_mems)
+		return -EINVAL;
+
+	/* Clear data memory in the partition */
+	mem = &pmems[0].mem;
+	range = &mem->range;
+
+	for (c = range->start.col;
+		c < range->start.col + range->size.col; c++) {
+		for (r = range->start.row;
+			r < range->start.row + range->size.row; r++) {
+			struct aie_location loc;
+			u32 memoff;
+
+			loc.col = c;
+			loc.row = r;
+			memoff = aie_cal_regoff(adev, loc, mem->offset);
+			memset_io(apart->aperture->base + memoff, 0, mem->size);
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_clear_context() - clear AI engine partition context
+ * @apart: AI engine partition
+ * @return: 0 for success and negative value for failure
+ *
+ * This function will:
+ * - Gate all columns
+ * - Reset AI engine partition columns
+ * - Ungate all columns
+ * - Reset shim tiles
+ * - Setup axi mm to raise events
+ * - Setup partition isolation
+ * - Zeroize data memory
+ * - Setup L2 intrupt
+ */
+int aie_part_clear_context(struct aie_partition *apart)
+{
+	u32 node_id = apart->adev->pm_node_id;
+	int ret;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_COL_RST);
+	if (ret < 0)
+		goto exit;
+
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_SHIM_RST);
+	if (ret < 0)
+		goto exit;
+
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_ENB_AXI_MM_ERR_EVENT);
+	if (ret < 0)
+		goto exit;
+
+	ret = aie_part_init_isolation(apart);
+	if (ret < 0)
+		goto exit;
+
+	ret = zynqmp_pm_feature(PM_IOCTL);
+	if ((ret < 0) || ((ret >= 0) &&
+			((ret & FIRMWARE_VERSION_MASK) < PM_API_VERSION_3))) {
+		if (aie_part_clear_data_mem(apart))
+			dev_warn(&apart->dev, "failed to clear data memory.\n");
+	} else {
+		ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+					apart->range.size.col,
+					XILINX_AIE_OPS_DATA_MEM_ZEROIZATION |
+					XILINX_AIE_OPS_MEM_TILE_ZEROIZATION);
+		if (ret < 0)
+			goto exit;
+	}
+
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+					apart->range.size.col,
+					XILINX_AIE_OPS_SET_L2_CTRL_NPI_INTR);
+exit:
+	mutex_unlock(&apart->mlock);
+
+	return ret;
+}
+
+/**
+ * aie_part_clean() - reset and clear AI engine partition
+ * @apart: AI engine partition
+ * @return: 0 for success and negative value for failure
+ *
+ * This function will:
+ *  * gate all the columns
+ *  * reset AI engine partition columns
+ *  * reset AI engine shims
+ *  * clear the memories
+ *  * clear core registers
+ *  * gate all the tiles in a partition
+ *  * update clock state bitmap
+ *
+ * This function will not validate the partition, the caller will need to
+ * provide a valid AI engine partition.
+ */
+int aie_part_clean(struct aie_partition *apart)
+{
+	u32 node_id = apart->adev->pm_node_id;
+	int ret;
+
+	if (apart->cntrflag & XAIE_PART_NOT_RST_ON_RELEASE)
+		return 0;
+
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_DIS_COL_CLK_BUFF);
+	if (ret < 0)
+		return ret;
+
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_COL_RST |
+				      XILINX_AIE_OPS_SHIM_RST);
+	if (ret < 0)
+		return ret;
+
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_ENB_COL_CLK_BUFF);
+	if (ret < 0)
+		return ret;
+
+	apart->adev->ops->mem_clear(apart);
+	aie_part_clear_core_regs(apart);
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_DIS_COL_CLK_BUFF);
+	if (ret < 0)
+		return ret;
+
+	aie_resource_clear_all(&apart->cores_clk_state);
+
+	return 0;
+}
+
+/**
+ * aie_part_reset() - reset AI engine partition
+ * @apart: AI engine partition
+ * @return: 0 for success and negative value for failure
+ *
+ * This function will:
+ * - gate all the columns
+ * - reset AI engine partition columns
+ * - ungate all the columns
+ * - reset AI engine shims
+ * - gate all the tiles in a partition.
+ *
+ * This function will not validate the partition, the caller will need to
+ * provide a valid AI engine partition.
+ */
+int aie_part_reset(struct aie_partition *apart)
+{
+	u32 node_id = apart->adev->pm_node_id;
+	int ret;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	/*
+	 * Check if any AI engine memories or registers in the
+	 * partition have been mapped. If yes, don't reset.
+	 */
+	if (aie_part_has_mem_mmapped(apart) ||
+	    aie_part_has_regs_mmapped(apart)) {
+		dev_err(&apart->dev,
+			"failed to reset, there are mmapped memories or registers.\n");
+		mutex_unlock(&apart->mlock);
+		return -EBUSY;
+	}
+
+	/* Clear tiles in use bitmap and clock state bitmap */
+	aie_resource_clear_all(&apart->tiles_inuse);
+	aie_resource_clear_all(&apart->cores_clk_state);
+
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_DIS_COL_CLK_BUFF);
+	if (ret < 0)
+		goto exit;
+
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_COL_RST);
+	if (ret < 0)
+		goto exit;
+
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_ENB_COL_CLK_BUFF);
+	if (ret < 0)
+		goto exit;
+
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_SHIM_RST);
+	if (ret < 0)
+		goto exit;
+
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_DIS_COL_CLK_BUFF);
+	if (ret < 0)
+		goto exit;
+
+	aie_part_clear_cached_events(apart);
+	aie_part_rscmgr_reset(apart);
+
+exit:
+	mutex_unlock(&apart->mlock);
+
+	return ret;
+}
+
+/**
+ * aie_part_post_reinit() - AI engine partition has been re-initialized
+ * @apart: AI engine partition
+ * @return: 0 for success and negative value for failure
+ *
+ * This function will:
+ * - scan which tiles are gated
+ * - update memories and registers mapping
+ *
+ * This function will scan which tiles are gated, and update the memories and
+ * registers setting. This function is called after the AI engine partition is
+ * reconfigured with PDI outside the AI engine driver.
+ */
+int aie_part_post_reinit(struct aie_partition *apart)
+{
+	int ret;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	ret = aie_part_scan_clk_state(apart);
+	mutex_unlock(&apart->mlock);
+	if (ret) {
+		dev_err(&apart->dev,
+			"failed to scan clock states after reset is done.\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_init_isolation() - Set isolation boundary of AI engine partition
+ * @apart: AI engine partition
+ * @return: return 0 if success negative value for failure.
+ */
+int aie_part_init_isolation(struct aie_partition *apart)
+{
+	struct aie_range *range = &apart->range;
+	int ret;
+	u32 c, r;
+	u8 dir;
+
+	for (c = range->start.col;
+	     c < range->start.col + range->size.col; c++) {
+		if (c == range->start.col)
+			dir = AIE_ISOLATE_WEST_MASK;
+		else if (c == (range->start.col + range->size.col - 1))
+			dir = AIE_ISOLATE_EAST_MASK;
+		else
+			dir = 0;
+
+		for (r = range->start.row;
+		     r < range->start.row + range->size.row; r++) {
+			struct aie_location loc;
+
+			loc.col = c;
+			loc.row = r;
+			ret = apart->adev->ops->set_tile_isolation(apart, &loc,
+								   dir);
+			if (ret < 0) {
+				dev_err(&apart->dev,
+					"failed to set partition isolation\n");
+				return ret;
+			}
+		}
+	}
+	return ret;
+}
+
+/**
+ * aie_part_initialize() - AI engine partition initialization
+ * @apart: AI engine partition
+ * @user_args: User initialization options
+ * @return: 0 for success and negative value for failure
+ *
+ * This function will:
+ * - gate all columns
+ * - enable column reset
+ * - ungate all columns
+ * - disable column reset
+ * - reset shim tiles
+ * - setup axi mm to raise events
+ * - setup partition isolation
+ * - zeroize memory
+ */
+int aie_part_initialize(struct aie_partition *apart, void __user *user_args)
+{
+	u32 node_id = apart->adev->pm_node_id;
+	struct aie_partition_init_args args;
+	struct aie_location *locs = NULL;
+	int ret;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	/* Clear resources */
+	aie_part_clear_cached_events(apart);
+	aie_part_rscmgr_reset(apart);
+	aie_resource_clear_all(&apart->tiles_inuse);
+	aie_resource_clear_all(&apart->cores_clk_state);
+
+	/* This operation will do first 4 steps of sequence */
+	if (args.init_opts & AIE_PART_INIT_OPT_COLUMN_RST) {
+		ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+					      apart->range.size.col,
+					      XILINX_AIE_OPS_COL_RST);
+		if (ret < 0)
+			goto exit;
+	}
+
+	/* Reset Shims */
+	if (args.init_opts & AIE_PART_INIT_OPT_SHIM_RST) {
+		ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+					      apart->range.size.col,
+					      XILINX_AIE_OPS_SHIM_RST);
+		if (ret < 0)
+			goto exit;
+	}
+
+	/* Setup AXIMM events */
+	if (args.init_opts & AIE_PART_INIT_OPT_BLOCK_NOCAXIMMERR) {
+		ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+					      apart->range.size.col,
+					      XILINX_AIE_OPS_ENB_AXI_MM_ERR_EVENT);
+		if (ret < 0)
+			goto exit;
+	}
+
+	/* Setup partition isolation */
+	if (args.init_opts & AIE_PART_INIT_OPT_ISOLATE) {
+		ret = aie_part_init_isolation(apart);
+		if (ret < 0)
+			goto exit;
+	}
+
+	/* Zeroize memory */
+	if (args.init_opts & AIE_PART_INIT_OPT_ZEROIZEMEM) {
+		ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+					      apart->range.size.col,
+					      XILINX_AIE_OPS_ZEROISATION);
+		if (ret < 0)
+			goto exit;
+	}
+
+	/* Set L2 interrupt */
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_SET_L2_CTRL_NPI_INTR);
+	if (ret < 0)
+		goto exit;
+
+	/* Request tile locations */
+	if (args.num_tiles) {
+		locs = kmalloc_array(args.num_tiles, sizeof(*locs),
+				     GFP_KERNEL);
+		if (!locs) {
+			ret = -ENOMEM;
+			goto exit;
+		}
+
+		if (copy_from_user(locs, (void __user *)args.locs,
+				   args.num_tiles * sizeof(*locs))) {
+			kfree(locs);
+			ret = -EFAULT;
+			goto exit;
+		}
+	}
+	ret = aie_part_request_tiles(apart, args.num_tiles, locs);
+	kfree(locs);
+
+exit:
+	mutex_unlock(&apart->mlock);
+	return ret;
+}
+
+/**
+ * aie_part_teardown() - AI engine partition teardown
+ * @apart: AI engine partition
+ * @return: 0 for success and negative value for failure
+ *
+ * This function will:
+ * - gate all columns
+ * - enable column reset
+ * - ungate all columns
+ * - disable column reset
+ * - reset shim tiles
+ * - zeroize memory
+ * - gate all columns
+ */
+int aie_part_teardown(struct aie_partition *apart)
+{
+	u32 node_id = apart->adev->pm_node_id;
+	int ret;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	/* This operation will do first 4 steps of sequence */
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_COL_RST);
+	if (ret < 0)
+		goto exit;
+
+	/* Reset shims */
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_SHIM_RST);
+	if (ret < 0)
+		goto exit;
+
+	/* Zeroize mem */
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_ZEROISATION);
+	if (ret < 0)
+		goto exit;
+
+	/* Gate all columns */
+	ret = zynqmp_pm_aie_operation(node_id, apart->range.start.col,
+				      apart->range.size.col,
+				      XILINX_AIE_OPS_DIS_COL_CLK_BUFF);
+	if (ret < 0)
+		goto exit;
+
+	/* Clear resources */
+	aie_resource_clear_all(&apart->tiles_inuse);
+	aie_resource_clear_all(&apart->cores_clk_state);
+	aie_part_clear_cached_events(apart);
+	aie_part_rscmgr_reset(apart);
+exit:
+	mutex_unlock(&apart->mlock);
+	return ret;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-rscmgr.c b/drivers/misc/xilinx-ai-engine/ai-engine-rscmgr.c
new file mode 100644
index 000000000..9319a3688
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-rscmgr.c
@@ -0,0 +1,1555 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine partition resource manager
+ *
+ * Copyright (C) 2021 Xilinx, Inc.
+ */
+
+#include "ai-engine-internal.h"
+#include <linux/slab.h>
+
+/*
+ * Macros for the AI engine resource bitmap element header
+ */
+#define AIE_RSC_BITMAP_TILETYPE_BITSHIFT	0U
+#define AIE_RSC_BITMAP_TILETYPE_BITWIDTH	4U
+#define AIE_RSC_BITMAP_MODTYPE_BITSHIFT		4U
+#define AIE_RSC_BITMAP_MODTYPE_BITWIDTH		4U
+#define AIE_RSC_BITMAP_RSCTYPE_BITSHIFT		8U
+#define AIE_RSC_BITMAP_RSCTYPE_BITWIDTH		8U
+#define AIE_RSC_BITMAP_LENU64_BITSHIFT		16U
+#define AIE_RSC_BITMAP_LENU64_BITWIDTH		32U
+
+#define AIE_RSC_BITMAP_GEN_MASK(N) \
+	GENMASK_ULL((AIE_RSC_BITMAP_##N ##_BITSHIFT + \
+		     AIE_RSC_BITMAP_##N ##_BITWIDTH - 1), \
+		    AIE_RSC_BITMAP_##N ##_BITSHIFT)
+#define AIE_RSC_BITMAP_TILETYPE_MASK	AIE_RSC_BITMAP_GEN_MASK(TILETYPE)
+#define AIE_RSC_BITMAP_MODTYPE_MASK	AIE_RSC_BITMAP_GEN_MASK(MODTYPE)
+#define AIE_RSC_BITMAP_RSCTYPE_MASK	AIE_RSC_BITMAP_GEN_MASK(RSCTYPE)
+#define AIE_RSC_BITMAP_LENU64_MASK	AIE_RSC_BITMAP_GEN_MASK(LENU64)
+
+#define AIE_RSC_BITMAP_HEAD_VAL(N, v) \
+	(((v) & AIE_RSC_BITMAP_##N ##_MASK) >> AIE_RSC_BITMAP_##N ##_BITSHIFT)
+
+/*
+ * enum for AI engine resource bitmap allocation types
+ */
+enum aie_rsc_alloc_type {
+	AIE_RSC_ALLOC_STATIC = 0,
+	AIE_RSC_ALLOC_AVAIL = 1,
+	AIE_RSC_ALLOC_MAX = 2
+};
+
+/**
+ * struct aie_rsc_meta_header - struct of a resource bitmaps meta data header
+ * @stat: statistics information of the bitmaps, such as number of bitmaps
+ * @bitmap_off: offset to the start of the binary of the first bitmap element
+ */
+struct aie_rsc_meta_header {
+	u64 stat;
+	u64 bitmap_off;
+};
+
+/**
+ * struct aie_rsc_bitmap - struct of a resource bitmap element
+ * @header: bitmap header, it contains the following information:
+ *	    tile type, module type, resource type, and the bitmap
+ *	    length.
+ * @bitmap: the pointer of bitmap
+ */
+struct aie_rsc_bitmap {
+	u64 header;
+	u64 bitmap[0];
+};
+
+/**
+ * aie_dev_get_tile_attr - helper function to get tile attributes
+ * @adev: AI engine device
+ * @ttype: tile type
+ * @return: attributes of an AI engine tile type
+ */
+static inline
+struct aie_tile_attr *aie_dev_get_tile_attr(struct aie_device *adev,
+					    enum aie_tile_type ttype)
+{
+	return &adev->ttype_attr[ttype];
+}
+
+/**
+ * aie_dev_get_tile_rsc_attr - helper function to get resource attribute of a
+ *			       tile type of an AI engine device
+ * @adev: AI engine device
+ * @ttype: tile type
+ * @rtype: resource type
+ * @return: attributes of an AI engine resource type of a tile type
+ */
+static inline const
+struct aie_tile_rsc_attr *aie_dev_get_tile_rsc_attr(struct aie_device *adev,
+						    enum aie_tile_type ttype,
+						    enum aie_rsc_type rtype)
+{
+	return &adev->ttype_attr[ttype].rscs_attr[rtype];
+}
+
+/**
+ * aie_dev_get_mod_id - helper function to get module id of a module of a tile
+ *			type. The module ID can be used to indexing the resource
+ *			attributes of a module type of a tile type or indexing
+ *			resource status bitmaps.
+ *
+ * @adev: AI engine device
+ * @ttype: tile type
+ * @mod: module type
+ * @return: module type index
+ */
+static int aie_dev_get_mod_id(struct aie_device *adev,
+			      enum aie_tile_type ttype,
+			      enum aie_module_type mod)
+{
+	struct aie_tile_attr *tattr = &adev->ttype_attr[ttype];
+
+	int ret;
+
+	if (ttype == AIE_TILE_TYPE_TILE)
+		ret = AIE_MOD_ID(TILE, mod);
+	else if (ttype == AIE_TILE_TYPE_MEMORY)
+		ret = AIE_MOD_ID(MEMORY, mod);
+	else if (ttype == AIE_TILE_TYPE_SHIMPL)
+		ret = AIE_MOD_ID(SHIMPL, mod);
+	else
+		ret = AIE_MOD_ID(SHIMNOC, mod);
+
+	if (ret < 0 || ret > tattr->num_mods)
+		return -EINVAL;
+
+	return ret;
+}
+
+/**
+ * aie_dev_get_mod_rsc_attr - helper function to get resource attribute of a
+ *			      module of an AI engine device
+ * @adev: AI engine device
+ * @ttype: tile type
+ * @mod: module type
+ * @rtype: resource type
+ * @return: module type resource attributes
+ */
+static const
+struct aie_mod_rsc_attr *aie_dev_get_mod_rsc_attr(struct aie_device *adev,
+						  enum aie_tile_type ttype,
+						  enum aie_module_type mod,
+						  enum aie_rsc_type rtype)
+{
+	const struct aie_tile_rsc_attr *rsc = aie_dev_get_tile_rsc_attr(adev,
+									ttype,
+									rtype);
+	const struct aie_mod_rsc_attr *mrsc = NULL;
+	int mod_id = aie_dev_get_mod_id(adev, ttype, mod);
+
+	if (mod_id < 0)
+		return NULL;
+
+	mrsc = &rsc->mod_attr[mod_id];
+	if (mrsc && !mrsc->num_rscs)
+		return NULL;
+
+	return mrsc;
+}
+
+/**
+ * aie_part_get_ttype_rsc_bitmaps - helper function to get bitmap of a resource
+ *				    with tile type, module type, and resource
+ *				    type
+ *
+ * @apart: AI engine partition
+ * @ttype: tile type
+ * @mod: module type
+ * @rtype: resource type
+ * @return: pointer to AI engine resource status bitmaps if resource is found,
+ *	    otherwise NULL
+ */
+static
+struct aie_rsc_stat *aie_part_get_ttype_rsc_bitmaps(struct aie_partition *apart,
+						    enum aie_tile_type ttype,
+						    enum aie_module_type mod,
+						    enum aie_rsc_type rtype)
+{
+	int mod_id;
+	struct aie_mod_rscs *mrscs;
+
+	if (ttype >= AIE_TILE_TYPE_MAX)
+		return NULL;
+
+	mod_id = aie_dev_get_mod_id(apart->adev, ttype, mod);
+	if (mod_id < 0)
+		return NULL;
+
+	if (rtype >= AIE_RSCTYPE_MAX)
+		return NULL;
+
+	mrscs = apart->trscs[ttype].mod_rscs[rtype];
+	if (!mrscs)
+		return NULL;
+
+	return mrscs[mod_id].rscs_stat;
+}
+
+/**
+ * aie_part_get_rsc_bitmaps - helper function to get bitmap of a resource
+ *
+ * @apart: AI engine partition
+ * @loc: tile location
+ * @mod: module type
+ * @rtype: resource type
+ * @return: pointer to AI engine resource status bitmaps if resource is found,
+ *	    otherwise NULL
+ */
+static
+struct aie_rsc_stat *aie_part_get_rsc_bitmaps(struct aie_partition *apart,
+					      struct aie_location loc,
+					      enum aie_module_type mod,
+					      enum aie_rsc_type rtype)
+{
+	u32 ttype = apart->adev->ops->get_tile_type(apart->adev, &loc);
+
+	return aie_part_get_ttype_rsc_bitmaps(apart, ttype, mod, rtype);
+}
+
+/**
+ * aie_part_get_mod_num_rscs - helper function to get number of resources
+ *			       of a module of a tile
+ *
+ * @apart: AI engine partition
+ * @loc: tile location
+ * @mod: module type
+ * @rtype: resource type
+ * @return: number of max resources of a module of a tile
+ */
+static
+int aie_part_get_mod_num_rscs(struct aie_partition *apart,
+			      struct aie_location loc,
+			      enum aie_module_type mod,
+			      enum aie_rsc_type rtype)
+{
+	u32 ttype = apart->adev->ops->get_tile_type(apart->adev, &loc);
+	const struct aie_mod_rsc_attr *mattr;
+
+	mattr = aie_dev_get_mod_rsc_attr(apart->adev, ttype, mod, rtype);
+	if (!mattr)
+		return 0;
+
+	return mattr->num_rscs;
+}
+
+/**
+ * aie_part_get_rsc_startbit - helper function to get the start bit of a
+ *			       resource of a module of a tile.
+ *
+ * @apart: AI engine partition
+ * @loc: tile location
+ * @mod: module type
+ * @rtype: resource type
+ * @return: pointer to AI engine resource status bitmaps if resource is found,
+ *	    otherwise NULL
+ *
+ */
+static
+int aie_part_get_rsc_startbit(struct aie_partition *apart,
+			      struct aie_location loc,
+			      enum aie_module_type mod,
+			      enum aie_rsc_type rtype)
+{
+	struct aie_device *adev = apart->adev;
+	u32 ttype;
+	const struct aie_mod_rsc_attr *mattr;
+	int num_rows;
+	struct aie_tile_attr *tattr;
+
+	ttype = adev->ops->get_tile_type(adev, &loc);
+
+	mattr = aie_dev_get_mod_rsc_attr(adev, ttype, mod, rtype);
+	if (!mattr)
+		return -EINVAL;
+
+	num_rows = aie_part_get_tile_rows(apart, ttype);
+	tattr = &adev->ttype_attr[ttype];
+	return mattr->num_rscs *
+	       ((loc.col - apart->range.start.col) * num_rows +
+		loc.row - tattr->start_row);
+}
+
+/**
+ * aie_part_adjust_loc - adjust relative tile location to partition to
+ *			 absolute location in AI engine device
+ * @apart: AI engine partition
+ * @rloc: relative location in AI engine partition
+ * @loc: returns absolute location in AI engine device
+ * @return: 0 for success, negative value for failure
+ */
+static
+int aie_part_adjust_loc(struct aie_partition *apart,
+			struct aie_location rloc, struct aie_location *loc)
+{
+	if (aie_validate_location(apart, rloc) < 0) {
+		dev_err(&apart->dev,
+			"invalid loc (%u,%u) in (%u,%u).\n",
+			rloc.col, rloc.row,
+			apart->range.size.col, apart->range.size.row);
+		return -EINVAL;
+	}
+
+	loc->col = rloc.col + apart->range.start.col;
+	loc->row = rloc.row + apart->range.start.row;
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_init() - initialize AI engine partition resource status
+ *			    bitmaps
+ * @apart: AI engine partition
+ * @return: 0 for success, negative value for failure.
+ *
+ * This function will create the hardware resources status bitmaps for the whole
+ * partition.
+ * Each partition contains an array of hardware resources status bitmaps of all
+ * defined tiles types.:
+ * aie_partition
+ *   |- trscs[<all_tile_types>]
+ *       |-mod_rscs[<all_resources_types>]
+ *         |-rscs_stat - resources status bitmaps of a module type of a tile
+ *			 type of the AI engine partition.
+ */
+int aie_part_rscmgr_init(struct aie_partition *apart)
+{
+	struct aie_device *adev = apart->adev;
+	u32 t;
+
+	for (t = AIE_TILE_TYPE_TILE; t < AIE_TILE_TYPE_MAX; t++) {
+		struct aie_tile_rscs *trscs = &apart->trscs[t];
+		struct aie_tile_attr *tattr;
+		int num_rows, num_cols;
+		u32 r;
+
+		/*
+		 * SHIMNOC tile resource bitmaps reuse the SHIMPL resource
+		 * bitmaps. In future, for DMA resources, SHIMNOC tile will
+		 * have DMA resources bitmap which will be the unique to
+		 * SHIMNOC tiles.
+		 */
+		if (t == AIE_TILE_TYPE_SHIMNOC) {
+			*trscs = apart->trscs[AIE_TILE_TYPE_SHIMPL];
+			continue;
+		}
+
+		/*
+		 * Get the number of rows of a tile type and the number
+		 * of columns of the partition, which will be used to
+		 * calculate the size of the bitmap is a resource.
+		 */
+		tattr = aie_dev_get_tile_attr(adev, t);
+		num_rows = aie_part_get_tile_rows(apart, t);
+		num_cols = apart->range.size.col;
+
+		for (r = AIE_RSCTYPE_PERF; r < AIE_RSCTYPE_MAX; r++) {
+			const struct aie_tile_rsc_attr *trsc_attr;
+			struct aie_mod_rscs *mod_rscs;
+			u32 m;
+
+			trsc_attr = aie_dev_get_tile_rsc_attr(adev, t, r);
+			if (!trsc_attr)
+				continue;
+
+			mod_rscs = kcalloc(tattr->num_mods,
+					   sizeof(*mod_rscs), GFP_KERNEL);
+			if (!mod_rscs) {
+				aie_part_rscmgr_finish(apart);
+				return -ENOMEM;
+			}
+
+			trscs->mod_rscs[r] = mod_rscs;
+			for (m = 0 ; m < tattr->num_mods; m++) {
+				struct aie_rsc_stat *rscs_stat;
+				int num_mrscs = trsc_attr->mod_attr[m].num_rscs;
+				int ret, total_rscs;
+
+				/*
+				 * if number of resources of this module type in
+				 * this tile type is 0, skip allocating bitmap
+				 * for the resource of this module type.
+				 */
+				if (!num_mrscs)
+					continue;
+
+				rscs_stat = kzalloc(sizeof(*rscs_stat),
+						    GFP_KERNEL);
+				if (!rscs_stat) {
+					aie_part_rscmgr_finish(apart);
+					return -ENOMEM;
+				}
+
+				mod_rscs[m].rscs_stat = rscs_stat;
+				total_rscs = num_mrscs * num_rows * num_cols;
+				/*
+				 * initialize bitmaps for static resources and
+				 * runtime allocated resources.
+				 */
+				ret = aie_resource_initialize(&rscs_stat->rbits,
+							      total_rscs);
+				if (ret) {
+					aie_part_rscmgr_finish(apart);
+					return ret;
+				}
+				ret = aie_resource_initialize(&rscs_stat->sbits,
+							      total_rscs);
+				if (ret) {
+					aie_part_rscmgr_finish(apart);
+					return ret;
+				}
+			}
+		}
+	}
+
+	/* Reserve resources for interrupts */
+	return aie_part_set_intr_rscs(apart);
+}
+
+/**
+ * aie_part_rscmgr_finish() - uninitialize AI engine partition resource status
+ *			      bitmaps.
+ * @apart: AI engine partition
+ */
+void aie_part_rscmgr_finish(struct aie_partition *apart)
+{
+	struct aie_device *adev = apart->adev;
+	u32 t;
+
+	for (t = AIE_TILE_TYPE_TILE; t < AIE_TILE_TYPE_MAX; t++) {
+		struct aie_tile_rscs *trscs = &apart->trscs[t];
+		struct aie_tile_attr *tattr;
+		u32 r;
+
+		/* SHIMNOC reuses SHIMPL resources bitmap */
+		if (t == AIE_TILE_TYPE_SHIMNOC)
+			continue;
+
+		tattr = aie_dev_get_tile_attr(adev, t);
+		for (r = AIE_RSCTYPE_PERF; r < AIE_RSCTYPE_MAX; r++) {
+			struct aie_mod_rscs *mod_rscs;
+			u32 m;
+
+			mod_rscs = trscs->mod_rscs[r];
+			if (!mod_rscs)
+				continue;
+
+			for (m = 0 ; m < tattr->num_mods; m++) {
+				struct aie_rsc_stat *rscs_stat;
+
+				rscs_stat = mod_rscs[m].rscs_stat;
+				if (!rscs_stat)
+					continue;
+
+				aie_resource_uninitialize(&rscs_stat->rbits);
+				aie_resource_uninitialize(&rscs_stat->sbits);
+				kfree(rscs_stat);
+			}
+
+			kfree(mod_rscs);
+			trscs->mod_rscs[r] = NULL;
+		}
+	}
+}
+
+/**
+ * aie_part_rscmgr_reset() - reset AI engine partition resource status bitmaps
+ *
+ * @apart: AI engine partition
+ *
+ * This function expect caller to lock the partition before calling this
+ * function.
+ */
+void aie_part_rscmgr_reset(struct aie_partition *apart)
+{
+	struct aie_device *adev = apart->adev;
+	u32 t;
+
+	for (t = AIE_TILE_TYPE_TILE; t < AIE_TILE_TYPE_MAX; t++) {
+		struct aie_tile_rscs *trscs = &apart->trscs[t];
+		struct aie_tile_attr *tattr;
+		u32 r;
+
+		/* SHIMNOC reuses SHIMPL resources bitmap */
+		if (t == AIE_TILE_TYPE_SHIMNOC)
+			continue;
+
+		tattr = aie_dev_get_tile_attr(adev, t);
+		for (r = AIE_RSCTYPE_PERF; r < AIE_RSCTYPE_MAX; r++) {
+			struct aie_mod_rscs *mod_rscs;
+			u32 m;
+
+			mod_rscs = trscs->mod_rscs[r];
+			if (!mod_rscs)
+				continue;
+
+			for (m = 0 ; m < tattr->num_mods; m++) {
+				struct aie_rsc_stat *rscs_stat;
+
+				rscs_stat = mod_rscs[m].rscs_stat;
+				if (!rscs_stat)
+					continue;
+
+				aie_resource_clear_all(&rscs_stat->rbits);
+				aie_resource_clear_all(&rscs_stat->sbits);
+			}
+		}
+	}
+
+	/* Always reserve resources for interrupt */
+	(void)aie_part_set_intr_rscs(apart);
+}
+
+/**
+ * aie_part_rscmgr_rsc_req() - request a type of resource from a module of a
+ *			       tile of an AI engine partition
+ *
+ * @apart: AI engine partition
+ * @user_args: user resource request arguments
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will check if there the specified number of free resources
+ * available. If yes, allocated the specified number of resources.
+ */
+long aie_part_rscmgr_rsc_req(struct aie_partition *apart,
+			     void __user *user_args)
+{
+	struct aie_rsc_req_rsp args;
+	struct aie_location loc;
+	struct aie_rsc_stat *rstat;
+	long ret;
+	int mod_num_rscs, start_bit;
+	struct aie_rsc *rscs;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	if (!args.rscs) {
+		dev_err(&apart->dev,
+			"invalid resource request, empty resources list.\n");
+		return -EINVAL;
+	}
+
+	ret = aie_part_adjust_loc(apart, args.req.loc, &loc);
+	if (ret < 0)
+		return ret;
+
+	if (args.req.type > AIE_RSCTYPE_MAX) {
+		dev_err(&apart->dev,
+			"invalid resource request, invalid resource type %d.\n",
+			args.req.type);
+		return -EINVAL;
+	}
+
+	rstat = aie_part_get_rsc_bitmaps(apart, loc, args.req.mod,
+					 args.req.type);
+	start_bit = aie_part_get_rsc_startbit(apart, loc, args.req.mod,
+					      args.req.type);
+	if (!rstat || start_bit < 0) {
+		dev_err(&apart->dev,
+			"invalid resource request(%u,%u), mod:%u, rsc:%u.\n",
+			args.req.loc.col, args.req.loc.row, args.req.mod,
+			args.req.type);
+		return -EINVAL;
+	}
+
+	mod_num_rscs = aie_part_get_mod_num_rscs(apart, loc, args.req.mod,
+						 args.req.type);
+	if (!args.req.num_rscs || args.req.num_rscs > mod_num_rscs) {
+		dev_err(&apart->dev,
+			"invalid resource req(%u,%u),mod:%u,rsc:%u,expect=%u,max=%u.\n",
+			args.req.loc.col, args.req.loc.row, args.req.mod,
+			args.req.type, args.req.num_rscs, mod_num_rscs);
+		return -EINVAL;
+	}
+
+	rscs = kmalloc_array(args.req.num_rscs, sizeof(*rscs), GFP_KERNEL);
+	if (!rscs)
+		return -ENOMEM;
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		kfree(rscs);
+		return ret;
+	}
+
+	/*
+	 * There can be some resources needs to be contiguous, such as combo events.
+	 * It needs to be 0,1; 2,3; or 0,1,2; or 0,1,2,3
+	 */
+	if (!(args.req.flag & XAIE_RSC_PATTERN_BLOCK)) {
+		ret = aie_resource_get_common_avail(&rstat->rbits, &rstat->sbits,
+						    start_bit,
+						    args.req.num_rscs,
+						    mod_num_rscs, rscs);
+	} else {
+		ret = aie_resource_get_common_pattern_region(&rstat->rbits,
+							     &rstat->sbits,
+							     start_bit,
+							     args.req.num_rscs,
+							     mod_num_rscs,
+							     rscs);
+	}
+	mutex_unlock(&apart->mlock);
+
+	if (ret < 0) {
+		if (!(args.req.flag & XAIE_RSC_PATTERN_BLOCK)) {
+			dev_warn(&apart->dev,
+				 "invalid resource req(%u,%u),mod:%u,rsc:%u,expect=%u not avail.\n",
+				args.req.loc.col, args.req.loc.row,
+				args.req.mod, args.req.type,
+				args.req.num_rscs);
+		} else {
+			dev_warn(&apart->dev,
+				 "invalid contiguous resource req(%u,%u),mod:%u,rsc:%u,expect=%u not avail.\n",
+				args.req.loc.col, args.req.loc.row,
+				args.req.mod, args.req.type, args.req.num_rscs);
+		}
+		kfree(rscs);
+		return ret;
+	}
+
+	if (copy_to_user((void __user *)args.rscs, rscs,
+			 sizeof(*rscs) * args.req.num_rscs))
+		ret = -EFAULT;
+	else
+		ret = 0;
+
+	kfree(rscs);
+	return ret;
+}
+
+/**
+ * aie_part_rscmgr_rsc_clearbit() - clear resource status of a module of a
+ *				    tile of an AI engine partition
+ *
+ * @apart: AI engine partition
+ * @user_args: user resource release arguments
+ * @is_release: true to clear the status from both runtime and static bitmaps
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will clear the status of a resource of both runtime status
+ * bitmap and static status bitmap or both based on the @is_release setting.
+ */
+static long aie_part_rscmgr_rsc_clearbit(struct aie_partition *apart,
+					 void __user *user_args,
+					 bool is_release)
+{
+	struct aie_rsc args;
+	struct aie_location loc, rloc;
+	struct aie_rsc_stat *rstat;
+	long ret;
+	int mod_num_rscs, start_bit;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	rloc.col = (u32)(args.loc.col & 0xFF);
+	rloc.row = (u32)(args.loc.row & 0xFF);
+	ret = aie_part_adjust_loc(apart, rloc, &loc);
+	if (ret < 0)
+		return ret;
+
+	if (args.type > AIE_RSCTYPE_MAX) {
+		dev_err(&apart->dev,
+			"invalid resource to release, invalid resource type %d.\n",
+			args.type);
+		return -EINVAL;
+	}
+
+	rstat = aie_part_get_rsc_bitmaps(apart, loc, args.mod, args.type);
+	start_bit = aie_part_get_rsc_startbit(apart, loc, args.mod,
+					      args.type);
+	if (!rstat || start_bit < 0) {
+		dev_err(&apart->dev,
+			"invalid resource to release(%u,%u),mod:%u,rsc:%u.\n",
+			rloc.col, rloc.row, args.mod, args.type);
+		return -EINVAL;
+	}
+
+	mod_num_rscs = aie_part_get_mod_num_rscs(apart, loc, args.mod,
+						 args.type);
+	if (args.id > mod_num_rscs) {
+		dev_err(&apart->dev,
+			"invalid resource to release(%u,%u),mod:%u,rsc:%u,id=%u.\n",
+			rloc.col, rloc.row, args.mod, args.type, args.id);
+		return -EINVAL;
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	if (!aie_resource_testbit(&rstat->rbits, start_bit + args.id)) {
+		dev_err(&apart->dev,
+			"invalid resource to release(%u,%u),mod:%u,rsc:%u,id=%u. not requested\n",
+			rloc.col, rloc.row, args.mod, args.type, args.id);
+		mutex_unlock(&apart->mlock);
+		return -EINVAL;
+	}
+
+	aie_resource_clear(&rstat->rbits, start_bit + args.id, 1);
+	if (is_release)
+		aie_resource_clear(&rstat->sbits, start_bit + args.id, 1);
+
+	mutex_unlock(&apart->mlock);
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_rsc_release() - release resource of a module of a tile of
+ *				   an AI engine partition
+ *
+ * @apart: AI engine partition
+ * @user_args: user resource releasearguments
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will clear the bit of the resource runtime and static status
+ * bitmap.
+ */
+long aie_part_rscmgr_rsc_release(struct aie_partition *apart,
+				 void __user *user_args)
+{
+	return aie_part_rscmgr_rsc_clearbit(apart, user_args, true);
+}
+
+/**
+ * aie_part_rscmgr_rsc_free() - free resource of a module of a tile of an AI
+ *				engine partition
+ *
+ * @apart: AI engine partition
+ * @user_args: user resource free arguments
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will clear the bit of the resource runtime status bitmap.
+ */
+long aie_part_rscmgr_rsc_free(struct aie_partition *apart,
+			      void __user *user_args)
+{
+	return aie_part_rscmgr_rsc_clearbit(apart, user_args, false);
+}
+
+/**
+ * aie_part_rscmgr_rsc_req_specific() - request for specific resource of a
+ *					module of a tile of an AI engine
+ *					partition
+ *
+ * @apart: AI engine partition
+ * @user_args: user resource free arguments
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function requires the specified resource is set in the static
+ * status bitmap
+ */
+long aie_part_rscmgr_rsc_req_specific(struct aie_partition *apart,
+				      void __user *user_args)
+{
+	struct aie_rsc args;
+	struct aie_location loc, rloc;
+	struct aie_rsc_stat *rstat;
+	long ret;
+	int mod_num_rscs, start_bit;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	rloc.col = (u32)(args.loc.col & 0xFF);
+	rloc.row = (u32)(args.loc.row & 0xFF);
+	ret = aie_part_adjust_loc(apart, rloc, &loc);
+	if (ret < 0)
+		return ret;
+
+	if (args.type > AIE_RSCTYPE_MAX) {
+		dev_err(&apart->dev,
+			"invalid resource to request, invalid resource type %d.\n",
+			args.type);
+		return -EINVAL;
+	}
+
+	rstat = aie_part_get_rsc_bitmaps(apart, loc, args.mod, args.type);
+	start_bit = aie_part_get_rsc_startbit(apart, loc, args.mod,
+					      args.type);
+	if (!rstat || start_bit < 0) {
+		dev_err(&apart->dev,
+			"invalid resource to request(%u,%u),mod:%u,rsc:%u.\n",
+			rloc.col, rloc.row, args.mod, args.type);
+		return -EINVAL;
+	}
+
+	mod_num_rscs = aie_part_get_mod_num_rscs(apart, loc, args.mod,
+						 args.type);
+	if (args.id > mod_num_rscs) {
+		dev_err(&apart->dev,
+			"invalid resource to request(%u,%u),mod:%u, rsc:%u,id=%u.\n",
+			rloc.col, rloc.row, args.mod, args.type, args.id);
+		return -EINVAL;
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	/* Check if the resource is in the runtime status bitmap */
+	if (aie_resource_testbit(&rstat->rbits, start_bit + args.id)) {
+		dev_err(&apart->dev,
+			"invalid resource to request(%u,%u),mod:%u,rsc:%u,id=%u, resource in use.\n",
+			rloc.col, rloc.row, args.mod, args.type, args.id);
+		mutex_unlock(&apart->mlock);
+		return -EBUSY;
+	}
+
+	aie_resource_set(&rstat->rbits, start_bit + args.id, 1);
+
+	mutex_unlock(&apart->mlock);
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_rsc_check_avail() - check how many resources vailable for
+ *				       the specified resource type
+ *
+ * @apart: AI engine partition
+ * @user_args: user resource free arguments
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function requires the specified resource is set in the static
+ * status bitmap
+ */
+long aie_part_rscmgr_rsc_check_avail(struct aie_partition *apart,
+				     void __user *user_args)
+{
+	struct aie_rsc_stat *rstat;
+	struct aie_location loc;
+	long ret;
+	int mod_num_rscs, start_bit;
+	struct aie_rsc_req args;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	ret = aie_part_adjust_loc(apart, args.loc, &loc);
+	if (ret < 0)
+		return ret;
+
+	if (args.type > AIE_RSCTYPE_MAX) {
+		dev_err(&apart->dev,
+			"invalid resource to request, invalid resource type %d.\n",
+			args.type);
+		return -EINVAL;
+	}
+
+	rstat = aie_part_get_rsc_bitmaps(apart, loc, args.mod, args.type);
+	start_bit = aie_part_get_rsc_startbit(apart, loc, args.mod,
+					      args.type);
+	if (!rstat || start_bit < 0) {
+		dev_err(&apart->dev,
+			"invalid resource to request(%u,%u),mod:%u,rsc:%u.\n",
+			args.loc.col, args.loc.row, args.mod, args.type);
+		return -EINVAL;
+	}
+
+	mod_num_rscs = aie_part_get_mod_num_rscs(apart, loc, args.mod,
+						 args.type);
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret)
+		return ret;
+
+	args.num_rscs = aie_resource_check_common_avail(&rstat->rbits,
+							&rstat->sbits,
+							start_bit,
+							mod_num_rscs);
+	mutex_unlock(&apart->mlock);
+
+	if (copy_to_user(user_args, &args, sizeof(args)))
+		return -EFAULT;
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_get_ungated_bc_mods() - find the ungated modules of the full
+ *					   partition and fill in the locations
+ *					   information to the resources array.
+ * @apart: AI engine partition
+ * @num_rscs: number of broadcast resources, each module of a tile has a
+ *	      broadcast resource in this array.
+ * @onum_rscs: returns the number of actual ungated broadcast resources of the
+ *	       whole partition.
+ *
+ * @rscs: broadcast resources array
+ * @return: 0 for success, negative value for failure
+ */
+static int aie_part_rscmgr_get_ungated_bc_mods(struct aie_partition *apart,
+					       u32 num_rscs, u32 *onum_rscs,
+					       struct aie_rsc *rscs)
+{
+	struct aie_device *adev = apart->adev;
+	u32 c, r, i = 0;
+
+	for (c = 0; c < apart->range.size.col; c++) {
+		for (r = 0; r < apart->range.size.row; r++) {
+			struct aie_location l;
+			u32 ttype, m;
+			const struct aie_tile_attr *tattr;
+			const struct aie_tile_rsc_attr *rattr;
+			enum aie_rsc_type rtype = AIE_RSCTYPE_BROADCAST;
+
+			l.col = apart->range.start.col + c;
+			l.row = r;
+			ttype = adev->ops->get_tile_type(adev, &l);
+			tattr = &adev->ttype_attr[ttype];
+			rattr = &tattr->rscs_attr[rtype];
+			for (m = 0; m < tattr->num_mods; m++) {
+				/*
+				 * if module doesn't have broadcast channel,
+				 * skipped. This is not the case today.
+				 */
+				if (!rattr->mod_attr[m].num_rscs)
+					continue;
+				/* Check if the broadcast resource is gated */
+				if (aie_part_check_clk_enable_loc(apart, &l)) {
+					if (i >= num_rscs) {
+						dev_err(&apart->dev,
+							"failed to returns all ungated tiles, not enough resource elements.\n");
+						return -EINVAL;
+					}
+					rscs[i].loc.col = (u8)(c & 0xFF);
+					rscs[i].loc.row = (u8)(r & 0xFF);
+					rscs[i].mod = tattr->mods[m];
+					i++;
+				}
+			}
+		}
+	}
+
+	*onum_rscs = i;
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_get_or_bc_stat() - get OR the broadcast resources stat of
+ *				      specified modules in the specified
+ *				      resources array.
+ *
+ * @apart: AI engine partition
+ * @num_rscs: number of broadcast resources, every module has one broadcast
+ *	      resource
+ * @rscs: array of broadcast resources, each element contains the tile
+ *	  location and module information of the broadcast channel
+ * @runtime_only: true to only check the runtime allocated resources bitmap,
+ *		  false to check both runtime and statically allocated resource
+ *		  bitmaps.
+ * @or_stat: returns result of OR all the modules broadcast resources status
+ * @return: 0 for success, negative value for failure
+ */
+static int aie_part_rscmgr_get_or_bc_stat(struct aie_partition *apart,
+					  u32 num_rscs, struct aie_rsc *rscs,
+					  bool runtime_only,
+					  unsigned long *or_stat)
+{
+	u32 i;
+
+	*or_stat = 0;
+	for (i = 0; i < num_rscs; i++) {
+		struct aie_location l;
+		struct aie_rsc_stat *rstat;
+		int mod_num_rscs, start_bit;
+
+		l.col = apart->range.start.col + rscs[i].loc.col;
+		l.row = rscs[i].loc.row;
+		rstat = aie_part_get_rsc_bitmaps(apart, l, rscs[i].mod,
+						 AIE_RSCTYPE_BROADCAST);
+		start_bit = aie_part_get_rsc_startbit(apart, l, rscs[i].mod,
+						      AIE_RSCTYPE_BROADCAST);
+		if (!rstat || start_bit < 0) {
+			dev_err(&apart->dev,
+				"failed to get broadcast bitmap for[%u]:tile(%u,%u), mod=%u.\n",
+				i, rscs[i].loc.col, rscs[i].loc.row,
+				rscs[i].mod);
+			return -EINVAL;
+		}
+		mod_num_rscs = aie_part_get_mod_num_rscs(apart, l, rscs[i].mod,
+							 AIE_RSCTYPE_BROADCAST);
+		*or_stat |= aie_resource_or_get_valueul(&rstat->rbits,
+							start_bit,
+							mod_num_rscs);
+		if (!runtime_only)
+			*or_stat |= aie_resource_or_get_valueul(&rstat->sbits,
+								start_bit,
+								mod_num_rscs);
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_get_common_bc() - get common broadcast id of specified
+ *				     modules in the specified resources array.
+ *
+ * @apart: AI engine partition
+ * @num_rscs: number of broadcast resources, every module has one broadcast
+ *	      resource
+ * @rscs: array of broadcast resources, each element contains the tile
+ *	  location and module information of the broadcast channel
+ * @return: common broadcast channel id for success, negative value for failure
+ *
+ * This function checks both runtime and static allocated resources bitmap.
+ */
+static int aie_part_rscmgr_get_common_bc(struct aie_partition *apart,
+					 u32 num_rscs, struct aie_rsc *rscs)
+{
+	unsigned long or_stat, b;
+	int ret;
+	struct aie_location l;
+	int mod_num_rscs;
+
+	l.col = apart->range.start.col + (u32)rscs[0].loc.row;
+	l.row = (u32)rscs[0].loc.row;
+
+	ret = aie_part_rscmgr_get_or_bc_stat(apart, num_rscs, rscs, false,
+					     &or_stat);
+	if (ret)
+		return ret;
+
+	mod_num_rscs = aie_part_get_mod_num_rscs(apart, l, rscs[0].mod,
+						 AIE_RSCTYPE_BROADCAST);
+	b = bitmap_find_next_zero_area(&or_stat, mod_num_rscs, 0, 1, 0);
+	if (b >= mod_num_rscs)
+		return -EINVAL;
+
+	return (int)b;
+}
+
+/**
+ * aie_part_rscmgr_check_common_bc() - validate the specified common broadcast
+ *				       id in the specified modules in the
+ *				       specified resources array.
+ *
+ * @apart: AI engine partition
+ * @bc: broadcast channel id to check
+ * @num_rscs: number of broadcast resources, every module has one broadcast
+ *	      resource
+ * @rscs: array of broadcast resources, each element contains the tile
+ *	  location and module information of the broadcast channel
+ * @return: 0 if the specified broadcast channel id is available for all the
+ *	    specified modules, negative value for failure
+ *
+ * This function only checks runtime allocated resources bitmap.
+ */
+static int aie_part_rscmgr_check_common_bc(struct aie_partition *apart,
+					   u32 bc, u32 num_rscs,
+					   struct aie_rsc *rscs)
+{
+	unsigned long or_stat;
+	int ret;
+	struct aie_location l;
+	int mod_num_rscs;
+
+	l.col = apart->range.start.col + (u32)rscs[0].loc.row;
+	l.row = (u32)rscs[0].loc.row;
+
+	mod_num_rscs = aie_part_get_mod_num_rscs(apart, l, rscs[0].mod,
+						 AIE_RSCTYPE_BROADCAST);
+	if (bc > mod_num_rscs) {
+		dev_err(&apart->dev,
+			"invalid specified broadcast id %u, max is %u.\n",
+			bc, mod_num_rscs);
+		return -EINVAL;
+	}
+
+	ret = aie_part_rscmgr_get_or_bc_stat(apart, num_rscs, rscs, true,
+					     &or_stat);
+	if (ret)
+		return ret;
+
+	if (test_bit(bc, &or_stat)) {
+		dev_err(&apart->dev,
+			"specified broadcast id %u is occupied.\n", bc);
+		return -EBUSY;
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_check_rscs_modules() - validate the modules of the array of
+ *					input resources
+ *
+ * @apart: AI engine partition
+ * @num_rscs: number of resources
+ * @rscs: array of resources, each element contains the tile
+ *	  location and module information of the resource
+ * @return: 0 if the modules of all the resources are valid, negative value
+ *	    for failure
+ *
+ * This function validate the modules and the tiles of the resources, and
+ * check if resource module is gated.
+ */
+static int aie_part_rscmgr_check_rscs_modules(struct aie_partition *apart,
+					      u32 num_rscs,
+					      struct aie_rsc *rscs)
+{
+	struct aie_device *adev = apart->adev;
+	u32 i;
+
+	for (i = 0; i < num_rscs; i++) {
+		struct aie_location l;
+
+		l.col = rscs[i].loc.col;
+		l.row = rscs[i].loc.row;
+		/* validate tile location */
+		if (aie_validate_location(apart, l)) {
+			dev_err(&apart->dev,
+				"failed resource check tile(%u,%u) invalid.\n",
+					rscs[i].loc.col, rscs[i].loc.row);
+			return -EINVAL;
+		}
+
+		l.col += apart->range.start.col;
+		/* validate module */
+		if (aie_dev_get_mod_id(adev, adev->ops->get_tile_type(adev, &l),
+				       rscs[i].mod) < 0) {
+			dev_err(&apart->dev,
+				"failed resource check, tile(%u,%u) mod %u invalid.\n",
+					rscs[i].loc.col, rscs[i].loc.row,
+					rscs[i].mod);
+			return -EINVAL;
+		}
+
+		/* check if the resource module is gated */
+		if (!aie_part_check_clk_enable_loc(apart, &l)) {
+			dev_err(&apart->dev,
+				"failed resource check, tile(%u,%u) mod=%u is gated.\n",
+				rscs[i].loc.col, rscs[i].loc.row,
+				rscs[i].mod);
+			return -EINVAL;
+		}
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_set_tile_broadcast() - set broadcast channel in use
+ *					  of a module of a tile
+ *
+ * @apart: AI engine partition
+ * @loc: tile location
+ * @mod: module
+ * @id: broadcast channel id
+ * @return: 0 for success, negative value for failure
+ *
+ * This function will set the bit of the specified broadcast channel in the
+ * runtime broadcast bitmap of the specified module of the specified tile.
+ */
+int aie_part_rscmgr_set_tile_broadcast(struct aie_partition *apart,
+				       struct aie_location loc,
+				       enum aie_module_type mod, uint32_t id)
+{
+	struct aie_rsc_stat *rstat;
+	int start_bit;
+
+	rstat = aie_part_get_rsc_bitmaps(apart, loc, mod,
+					 AIE_RSCTYPE_BROADCAST);
+	/* bitmap pointer cannot be NULL. */
+	if (WARN_ON(!rstat || !rstat->rbits.bitmap))
+		return -EFAULT;
+
+	start_bit = aie_part_get_rsc_startbit(apart, loc, mod,
+					      AIE_RSCTYPE_BROADCAST);
+	aie_resource_set(&rstat->rbits, start_bit + id, 1);
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_get_broadcast() - get common broadcast channel of
+ *				     the specified modules or the whole
+ *				     partition.
+ *
+ * @apart: AI engine partition
+ * @user_args: user resource free arguments
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function get a common broadcast channel for the specified set
+ * of AI engine modules in the resources array. If the any of the input set of
+ * tiles is gated, it will return failure. This ioctl will not check the
+ * connection of the input modules set.
+ * The driver will fill in the resource ID with the assigned broadcast channel
+ * ID of the resources array.
+ * If the XAIE_BROADCAST_ALL is set in the request flag, it will get the
+ * broadcast channel for all the ungated tiles of the partition.
+ * If a particular broadcast channel id is specified in the request, if will
+ * check if the channel is available for the specified modules, or the whole
+ * partition depends on if XAIE_BROADCAST_ALL is set.
+ */
+long aie_part_rscmgr_get_broadcast(struct aie_partition *apart,
+				   void __user *user_args)
+{
+	struct aie_rsc_bc_req args;
+	struct aie_rsc *rscs;
+	u32 i;
+	long ret;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	rscs = kmalloc_array(args.num_rscs, sizeof(*rscs), GFP_KERNEL);
+	if (!rscs)
+		return -ENOMEM;
+
+	if (!(args.flag & XAIE_BROADCAST_ALL)) {
+		if (copy_from_user(rscs, (void __user *)args.rscs,
+				   sizeof(*rscs) * args.num_rscs)) {
+			kfree(rscs);
+			return -EFAULT;
+		}
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		kfree(rscs);
+		return ret;
+	}
+
+	if (args.flag & XAIE_BROADCAST_ALL)
+		/*
+		 * It is to broadcast to the whole partition.
+		 * Get all the ungated modules.
+		 */
+		ret = aie_part_rscmgr_get_ungated_bc_mods(apart, args.num_rscs,
+							  &args.num_rscs,
+							  rscs);
+	else
+		/*
+		 * validate tiles and modules, and check if there are modules
+		 * gated
+		 */
+		ret = aie_part_rscmgr_check_rscs_modules(apart, args.num_rscs,
+							 rscs);
+	if (ret)
+		goto error;
+
+	/* find the common broadcast signal among the specified modules */
+	if (args.id == XAIE_BROADCAST_ID_ANY) {
+		ret = aie_part_rscmgr_get_common_bc(apart, args.num_rscs, rscs);
+		if (ret >= 0) {
+			args.id = (u32)ret;
+			ret = 0;
+		} else {
+			dev_warn(&apart->dev, "no available broadcast channel.\n");
+		}
+	} else {
+		ret = aie_part_rscmgr_check_common_bc(apart, args.id,
+						      args.num_rscs,
+						      rscs);
+	}
+	if (ret)
+		goto error;
+
+	/* set the broadcast channel resource runtime status bit */
+	for (i = 0; i < args.num_rscs; i++) {
+		struct aie_location l;
+
+		l.col = apart->range.start.col + rscs[i].loc.col;
+		l.row = rscs[i].loc.row;
+		ret = aie_part_rscmgr_set_tile_broadcast(apart, l, rscs[i].mod,
+							 args.id);
+		if (ret)
+			goto error;
+
+		rscs[i].id = args.id;
+	}
+
+	mutex_unlock(&apart->mlock);
+
+	if (copy_to_user((void __user *)args.rscs, rscs,
+			 sizeof(*rscs) * args.num_rscs)) {
+		kfree(rscs);
+		return -EFAULT;
+	}
+
+	/*
+	 * If it is required to broadcast to whole partition, it needs to
+	 * return the actual number of broadcast resources as some tiles
+	 * can be gated
+	 */
+	if (args.flag & XAIE_BROADCAST_ALL) {
+		struct aie_rsc_bc_req __user *uargs = user_args;
+
+		if (copy_to_user((void __user *)&uargs->num_rscs,
+				 &args.num_rscs, sizeof(args.num_rscs))) {
+			kfree(rscs);
+			return -EFAULT;
+		}
+	}
+
+	kfree(rscs);
+	return 0;
+error:
+	mutex_unlock(&apart->mlock);
+	kfree(rscs);
+	return ret;
+}
+
+/**
+ * aie_part_rscmgr_set_static() - sets statically allocated resources bitmaps
+ *
+ * @apart: AI engine partition
+ * @meta: meta data which contains the statically allocated resources bitmaps
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function takes the static bitmap information from meta data and fill
+ * in the static bitmap.
+ */
+int aie_part_rscmgr_set_static(struct aie_partition *apart, void *meta)
+{
+	struct aie_rsc_meta_header *header = meta;
+	struct aie_rsc_bitmap *bitmap;
+	u64 i, num_bitmaps, offset;
+
+	if (!header) {
+		dev_err(&apart->dev,
+			"failed to get static resources, meta data is NULL.\n");
+		return -EINVAL;
+	}
+
+	/*
+	 * For now, the stat field of the header only contains the number of
+	 * bitmaps.
+	 */
+	num_bitmaps = header->stat;
+	offset = header->bitmap_off;
+	if (!num_bitmaps || offset < sizeof(*header)) {
+		dev_err(&apart->dev,
+			"failed to get static resources, invalid header.\n");
+		return -EINVAL;
+	}
+
+	bitmap = (struct aie_rsc_bitmap *)(meta + offset);
+	for (i = 0; i < num_bitmaps; i++) {
+		struct aie_rsc_stat *rstat;
+		const struct aie_mod_rsc_attr *mrattr;
+		u64 header = bitmap->header;
+		u32 lrlen, rlen, ttype, mtype, rtype, total;
+
+		ttype = AIE_RSC_BITMAP_HEAD_VAL(TILETYPE, header);
+		mtype = AIE_RSC_BITMAP_HEAD_VAL(MODTYPE, header);
+		rtype = AIE_RSC_BITMAP_HEAD_VAL(RSCTYPE, header);
+		rlen = AIE_RSC_BITMAP_HEAD_VAL(LENU64, header);
+
+		if (!rlen) {
+			dev_err(&apart->dev,
+				"invalid static bitmap[%llu], length is 0.\n",
+				i);
+			return -EINVAL;
+		}
+
+		mrattr = aie_dev_get_mod_rsc_attr(apart->adev, ttype, mtype,
+						  rtype);
+		if (!mrattr) {
+			dev_err(&apart->dev,
+				"invalid static bitmap[%llu], invalid tile(%u)/module(%u)/rsce(%u) types combination.\n",
+				i, ttype, mtype, rtype);
+			return -EINVAL;
+		}
+
+		total = mrattr->num_rscs * apart->range.size.col *
+			aie_part_get_tile_rows(apart, ttype);
+		lrlen = BITS_TO_LONGS(total);
+		if (rlen != lrlen) {
+			dev_err(&apart->dev,
+				"invalid static bitmap[%llu], tile(%u)/module(%u)/rscs(%u), expect len(%u), actual(%u).\n",
+				i, ttype, mtype, rtype, lrlen, rlen);
+			return -EINVAL;
+		}
+
+		rstat = aie_part_get_ttype_rsc_bitmaps(apart, ttype, mtype,
+						       rtype);
+		/* if bitmap length is not 0, bitmap pointer cannot be NULL. */
+		if (WARN_ON(!rstat || !rstat->sbits.bitmap))
+			return -EFAULT;
+
+		/* copy the bitmap from meta data */
+		bitmap_copy(rstat->sbits.bitmap,
+			    (unsigned long *)bitmap->bitmap, total);
+
+		bitmap = (struct aie_rsc_bitmap *)((void *)bitmap +
+						   sizeof(header) +
+						   rlen * sizeof(u64));
+	}
+
+	return 0;
+}
+
+/**
+ * aie_part_rscmgr_check_static() - check the number of static resources
+ *
+ * @rstat: resource statistics structure which contains bitmaps of a resource
+ *	   type of a module type of a tile type.
+ * @sbit: start bit of the resource bitmap of a tile of a module
+ * @total: number of total resources bits to check
+ *
+ * @return: number of static resources
+ *
+ * This function returns the number of static resources of a resource
+ * bitmap.
+ */
+static int aie_part_rscmgr_check_static(struct aie_rsc_stat *rstat,
+					u32 sbit, u32 total)
+{
+	u32 i;
+	int num_static = 0;
+
+	for (i = sbit; i < sbit + total; i++) {
+		if (aie_resource_testbit(&rstat->sbits, i))
+			num_static++;
+	}
+
+	return num_static;
+}
+
+/**
+ * aie_part_rscmgr_check_avail() - check the number of available resources
+ *
+ * @rstat: resource statistics structure which contains bitmaps of a resource
+ *	   type of a module type of a tile type.
+ * @sbit: start bit of the resource bitmap of a tile of a module
+ * @total: number of total resources bits to check
+ *
+ * @return: number of available resources for success, negative value for
+ *	    failure
+ *
+ * This function returns the number of available resources of a resource
+ * bitmap.
+ */
+static int aie_part_rscmgr_check_avail(struct aie_rsc_stat *rstat,
+				       u32 sbit, u32 total)
+{
+	return aie_resource_check_common_avail(&rstat->rbits,
+					       &rstat->sbits,
+					       sbit, total);
+}
+
+/**
+ * aie_part_rscmgr_get_statistics() - get resource statistics based on user
+ *				      request
+ *
+ * @apart: AI engine partition
+ * @user_args: user resource statistics request. it contains the number of
+ *	       resource statistics wants to get followed by the statistics
+ *	       array and the statistics type to specify if it is for static
+ *	       allocated resources or available resources. Each statistics
+ *	       element contains the tile location, module type and the resource
+ *	       type.
+ *
+ * @return: 0 for success, negative value for failure
+ *
+ * This function returns the resource statistics based on the user request.
+ * If user requests for available resource statistics, it returns the number
+ * of available resources of each resource statistics entry. If user requests
+ * for static resources statistics, it returns the number of static resources
+ * of each resource statistics entry.
+ */
+long aie_part_rscmgr_get_statistics(struct aie_partition *apart,
+				    void __user *user_args)
+{
+	struct aie_rsc_user_stat_array args;
+	struct aie_rsc_user_stat __user *ustat_ptr;
+	u32 i;
+
+	if (copy_from_user(&args, user_args, sizeof(args)))
+		return -EFAULT;
+
+	if (args.stats_type >= AIE_RSC_STAT_TYPE_MAX) {
+		dev_err(&apart->dev,
+			"get rsc statistics failed, invalid rsc stat type %u.\n",
+			args.stats_type);
+		return -EINVAL;
+	}
+
+	ustat_ptr = (struct aie_rsc_user_stat __user *)args.stats;
+	for (i = 0; i < args.num_stats; i++) {
+		struct aie_rsc_user_stat ustat;
+		struct aie_rsc_stat *rstat;
+		struct aie_location rloc, loc;
+		long ret;
+		int max_rscs, start_bit;
+
+		if (copy_from_user(&ustat, (void __user *)ustat_ptr,
+				   sizeof(ustat)))
+			return -EFAULT;
+
+		/* convert user tile loc to kernel tile loc format */
+		rloc.col = (u32)(ustat.loc.col & 0xFF);
+		rloc.row = (u32)(ustat.loc.row & 0xFF);
+		ret = aie_part_adjust_loc(apart, rloc, &loc);
+		if (ret < 0)
+			return ret;
+
+		if (ustat.type > AIE_RSCTYPE_MAX) {
+			dev_err(&apart->dev,
+				"get rsc statistics failed, invalid resource type %d.\n",
+				ustat.type);
+			return -EINVAL;
+		}
+
+		rstat = aie_part_get_rsc_bitmaps(apart, loc, ustat.mod,
+						 ustat.type);
+		start_bit = aie_part_get_rsc_startbit(apart, loc, ustat.mod,
+						      ustat.type);
+		if (!rstat || start_bit < 0) {
+			dev_err(&apart->dev,
+				"get rsc statistics failed, invalid resource(%u,%u),mod:%u,rsc:%u.\n",
+				loc.col, loc.row, ustat.mod, ustat.type);
+			return -EINVAL;
+		}
+
+		max_rscs = aie_part_get_mod_num_rscs(apart, loc, ustat.mod,
+						     ustat.type);
+		ret = mutex_lock_interruptible(&apart->mlock);
+		if (ret)
+			return ret;
+
+		if (args.stats_type == AIE_RSC_STAT_TYPE_STATIC)
+			ustat.num_rscs = aie_part_rscmgr_check_static(rstat,
+								      start_bit,
+								      max_rscs);
+		else
+			ustat.num_rscs = aie_part_rscmgr_check_avail(rstat,
+								     start_bit,
+								     max_rscs);
+
+		mutex_unlock(&apart->mlock);
+		if (WARN_ON(ustat.num_rscs < 0))
+			return -EFAULT;
+
+		/* copy the information back to userspace */
+		if (copy_to_user((void __user *)ustat_ptr, &ustat,
+				 sizeof(ustat)))
+			return -EFAULT;
+
+		ustat_ptr++;
+	}
+
+	return 0;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-status-dump.c b/drivers/misc/xilinx-ai-engine/ai-engine-status-dump.c
new file mode 100644
index 000000000..9f3458ed1
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-status-dump.c
@@ -0,0 +1,378 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine Status Dump.
+ *
+ * Copyright (C) 2023 AMD, Inc.
+ */
+#include "ai-engine-internal.h"
+#include "linux/xlnx-ai-engine.h"
+
+/*
+ * Version Number to maintain Tile and Column Structure change
+ * between Linux Driver and Application
+ */
+#define MAJOR_VERSION 1
+#define MINOR_VERSION 1
+
+/**
+ * aie_tile_core_status() - Stores AI engine core status, value of program
+ *                        counter, stack pointer, and link register to a tile
+ *                        column structure.
+ * @apart: AI engine Partition.
+ * @status: Pointer to Structure which store status value of
+ *		Tile core registers
+ * @loc: Location of AI Engine Tile
+ * @return: 0 for success, negative value for failure
+ */
+static int aie_tile_core_status(struct aie_partition *apart,
+				struct aie_col_status *status,
+				struct aie_location *loc)
+{
+	u8 tile_st;
+	u32 ttype;
+	int ret;
+
+	if (apart->adev->dev_gen != AIE_DEVICE_GEN_AIEML) {
+		dev_warn(&apart->dev, "Skipping Tile core status For Non AIEML Devices\n");
+		return 0;
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock Process was interrupted by fatal signals\n");
+		return ret;
+	}
+
+	tile_st = apart->adev->ttype_attr[AIE_TILE_TYPE_TILE].start_row;
+	ttype = apart->adev->ops->get_tile_type(apart->adev, loc);
+
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		if (aie_part_check_clk_enable_loc(apart, loc)) {
+			status[loc->col].core_tile[loc->row - tile_st].prg_cntr =
+				aie_get_core_pc(apart, loc);
+			status[loc->col].core_tile[loc->row - tile_st].link_reg =
+				aie_get_core_lr(apart, loc);
+			status[loc->col].core_tile[loc->row - tile_st].stack_ptr =
+				aie_get_core_sp(apart, loc);
+			status[loc->col].core_tile[loc->row - tile_st].core_status =
+				apart->adev->ops->get_core_status(apart, loc);
+		}
+	}
+
+	mutex_unlock(&apart->mlock);
+
+	return 0;
+}
+
+/**
+ * aie_dma_status() - Stores AI engine DMA status values to a tile
+ *                        column structure.
+ * @apart: AI engine Partition.
+ * @status: Pointer to Structure which store status value of Tile DMA registers
+ * @loc: Location of AI Engine Tile
+ * @return: 0 for success, negative value for failure
+ */
+static int aie_dma_status(struct aie_partition *apart, struct aie_col_status *status,
+			  struct aie_location *loc)
+{
+	u32 ttype, i, num_s2mm_chan, num_mm2s_chan, status_val, value;
+	struct aie_core_tile_status *sts_coretile;
+	struct aie_shim_tile_status *sts_shimtile;
+	struct aie_mem_tile_status *sts_memtile;
+	u8 tile_st, memtile_st, index;
+	int ret;
+
+	if (apart->adev->dev_gen != AIE_DEVICE_GEN_AIEML) {
+		dev_warn(&apart->dev, "Skipping DMA Status For Non AIEML Devices\n");
+		return 0;
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock Process was interrupted by fatal signals\n");
+		return ret;
+	}
+
+	tile_st = apart->adev->ttype_attr[AIE_TILE_TYPE_TILE].start_row;
+	memtile_st = apart->adev->ttype_attr[AIE_TILE_TYPE_MEMORY].start_row;
+
+	ttype = apart->adev->ops->get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		num_mm2s_chan = apart->adev->tile_dma->num_mm2s_chan;
+		num_s2mm_chan = apart->adev->tile_dma->num_s2mm_chan;
+	} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+		num_mm2s_chan = apart->adev->memtile_dma->num_mm2s_chan;
+		num_s2mm_chan = apart->adev->memtile_dma->num_s2mm_chan;
+	} else {
+		num_mm2s_chan = apart->adev->shim_dma->num_mm2s_chan;
+		num_s2mm_chan = apart->adev->shim_dma->num_s2mm_chan;
+	}
+
+	if (aie_part_check_clk_enable_loc(apart, loc)) {
+		const struct aie_tile_operations *ops;
+
+		ops = apart->adev->ops;
+		sts_coretile = status[loc->col].core_tile;
+		sts_memtile = status[loc->col].mem_tile;
+		sts_shimtile = status[loc->col].shim_tile;
+
+		for (i = 0; i < num_s2mm_chan; i++) {
+			status_val = ops->get_dma_s2mm_status(apart, loc, i);
+			value = ops->get_chan_status(apart, loc, status_val);
+
+			if (ttype == AIE_TILE_TYPE_TILE) {
+				index = loc->row - tile_st;
+				sts_coretile[index].dma[i].s2mm_sts = value;
+			} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+				index = loc->row - memtile_st;
+				sts_memtile[index].dma[i].s2mm_sts = value;
+			} else {
+				sts_shimtile[loc->row].dma[i].s2mm_sts = value;
+			}
+		}
+
+		for (i = 0; i < num_mm2s_chan; i++) {
+			status_val = ops->get_dma_mm2s_status(apart, loc, i);
+			value = ops->get_chan_status(apart, loc, status_val);
+
+			if (ttype == AIE_TILE_TYPE_TILE) {
+				index = loc->row - tile_st;
+				sts_coretile[index].dma[i].mm2s_sts = value;
+			} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+				index = loc->row - memtile_st;
+				sts_memtile[index].dma[i].mm2s_sts = value;
+			} else {
+				sts_shimtile[loc->row].dma[i].mm2s_sts = value;
+			}
+		}
+	}
+	mutex_unlock(&apart->mlock);
+
+	return 0;
+}
+
+/**
+ * aie_lock_status() - Stores AI engine Tile Lock status values to a tile
+ *                        column structure.
+ * @apart: AI engine Partition.
+ * @status: Pointer to Structure which store status value of Tile Lock registers
+ * @loc: Location of AI Engine Tile
+ * @return: 0 for success, negative value for failure
+ */
+static int aie_lock_status(struct aie_partition *apart, struct aie_col_status *status,
+			   struct aie_location *loc)
+{
+	struct aie_core_tile_status *sts_coretile;
+	struct aie_shim_tile_status *sts_shimtile;
+	u32 ttype, num_locks, lock_val, row, col;
+	struct aie_mem_tile_status *sts_memtile;
+	u8 i, tile_st, memtile_st, index;
+	int ret;
+
+	if (apart->adev->dev_gen != AIE_DEVICE_GEN_AIEML) {
+		dev_warn(&apart->dev, "Skipping Lock Status For Non AIEML Devices\n");
+		return 0;
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dev_err(&apart->dev, "Failed to acquire lock Process was interrupted by fatal signals\n");
+		return ret;
+	}
+
+	tile_st = apart->adev->ttype_attr[AIE_TILE_TYPE_TILE].start_row;
+	memtile_st = apart->adev->ttype_attr[AIE_TILE_TYPE_MEMORY].start_row;
+
+	loc->row = row;
+	loc->col = col;
+	ttype = apart->adev->ops->get_tile_type(apart->adev,
+						loc);
+
+	if (aie_part_check_clk_enable_loc(apart, loc)) {
+		const struct aie_tile_operations *ops;
+
+		ops = apart->adev->ops;
+		sts_coretile = status[loc->col].core_tile;
+		sts_memtile = status[loc->col].mem_tile;
+		sts_shimtile = status[loc->col].shim_tile;
+
+		if (ttype == AIE_TILE_TYPE_TILE)
+			num_locks = apart->adev->mem_lock->num_locks;
+		else if (ttype == AIE_TILE_TYPE_MEMORY)
+			num_locks = apart->adev->memtile_lock->num_locks;
+		else
+			num_locks = apart->adev->pl_lock->num_locks;
+
+		for (i = 0; i < num_locks; i++) {
+			lock_val = ops->get_lock_status(apart, loc, i);
+
+			if (ttype == AIE_TILE_TYPE_TILE) {
+				index = loc->row - tile_st;
+				sts_coretile[index].lock_value[i] = lock_val;
+			} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+				index = loc->row - memtile_st;
+				sts_memtile[index].lock_value[i] = lock_val;
+			} else {
+				sts_shimtile[loc->row].lock_value[i] = lock_val;
+			}
+		}
+	}
+	mutex_unlock(&apart->mlock);
+
+	return 0;
+}
+
+/**
+ * aie_event_status() - Stores AI engine Tile Event status values to a tile
+ *                        column structure.
+ * @apart: AI engine partition.
+ * @status: Pointer to Structure which store status value of Tile Lock registers
+ * @loc: Location of AI Engine Tile
+ * @return: 0 for success, negative value for failure
+ */
+static int aie_event_status(struct aie_partition *apart, struct aie_col_status *status,
+			    struct aie_location *loc)
+{
+	struct aie_core_tile_status *sts_coretile;
+	struct aie_shim_tile_status *sts_shimtile;
+	struct aie_mem_tile_status *sts_memtile;
+	u8 tile_st, memtile_st, index;
+	u32 ttype;
+	int ret;
+
+	if (apart->adev->dev_gen != AIE_DEVICE_GEN_AIEML) {
+		dev_warn(&apart->dev, "Skipping Event Status For Non AIEML Devices\n");
+		return 0;
+	}
+
+	ret = mutex_lock_interruptible(&apart->mlock);
+	if (ret) {
+		dev_err(&apart->dev, "Failed to acquire lock Process was interrupted by fatal signals\n");
+		return ret;
+	}
+
+	tile_st = apart->adev->ttype_attr[AIE_TILE_TYPE_TILE].start_row;
+	memtile_st = apart->adev->ttype_attr[AIE_TILE_TYPE_MEMORY].start_row;
+
+	ttype = apart->adev->ops->get_tile_type(apart->adev, loc);
+	if (aie_part_check_clk_enable_loc(apart, loc)) {
+		sts_coretile = status[loc->col].core_tile;
+		sts_memtile = status[loc->col].mem_tile;
+		sts_shimtile = status[loc->col].shim_tile;
+
+		if (ttype == AIE_TILE_TYPE_TILE) {
+			index = loc->row - tile_st;
+			aie_read_event_status(apart, loc, AIE_CORE_MOD,
+					      sts_coretile[index].core_mode_event_sts);
+			aie_read_event_status(apart, loc, AIE_MEM_MOD,
+					      sts_coretile[index].mem_mode_event_sts);
+		} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+			index = loc->row - memtile_st;
+			aie_read_event_status(apart, loc, AIE_MEM_MOD,
+					      sts_memtile[index].event_sts);
+		} else {
+			aie_read_event_status(apart, loc, AIE_PL_MOD,
+					      sts_shimtile[loc->row].event_sts);
+		}
+	}
+	mutex_unlock(&apart->mlock);
+
+	return 0;
+}
+
+/**
+ * aie_get_status_dump() - exports AI engine core status, value of program
+ *                        counter, stack pointer, and link register to a tile
+ *                        level sysfs node.
+ * @dev: AI engine tile device.
+ * @status: Pointer to Structure which stores status value of
+ * Tile Core regisers and Tile DMA registers
+ * @return: 0 for success, negative value for failure
+ */
+int aie_get_status_dump(struct device *dev, struct aie_col_status *status)
+{
+	struct aie_partition *apart = dev_to_aiepart(dev);
+	struct aie_location loc;
+	u32 row, col, ttype;
+	int ret;
+
+	if (!status)
+		return -EFAULT;
+
+	for (col = apart->range.start.col; col < apart->range.size.col; col++) {
+		for (row = apart->range.start.row; row < apart->range.size.row; row++) {
+			loc.row = row;
+			loc.col = col;
+
+			ttype = apart->adev->ops->get_tile_type(apart->adev, &loc);
+			if (ttype == AIE_TILE_TYPE_SHIMPL)
+				continue;
+			/* Get Status of Core Tile and DMA */
+			ret = aie_tile_core_status(apart, status, &loc);
+			if (ret) {
+				dev_err(dev, "aie_tile_core_status API Failed\n");
+				return ret;
+			}
+
+			ret = aie_dma_status(apart, status, &loc);
+			if (ret) {
+				dev_err(dev, "aie_dma_status API Failed\n");
+				return ret;
+			}
+
+			ret = aie_lock_status(apart, status, &loc);
+			if (ret) {
+				dev_err(dev, "aie_lock_status API Failed\n");
+				return ret;
+			}
+
+			ret = aie_event_status(apart, status, &loc);
+			if (ret) {
+				dev_err(dev, "aie_event_status API Failed\n");
+				return ret;
+			}
+		}
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(aie_get_status_dump);
+
+/**
+ * aie_get_tile_info() - exports AI engine tile information
+ * @dev: AI engine tile device.
+ * @tile_info: Pointer to Structure which stores Tile information
+ * @return: 0 for success, negative value for failure
+ */
+int aie_get_tile_info(struct device *dev, struct aie_tile_info *tile_info)
+{
+	struct aie_partition *apart = dev_to_aiepart(dev);
+
+	if (!tile_info)
+		return -EFAULT;
+
+	tile_info->minor = MINOR_VERSION;
+	tile_info->major = MAJOR_VERSION;
+	tile_info->cols = apart->range.size.col;
+	tile_info->rows = apart->range.size.row;
+	tile_info->core_rows = apart->adev->ttype_attr[AIE_TILE_TYPE_TILE].num_rows;
+	tile_info->mem_rows = apart->adev->ttype_attr[AIE_TILE_TYPE_MEMORY].num_rows;
+	tile_info->shim_rows = apart->adev->ttype_attr[AIE_TILE_TYPE_SHIMPL].num_rows;
+	tile_info->core_row_start = apart->adev->ttype_attr[AIE_TILE_TYPE_TILE].start_row;
+	tile_info->mem_row_start = apart->adev->ttype_attr[AIE_TILE_TYPE_MEMORY].start_row;
+	tile_info->shim_row_start = apart->adev->ttype_attr[AIE_TILE_TYPE_SHIMPL].start_row;
+	tile_info->core_dma_channels = apart->adev->tile_dma->num_s2mm_chan;
+	tile_info->shim_dma_channels = apart->adev->shim_dma->num_s2mm_chan;
+	tile_info->mem_dma_channels = apart->adev->memtile_dma->num_mm2s_chan;
+	tile_info->core_locks = apart->adev->mem_lock->num_locks;
+	tile_info->mem_locks = apart->adev->memtile_lock->num_locks;
+	tile_info->shim_locks = apart->adev->pl_lock->num_locks;
+	tile_info->core_events = (apart->adev->core_events->num_events) / 32;
+	tile_info->mem_events = (apart->adev->memtile_events->num_events) / 32;
+	tile_info->shim_events = (apart->adev->pl_events->num_events) / 32;
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(aie_get_tile_info);
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-clock.c b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-clock.c
new file mode 100644
index 000000000..33b3b61e0
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-clock.c
@@ -0,0 +1,37 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver sysfs for clock.
+ *
+ * Copyright (C) 2022 Xilinx, Inc.
+ */
+#include "ai-engine-internal.h"
+
+/**
+ * aie_part_show_current_freq() - exports AI engine partition current frequency
+ *
+ * @dev: AI engine tile device.
+ * @attr: sysfs device attribute.
+ * @buffer: export buffer.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_part_show_current_freq(struct device *dev,
+				   struct device_attribute *attr, char *buffer)
+{
+	struct aie_partition *apart = dev_to_aiepart(dev);
+	int ret = 0;
+	u64 freq;
+
+	if (mutex_lock_interruptible(&apart->mlock))
+		return ret;
+
+	ret = aie_part_get_freq(apart, &freq);
+	if (ret) {
+		dev_err(dev, "Failed to get partition frequency.\n");
+		mutex_unlock(&apart->mlock);
+		return 0;
+	}
+
+	mutex_unlock(&apart->mlock);
+
+	return scnprintf(buffer, PAGE_SIZE, "%llu\n", freq);
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-core.c b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-core.c
new file mode 100644
index 000000000..471f03998
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-core.c
@@ -0,0 +1,191 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver.
+ *
+ * Copyright (C) 2021 Xilinx, Inc.
+ */
+#include "ai-engine-internal.h"
+
+#define AIE_CORE_STS_ENABLE_MASK	0x3U
+
+/**
+ * aie_get_core_pc() - reads the AI engine core program counter value.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine core.
+ * @return: 32-bit register value.
+ */
+u32 aie_get_core_pc(struct aie_partition *apart,
+		    struct aie_location *loc)
+{
+	u32 regoff;
+
+	regoff = aie_cal_regoff(apart->adev, *loc,
+				apart->adev->core_pc->regoff);
+	return ioread32(apart->aperture->base + regoff);
+}
+
+/**
+ * aie_get_core_lr() - reads the AI engine core link register value.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine core.
+ * @return: 32-bit register value.
+ */
+u32 aie_get_core_lr(struct aie_partition *apart,
+		    struct aie_location *loc)
+{
+	u32 regoff;
+
+	regoff = aie_cal_regoff(apart->adev, *loc,
+				apart->adev->core_lr->regoff);
+	return ioread32(apart->aperture->base + regoff);
+}
+
+/**
+ * aie_get_core_sp() - reads the AI engine core stack pointer value.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine core.
+ * @return: 32-bit register value.
+ */
+u32 aie_get_core_sp(struct aie_partition *apart,
+		    struct aie_location *loc)
+{
+	u32 regoff;
+
+	regoff = aie_cal_regoff(apart->adev, *loc,
+				apart->adev->core_sp->regoff);
+	return ioread32(apart->aperture->base + regoff);
+}
+
+/**
+ * aie_sysfs_get_core_status() - returns the status of core in string format
+ *				 with each status value separated by a '|'
+ *				 symbol.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine core.
+ * @buffer: location to return core status string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_sysfs_get_core_status(struct aie_partition *apart,
+				  struct aie_location *loc, char *buffer,
+				  ssize_t size)
+{
+	ssize_t len = 0;
+	u32 ttype, n;
+	unsigned long status;
+	bool is_delimit_req = false;
+	char **str = apart->adev->core_status_str;
+
+	ttype = apart->adev->ops->get_tile_type(apart->adev, loc);
+	if (ttype != AIE_TILE_TYPE_TILE)
+		return len;
+
+	if (!aie_part_check_clk_enable_loc(apart, loc)) {
+		len += scnprintf(buffer, max(0L, size), "clock_gated");
+		return len;
+	}
+
+	/*
+	 * core is in disabled state when neither the enable nor reset bit is
+	 * high
+	 */
+	status = apart->adev->ops->get_core_status(apart, loc);
+	if (!(status & AIE_CORE_STS_ENABLE_MASK)) {
+		len += scnprintf(&buffer[len], max(0L, size - len), "disabled");
+		is_delimit_req = true;
+	}
+
+	for_each_set_bit(n, &status, 32) {
+		if (is_delimit_req) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 DELIMITER_LEVEL0);
+		}
+		len += scnprintf(&buffer[len], max(0L, size - len), str[n]);
+		is_delimit_req = true;
+	}
+
+	return len;
+}
+
+/**
+ * aie_tile_show_core() - exports AI engine core status, value of program
+ *			  counter, stack pointer, and link register to a tile
+ *			  level sysfs node.
+ * @dev: AI engine tile device.
+ * @attr: sysfs device attribute.
+ * @buffer: export buffer.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_tile_show_core(struct device *dev, struct device_attribute *attr,
+			   char *buffer)
+{
+	struct aie_tile *atile = container_of(dev, struct aie_tile, dev);
+	struct aie_partition *apart = atile->apart;
+	ssize_t len = 0, size = PAGE_SIZE;
+	u32 pc = 0, lr = 0, sp = 0;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	len += scnprintf(&buffer[len], max(0L, size - len), "status: ");
+	len += aie_sysfs_get_core_status(apart, &atile->loc, &buffer[len],
+					 size - len);
+	len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+
+	if (aie_part_check_clk_enable_loc(apart, &atile->loc)) {
+		pc = aie_get_core_pc(apart, &atile->loc);
+		lr = aie_get_core_lr(apart, &atile->loc);
+		sp = aie_get_core_sp(apart, &atile->loc);
+	}
+
+	len += scnprintf(&buffer[len], max(0L, size - len), "pc: %#.8x\n", pc);
+	len += scnprintf(&buffer[len], max(0L, size - len), "lr: %#.8x\n", lr);
+	len += scnprintf(&buffer[len], max(0L, size - len), "sp: %#.8x\n", sp);
+
+	mutex_unlock(&apart->mlock);
+	return len;
+}
+
+/**
+ * aie_part_read_cb_core() - exports status of all cores within a given
+ *			     partition to partition level node.
+ * @kobj: kobject used to create sysfs node.
+ * @buffer: export buffer.
+ * @size: length of export buffer available.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_part_read_cb_core(struct kobject *kobj, char *buffer, ssize_t size)
+{
+	struct device *dev = container_of(kobj, struct device, kobj);
+	struct aie_partition *apart = dev_to_aiepart(dev);
+	struct aie_tile *atile = apart->atiles;
+	ssize_t len = 0;
+	u32 index;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	for (index = 0; index < apart->range.size.col * apart->range.size.row;
+	     index++, atile++) {
+		u32 ttype = apart->adev->ops->get_tile_type(apart->adev,
+							    &atile->loc);
+
+		if (ttype != AIE_TILE_TYPE_TILE)
+			continue;
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d_%d: ",
+				 atile->loc.col, atile->loc.row);
+		len += aie_sysfs_get_core_status(apart, &atile->loc,
+						 &buffer[len], size - len);
+		len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	}
+
+	mutex_unlock(&apart->mlock);
+	return len;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-dma.c b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-dma.c
new file mode 100644
index 000000000..95c53ca5e
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-dma.c
@@ -0,0 +1,108 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver.
+ *
+ * Copyright (C) 2021 Xilinx, Inc.
+ */
+#include "ai-engine-internal.h"
+
+/**
+ * aie_tile_show_bd() - exports AI engine DMA buffer descriptor metadata for
+ *			all buffer descriptors to a tile level sysfs node.
+ * @dev: AI engine tile device.
+ * @attr: sysfs device attribute.
+ * @buffer: export buffer.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_tile_show_bd(struct device *dev, struct device_attribute *attr,
+			 char *buffer)
+{
+	struct aie_tile *atile = container_of(dev, struct aie_tile, dev);
+	struct aie_partition *apart = atile->apart;
+	ssize_t len = 0, size = PAGE_SIZE;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	len = apart->adev->ops->get_tile_sysfs_bd_metadata(apart, &atile->loc,
+							   &buffer[len],
+							   size);
+
+	mutex_unlock(&apart->mlock);
+	return len;
+}
+
+/**
+ * aie_tile_show_dma() - exports AI engine DMA channel status, queue size,
+ *			 queue status, and current buffer descriptor ID being
+ *			 processed by DMA channel to a tile level sysfs node.
+ * @dev: AI engine tile device.
+ * @attr: sysfs device attribute.
+ * @buffer: export buffer.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_tile_show_dma(struct device *dev, struct device_attribute *attr,
+			  char *buffer)
+{
+	struct aie_tile *atile = container_of(dev, struct aie_tile, dev);
+	struct aie_partition *apart = atile->apart;
+	ssize_t len = 0, size = PAGE_SIZE;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	len = apart->adev->ops->get_tile_sysfs_dma_status(apart, &atile->loc,
+							  &buffer[len],
+							  size - len);
+
+	mutex_unlock(&apart->mlock);
+	return len;
+}
+
+/**
+ * aie_part_read_cb_dma() - exports status of all DMAs within a given
+ *			    partition to partition level node.
+ * @kobj: kobject used to create sysfs node.
+ * @buffer: export buffer.
+ * @size: length of export buffer available.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_part_read_cb_dma(struct kobject *kobj, char *buffer, ssize_t size)
+{
+	struct device *dev = container_of(kobj, struct device, kobj);
+	struct aie_partition *apart = dev_to_aiepart(dev);
+	struct aie_tile *atile = apart->atiles;
+	const struct aie_tile_operations *ops = apart->adev->ops;
+	ssize_t len = 0;
+	u32 index;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	for (index = 0; index < apart->range.size.col * apart->range.size.row;
+	     index++, atile++) {
+		u32 ttype = apart->adev->ops->get_tile_type(apart->adev,
+							    &atile->loc);
+
+		if (ttype == AIE_TILE_TYPE_SHIMPL)
+			continue;
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d_%d: ",
+				 atile->loc.col, atile->loc.row);
+		len += ops->get_part_sysfs_dma_status(apart, &atile->loc,
+						      &buffer[len], size - len);
+		len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	}
+
+	mutex_unlock(&apart->mlock);
+	return len;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-error.c b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-error.c
new file mode 100644
index 000000000..ab5d82e00
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-error.c
@@ -0,0 +1,382 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver.
+ *
+ * Copyright (C) 2021 Xilinx, Inc.
+ */
+#include "ai-engine-internal.h"
+#include "linux/xlnx-ai-engine.h"
+
+static char *aie_error_category_str[] = {
+	"saturation",
+	"floating_point",
+	"stream_switch",
+	"access",
+	"bus",
+	"instruction",
+	"ecc",
+	"lock",
+	"dma",
+	"memory_parity",
+};
+
+/**
+ * aie_get_errors_str() - returns errors in string format. Errors of the same
+ *			  category are separated by a '|' symbol with the error
+ *			  category sting as a label prefix.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine tile.
+ * @module: module type.
+ * @err_attr: error attribute for given module type.
+ * @buffer: location to return error string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+static ssize_t aie_get_errors_str(struct aie_partition *apart,
+				  struct aie_location loc,
+				  enum aie_module_type module,
+				  const struct aie_error_attr *err_attr,
+				  char *buffer, ssize_t size)
+{
+	ssize_t len = 0;
+	u32 i, j;
+	char *mod;
+
+	if (module == AIE_CORE_MOD)
+		mod = "core";
+	else if (module == AIE_MEM_MOD)
+		mod = "memory";
+	else
+		mod = "pl";
+
+	for (i = 0; i < err_attr->num_err_categories; i++) {
+		const struct aie_err_category *category;
+		bool is_delimit_req = false, preamble = true;
+		u8 index;
+
+		category = &err_attr->err_category[i];
+		index = category->err_category;
+		for (j = 0; j < category->num_events; j++) {
+			u8 event = category->prop[j].event;
+			char *str = category->prop[j].event_str;
+
+			if (!aie_check_error_bitmap(apart, loc, module, event))
+				continue;
+
+			if (preamble) {
+				len += scnprintf(&buffer[len],
+						 max(0L, size - len),
+						 "%s: %s: ", mod,
+						 aie_error_category_str[index]);
+				preamble = false;
+			}
+
+			if (is_delimit_req) {
+				len += scnprintf(&buffer[len],
+						 max(0L, size - len),
+						 DELIMITER_LEVEL0);
+			}
+
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 str);
+
+			is_delimit_req = true;
+		}
+
+		if (!preamble) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "\n");
+		}
+	}
+	return len;
+}
+
+/**
+ * aie_get_error_category_str() - returns error categories in string format.
+ *				  Errors categories are separated by a '|'
+ *				  symbol.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine tile.
+ * @module: module type.
+ * @err_attr: error attribute for given module type.
+ * @buffer: location to return error string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+static ssize_t aie_get_error_category_str(struct aie_partition *apart,
+					  struct aie_location loc,
+					  enum aie_module_type module,
+					  const struct aie_error_attr *err_attr,
+					  char *buffer, ssize_t size)
+{
+	ssize_t len = 0;
+	u32 i, j;
+	bool is_delimit_req = false;
+
+	for (i = 0; i < err_attr->num_err_categories; i++) {
+		const struct aie_err_category *category;
+
+		category = &err_attr->err_category[i];
+		for (j = 0; j < category->num_events; j++) {
+			u8 event = category->prop[j].event;
+			u8 index = category->err_category;
+
+			if (!aie_check_error_bitmap(apart, loc, module, event))
+				continue;
+
+			if (is_delimit_req) {
+				len += scnprintf(&buffer[len], max(0L, size - len),
+						 DELIMITER_LEVEL0);
+			}
+
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					aie_error_category_str[index]);
+			is_delimit_req = true;
+			break;
+		}
+	}
+	return len;
+}
+
+/**
+ * aie_tile_show_error() - exports detailed error information to a tile level
+ *			   sysfs node.
+ * @dev: AI engine tile device.
+ * @attr: sysfs device attribute.
+ * @buffer: export buffer.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_tile_show_error(struct device *dev, struct device_attribute *attr,
+			    char *buffer)
+{
+	struct aie_tile *atile = container_of(dev, struct aie_tile, dev);
+	struct aie_partition *apart = atile->apart;
+	const struct aie_error_attr *core_attr, *mem_attr, *memtile_attr, *pl_attr;
+	ssize_t len = 0, size = PAGE_SIZE;
+	u32 ttype, core_count = 0, mem_count = 0, memtile_count = 0, pl_count = 0;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	ttype = apart->adev->ops->get_tile_type(apart->adev, &atile->loc);
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		core_attr = apart->adev->core_errors;
+		mem_attr = apart->adev->mem_errors;
+		core_count = aie_get_module_error_count(apart, atile->loc,
+							AIE_CORE_MOD,
+							core_attr);
+		mem_count = aie_get_module_error_count(apart, atile->loc,
+						       AIE_MEM_MOD, mem_attr);
+	} else {
+		pl_attr = apart->adev->shim_errors;
+		pl_count = aie_get_module_error_count(apart, atile->loc,
+						      AIE_PL_MOD, pl_attr);
+	}
+
+	if (!(core_count || mem_count || memtile_count || pl_count)) {
+		mutex_unlock(&apart->mlock);
+		return len;
+	}
+
+	if (core_count) {
+		len += aie_get_errors_str(apart, atile->loc, AIE_CORE_MOD,
+					  core_attr, &buffer[len], size - len);
+	}
+
+	if (mem_count) {
+		len += aie_get_errors_str(apart, atile->loc, AIE_MEM_MOD,
+					  mem_attr, &buffer[len], size - len);
+	}
+
+	if (memtile_count) {
+		len += aie_get_errors_str(apart, atile->loc, AIE_MEM_MOD,
+					  memtile_attr, &buffer[len],
+					  size - len);
+	}
+
+	if (pl_count) {
+		len += aie_get_errors_str(apart, atile->loc, AIE_PL_MOD,
+					  pl_attr, &buffer[len], size - len);
+	}
+
+	mutex_unlock(&apart->mlock);
+	return len;
+}
+
+/**
+ * aie_part_show_error_stat() - exports error count in a partition to a
+ *				partition level sysfs node.
+ * @dev: AI engine tile device.
+ * @attr: sysfs device attribute.
+ * @buffer: export buffer.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_part_show_error_stat(struct device *dev,
+				 struct device_attribute *attr, char *buffer)
+{
+	struct aie_partition *apart = dev_to_aiepart(dev);
+	struct aie_tile *atile = apart->atiles;
+	const struct aie_error_attr *core_attr, *mem_attr, *memtile_attr, *pl_attr;
+	ssize_t len = 0, size = PAGE_SIZE;
+	u32 index, core = 0, mem = 0, memtile = 0, pl = 0;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	for (index = 0; index < apart->range.size.col * apart->range.size.row;
+	     index++, atile++) {
+		u32 ttype = apart->adev->ops->get_tile_type(apart->adev,
+							    &atile->loc);
+
+		if (ttype == AIE_TILE_TYPE_TILE) {
+			core_attr = apart->adev->core_errors;
+			mem_attr = apart->adev->mem_errors;
+			core += aie_get_module_error_count(apart, atile->loc,
+							   AIE_CORE_MOD,
+							   core_attr);
+			mem += aie_get_module_error_count(apart, atile->loc,
+							  AIE_MEM_MOD,
+							  mem_attr);
+		} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+			memtile_attr = apart->adev->memtile_errors;
+			memtile = aie_get_module_error_count(apart, atile->loc,
+							     AIE_MEM_MOD,
+							     memtile_attr);
+		} else {
+			pl_attr = apart->adev->shim_errors;
+			pl += aie_get_module_error_count(apart, atile->loc,
+							 AIE_PL_MOD, pl_attr);
+		}
+	}
+
+	mutex_unlock(&apart->mlock);
+
+	len += scnprintf(&buffer[len], max(0L, size - len), "core: %d\n", core);
+	len += scnprintf(&buffer[len], max(0L, size - len), "memory: %d\n",
+			 mem);
+	len += scnprintf(&buffer[len], max(0L, size - len), "memory_tile: %d\n",
+			 memtile);
+	len += scnprintf(&buffer[len], max(0L, size - len), "pl: %d\n", pl);
+	return len;
+}
+
+/**
+ * aie_sysfs_get_errors() - returns all asserted error categories in string
+ *			    format. Error categories within module label,
+ *			    are separated by a '|' symbol.
+ * @apart: AI engine partition.
+ * @loc: location of AI engine tile.
+ * @buffer: location to return error string.
+ * @size: total size of buffer available.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_sysfs_get_errors(struct aie_partition *apart,
+			     struct aie_location *loc, char *buffer,
+			     ssize_t size)
+{
+	u32 ttype, core_count = 0, mem_count = 0, memtile_count = 0, pl_count = 0;
+	const struct aie_error_attr *core_attr, *mem_attr, *memtile_attr, *pl_attr;
+	ssize_t len = 0;
+
+	ttype = apart->adev->ops->get_tile_type(apart->adev, loc);
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		core_attr = apart->adev->core_errors;
+		mem_attr = apart->adev->mem_errors;
+		core_count = aie_get_module_error_count(apart, *loc,
+							AIE_CORE_MOD,
+							core_attr);
+		mem_count  = aie_get_module_error_count(apart, *loc,
+							AIE_MEM_MOD, mem_attr);
+	} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+		memtile_attr = apart->adev->memtile_errors;
+		memtile_count = aie_get_module_error_count(apart, *loc,
+							   AIE_MEM_MOD,
+							   memtile_attr);
+	} else {
+		pl_attr = apart->adev->shim_errors;
+		pl_count = aie_get_module_error_count(apart, *loc, AIE_PL_MOD,
+						      pl_attr);
+	}
+
+	if (!(core_count || mem_count || memtile_count || pl_count))
+		return len;
+
+	len += scnprintf(&buffer[len], max(0L, size - len), "%d_%d: ", loc->col,
+			 loc->row);
+
+	if (core_count) {
+		len += scnprintf(&buffer[len], max(0L, size - len), "core: ");
+		len += aie_get_error_category_str(apart, *loc, AIE_CORE_MOD,
+						  core_attr, &buffer[len],
+						  size - len);
+	}
+
+	if (mem_count) {
+		len += scnprintf(&buffer[len], max(0L, size - len), "%smemory: ",
+				 core_count ? DELIMITER_LEVEL1 : "");
+		len += aie_get_error_category_str(apart, *loc, AIE_MEM_MOD,
+						  mem_attr, &buffer[len],
+						  size - len);
+	}
+
+	if (memtile_count) {
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "memory_tile: ");
+		len += aie_get_error_category_str(apart, *loc, AIE_MEM_MOD,
+						  memtile_attr, &buffer[len],
+						  size - len);
+	}
+
+	if (pl_count) {
+		len += scnprintf(&buffer[len], max(0L, size - len), "pl: ");
+		len += aie_get_error_category_str(apart, *loc, AIE_PL_MOD,
+						  pl_attr, &buffer[len],
+						  size - len);
+	}
+
+	len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	return len;
+}
+
+/**
+ * aie_part_read_cb_error() - exports errors with a given partition to
+ *			      partition level node.
+ * @kobj: kobject used to create sysfs node.
+ * @buffer: export buffer.
+ * @size: length of export buffer available.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_part_read_cb_error(struct kobject *kobj, char *buffer, ssize_t size)
+{
+	struct device *dev = container_of(kobj, struct device, kobj);
+	struct aie_partition *apart = dev_to_aiepart(dev);
+	struct aie_tile *atile = apart->atiles;
+	ssize_t len = 0;
+	u32 index;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	if (!(aie_get_error_count(apart))) {
+		mutex_unlock(&apart->mlock);
+		return len;
+	}
+
+	for (index = 0; index < apart->range.size.col * apart->range.size.row;
+	     index++, atile++) {
+		len += aie_sysfs_get_errors(apart, &atile->loc, &buffer[len],
+					    size - len);
+	}
+
+	mutex_unlock(&apart->mlock);
+	return len;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-event.c b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-event.c
new file mode 100644
index 000000000..704d94891
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-event.c
@@ -0,0 +1,134 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver.
+ *
+ * Copyright (C) 2021 Xilinx, Inc.
+ */
+#include "ai-engine-internal.h"
+
+/**
+ * aie_tile_show_event() - exports all active events in a given tile to a
+ *			   tile level sysfs node.
+ * @dev: AI engine tile device.
+ * @attr: sysfs device attribute.
+ * @buffer: export buffer.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_tile_show_event(struct device *dev, struct device_attribute *attr,
+			    char *buffer)
+{
+	struct aie_tile *atile = container_of(dev, struct aie_tile, dev);
+	struct aie_partition *apart = atile->apart;
+	ssize_t len = 0, size = PAGE_SIZE;
+	unsigned long cs[AIE_NUM_EVENT_STS_CORETILE] = {0},
+		      ms[AIE_NUM_EVENT_STS_MEMTILE] = {0},
+		      ps[AIE_NUM_EVENT_STS_SHIMTILE] = {0};
+	u32 ttype, n;
+	bool is_delimit_req = false;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return 0;
+	}
+
+	ttype = apart->adev->ops->get_tile_type(apart->adev, &atile->loc);
+
+	if (ttype == AIE_TILE_TYPE_TILE) {
+		if (!aie_part_check_clk_enable_loc(apart, &atile->loc)) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "core: clock_gated\n");
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "memory: clock_gated\n");
+			goto exit;
+		}
+
+		aie_read_event_status(apart, &atile->loc, AIE_CORE_MOD,
+				      (u32 *)cs);
+		aie_read_event_status(apart, &atile->loc, AIE_MEM_MOD,
+				      (u32 *)ms);
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "core: ");
+
+		for_each_set_bit(n, cs, apart->adev->core_events->num_events) {
+			if (is_delimit_req) {
+				len += scnprintf(&buffer[len],
+						 max(0L, size - len),
+						 DELIMITER_LEVEL0);
+			}
+
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%d", n);
+			is_delimit_req = true;
+		}
+
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "\nmemory: ");
+
+		is_delimit_req = false;
+		for_each_set_bit(n, ms, apart->adev->mem_events->num_events) {
+			if (is_delimit_req) {
+				len += scnprintf(&buffer[len],
+						 max(0L, size - len),
+						 DELIMITER_LEVEL0);
+			}
+
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%d", n);
+			is_delimit_req = true;
+		}
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	} else if (ttype == AIE_TILE_TYPE_MEMORY) {
+		if (!aie_part_check_clk_enable_loc(apart, &atile->loc)) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "memory: clock_gated\n");
+			goto exit;
+		}
+
+		aie_read_event_status(apart, &atile->loc, AIE_MEM_MOD,
+				      (u32 *)ms);
+
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "memory_tile: ");
+
+		is_delimit_req = false;
+		for_each_set_bit(n, ms,
+				 apart->adev->memtile_events->num_events) {
+			if (is_delimit_req) {
+				len += scnprintf(&buffer[len],
+						 max(0L, size - len),
+						 DELIMITER_LEVEL0);
+			}
+
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%d", n);
+			is_delimit_req = true;
+		}
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	} else {
+		aie_read_event_status(apart, &atile->loc, AIE_PL_MOD,
+				      (u32 *)ps);
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "pl: ");
+
+		for_each_set_bit(n, ps, apart->adev->pl_events->num_events) {
+			if (is_delimit_req) {
+				len += scnprintf(&buffer[len],
+						 max(0L, size - len),
+						 DELIMITER_LEVEL0);
+			}
+
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "%d", n);
+			is_delimit_req = true;
+		}
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	}
+
+exit:
+	mutex_unlock(&apart->mlock);
+	return len;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-info.c b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-info.c
new file mode 100644
index 000000000..ebc8d2506
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-info.c
@@ -0,0 +1,79 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver sysfs for clock.
+ *
+ * Copyright (C) 2022 Xilinx, Inc.
+ */
+#include "ai-engine-internal.h"
+
+/**
+ * aie_aperture_show_hardware_info() - exports AI engine hardware information
+ *
+ * @dev: AI engine device.
+ * @attr: sysfs device attribute.
+ * @buffer: export buffer.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_aperture_show_hardware_info(struct device *dev,
+					struct device_attribute *attr,
+					char *buffer)
+{
+	struct aie_aperture *aperture = dev_to_aieaperture(dev);
+	ssize_t len = 0, size = PAGE_SIZE;
+	u32 num_rows[AIE_TILE_TYPE_MAX], start_rows[AIE_TILE_TYPE_MAX];
+	u32 total_rows = 0, total_cols = 0;
+	u32 version, ttype;
+
+	if (mutex_lock_interruptible(&aperture->mlock))
+		return 0;
+
+	version = aperture->adev->dev_gen;
+	len += scnprintf(&buffer[len], max(0L, size - len), "generation: ");
+	switch (version) {
+	case AIE_DEVICE_GEN_AIE:
+		len += scnprintf(&buffer[len], max(0L, size - len), "aie\n");
+		break;
+	case AIE_DEVICE_GEN_AIEML:
+		len += scnprintf(&buffer[len], max(0L, size - len), "aieml\n");
+		break;
+	default:
+		len += scnprintf(&buffer[len], max(0L, size - len),
+				 "invalid\n");
+		memset(num_rows, 0, sizeof(num_rows));
+		memset(start_rows, 0, sizeof(start_rows));
+		goto exit;
+	}
+
+	total_cols = aperture->range.size.col;
+	for (ttype = 0; ttype < AIE_TILE_TYPE_MAX; ttype++) {
+		if (ttype == AIE_TILE_TYPE_SHIMNOC)
+			continue;
+		num_rows[ttype] = aperture->adev->ttype_attr[ttype].num_rows;
+		start_rows[ttype] = aperture->adev->ttype_attr[ttype].start_row;
+		total_rows += num_rows[ttype];
+	}
+
+exit:
+	len += scnprintf(&buffer[len], max(0L, size - len),
+			 "total_cols: %d\n", total_cols);
+	len += scnprintf(&buffer[len], max(0L, size - len),
+			 "total_rows: %d\n", total_rows);
+
+	len += scnprintf(&buffer[len], max(0L, size - len),
+			 "shim_tile: start row: %d%snum_rows: %d\n",
+			 start_rows[AIE_TILE_TYPE_SHIMPL], DELIMITER_LEVEL1,
+			 num_rows[AIE_TILE_TYPE_SHIMPL]);
+
+	len += scnprintf(&buffer[len], max(0L, size - len),
+			 "memory_tile: start row: %d%snum_rows: %d\n",
+			 start_rows[AIE_TILE_TYPE_MEMORY], DELIMITER_LEVEL1,
+			 num_rows[AIE_TILE_TYPE_MEMORY]);
+
+	len += scnprintf(&buffer[len], max(0L, size - len),
+			 "aie_tile: start row: %d%snum_rows: %d\n",
+			 start_rows[AIE_TILE_TYPE_TILE], DELIMITER_LEVEL1,
+			 num_rows[AIE_TILE_TYPE_TILE]);
+
+	mutex_unlock(&aperture->mlock);
+	return len;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-lock.c b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-lock.c
new file mode 100644
index 000000000..6ed7c66d3
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-lock.c
@@ -0,0 +1,78 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver.
+ *
+ * Copyright (C) 2021 Xilinx, Inc.
+ */
+#include "ai-engine-internal.h"
+
+/**
+ * aie_tile_show_lock() - exports AI engine lock status to a tile level sysfs
+ *			  node.
+ * @dev: AI engine tile device.
+ * @attr: sysfs device attribute.
+ * @buffer: export buffer.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_tile_show_lock(struct device *dev, struct device_attribute *attr,
+			   char *buffer)
+{
+	struct aie_tile *atile = container_of(dev, struct aie_tile, dev);
+	struct aie_partition *apart = atile->apart;
+	ssize_t len = 0, size = PAGE_SIZE;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	len = apart->adev->ops->get_tile_sysfs_lock_status(apart, &atile->loc,
+							   &buffer[len],
+							   size - len);
+
+	mutex_unlock(&apart->mlock);
+	return len;
+}
+
+/**
+ * aie_part_read_cb_lock() - exports status of all lock modules within a given
+ *			     partition to partition level node.
+ * @kobj: kobject used to create sysfs node.
+ * @buffer: export buffer.
+ * @size: length of export buffer available.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_part_read_cb_lock(struct kobject *kobj, char *buffer, ssize_t size)
+{
+	struct device *dev = container_of(kobj, struct device, kobj);
+	struct aie_partition *apart = dev_to_aiepart(dev);
+	struct aie_tile *atile = apart->atiles;
+	const struct aie_tile_operations *ops = apart->adev->ops;
+	ssize_t len = 0;
+	u32 index;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	for (index = 0; index < apart->range.size.col * apart->range.size.row;
+	     index++, atile++) {
+		u32 ttype = ops->get_tile_type(apart->adev, &atile->loc);
+
+		if (ttype == AIE_TILE_TYPE_SHIMPL)
+			continue;
+
+		len += scnprintf(&buffer[len], max(0L, size - len), "%d_%d: ",
+				 atile->loc.col, atile->loc.row);
+		len += ops->get_part_sysfs_lock_status(apart, &atile->loc,
+						       &buffer[len],
+						       size - len);
+		len += scnprintf(&buffer[len], max(0L, size - len), "\n");
+	}
+
+	mutex_unlock(&apart->mlock);
+	return len;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-status.c b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-status.c
new file mode 100644
index 000000000..2ce39f9e6
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs-status.c
@@ -0,0 +1,124 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine driver AIE device specific implementation
+ *
+ * Copyright (C) 2021 Xilinx, Inc.
+ */
+#include <linux/slab.h>
+
+#include "ai-engine-internal.h"
+
+/**
+ * aie_part_read_cb_status() - exports status of cores, DMAs, errors, and locks
+ *			       within a partition at a partition level node.
+ *			       this node serves as a single access point to
+ *			       query the status of a partition by a
+ *			       script/tool. For a given tile location, core
+ *			       status, DMAs, etc are separated by a ';' symbol.
+ *			       Core status information is captured under 'cs'
+ *			       label, DMA under 'ds', errors under 'es', and
+ *			       lock status under 'ls'.
+ * @kobj: kobject used to create sysfs node.
+ * @buffer: export buffer.
+ * @size: length of export buffer available.
+ * @return: length of string copied to buffer.
+ */
+ssize_t aie_part_read_cb_status(struct kobject *kobj, char *buffer,
+				ssize_t size)
+{
+	struct device *dev = container_of(kobj, struct device, kobj);
+	struct aie_partition *apart = dev_to_aiepart(dev);
+	struct aie_tile *atile = apart->atiles;
+	ssize_t len = 0;
+	u32 index;
+
+	if (mutex_lock_interruptible(&apart->mlock)) {
+		dev_err(&apart->dev,
+			"Failed to acquire lock. Process was interrupted by fatal signals\n");
+		return len;
+	}
+
+	for (index = 0; index < apart->range.size.col * apart->range.size.row;
+	     index++, atile++) {
+		struct aie_location loc = atile->loc;
+		const struct aie_tile_operations *ops = apart->adev->ops;
+		bool preamble = true;
+		u32 ttype;
+
+		ttype = ops->get_tile_type(apart->adev, &loc);
+
+		if (ttype == AIE_TILE_TYPE_TILE) {
+			if (preamble) {
+				len += scnprintf(&buffer[len],
+						 max(0L, size - len),
+						 "%d_%d: cs: ", loc.col,
+						 loc.row);
+				preamble = false;
+			}
+
+			len += aie_sysfs_get_core_status(apart, &loc,
+							 &buffer[len],
+							 size - len);
+		}
+
+		if (ttype == AIE_TILE_TYPE_TILE ||
+		    ttype == AIE_TILE_TYPE_SHIMNOC) {
+			if (preamble) {
+				len += scnprintf(&buffer[len],
+						 max(0L, size - len),
+						 "%d_%d: ds: ", loc.col,
+						 loc.row);
+				preamble = false;
+			} else {
+				len += scnprintf(&buffer[len],
+						 max(0L, size - len),
+						 "%sds: ", DELIMITER_LEVEL2);
+			}
+
+			len += ops->get_part_sysfs_dma_status(apart, &loc,
+							      &buffer[len],
+							      size - len);
+
+			if (preamble) {
+				len += scnprintf(&buffer[len],
+						 max(0L, size - len),
+						 "%d_%d: ls: ", loc.col,
+						 loc.row);
+				preamble = false;
+			} else {
+				len += scnprintf(&buffer[len],
+						 max(0L, size - len),
+						 "%sls: ", DELIMITER_LEVEL2);
+			}
+
+			len += ops->get_part_sysfs_lock_status(apart, &loc,
+							       &buffer[len],
+							       size - len);
+		}
+
+		if (aie_check_tile_error(apart, loc)) {
+			if (preamble) {
+				len += scnprintf(&buffer[len],
+						 max(0L, size - len),
+						 "%d_%d: es: ", loc.col,
+						 loc.row);
+				preamble = false;
+			} else {
+				len += scnprintf(&buffer[len],
+						 max(0L, size - len),
+						 "%ses: ", DELIMITER_LEVEL2);
+			}
+
+			len += aie_sysfs_get_errors(apart, &loc, &buffer[len],
+						    size - len);
+		}
+
+		if (!preamble) {
+			len += scnprintf(&buffer[len], max(0L, size - len),
+					 "\n");
+		}
+	}
+
+	mutex_unlock(&apart->mlock);
+	return len;
+}
diff --git a/drivers/misc/xilinx-ai-engine/ai-engine-sysfs.c b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs.c
new file mode 100644
index 000000000..189b523c5
--- /dev/null
+++ b/drivers/misc/xilinx-ai-engine/ai-engine-sysfs.c
@@ -0,0 +1,448 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx AI Engine device driver.
+ *
+ * Copyright (C) 2021 Xilinx, Inc.
+ */
+#include "ai-engine-internal.h"
+
+/**
+ * aie_sysfs_read_handler() - sysfs binary attribute read handler.
+ * @filp: file pointer.
+ * @kobj: pointer to the kobject.
+ * @attr: sysfs binary attribute.
+ * @buf: buffer to copy the data to.
+ * @offset: offset into the sysfs file.
+ * @max_size: maximum length of data that could be copied to buf.
+ * @return: length of data actually copied to buf.
+ */
+ssize_t aie_sysfs_read_handler(struct file *filp, struct kobject *kobj,
+			       struct bin_attribute *attr, char *buf,
+			       loff_t offset, size_t max_size)
+{
+	ssize_t len = max_size;
+	struct aie_sysfs_prop *prop;
+
+	prop = attr->private;
+	if (!prop->data)
+		return 0;
+
+	if (!offset)
+		prop->size = prop->read_callback(kobj, prop->data,
+						 prop->max_size);
+
+	if (offset >= prop->size)
+		return 0;
+
+	if (offset + max_size > prop->size)
+		len = prop->size - offset;
+
+	memcpy(buf,  prop->data + offset, len);
+	return len;
+}
+
+/**
+ * aie_sysfs_create_dev_attr() - dynamically allocates and initialize a device
+ *				 attribute
+ * @dev: device to allocate attribute for.
+ * @attr: AI engine device attribute.
+ * @return: pointer to the allocated device attribute.
+ */
+static struct device_attribute *
+aie_sysfs_create_dev_attr(struct device *dev, const struct aie_dev_attr *attr)
+{
+	struct device_attribute *node;
+
+	node = devm_kzalloc(dev, sizeof(struct device_attribute), GFP_KERNEL);
+	if (!node)
+		return ERR_PTR(-ENOMEM);
+
+	sysfs_attr_init(&node->attr);
+
+	node->attr.name = attr->name;
+	node->attr.mode = attr->mode;
+	node->show = attr->show;
+	return node;
+}
+
+/**
+ * aie_sysfs_create_bin_attr() - dynamically allocates and initialize a binary
+ *				 attribute
+ * @dev: device to allocate attribute for.
+ * @attr: AI engine binary attribute.
+ * @return: pointer to the allocated binary attribute.
+ */
+static struct bin_attribute *
+aie_sysfs_create_bin_attr(struct device *dev, const struct aie_bin_attr *attr)
+{
+	struct bin_attribute *node;
+	struct aie_sysfs_prop *prop;
+
+	node = devm_kzalloc(dev, sizeof(struct bin_attribute), GFP_KERNEL);
+	if (!node)
+		return ERR_PTR(-ENOMEM);
+
+	sysfs_bin_attr_init(node);
+
+	node->attr.name = attr->name;
+	node->attr.mode = attr->mode;
+	node->size = attr->size;
+	node->read = attr->read;
+
+	prop = devm_kzalloc(dev, sizeof(struct aie_sysfs_prop), GFP_KERNEL);
+	if (!prop)
+		return ERR_PTR(-ENOMEM);
+
+	prop->data = devm_kzalloc(dev, node->size, GFP_KERNEL);
+	if (!prop->data)
+		return ERR_PTR(-ENOMEM);
+
+	prop->max_size = node->size;
+	prop->read_callback = attr->read_callback;
+	node->private = prop;
+	return node;
+}
+
+/**
+ * aie_tile_sysfs_create() - creates sysfs nodes at the tile level.
+ * @atile: AI engine tile.
+ * @return: 0 for success, error code for failure.
+ */
+static int aie_tile_sysfs_create(struct aie_tile *atile)
+{
+	struct attribute_group *attr_grp;
+	struct bin_attribute **bin_attrs;
+	struct attribute **dev_attrs;
+	const struct aie_sysfs_attr *attr;
+	int ret = 0;
+	u32 ttype;
+	u32 index, i = 0, j = 0;
+
+	attr = atile->apart->adev->tile_sysfs_attr;
+	ttype = atile->apart->adev->ops->get_tile_type(atile->apart->adev,
+						       &atile->loc);
+
+	if (attr->num_dev_attrs) {
+		dev_attrs = devm_kzalloc(&atile->dev, sizeof(*dev_attrs) *
+					 (attr->num_dev_attrs + 1), GFP_KERNEL);
+		if (!dev_attrs)
+			return -ENOMEM;
+
+		for (index = 0; index < attr->num_dev_attrs; index++) {
+			struct device_attribute *node;
+			const struct aie_dev_attr *dev_attr;
+
+			dev_attr = &attr->dev_attr[index];
+
+			if (!(BIT(ttype) & attr->dev_attr[index].tile_type))
+				continue;
+
+			node = aie_sysfs_create_dev_attr(&atile->dev, dev_attr);
+			if (IS_ERR_VALUE(node))
+				return PTR_ERR(node);
+
+			dev_attrs[i++] = &node->attr;
+		}
+	}
+
+	if (attr->num_bin_attrs) {
+		bin_attrs = devm_kzalloc(&atile->dev, sizeof(*bin_attrs) *
+					 (attr->num_bin_attrs + 1), GFP_KERNEL);
+		if (!bin_attrs)
+			return -ENOMEM;
+
+		for (index = 0; index < attr->num_bin_attrs; index++) {
+			struct bin_attribute *node;
+			const struct aie_bin_attr *bin_attr;
+
+			bin_attr = &attr->bin_attr[index];
+
+			if (!(BIT(ttype) & attr->bin_attr[index].tile_type))
+				continue;
+
+			node = aie_sysfs_create_bin_attr(&atile->dev, bin_attr);
+			if (IS_ERR_VALUE(node))
+				return PTR_ERR(node);
+
+			bin_attrs[j++] = node;
+		}
+	}
+
+	if (attr->num_dev_attrs || attr->num_bin_attrs) {
+		attr_grp = devm_kzalloc(&atile->dev,
+					sizeof(struct attribute_group),
+					GFP_KERNEL);
+		if (!attr_grp)
+			return -ENOMEM;
+
+		atile->attr_grp = attr_grp;
+
+		if (attr->num_dev_attrs)
+			attr_grp->attrs = dev_attrs;
+
+		if (attr->num_bin_attrs)
+			attr_grp->bin_attrs = bin_attrs;
+
+		/* TODO - use the non managed api to create sysfs group.
+		 * This workaround solves an issue where the device_del()
+		 * removes the SYSFS files before the managed create group.
+		 * This results in an error where files cannot be found.
+		 */
+		ret = sysfs_create_group(&atile->dev.kobj, attr_grp);
+		if (ret) {
+			dev_err(&atile->dev,
+				"Failed to add sysfs attributes group\n");
+		}
+	}
+	return ret;
+}
+
+/**
+ * aie_part_sysfs_create() - creates sysfs nodes at the partition level.
+ * @apart: AI engine partition.
+ * @return: 0 for success, error code for failure.
+ */
+static int aie_part_sysfs_create(struct aie_partition *apart)
+{
+	const struct aie_sysfs_attr *attr;
+	struct attribute_group *attr_grp;
+	struct bin_attribute **bin_attrs;
+	struct attribute **dev_attrs;
+	int ret = 0;
+	u32 index;
+
+	attr = apart->adev->part_sysfs_attr;
+
+	if (attr->num_dev_attrs) {
+		dev_attrs = devm_kzalloc(&apart->dev, sizeof(*dev_attrs) *
+					 (attr->num_dev_attrs + 1), GFP_KERNEL);
+		if (!dev_attrs)
+			return -ENOMEM;
+
+		for (index = 0; index < attr->num_dev_attrs; index++) {
+			struct device_attribute *node;
+			const struct aie_dev_attr *dev_attr;
+
+			dev_attr = &attr->dev_attr[index];
+
+			node = aie_sysfs_create_dev_attr(&apart->dev, dev_attr);
+			if (IS_ERR_VALUE(node))
+				return PTR_ERR(node);
+
+			dev_attrs[index] = &node->attr;
+		}
+	}
+
+	if (attr->num_bin_attrs) {
+		bin_attrs = devm_kzalloc(&apart->dev, sizeof(*bin_attrs) *
+					 (attr->num_bin_attrs + 1), GFP_KERNEL);
+		if (!bin_attrs)
+			return -ENOMEM;
+
+		for (index = 0; index < attr->num_bin_attrs; index++) {
+			struct bin_attribute *node;
+			const struct aie_bin_attr *bin_attr;
+
+			bin_attr = &attr->bin_attr[index];
+
+			node = aie_sysfs_create_bin_attr(&apart->dev, bin_attr);
+			if (IS_ERR_VALUE(node))
+				return PTR_ERR(node);
+
+			bin_attrs[index] = node;
+		}
+	}
+
+	if (attr->num_dev_attrs || attr->num_bin_attrs) {
+		attr_grp = devm_kzalloc(&apart->dev,
+					sizeof(struct attribute_group),
+					GFP_KERNEL);
+		if (!attr_grp)
+			return -ENOMEM;
+
+		apart->attr_grp = attr_grp;
+
+		if (attr->num_dev_attrs)
+			attr_grp->attrs = dev_attrs;
+
+		if (attr->num_bin_attrs)
+			attr_grp->bin_attrs = bin_attrs;
+
+		/* TODO - use the non managed api to create sysfs group.
+		 * This workaround solves an issue where the device_del()
+		 * removes the SYSFS files before the managed create group.
+		 * This results in an error where files cannot be found.
+		 */
+		ret = sysfs_create_group(&apart->dev.kobj, attr_grp);
+		if (ret) {
+			dev_err(&apart->dev,
+				"Failed to add sysfs attributes group\n");
+		}
+	}
+	return ret;
+}
+
+/**
+ * aie_aperture_sysfs_create() - creates sysfs nodes at the aperture level.
+ * @aperture: AI engine aperture.
+ * @return: 0 for success, error code for failure.
+ */
+static int aie_aperture_sysfs_create(struct aie_aperture *aperture)
+{
+	const struct aie_sysfs_attr *attr;
+	struct attribute_group *attr_grp;
+	struct bin_attribute **bin_attrs;
+	struct attribute **dev_attrs;
+	int ret = 0;
+	u32 index;
+
+	attr = aperture->adev->aperture_sysfs_attr;
+
+	if (attr->num_dev_attrs) {
+		dev_attrs = devm_kzalloc(&aperture->dev, sizeof(*dev_attrs) *
+					 (attr->num_dev_attrs + 1), GFP_KERNEL);
+		if (!dev_attrs)
+			return -ENOMEM;
+
+		for (index = 0; index < attr->num_dev_attrs; index++) {
+			struct device_attribute *node;
+			const struct aie_dev_attr *dev_attr;
+
+			dev_attr = &attr->dev_attr[index];
+
+			node = aie_sysfs_create_dev_attr(&aperture->dev,
+							 dev_attr);
+			if (IS_ERR_VALUE(node))
+				return PTR_ERR(node);
+
+			dev_attrs[index] = &node->attr;
+		}
+	}
+
+	if (attr->num_bin_attrs) {
+		bin_attrs = devm_kzalloc(&aperture->dev, sizeof(*bin_attrs) *
+					 (attr->num_bin_attrs + 1), GFP_KERNEL);
+		if (!bin_attrs)
+			return -ENOMEM;
+
+		for (index = 0; index < attr->num_bin_attrs; index++) {
+			struct bin_attribute *node;
+			const struct aie_bin_attr *bin_attr;
+
+			bin_attr = &attr->bin_attr[index];
+
+			node = aie_sysfs_create_bin_attr(&aperture->dev,
+							 bin_attr);
+			if (IS_ERR_VALUE(node))
+				return PTR_ERR(node);
+
+			bin_attrs[index] = node;
+		}
+	}
+
+	if (attr->num_dev_attrs || attr->num_bin_attrs) {
+		attr_grp = devm_kzalloc(&aperture->dev,
+					sizeof(struct attribute_group),
+					GFP_KERNEL);
+		if (!attr_grp)
+			return -ENOMEM;
+
+		aperture->attr_grp = attr_grp;
+
+		if (attr->num_dev_attrs)
+			attr_grp->attrs = dev_attrs;
+
+		if (attr->num_bin_attrs)
+			attr_grp->bin_attrs = bin_attrs;
+
+		/* TODO - use the non managed api to create sysfs group.
+		 * This workaround solves an issue where the device_del()
+		 * removes the SYSFS files before the managed create group.
+		 * This results in an error where files cannot be found.
+		 */
+		ret = sysfs_create_group(&aperture->dev.kobj, attr_grp);
+		if (ret) {
+			dev_err(&aperture->dev,
+				"Failed to add sysfs attributes group\n");
+		}
+	}
+	return ret;
+}
+
+/**
+ * aie_aperture_sysfs_create_entries() - creates sysfs group for aperture.
+ * @aperture: AI engine aperture.
+ * @return: 0 for success, error code for failure.
+ */
+int aie_aperture_sysfs_create_entries(struct aie_aperture *aperture)
+{
+	int ret;
+
+	ret = aie_aperture_sysfs_create(aperture);
+	if (ret < 0) {
+		dev_err(&aperture->dev, "Failed to create aperture sysfs\n");
+		return ret;
+	}
+	return ret;
+}
+
+/**
+ * aie_part_sysfs_create_entries() - creates sysfs group for partition device.
+ * @apart: AI engine partition.
+ * @return: 0 for success, error code for failure.
+ */
+int aie_part_sysfs_create_entries(struct aie_partition *apart)
+{
+	int ret;
+
+	ret = aie_part_sysfs_create(apart);
+	if (ret < 0) {
+		dev_err(&apart->dev, "Failed to create sysfs partition\n");
+		return ret;
+	}
+	return ret;
+}
+
+/**
+ * aie_tile_sysfs_create_entries() - creates sysfs group for tile device.
+ * @atile: AI engine tile.
+ * @return: 0 for success, error code for failure.
+ */
+int aie_tile_sysfs_create_entries(struct aie_tile *atile)
+{
+	int ret;
+
+	ret = aie_tile_sysfs_create(atile);
+	if (ret < 0) {
+		dev_err(&atile->dev, "Failed to create sysfs tile\n");
+		return ret;
+	}
+	return ret;
+}
+
+/**
+ * aie_aperture_sysfs_remove_entries() - removes sysfs group from aperture.
+ * @aperture: AI engine aperture.
+ */
+void aie_aperture_sysfs_remove_entries(struct aie_aperture *aperture)
+{
+	sysfs_remove_group(&aperture->dev.kobj, aperture->attr_grp);
+}
+
+/**
+ * aie_part_sysfs_remove_entries() - removes sysfs group from partition device.
+ * @apart: AI engine partition.
+ */
+void aie_part_sysfs_remove_entries(struct aie_partition *apart)
+{
+	sysfs_remove_group(&apart->dev.kobj, apart->attr_grp);
+}
+
+/**
+ * aie_tile_sysfs_remove_entries() - removes sysfs group from tile device.
+ * @atile: AI engine tile.
+ */
+void aie_tile_sysfs_remove_entries(struct aie_tile *atile)
+{
+	sysfs_remove_group(&atile->dev.kobj, atile->attr_grp);
+}
diff --git a/drivers/misc/xilinx_puf.c b/drivers/misc/xilinx_puf.c
new file mode 100644
index 000000000..76d522870
--- /dev/null
+++ b/drivers/misc/xilinx_puf.c
@@ -0,0 +1,308 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Driver for Xilinx PUF device.
+ *
+ * Copyright (C) 2022 - 2023, Advanced Micro Devices, Inc.
+ *
+ * Description:
+ * This driver is developed for PUF registration and regeneration support.
+ */
+
+#include <linux/dma-mapping.h>
+#include <linux/module.h>
+#include <linux/miscdevice.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/platform_device.h>
+#include <linux/firmware/xlnx-zynqmp.h>
+#include <uapi/misc/xilinx_puf.h>
+
+/**
+ * struct puf_params - parameters for PUF
+ * @pufoperation: PUF registration or regeneration operation
+ * @globalvarfilter: global variation filter
+ * @readoption: option to read PUF data from efuse cache or ram address
+ * @shuttervalue: shutter value for PUF registration/regeneration
+ * @readsyndromeaddr: address to store the syndrome data during registration
+ * @chashaddr: CHASH address
+ * @auxaddr: AUX address
+ * @pufidaddr: PUF ID address
+ * @writesyndromeaddr: address where syndrome data is present and it is passed to the user
+ * @trimsyndataaddr: trimmed syndrome data will be stored
+ */
+struct puf_params {
+	u8 pufoperation;
+	u8 globalvarfilter;
+	u8 readoption;
+	u32 shuttervalue;
+	u64 readsyndromeaddr;
+	u64 chashaddr;
+	u64 auxaddr;
+	u64 pufidaddr;
+	u64 writesyndromeaddr;
+	u64 trimsyndataaddr;
+};
+
+/**
+ * struct xpuf_dev - Driver data for PUF
+ * @dev: pointer to device struct
+ * @miscdev: misc device handle
+ */
+struct xpuf_dev {
+	struct device	*dev;
+	struct miscdevice	miscdev;
+};
+
+static int xlnx_puf_cfg(struct xpuf_dev *puf, struct puf_usrparams *pufreq)
+{
+	struct pufdata *pufdat;
+	struct device *dev = puf->dev;
+	struct puf_params *pufin;
+	struct puf_helperdata *pufhd;
+	dma_addr_t dma_addr_in;
+	dma_addr_t dma_addr_data;
+	int ret;
+
+	pufin = dma_alloc_coherent(dev, sizeof(struct puf_params), &dma_addr_in, GFP_KERNEL);
+	if (!pufin)
+		return -ENOMEM;
+
+	pufin->pufoperation = pufreq->pufoperation;
+	pufin->globalvarfilter = pufreq->globalvarfilter;
+	pufin->shuttervalue = pufreq->shuttervalue;
+	if (pufin->pufoperation == PUF_REGIS) {
+		pufdat = dma_alloc_coherent(dev, sizeof(struct pufdata), &dma_addr_data,
+					    GFP_KERNEL);
+		if (!pufdat)
+			goto cleanup_pufin;
+
+		pufin->readsyndromeaddr = (u64)(dma_addr_data);
+		pufin->chashaddr = (u64)(pufin->readsyndromeaddr + sizeof(pufdat->pufhd.syndata));
+		pufin->auxaddr = (u64)(pufin->chashaddr + sizeof(pufdat->pufhd.chash));
+		pufin->pufidaddr = (u64)(pufin->auxaddr + sizeof(pufdat->pufhd.aux));
+		pufin->trimsyndataaddr = (u64)(pufin->pufidaddr + sizeof(pufdat->pufid));
+
+		ret = versal_pm_puf_registration(dma_addr_in);
+		if (ret != 0)
+			goto cleanup_pufdata;
+
+		if (copy_to_user((void *)pufreq->pufdataaddr, pufdat, sizeof(struct pufdata))) {
+			ret = -EFAULT;
+			goto cleanup_pufdata;
+		}
+	} else if (pufin->pufoperation == PUF_REGEN) {
+		pufin->readoption = pufreq->readoption;
+		pufhd = dma_alloc_coherent(dev, (sizeof(struct puf_helperdata) +
+					   PUF_ID_LEN_IN_BYTES), &dma_addr_data,
+					   GFP_KERNEL);
+		if (!pufhd)
+			goto cleanup_pufin;
+
+		if (copy_from_user(pufhd, (void *)pufreq->pufdataaddr,
+				   sizeof(struct puf_helperdata))) {
+			ret = -EFAULT;
+			goto cleanup_pufdata;
+		}
+
+		pufin->writesyndromeaddr = (u64)(dma_addr_data);
+		pufin->chashaddr = (u64)(pufin->writesyndromeaddr + sizeof(pufhd->syndata));
+		pufin->auxaddr = (u64)(pufin->chashaddr + sizeof(pufhd->chash));
+		pufin->pufidaddr = (u64)(pufin->auxaddr + sizeof(pufhd->aux));
+		ret = versal_pm_puf_regeneration(dma_addr_in);
+		if (ret != 0)
+			goto cleanup_pufdata;
+
+		if (copy_to_user((void *)pufreq->pufidaddr, ((char *)pufhd +
+				 sizeof(struct puf_helperdata)),
+				 PUF_ID_LEN_IN_BYTES)) {
+			ret = -EFAULT;
+			goto cleanup_pufdata;
+		}
+	} else {
+		ret = -EINVAL;
+		goto cleanup_pufin;
+	}
+
+cleanup_pufdata:
+	if (pufin->pufoperation == PUF_REGIS)
+		dma_free_coherent(dev, sizeof(struct pufdata), pufdat, dma_addr_data);
+	else if (pufin->pufoperation == PUF_REGEN)
+		dma_free_coherent(dev, (sizeof(struct puf_helperdata) + PUF_ID_LEN_IN_BYTES),
+				  pufhd, dma_addr_data);
+
+cleanup_pufin:
+	dma_free_coherent(dev, sizeof(struct puf_params), pufin, dma_addr_in);
+
+	return ret;
+}
+
+static long xlnx_puf_ioctl(struct file *file, unsigned int cmd, unsigned long arg)
+{
+	struct puf_usrparams pufreq;
+	struct xpuf_dev *puf = file->private_data;
+	void __user *data = NULL;
+	int ret;
+
+	if (_IOC_TYPE(cmd) != PUF_IOC_MAGIC)
+		return -ENOTTY;
+
+	/* check if ioctl argument is present and valid */
+	if (_IOC_DIR(cmd) != _IOC_NONE) {
+		data = (void __user *)arg;
+		if (!data)
+			return -EINVAL;
+	}
+
+	switch (cmd) {
+	case PUF_REGISTRATION:
+	case PUF_REGENERATION:
+		if (copy_from_user(&pufreq, data,
+				   sizeof(struct puf_usrparams)))
+			return -EINVAL;
+
+		ret = xlnx_puf_cfg(puf, &pufreq);
+		break;
+	default:
+		ret = -EOPNOTSUPP;
+		break;
+	}
+
+	return ret;
+}
+
+/**
+ * xlnx_puf_open - open puf device
+ * @inode:	inode object
+ * @file:	file object
+ *
+ * Return:	0 if successful; otherwise -errno
+ */
+static int xlnx_puf_open(struct inode *inode, struct file *file)
+{
+	struct xpuf_dev *xpuf;
+
+	xpuf = container_of(file->private_data, struct xpuf_dev, miscdev);
+	file->private_data = xpuf;
+
+	return 0;
+}
+
+/**
+ * xlnx_puf_release - release puf resources
+ * @inode:	inode object
+ * @file:	file object
+ *
+ * Return:	0 if successful; otherwise -errno
+ */
+static int xlnx_puf_release(struct inode *inode, struct file *file)
+{
+	struct xpuf_dev *xpuf = file->private_data;
+
+	dev_dbg(xpuf->dev, "%s: device /dev/xpuf unregistered\n", __func__);
+	return 0;
+}
+
+static const struct file_operations dev_fops = {
+	.owner          = THIS_MODULE,
+	.open           = xlnx_puf_open,
+	.release        = xlnx_puf_release,
+	.unlocked_ioctl = xlnx_puf_ioctl,
+};
+
+/**
+ * xlnx_puf_probe - probe puf device
+ * @pdev: Pointer to puf platform device structure
+ *
+ * Return: 0 if successful; otherwise -errno
+ */
+static int xlnx_puf_probe(struct platform_device *pdev)
+{
+	int ret;
+	struct xpuf_dev *xpuf;
+	struct device *dev = &pdev->dev;
+
+	xpuf = devm_kzalloc(dev, sizeof(*xpuf), GFP_KERNEL);
+	if (!xpuf)
+		return -ENOMEM;
+
+	xpuf->dev = dev;
+
+	ret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(64));
+	if (ret < 0) {
+		ret = dma_set_mask_and_coherent(dev, DMA_BIT_MASK(32));
+		if (ret < 0) {
+			dev_err(dev, "no usable DMA configuration\n");
+			return ret;
+		}
+	}
+
+	xpuf->miscdev.minor = MISC_DYNAMIC_MINOR;
+	xpuf->miscdev.name = "xpuf";
+	xpuf->miscdev.fops = &dev_fops;
+	xpuf->miscdev.parent = dev;
+
+	if (misc_register(&xpuf->miscdev))
+		return -ENODEV;
+
+	platform_set_drvdata(pdev, xpuf);
+
+	dev_dbg(dev, "puf registered as /dev/xpuf successfully");
+
+	return 0;
+}
+
+/**
+ * xlnx_puf_remove - clean up structures
+ * @pdev:	The structure containing the device's details
+ *
+ * Return: 0 on success.
+ */
+static int xlnx_puf_remove(struct platform_device *pdev)
+{
+	struct xpuf_dev *xpuf = platform_get_drvdata(pdev);
+
+	platform_set_drvdata(pdev, NULL);
+	misc_deregister(&xpuf->miscdev);
+
+	dev_dbg(xpuf->dev, "device /dev/xpuf removed\n");
+
+	return 0;
+}
+
+static struct platform_driver xlnx_puf_drv = {
+	.probe = xlnx_puf_probe,
+	.remove = xlnx_puf_remove,
+	.driver = {
+		.name = "xlnx-puf",
+	},
+};
+
+static int __init xlnx_puf_driver_init(void)
+{
+	struct platform_device *pdev;
+	int ret;
+
+	ret = platform_driver_register(&xlnx_puf_drv);
+	if (ret)
+		return ret;
+
+	pdev = platform_device_register_simple(xlnx_puf_drv.driver.name,
+					       0, NULL, 0);
+	if (IS_ERR(pdev)) {
+		ret = PTR_ERR(pdev);
+		platform_driver_unregister(&xlnx_puf_drv);
+	}
+
+	return ret;
+}
+
+static void __exit xlnx_puf_driver_exit(void)
+{
+	platform_driver_unregister(&xlnx_puf_drv);
+}
+
+module_init(xlnx_puf_driver_init);
+module_exit(xlnx_puf_driver_exit);
+
+MODULE_AUTHOR("Praveen Teja Kundanala <praveen.teja.kundanala@amd.com>");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("Xilinx Versal PUF driver");
diff --git a/drivers/misc/xilinx_tmr_inject.c b/drivers/misc/xilinx_tmr_inject.c
new file mode 100644
index 000000000..0d6065387
--- /dev/null
+++ b/drivers/misc/xilinx_tmr_inject.c
@@ -0,0 +1,196 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Driver for Xilinx TMR Inject IP.
+ *
+ * Copyright (C) 2022 Xilinx, Inc.
+ *
+ * Description:
+ * This driver is developed for TMR Inject IP,The Triple Modular Redundancy(TMR)
+ * Inject provides fault injection.
+ * Fault injection and detection features are provided through sysfs entries
+ * which allow the user to generate a fault.
+ */
+
+#include <asm/xilinx_mb_manager.h>
+#include <linux/module.h>
+#include <linux/of_device.h>
+
+/* TMR Inject Register offsets */
+#define XTMR_INJECT_CR_OFFSET		0x0
+#define XTMR_INJECT_AIR_OFFSET		0x4
+#define XTMR_INJECT_IIR_OFFSET		0xC
+#define XTMR_INJECT_EAIR_OFFSET		0x10
+#define XTMR_INJECT_ERR_OFFSET		0x204
+
+/* Register Bitmasks/shifts */
+#define XTMR_INJECT_CR_CPUID_SHIFT	8
+#define XTMR_INJECT_CR_IE_SHIFT		10
+#define XTMR_INJECT_IIR_ADDR_MASK	GENMASK(31, 16)
+
+/**
+ * struct xtmr_inject_dev - Driver data for TMR Inject
+ * @regs: device physical base address
+ * @dev: pointer to device struct
+ * @cr_val: control register value
+ * @magic: Magic hardware configuration value
+ * @err_cnt: error statistics count
+ */
+struct xtmr_inject_dev {
+	void __iomem *regs;
+	struct device *dev;
+	u32 cr_val;
+	u32 magic;
+	u32 err_cnt;
+};
+
+/* IO accessors */
+static inline void xtmr_inject_write(struct xtmr_inject_dev *xtmr_inject, u32 addr,
+				     u32 value)
+{
+	iowrite32(value, xtmr_inject->regs + addr);
+}
+
+static inline u32 xtmr_inject_read(struct xtmr_inject_dev *xtmr_inject, u32 addr)
+{
+	return ioread32(xtmr_inject->regs + addr);
+}
+
+static ssize_t inject_err_store(struct device *dev,
+				struct device_attribute *attr, const char *buf,
+				size_t size)
+{
+	int ret;
+	long value;
+
+	ret = kstrtol(buf, 16, &value);
+	if (ret)
+		return ret;
+
+	if (value > 1)
+		return -EINVAL;
+
+	xmb_inject_err();
+
+	return size;
+}
+static DEVICE_ATTR_WO(inject_err);
+
+static ssize_t inject_cpuid_store(struct device *dev,
+				  struct device_attribute *attr,
+				  const char *buf, size_t size)
+{
+	struct xtmr_inject_dev *xtmr_inject = dev_get_drvdata(dev);
+	int ret;
+	long value;
+
+	ret = kstrtol(buf, 0, &value);
+	if (ret)
+		return ret;
+
+	if (value > 3)
+		return -EINVAL;
+
+	xtmr_inject->cr_val |= (value << XTMR_INJECT_CR_CPUID_SHIFT);
+	xtmr_inject_write(xtmr_inject, XTMR_INJECT_CR_OFFSET,
+			  xtmr_inject->cr_val);
+
+	return size;
+}
+static DEVICE_ATTR_WO(inject_cpuid);
+
+static struct attribute *xtmr_inject_attrs[] = {
+	&dev_attr_inject_err.attr,
+	&dev_attr_inject_cpuid.attr,
+	NULL,
+};
+ATTRIBUTE_GROUPS(xtmr_inject);
+
+static void xtmr_inject_init(struct xtmr_inject_dev *xtmr_inject)
+{
+	/* Allow fault injection */
+	xtmr_inject->cr_val = xtmr_inject->magic |
+				(1 << XTMR_INJECT_CR_IE_SHIFT) |
+				(1 << XTMR_INJECT_CR_CPUID_SHIFT);
+	xtmr_inject_write(xtmr_inject, XTMR_INJECT_CR_OFFSET,
+			  xtmr_inject->cr_val);
+	/* Initialize the address inject and instruction inject registers */
+	xtmr_inject_write(xtmr_inject, XTMR_INJECT_AIR_OFFSET,
+			  XMB_INJECT_ERR_OFFSET);
+	xtmr_inject_write(xtmr_inject, XTMR_INJECT_IIR_OFFSET,
+			  XMB_INJECT_ERR_OFFSET & XTMR_INJECT_IIR_ADDR_MASK);
+}
+
+/**
+ * xtmr_inject_probe - Driver probe function
+ * @pdev: Pointer to the platform_device structure
+ *
+ * This is the driver probe routine. It does all the memory
+ * allocation and creates sysfs entries for the device.
+ *
+ * Return: 0 on success and failure value on error
+ */
+static int xtmr_inject_probe(struct platform_device *pdev)
+{
+	struct xtmr_inject_dev *xtmr_inject;
+	struct resource *res;
+	int err;
+
+	xtmr_inject = devm_kzalloc(&pdev->dev, sizeof(*xtmr_inject), GFP_KERNEL);
+	if (!xtmr_inject)
+		return -ENOMEM;
+
+	xtmr_inject->dev = &pdev->dev;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	xtmr_inject->regs = devm_ioremap_resource(xtmr_inject->dev, res);
+	if (IS_ERR(xtmr_inject->regs))
+		return PTR_ERR(xtmr_inject->regs);
+
+	err = of_property_read_u32(pdev->dev.of_node, "xlnx,magic",
+				   &xtmr_inject->magic);
+	if (err < 0) {
+		dev_err(&pdev->dev, "unable to read xlnx,magic property");
+		return err;
+	}
+
+	/* Initialize TMR Inject */
+	xtmr_inject_init(xtmr_inject);
+
+	err = sysfs_create_groups(&xtmr_inject->dev->kobj, xtmr_inject_groups);
+	if (err < 0) {
+		dev_err(&pdev->dev, "unable to create sysfs entries\n");
+		return err;
+	}
+
+	platform_set_drvdata(pdev, xtmr_inject);
+
+	return 0;
+}
+
+static int xtmr_inject_remove(struct platform_device *pdev)
+{
+	sysfs_remove_groups(&pdev->dev.kobj, xtmr_inject_groups);
+
+	return 0;
+}
+
+static const struct of_device_id xtmr_inject_of_match[] = {
+	{
+		.compatible = "xlnx,tmr-inject-1.0",
+	},
+	{ /* end of table */ }
+};
+MODULE_DEVICE_TABLE(of, xtmr_inject_of_match);
+
+static struct platform_driver xtmr_inject_driver = {
+	.driver = {
+		.name = "xilinx-tmr_inject",
+		.of_match_table = xtmr_inject_of_match,
+	},
+	.probe = xtmr_inject_probe,
+	.remove = xtmr_inject_remove,
+};
+module_platform_driver(xtmr_inject_driver);
+MODULE_AUTHOR("Xilinx, Inc");
+MODULE_DESCRIPTION("Xilinx TMR Inject Driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/misc/xilinx_tmr_manager.c b/drivers/misc/xilinx_tmr_manager.c
new file mode 100644
index 000000000..2fdda19dd
--- /dev/null
+++ b/drivers/misc/xilinx_tmr_manager.c
@@ -0,0 +1,282 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Xilinx TMR Subsystem.
+ *
+ * Copyright (C) 2022 Xilinx, Inc.
+ *
+ * Description:
+ * This driver is developed for TMR Manager,The Triple Modular Redundancy(TMR)
+ * Manager is responsible for handling the TMR subsystem state, including
+ * fault detection and error recovery. The core is triplicated in each of
+ * the sub-blocks in the TMR subsystem, and provides majority voting of
+ * its internal state provides soft error detection, correction and
+ * recovery. Error detection feature is provided through sysfs
+ * entries which allow the user to observer the TMR microblaze
+ * status.
+ */
+
+#include <asm/xilinx_mb_manager.h>
+#include <linux/module.h>
+#include <linux/of_device.h>
+
+/* TMR Manager Register offsets */
+#define XTMR_MANAGER_CR_OFFSET		0x0
+#define XTMR_MANAGER_FFR_OFFSET		0x4
+#define XTMR_MANAGER_CMR0_OFFSET	0x8
+#define XTMR_MANAGER_CMR1_OFFSET	0xC
+#define XTMR_MANAGER_BDIR_OFFSET	0x10
+#define XTMR_MANAGER_SEMIMR_OFFSET	0x1C
+
+/* Register Bitmasks/shifts */
+#define XTMR_MANAGER_CR_MAGIC1_MASK	0x00ff
+#define XTMR_MANAGER_CR_MAGIC2_MASK	0xff00
+#define XTMR_MANAGER_CR_RIR_MASK	0x10000
+#define XTMR_MANAGER_CR_MAGIC2_SHIFT	4
+#define XTMR_MANAGER_CR_RIR_SHIFT	16
+#define XTMR_MANAGER_CR_BB_SHIFT	18
+
+#define XTMR_MANAGER_FFR_LM12_MASK	BIT(0)
+#define XTMR_MANAGER_FFR_LM13_MASK	BIT(1)
+#define XTMR_MANAGER_FFR_LM23_MASK	BIT(2)
+
+/**
+ * struct xtmr_manager_dev - Driver data for TMR Manager
+ * @regs: device physical base address
+ * @dev: pointer to device struct
+ * @cr_val: control register value
+ * @magic1: Magic 1 hardware configuration value
+ * @err_cnt: error statistics count
+ * @phys_baseaddr: Physical base address
+ */
+struct xtmr_manager_dev {
+	void __iomem *regs;
+	struct device *dev;
+	u32 cr_val;
+	u32 magic1;
+	u32 err_cnt;
+	uintptr_t phys_baseaddr;
+};
+
+/* IO accessors */
+static inline void xtmr_manager_write(struct xtmr_manager_dev *xtmr_manager, u32 addr,
+				      u32 value)
+{
+	iowrite32(value, xtmr_manager->regs + addr);
+}
+
+static inline u32 xtmr_manager_read(struct xtmr_manager_dev *xtmr_manager, u32 addr)
+{
+	return ioread32(xtmr_manager->regs + addr);
+}
+
+/**
+ * xtmr_manager_unblock_break - unblocks the break signal
+ * @xtmr_manager: Pointer to xtmr_manager_dev structure
+ */
+static void xtmr_manager_unblock_break(struct xtmr_manager_dev *xtmr_manager)
+{
+	xtmr_manager->cr_val &= ~(1 << XTMR_MANAGER_CR_BB_SHIFT);
+	xtmr_manager_write(xtmr_manager, XTMR_MANAGER_CR_OFFSET, xtmr_manager->cr_val);
+}
+
+/**
+ * xmb_manager_reset_handler - clears the ffr register contents
+ * @priv: Private pointer
+ */
+static void xmb_manager_reset_handler(void *priv)
+{
+	struct xtmr_manager_dev *xtmr_manager = (struct xtmr_manager_dev *)priv;
+	/*
+	 * Clear the FFR Register contents as a part of recovery process.
+	 */
+	xtmr_manager_write(xtmr_manager, XTMR_MANAGER_FFR_OFFSET, 0);
+}
+
+/**
+ * xmb_manager_update_errcnt - update the error inject count
+ * @priv: Private pointer
+ */
+static void xmb_manager_update_errcnt(void *priv)
+{
+	struct xtmr_manager_dev *xtmr_manager = (struct xtmr_manager_dev *)priv;
+
+	xtmr_manager->err_cnt++;
+}
+
+static ssize_t errcnt_show(struct device *dev, struct device_attribute *attr,
+			   char *buf)
+{
+	struct xtmr_manager_dev *xtmr_manager = dev_get_drvdata(dev);
+
+	return sprintf(buf, "%x\n", xtmr_manager->err_cnt);
+}
+static DEVICE_ATTR_RO(errcnt);
+
+static ssize_t status_show(struct device *dev, struct device_attribute *attr,
+			   char *buf)
+{
+	struct xtmr_manager_dev *xtmr_manager = dev_get_drvdata(dev);
+	size_t ffr;
+	int len = 0;
+
+	ffr = xtmr_manager_read(xtmr_manager, XTMR_MANAGER_FFR_OFFSET);
+	if ((ffr & XTMR_MANAGER_FFR_LM12_MASK) == XTMR_MANAGER_FFR_LM12_MASK) {
+		len += sprintf(buf + len,
+			       "Lockstep mismatch between processor 1 and 2\n");
+	}
+
+	if ((ffr & XTMR_MANAGER_FFR_LM13_MASK) == XTMR_MANAGER_FFR_LM13_MASK) {
+		len += sprintf(buf + len,
+			       "Lockstep mismatch between processor 1 and 3\n");
+	}
+
+	if ((ffr & XTMR_MANAGER_FFR_LM23_MASK) == XTMR_MANAGER_FFR_LM23_MASK) {
+		len += sprintf(buf + len,
+			       "Lockstep mismatch between processor 2 and 3\n");
+	}
+
+	return len;
+}
+static DEVICE_ATTR_RO(status);
+
+static ssize_t dis_block_break_store(struct device *dev,
+				     struct device_attribute *attr,
+				     const char *buf, size_t size)
+{
+	struct xtmr_manager_dev *xtmr_manager = dev_get_drvdata(dev);
+	int ret;
+	long value;
+
+	ret = kstrtol(buf, 16, &value);
+	if (ret)
+		return ret;
+
+	if (value > 1)
+		return -EINVAL;
+
+	xtmr_manager_unblock_break(xtmr_manager);
+	return size;
+}
+static DEVICE_ATTR_WO(dis_block_break);
+
+static struct attribute *xtmr_manager_attrs[] = {
+	&dev_attr_dis_block_break.attr,
+	&dev_attr_status.attr,
+	&dev_attr_errcnt.attr,
+	NULL,
+};
+ATTRIBUTE_GROUPS(xtmr_manager);
+
+static void xtmr_manager_init(struct xtmr_manager_dev *xtmr_manager)
+{
+	/* Clear the SEM interrupt mask register to disable the interrupt */
+	xtmr_manager_write(xtmr_manager, XTMR_MANAGER_SEMIMR_OFFSET, 0);
+
+	/* Allow recovery reset by default */
+	xtmr_manager->cr_val = (1 << XTMR_MANAGER_CR_RIR_SHIFT) |
+				xtmr_manager->magic1;
+	xtmr_manager_write(xtmr_manager, XTMR_MANAGER_CR_OFFSET,
+			   xtmr_manager->cr_val);
+	/*
+	 * Configure Break Delay Initialization Register to zero so that
+	 * break occurs immediately
+	 */
+	xtmr_manager_write(xtmr_manager, XTMR_MANAGER_BDIR_OFFSET, 0);
+
+	/*
+	 * To come out of break handler need to block the break signal
+	 * in the tmr manager, update the xtmr_manager cr_val for the same
+	 */
+	xtmr_manager->cr_val |= (1 << XTMR_MANAGER_CR_BB_SHIFT);
+
+	/*
+	 * When the break vector gets asserted because of error injection,
+	 * the break signal must be blocked before exiting from the
+	 * break handler, Below api updates the TMR manager address and
+	 * control register and error counter callback arguments,
+	 * which will be used by the break handler to block the
+	 * break and call the callback function.
+	 */
+	xmb_manager_register(xtmr_manager->phys_baseaddr, xtmr_manager->cr_val,
+			     xmb_manager_update_errcnt,
+			     xtmr_manager, xmb_manager_reset_handler);
+}
+
+/**
+ * xtmr_manager_probe - Driver probe function
+ * @pdev: Pointer to the platform_device structure
+ *
+ * This is the driver probe routine. It does all the memory
+ * allocation and creates sysfs entries for the device.
+ *
+ * Return: 0 on success and failure value on error
+ */
+static int xtmr_manager_probe(struct platform_device *pdev)
+{
+	struct xtmr_manager_dev *xtmr_manager;
+	struct resource *res;
+	int err;
+
+	xtmr_manager = devm_kzalloc(&pdev->dev, sizeof(*xtmr_manager), GFP_KERNEL);
+	if (!xtmr_manager)
+		return -ENOMEM;
+
+	xtmr_manager->dev = &pdev->dev;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	xtmr_manager->regs = devm_ioremap_resource(xtmr_manager->dev, res);
+	if (IS_ERR(xtmr_manager->regs))
+		return PTR_ERR(xtmr_manager->regs);
+
+	xtmr_manager->phys_baseaddr = res->start;
+
+	err = of_property_read_u32(pdev->dev.of_node, "xlnx,magic1",
+				   &xtmr_manager->magic1);
+	if (err < 0) {
+		dev_err(&pdev->dev, "unable to read xlnx,magic1 property");
+		return err;
+	}
+
+	/* Initialize TMR Manager */
+	xtmr_manager_init(xtmr_manager);
+
+	err = sysfs_create_groups(&xtmr_manager->dev->kobj,
+				  xtmr_manager_groups);
+	if (err < 0) {
+		dev_err(&pdev->dev, "unable to create sysfs entries\n");
+		return err;
+	}
+
+	platform_set_drvdata(pdev, xtmr_manager);
+
+	return 0;
+}
+
+static int xtmr_manager_remove(struct platform_device *pdev)
+{
+	sysfs_remove_groups(&pdev->dev.kobj, xtmr_manager_groups);
+
+	return 0;
+}
+
+static const struct of_device_id xtmr_manager_of_match[] = {
+	{
+		.compatible = "xlnx,tmr-manager-1.0",
+	},
+	{ /* end of table */ }
+};
+MODULE_DEVICE_TABLE(of, xtmr_manager_of_match);
+
+static struct platform_driver xtmr_manager_driver = {
+	.driver = {
+		.name = "xilinx-tmr_manager",
+		.of_match_table = xtmr_manager_of_match,
+	},
+	.probe = xtmr_manager_probe,
+	.remove = xtmr_manager_remove,
+};
+module_platform_driver(xtmr_manager_driver);
+
+MODULE_AUTHOR("Xilinx, Inc");
+MODULE_DESCRIPTION("Xilinx TMR Manager Driver");
+MODULE_LICENSE("GPL");
diff --git a/drivers/misc/xlnx_dpu.c b/drivers/misc/xlnx_dpu.c
new file mode 100644
index 000000000..65cbc052d
--- /dev/null
+++ b/drivers/misc/xlnx_dpu.c
@@ -0,0 +1,1368 @@
+// SPDX-License-Identifier: GPL-2.0 OR Apache-2.0
+/*
+ * Xilinx Vivado Flow Deep learning Processing Unit (DPU) Driver
+ *
+ * Copyright (C) 2022 Xilinx, Inc. All rights reserved.
+ *
+ * Authors:
+ *    Ye Yang <ye.yang@xilinx.com>
+ *
+ * This file is dual-licensed; you may select either the GNU General Public
+ * License version 2 or Apache License, Version 2.0.
+ */
+
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/miscdevice.h>
+#include <linux/interrupt.h>
+#include <linux/io-64-nonatomic-lo-hi.h>
+#include <linux/iopoll.h>
+#include <linux/clk.h>
+#include <linux/dma-mapping.h>
+#include <linux/nospec.h>
+#include <linux/slab.h>
+#include <linux/iommu.h>
+#ifdef CONFIG_DEBUG_FS
+#include <linux/debugfs.h>
+#endif
+#include "xlnx_dpu.h"
+
+#define DEVICE_NAME "dpu"
+#define DRV_NAME "xlnx-dpu"
+#define DRIVER_DESC "Xilinx Deep Learning Processing Unit driver"
+
+static int timeout = 5;
+module_param(timeout, int, 0644);
+MODULE_PARM_DESC(timeout, "Set DPU timeout val in secs (default 5s)");
+
+static bool force_poll;
+/*
+ * this parameter is intended to be used only at probe time as there is
+ * no way to disable interrupts from DPU at run time.
+ */
+module_param(force_poll, bool, 0444);
+MODULE_PARM_DESC(force_poll, "polling or interrupt mode (default interrupt)");
+
+/*
+ * This parameter is intended to be used only when IOMMU is enabled.
+ * By specifying it the allocated buffer is forced to be contiguous
+ * in physical memory.
+ */
+static bool force_contig;
+module_param(force_contig, bool, 0444);
+MODULE_PARM_DESC(force_contig, "buffer is forced to be contiguous, default 0");
+
+/**
+ * struct cu - Computer Unit (cu) structure
+ * @mutex: protects from simultaneous access
+ * @done: completion of cu
+ * @irq: indicates cu IRQ number
+ */
+struct cu {
+	struct mutex	mutex; /* protects from simultaneous accesses */
+	struct completion	done;
+	int	irq;
+};
+
+/**
+ * struct xdpu_dev - Driver data for DPU
+ * @dev: pointer to device struct
+ * @regs: virtual base address for the dpu regmap
+ * @cu: indicates computer unit struct
+ * @axi_clk: AXI Lite clock
+ * @dpu_clk: DPU clock used for DPUCZDX8G general logic
+ * @dsp_clk: DSP clock used for DSP blocks
+ * @miscdev: misc device handle
+ * @mutex: protect client
+ * @root: debugfs dentry
+ * @client_list: indicates how many dpu clients link to xdpu
+ * @dpu_cnt: indicates how many dpu core/cu enabled in IP, up to 4
+ * @sfm_cnt: indicates softmax core enabled or not
+ */
+struct xdpu_dev {
+	struct device	*dev;
+	void __iomem	*regs;
+	struct cu	cu[MAX_CU_NUM];
+	struct clk	*axi_clk;
+	struct clk	*dpu_clk;
+	struct clk	*dsp_clk;
+	struct miscdevice	miscdev;
+	struct mutex	mutex; /* guards client */
+#ifdef CONFIG_DEBUG_FS
+	struct dentry	*root;
+	struct list_head	client_list;
+#endif
+	u8	dpu_cnt;
+	u8	sfm_cnt;
+};
+
+/**
+ * struct xdpu_client - DPU client
+ * @dev: pointer to dpu device struct
+ * @head: indicates dma memory pool list head
+ * @node: client node
+ */
+struct xdpu_client {
+	struct xdpu_dev	*dev;
+	struct list_head	head;
+	struct list_head	node;
+};
+
+/**
+ * struct dpu_buffer_block - DPU buffer block
+ * @head: list head
+ * @cpu_addr: cpu virtual address of the blocks memory
+ * @dma_addr: dma address of the blocks memory
+ * @phy_addr: physical address of the blocks memory
+ * @size: total size of the block in bytes
+ * @attrs: dma buffer attributes
+ */
+struct dpu_buffer_block {
+	struct list_head	head;
+	void	*cpu_addr;
+	dma_addr_t	dma_addr;
+	phys_addr_t	phy_addr;
+	size_t	size;
+	unsigned long	attrs;
+};
+
+#ifdef CONFIG_DEBUG_FS
+static int dpu_debugfs_init(struct xdpu_dev *xdpu);
+#endif
+
+/**
+ * xlnx_dpu_regs_init - initialize dpu register
+ * @xdpu:	dpu structure
+ */
+static void xlnx_dpu_regs_init(struct xdpu_dev *xdpu)
+{
+	int cu;
+
+	iowrite32(0, xdpu->regs + DPU_PMU_IP_RST);
+
+	for (cu = 0; cu < xdpu->dpu_cnt; cu++) {
+		iowrite32(DPU_HPBUS_VAL, xdpu->regs + DPU_HPBUS(cu));
+		iowrite32(0, xdpu->regs + DPU_IPSTART(cu));
+	}
+
+	iowrite32(DPU_RST_ALL_CORES, xdpu->regs + DPU_PMU_IP_RST);
+	iowrite32(0, xdpu->regs + DPU_SFM_RESET);
+	iowrite32(1, xdpu->regs + DPU_SFM_RESET);
+}
+
+/**
+ * xlnx_dpu_dump_regs - dump all dpu registers
+ * @p:	dpu structure
+ */
+static void xlnx_dpu_dump_regs(struct xdpu_dev *p)
+{
+	struct device *dev = p->dev;
+	int i;
+
+#define FMT8	"%-27s %08x\n"
+#define FMT16	"%-27s %016llx\n"
+	dev_warn(dev, "------------[ cut here ]------------\n");
+	dev_warn(dev, "Dump DPU Registers:\n");
+	dev_info(dev, FMT16, "TARGET_ID",
+		 lo_hi_readq(p->regs + DPU_TARGETID_L));
+	dev_info(dev, FMT8, "PMU_RST", ioread32(p->regs + DPU_PMU_IP_RST));
+	dev_info(dev, FMT8, "IP_VER_INFO", ioread32(p->regs + DPU_IPVER_INFO));
+	dev_info(dev, FMT8, "IP_FREQENCY", ioread32(p->regs + DPU_IPFREQENCY));
+	dev_info(dev, FMT8, "INT_STS", ioread32(p->regs + DPU_INT_STS));
+	dev_info(dev, FMT8, "INT_MSK", ioread32(p->regs + DPU_INT_MSK));
+	dev_info(dev, FMT8, "INT_RAW", ioread32(p->regs + DPU_INT_RAW));
+	dev_info(dev, FMT8, "INT_ICR", ioread32(p->regs + DPU_INT_ICR));
+	for (i = 0; i < p->dpu_cnt; i++) {
+		dev_warn(dev, "[CU-%d]\n", i);
+		dev_info(dev, FMT8, "HPBUS", ioread32(p->regs + DPU_HPBUS(i)));
+		dev_info(dev, FMT8, "INSTR",
+			 ioread32(p->regs + DPU_INSADDR(i)));
+		dev_info(dev, FMT8, "START",
+			 ioread32(p->regs + DPU_IPSTART(i)));
+		dev_info(dev, FMT16, "ADDR0",
+			 lo_hi_readq(p->regs + DPU_ADDR0_L(i)));
+		dev_info(dev, FMT16, "ADDR1",
+			 lo_hi_readq(p->regs + DPU_ADDR1_L(i)));
+		dev_info(dev, FMT16, "ADDR2",
+			 lo_hi_readq(p->regs + DPU_ADDR2_L(i)));
+		dev_info(dev, FMT16, "ADDR3",
+			 lo_hi_readq(p->regs + DPU_ADDR3_L(i)));
+		dev_info(dev, FMT16, "ADDR4",
+			 lo_hi_readq(p->regs + DPU_ADDR4_L(i)));
+		dev_info(dev, FMT16, "ADDR5",
+			 lo_hi_readq(p->regs + DPU_ADDR5_L(i)));
+		dev_info(dev, FMT16, "ADDR6",
+			 lo_hi_readq(p->regs + DPU_ADDR6_L(i)));
+		dev_info(dev, FMT16, "ADDR7",
+			 lo_hi_readq(p->regs + DPU_ADDR7_L(i)));
+		dev_info(dev, FMT8, "PSTART",
+			 ioread32(p->regs + DPU_P_STA_C(i)));
+		dev_info(dev, FMT8, "PEND",
+			 ioread32(p->regs + DPU_P_END_C(i)));
+		dev_info(dev, FMT8, "CSTART",
+			 ioread32(p->regs + DPU_C_STA_C(i)));
+		dev_info(dev, FMT8, "CEND",
+			 ioread32(p->regs + DPU_C_END_C(i)));
+		dev_info(dev, FMT8, "SSTART",
+			 ioread32(p->regs + DPU_S_STA_C(i)));
+		dev_info(dev, FMT8, "SEND",
+			 ioread32(p->regs + DPU_S_END_C(i)));
+		dev_info(dev, FMT8, "LSTART",
+			 ioread32(p->regs + DPU_L_STA_C(i)));
+		dev_info(dev, FMT8, "LEND",
+			 ioread32(p->regs + DPU_L_END_C(i)));
+		dev_info(dev, FMT16, "CYCLE",
+			 lo_hi_readq(p->regs + DPU_CYCLE_L(i)));
+		dev_info(dev, FMT8, "AXI", ioread32(p->regs + DPU_AXI_STS(i)));
+	}
+	dev_warn(dev, "[SOFTMAX]\n");
+	if (p->sfm_cnt) {
+#define DUMPREG(r) \
+	dev_info(dev, FMT8, #r, ioread32(p->regs + DPU_SFM_##r))
+		DUMPREG(INT_DONE);
+		DUMPREG(CMD_XLEN);
+		DUMPREG(CMD_YLEN);
+		DUMPREG(SRC_ADDR);
+		DUMPREG(SRC_ADDR_H);
+		DUMPREG(DST_ADDR);
+		DUMPREG(DST_ADDR_H);
+		DUMPREG(CMD_SCAL);
+		DUMPREG(CMD_OFF);
+		DUMPREG(INT_CLR);
+		DUMPREG(START);
+		DUMPREG(RESET);
+#undef DUMPREG
+	}
+	dev_warn(dev, "------------[ cut here ]------------\n");
+}
+
+/**
+ * xlnx_dpu_int_clear - clean DPU interrupt
+ * @xdpu:	dpu structure
+ * @id:	indicates which cu needs to be clean interrupt
+ */
+static void xlnx_dpu_int_clear(struct xdpu_dev *xdpu, int id)
+{
+	iowrite32(BIT(id), xdpu->regs + DPU_INT_ICR);
+	iowrite32(0, xdpu->regs + DPU_IPSTART(id));
+	iowrite32(ioread32(xdpu->regs + DPU_INT_ICR) & ~BIT(id),
+		  xdpu->regs + DPU_INT_ICR);
+}
+
+/**
+ * xlnx_sfm_int_clear - clean softmax interrupt
+ * @xdpu:	dpu structure
+ */
+static void xlnx_sfm_int_clear(struct xdpu_dev *xdpu)
+{
+	iowrite32(1, xdpu->regs + DPU_SFM_INT_CLR);
+	iowrite32(0, xdpu->regs + DPU_SFM_INT_CLR);
+}
+
+/**
+ * xlnx_dpu_softmax - softmax calculation acceleration using softmax IP
+ * @xdpu:	dpu structure
+ * @p :	softmax pmeter structure
+ *
+ * Return:	0 if successful; otherwise -errno
+ */
+static int xlnx_dpu_softmax(struct xdpu_dev *xdpu, struct ioc_softmax_t *p)
+{
+	int ret = -ETIMEDOUT;
+	int val;
+
+	iowrite32(p->width, xdpu->regs + DPU_SFM_CMD_XLEN);
+	iowrite32(p->height, xdpu->regs + DPU_SFM_CMD_YLEN);
+
+	iowrite32(p->input, xdpu->regs + DPU_SFM_SRC_ADDR);
+	iowrite32(p->input >> 32, xdpu->regs + DPU_SFM_SRC_ADDR_H);
+
+	iowrite32(p->output, xdpu->regs + DPU_SFM_DST_ADDR);
+	iowrite32(p->output >> 32, xdpu->regs + DPU_SFM_DST_ADDR_H);
+
+	iowrite32(p->scale, xdpu->regs + DPU_SFM_CMD_SCAL);
+	iowrite32(p->offset, xdpu->regs + DPU_SFM_CMD_OFF);
+	iowrite32(1, xdpu->regs + DPU_SFM_RESET);
+	iowrite32(0, xdpu->regs + DPU_SFM_MODE);
+
+	iowrite32(1, xdpu->regs + DPU_SFM_START);
+	iowrite32(0, xdpu->regs + DPU_SFM_START);
+
+	if (!force_poll) {
+		if (!wait_for_completion_timeout(&xdpu->cu[xdpu->dpu_cnt].done,
+						 TIMEOUT))
+			goto err_out;
+	} else {
+		ret = readx_poll_timeout(ioread32,
+					 xdpu->regs + DPU_SFM_INT_DONE,
+					 val,
+					 val & 0x1,
+					 POLL_PERIOD_US,
+					 TIMEOUT_US);
+		if (ret < 0)
+			goto err_out;
+
+		xlnx_sfm_int_clear(xdpu);
+	}
+
+	dev_dbg(xdpu->dev, "%s: PID=%d CPU=%d\n",
+		__func__, current->pid, raw_smp_processor_id());
+
+	return 0;
+
+err_out:
+	dev_warn(xdpu->dev, "timeout waiting for softmax\n");
+	xlnx_dpu_dump_regs(xdpu);
+
+	return ret;
+}
+
+/**
+ * xlnx_dpu_run - run dpu
+ * @xdpu:	dpu structure
+ * @p:	dpu run struct, contains the necessary address info
+ * @id:	indicates which cu is running
+ *
+ * Return:	0 if successful; otherwise -errno
+ */
+static inline int xlnx_dpu_run(struct xdpu_dev *xdpu,
+			       struct ioc_kernel_run_t *p, int id)
+{
+	int val, ret;
+
+	iowrite32(p->addr_code >> DPU_INSTR_OFFSET,
+		  xdpu->regs + DPU_INSADDR(id));
+
+	/*
+	 * Addr0: bias/weights
+	 * Addr1: the inter-layer workspacce
+	 * Addr2: the 1st input layer
+	 * Addr3: the output layer
+	 * AddrX: ULLONG_MAX as default
+	 */
+	lo_hi_writeq(p->addr0, xdpu->regs + DPU_ADDR0_L(id));
+	lo_hi_writeq(p->addr1, xdpu->regs + DPU_ADDR1_L(id));
+	lo_hi_writeq(p->addr2, xdpu->regs + DPU_ADDR2_L(id));
+	lo_hi_writeq(p->addr3, xdpu->regs + DPU_ADDR3_L(id));
+
+	if (p->addr4 != ULLONG_MAX)
+		lo_hi_writeq(p->addr4, xdpu->regs + DPU_ADDR4_L(id));
+	if (p->addr5 != ULLONG_MAX)
+		lo_hi_writeq(p->addr5, xdpu->regs + DPU_ADDR5_L(id));
+	if (p->addr6 != ULLONG_MAX)
+		lo_hi_writeq(p->addr6, xdpu->regs + DPU_ADDR6_L(id));
+	if (p->addr7 != ULLONG_MAX)
+		lo_hi_writeq(p->addr7, xdpu->regs + DPU_ADDR7_L(id));
+
+	iowrite32(1, xdpu->regs + DPU_IPSTART(id));
+
+	p->time_start = ktime_get();
+
+	if (!force_poll) {
+		if (!wait_for_completion_timeout(&xdpu->cu[id].done,
+						 TIMEOUT))
+			goto err_out;
+	} else {
+		ret = readx_poll_timeout(ioread32,
+					 xdpu->regs + DPU_INT_RAW,
+					 val,
+					 val & BIT(id),
+					 POLL_PERIOD_US,
+					 TIMEOUT_US);
+		if (ret < 0)
+			goto err_out;
+
+		xlnx_dpu_int_clear(xdpu, id);
+	}
+
+	p->time_end = ktime_get();
+	p->core_id = id;
+	p->pend_cnt = ioread32(xdpu->regs + DPU_P_END_C(id));
+	p->cend_cnt = ioread32(xdpu->regs + DPU_C_END_C(id));
+	p->send_cnt = ioread32(xdpu->regs + DPU_S_END_C(id));
+	p->lend_cnt = ioread32(xdpu->regs + DPU_L_END_C(id));
+	p->pstart_cnt = ioread32(xdpu->regs + DPU_P_STA_C(id));
+	p->cstart_cnt = ioread32(xdpu->regs + DPU_C_STA_C(id));
+	p->sstart_cnt = ioread32(xdpu->regs + DPU_S_STA_C(id));
+	p->lstart_cnt = ioread32(xdpu->regs + DPU_L_STA_C(id));
+	p->counter = lo_hi_readq(xdpu->regs + DPU_CYCLE_L(id));
+
+	dev_dbg(xdpu->dev,
+		"%s: PID=%d DPU=%d CPU=%d TIME=%lldus complete!\n",
+		__func__, current->pid, id, raw_smp_processor_id(),
+		ktime_us_delta(p->time_end, p->time_start));
+
+	return 0;
+
+err_out:
+	dev_warn(xdpu->dev, "cu[%d] timeout", id);
+	xlnx_dpu_dump_regs(xdpu);
+
+	return -ETIMEDOUT;
+}
+
+static inline phys_addr_t get_pa(void *addr)
+{
+	if (likely(is_vmalloc_addr(addr)))
+		return page_to_phys(vmalloc_to_page(addr)) +
+			offset_in_page(addr);
+	return __pa(addr);
+}
+
+/**
+ * xlnx_dpu_alloc_bo - alloc contiguous physical memory for dpu
+ * @client:	dpu client
+ * @req:	dpcma_req_alloc struct, contains the request info
+ *
+ * Return:	0 if successful; otherwise -errno
+ */
+static long xlnx_dpu_alloc_bo(struct xdpu_client *client,
+			      struct dpcma_req_alloc __user *req)
+{
+	struct dpu_buffer_block *pb;
+	size_t size;
+	struct xdpu_dev *xdpu = client->dev;
+
+	pb = kzalloc(sizeof(*pb), GFP_KERNEL);
+	if (!pb)
+		return -ENOMEM;
+
+	if (get_user(size, &req->size))
+		goto err_pb;
+
+	if (size > SIZE_MAX - PAGE_SIZE)
+		goto err_pb;
+
+	pb->size = size;
+
+	if (put_user(pb->size, &req->capacity))
+		goto err_pb;
+
+	if (iommu_present(xdpu->dev->bus) && force_contig)
+		pb->attrs = DMA_ATTR_FORCE_CONTIGUOUS;
+
+	pb->cpu_addr = dma_alloc_attrs(xdpu->dev, pb->size, &pb->dma_addr,
+				       GFP_KERNEL | __GFP_ZERO, pb->attrs);
+	if (!pb->cpu_addr)
+		goto err_pb;
+
+	if (put_user(pb->dma_addr, &req->dma_addr))
+		goto err_out;
+
+	if (!(iommu_present(xdpu->dev->bus)))
+		pb->phy_addr = pb->dma_addr;
+	else
+		pb->phy_addr = get_pa(pb->cpu_addr);
+
+	mutex_lock(&xdpu->mutex);
+	list_add(&pb->head, &client->head);
+	mutex_unlock(&xdpu->mutex);
+
+	return 0;
+err_out:
+	dma_free_attrs(xdpu->dev, pb->size, pb->cpu_addr, pb->dma_addr,
+		       pb->attrs);
+err_pb:
+	kfree(pb);
+	return -EFAULT;
+}
+
+/**
+ * xlnx_dpu_free_bo - free contiguous physical memory allocated
+ * @client:	dpu client
+ * @req:	dpcma_req_free struct, contains the request info
+ *
+ * Return:	0 if successful; otherwise -errno
+ */
+static long xlnx_dpu_free_bo(struct xdpu_client *client,
+			     struct dpcma_req_free __user *req)
+{
+	dma_addr_t dma_addr = 0;
+	struct xdpu_dev *xdpu = client->dev;
+	struct dpu_buffer_block *h, *n;
+
+	if (get_user(dma_addr, &req->dma_addr))
+		return -EFAULT;
+
+	mutex_lock(&xdpu->mutex);
+	list_for_each_entry_safe(h, n, &client->head, head) {
+		if (in_range(dma_addr, h->dma_addr, h->size)) {
+			dma_free_attrs(xdpu->dev, h->size, h->cpu_addr,
+				       h->dma_addr, h->attrs);
+			list_del(&h->head);
+			kfree(h);
+		}
+	}
+	mutex_unlock(&xdpu->mutex);
+
+	return 0;
+}
+
+/**
+ * xlnx_dpu_sync_bo - flush/invalidate cache for allocated memory
+ * @client:	dpu client
+ * @req:	dpcma_req_sync struct, contains the request info
+ *
+ * Return:	0 if successful; otherwise -errno
+ */
+static inline long xlnx_dpu_sync_bo(struct xdpu_client *client,
+				    struct dpcma_req_sync __user *req)
+{
+	dma_addr_t dma_addr;
+	int dir;
+	size_t size;
+	struct dpu_buffer_block *h = NULL, *n = NULL;
+	struct xdpu_dev *xdpu = client->dev;
+
+	if (get_user(dma_addr, &req->dma_addr) ||
+	    get_user(size, &req->size) || get_user(dir, &req->direction))
+		return -EFAULT;
+
+	if (dir != DPU_TO_CPU && dir != CPU_TO_DPU) {
+		dev_err(xdpu->dev, "invalid direction. direction = %d\n", dir);
+		return -EINVAL;
+	}
+
+	mutex_lock(&xdpu->mutex);
+	list_for_each_entry_safe(h, n, &client->head, head) {
+		if (in_range(dma_addr, h->dma_addr, h->size)) {
+			if (dir == DPU_TO_CPU)
+				dma_sync_single_for_cpu(xdpu->dev,
+							h->phy_addr,
+							size,
+							DMA_FROM_DEVICE);
+			else
+				dma_sync_single_for_device(xdpu->dev,
+							   h->phy_addr,
+							   size,
+							   DMA_TO_DEVICE);
+			break;
+		}
+	}
+	mutex_unlock(&xdpu->mutex);
+
+	return 0;
+}
+
+/**
+ * xlnx_dpu_ioctl - control ioctls for the DPU
+ * @file:	file handle of the DPU device
+ * @cmd:	ioctl code
+ * @arg:	pointer to user passed structure
+ *
+ * Return:	0 if successful; otherwise -errno
+ */
+static long xlnx_dpu_ioctl(struct file *file, unsigned int cmd,
+			   unsigned long arg)
+{
+	int ret = 0;
+	struct xdpu_client *client = file->private_data;
+	struct xdpu_dev *xdpu = client->dev;
+	void __user *data = NULL;
+
+	if (_IOC_TYPE(cmd) != DPU_IOC_MAGIC)
+		return -ENOTTY;
+
+	/* check if ioctl argument is present and valid */
+	if (_IOC_DIR(cmd) != _IOC_NONE) {
+		data = (void __user *)arg;
+		if (!data)
+			return -EINVAL;
+	}
+
+	switch (cmd) {
+	case DPUIOC_RUN:
+	{
+		struct ioc_kernel_run_t t;
+		int id;
+
+		if (copy_from_user(&t, data,
+				   sizeof(struct ioc_kernel_run_t))) {
+			return -EINVAL;
+		}
+
+		id = t.core_id;
+		if (id >= xdpu->dpu_cnt)
+			return -EINVAL;
+
+		dev_dbg(xdpu->dev,
+			"%s: PID=%d DPU=%d CPU=%d Comm=%.20s waiting",
+			__func__, current->pid, id, raw_smp_processor_id(),
+			current->comm);
+
+		id = array_index_nospec(id, xdpu->dpu_cnt);
+		/* Allows one process to run the cu by using a mutex */
+		mutex_lock(&xdpu->cu[id].mutex);
+
+		ret = xlnx_dpu_run(xdpu, &t, id);
+
+		mutex_unlock(&xdpu->cu[id].mutex);
+
+		if (copy_to_user(data, &t, sizeof(struct ioc_kernel_run_t)))
+			return -EINVAL;
+
+		break;
+	}
+	case DPUIOC_CREATE_BO:
+		return xlnx_dpu_alloc_bo(client,
+					 (struct dpcma_req_alloc __user *)arg);
+	case DPUIOC_FREE_BO:
+		return xlnx_dpu_free_bo(client,
+					(struct dpcma_req_free __user *)arg);
+	case DPUIOC_SYNC_BO:
+		return xlnx_dpu_sync_bo(client,
+					(struct dpcma_req_sync __user *)arg);
+	case DPUIOC_G_INFO:
+	{
+		u32 dpu_info = ioread32(xdpu->regs + DPU_IPVER_INFO);
+
+		if (copy_to_user(data, &dpu_info, sizeof(dpu_info)))
+			return -EFAULT;
+		break;
+	}
+	case DPUIOC_G_TGTID:
+	{
+		u64 fingerprint = lo_hi_readq(xdpu->regs + DPU_TARGETID_L);
+
+		if (copy_to_user(data, &fingerprint, sizeof(fingerprint)))
+			return -EFAULT;
+		break;
+	}
+	case DPUIOC_RUN_SOFTMAX:
+	{
+		struct ioc_softmax_t t;
+
+		if (copy_from_user(&t, data, sizeof(struct ioc_softmax_t))) {
+			dev_err(xdpu->dev, "copy_from_user softmax_t fail\n");
+			return -EINVAL;
+		}
+
+		mutex_lock(&xdpu->cu[xdpu->dpu_cnt].mutex);
+
+		ret = xlnx_dpu_softmax(xdpu, &t);
+
+		mutex_unlock(&xdpu->cu[xdpu->dpu_cnt].mutex);
+
+		break;
+	}
+	case DPUIOC_REG_READ:
+	{
+		u32 val = 0;
+		u32 off = 0;
+
+		if (copy_from_user(&off, data, sizeof(off))) {
+			dev_err(xdpu->dev, "copy_from_user off failed\n");
+			return -EINVAL;
+		}
+
+		val = ioread32(xdpu->regs + off);
+
+		if (copy_to_user(data, &val, sizeof(val)))
+			return -EFAULT;
+		break;
+	}
+	default:
+		ret = -EOPNOTSUPP;
+	}
+
+	return ret;
+}
+
+/**
+ * xlnx_dpu_isr - interrupt handler for DPU.
+ * @irq:	Interrupt number.
+ * @data:	DPU device structure.
+ *
+ * Return: IRQ_HANDLED.
+ */
+static irqreturn_t xlnx_dpu_isr(int irq, void *data)
+{
+	struct xdpu_dev *xdpu = data;
+	int i;
+
+	for (i = 0; i < xdpu->dpu_cnt; i++) {
+		if (irq == xdpu->cu[i].irq) {
+			xlnx_dpu_int_clear(xdpu, i);
+			dev_dbg(xdpu->dev, "%s: DPU=%d IRQ=%d",
+				__func__, i, irq);
+			complete(&xdpu->cu[i].done);
+		}
+	}
+
+	if (irq == xdpu->cu[xdpu->dpu_cnt].irq) {
+		xlnx_sfm_int_clear(xdpu);
+		dev_dbg(xdpu->dev, "%s: softmax IRQ=%d", __func__, irq);
+		complete(&xdpu->cu[xdpu->dpu_cnt].done);
+	}
+
+	return IRQ_HANDLED;
+}
+
+/**
+ * xlnx_dpu_mmap - maps cma ranges into userspace
+ * @file:	file structure for the device
+ * @vma:	VMA to map the registers into
+ *
+ * Return:	0 if successful; otherwise -errno
+ */
+static int xlnx_dpu_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	int found = 0;
+	struct xdpu_client *client = file->private_data;
+	struct xdpu_dev *xdpu = client->dev;
+	struct dpu_buffer_block *h = NULL, *n = NULL;
+	size_t size = vma->vm_end - vma->vm_start;
+	dma_addr_t offset = (dma_addr_t)vma->vm_pgoff << PAGE_SHIFT;
+
+	if ((offset >> PAGE_SHIFT) != vma->vm_pgoff)
+		return -EINVAL;
+
+	if ((offset + (phys_addr_t)size - 1) < offset)
+		return -EINVAL;
+
+	if (!((vma->vm_pgoff + size) <= __pa(high_memory)))
+		return -EINVAL;
+
+	mutex_lock(&xdpu->mutex);
+	list_for_each_entry_safe(h, n, &client->head, head) {
+		if (in_range(offset, h->dma_addr, h->size)) {
+			found = 1;
+			break;
+		}
+	}
+	mutex_unlock(&xdpu->mutex);
+
+	if (!found)
+		return -EINVAL;
+
+	/* map the whole buffer */
+	vma->vm_pgoff = 0;
+
+	return dma_mmap_attrs(xdpu->dev, vma, h->cpu_addr, h->dma_addr,
+			size, 0);
+}
+
+/**
+ * xlnx_dpu_open - open dpu device
+ * @inode:	inode object
+ * @filp:	file object
+ * Return:	0 if successful; otherwise -errno
+ */
+static int xlnx_dpu_open(struct inode *inode, struct file *filp)
+{
+	struct xdpu_dev *xdpu;
+	struct xdpu_client *client;
+
+	client = kzalloc(sizeof(*client), GFP_KERNEL);
+	if (!client)
+		return -ENOMEM;
+
+	xdpu = container_of(filp->private_data, struct xdpu_dev, miscdev);
+	client->dev = xdpu;
+	INIT_LIST_HEAD(&client->head);
+
+	filp->private_data = client;
+
+#ifdef CONFIG_DEBUG_FS
+	mutex_lock(&xdpu->mutex);
+	list_add_tail(&client->node, &xdpu->client_list);
+	mutex_unlock(&xdpu->mutex);
+#endif
+	return 0;
+}
+
+/**
+ * xlnx_dpu_release - release dpu resources
+ * @inode:	inode object
+ * @filp:	file object
+ *
+ * Return:	0 if successful; otherwise -errno
+ */
+static int xlnx_dpu_release(struct inode *inode, struct file *filp)
+{
+	struct xdpu_client *client = filp->private_data;
+	struct xdpu_dev *xdpu = client->dev;
+	struct dpu_buffer_block *h = NULL, *n = NULL;
+#ifdef CONFIG_DEBUG_FS
+	struct xdpu_client *p = NULL, *t = NULL;
+#endif
+
+	mutex_lock(&xdpu->mutex);
+	/* Drain the remaining buffer entries when abnormal close */
+	if (!list_empty(&client->head)) {
+		list_for_each_entry_safe(h, n, &client->head, head) {
+			dma_free_attrs(xdpu->dev,
+				       h->size,
+				       h->cpu_addr,
+				       h->dma_addr,
+				       h->attrs);
+			list_del(&h->head);
+			kfree(h);
+		};
+	}
+
+#ifdef CONFIG_DEBUG_FS
+	list_for_each_entry_safe(p, t, &xdpu->client_list, node) {
+		if (p == client) {
+			list_del(&p->node);
+			kfree(p);
+			break;
+		};
+	};
+#endif
+	mutex_unlock(&xdpu->mutex);
+
+	return 0;
+}
+
+static const struct file_operations dev_fops = {
+	.owner = THIS_MODULE,
+	.open = xlnx_dpu_open,
+	.mmap = xlnx_dpu_mmap,
+	.unlocked_ioctl = xlnx_dpu_ioctl,
+	.release = xlnx_dpu_release,
+};
+
+/**
+ * get_irq - get irq
+ * @pdev:	dpu platform device
+ * @xdpu:	dpu structure
+ *
+ * Return:	0 if successful; otherwise -errno
+ */
+static int get_irq(struct platform_device *pdev, struct xdpu_dev *xdpu)
+{
+	int ret, i;
+	int sfm_no = xdpu->dpu_cnt;
+	struct device *dev = xdpu->dev;
+
+	if (force_poll) {
+		dev_warn(dev, "no IRQ, using polling mode\n");
+		return 0;
+	}
+
+	for (i = 0; i < xdpu->dpu_cnt; i++) {
+		xdpu->cu[i].irq = platform_get_irq(pdev, i);
+		if (xdpu->cu[i].irq <= 0)
+			return -EINVAL;
+
+		ret = devm_request_irq(dev,
+				       xdpu->cu[i].irq,
+				       xlnx_dpu_isr,
+				       0,
+				       devm_kasprintf(dev,
+						      GFP_KERNEL,
+						      "%s-cu[%d]",
+						      dev_name(dev),
+						      i),
+				       xdpu);
+		if (ret < 0)
+			return ret;
+	}
+
+	if (xdpu->sfm_cnt) {
+		xdpu->cu[sfm_no].irq = platform_get_irq(pdev, sfm_no);
+		if (xdpu->cu[sfm_no].irq <= 0)
+			return -EINVAL;
+
+		ret = devm_request_irq(dev,
+				       xdpu->cu[sfm_no].irq,
+				       xlnx_dpu_isr,
+				       0,
+				       devm_kasprintf(dev,
+						      GFP_KERNEL,
+						      "%s-softmax",
+						      dev_name(dev)),
+				       xdpu);
+		if (ret < 0)
+			return ret;
+	}
+
+	return 0;
+}
+
+/**
+ * xlnx_dpu_probe - probe dpu device
+ * @pdev: Pointer to dpu platform device structure
+ *
+ * Return: 0 if successful; otherwise -errno
+ */
+static int xlnx_dpu_probe(struct platform_device *pdev)
+{
+	int i, ret;
+	u32 val;
+	struct xdpu_dev *xdpu;
+	struct resource *res;
+	struct device *dev = &pdev->dev;
+
+	xdpu = devm_kzalloc(dev, sizeof(*xdpu), GFP_KERNEL);
+	if (!xdpu)
+		return -ENOMEM;
+
+	xdpu->dev = dev;
+
+	xdpu->regs = devm_platform_get_and_ioremap_resource(pdev, 0, &res);
+	if (IS_ERR(xdpu->regs))
+		return -ENOMEM;
+
+	/*
+	 * DTG doesn't generate clock nodes in DT for Microblaze(MB),
+	 * using devm_clk_get_optional to return NULL in MB case, and return
+	 * required clock in ZynqMP case.
+	 */
+	xdpu->axi_clk = devm_clk_get_optional(xdpu->dev, "s_axi_aclk");
+	if (IS_ERR(xdpu->axi_clk))
+		return dev_err_probe(xdpu->dev, PTR_ERR(xdpu->axi_clk),
+				     "unable to get axi reference clock\n");
+
+	ret = clk_prepare_enable(xdpu->axi_clk);
+	if (ret) {
+		dev_err(xdpu->dev, "failed to enable s_axi_aclk(%d)\n", ret);
+		return ret;
+	}
+
+	xdpu->dpu_clk = devm_clk_get_optional(xdpu->dev, "m_axi_dpu_aclk");
+	if (IS_ERR(xdpu->dpu_clk))
+		return dev_err_probe(xdpu->dev, PTR_ERR(xdpu->dpu_clk),
+				     "unable to get m_axi_dpu_aclk\n");
+
+	ret = clk_prepare_enable(xdpu->dpu_clk);
+	if (ret) {
+		dev_err(xdpu->dev, "unable to enable dpu_clk(%d)\n", ret);
+		goto err_dpuclk;
+	}
+
+	xdpu->dsp_clk = devm_clk_get_optional(xdpu->dev, "dpu_2x_clk");
+	if (IS_ERR(xdpu->dsp_clk))
+		return dev_err_probe(xdpu->dev, PTR_ERR(xdpu->dsp_clk),
+				     "unable to get dsp clock\n");
+
+	ret = clk_prepare_enable(xdpu->dsp_clk);
+	if (ret) {
+		dev_err(xdpu->dev, "unable to enable dpu_2x_clk(%d)\n", ret);
+		goto err_dspclk;
+	}
+
+	/* dsp_clk should be dpu_clk * 2 */
+	if (xdpu->axi_clk && xdpu->dpu_clk && xdpu->dsp_clk)
+		dev_dbg(xdpu->dev,
+			"Freq: axilite: %lu MHz, dpu: %lu MHz, dsp: %lu MHz",
+			clk_get_rate(xdpu->axi_clk) / 1000000,
+			clk_get_rate(xdpu->dpu_clk) / 1000000,
+			clk_get_rate(xdpu->dsp_clk) / 1000000);
+
+	val = ioread32(xdpu->regs + DPU_IPVER_INFO);
+	if (DPU_VER(val) < DPU_IP_V3_4) {
+		dev_err(dev, "DPU IP need upgrade to 3.4 or later");
+		goto err_out;
+	}
+
+	xdpu->dpu_cnt = DPU_NUM(val);
+	xdpu->sfm_cnt = SFM_NUM(val);
+
+	val = ioread32(xdpu->regs + DPU_IPFREQENCY);
+	dev_dbg(dev, "found %d dpu core @%ldMHz and %d softmax core",
+		xdpu->dpu_cnt, DPU_FREQ(val), xdpu->sfm_cnt);
+
+	if (get_irq(pdev, xdpu))
+		goto err_out;
+
+	/* Try the reserved memory. Proceed if there's none */
+	ret = of_reserved_mem_device_init(dev);
+	if (ret && ret != -ENODEV)
+		goto err_out;
+
+	/* The DMA of the DPU is capable of 40-bit physical addresses */
+	if (dma_set_mask_and_coherent(dev, DMA_BIT_MASK(40)))
+		/* fall back to 32-bit DMA mask */
+		if (dma_set_mask_and_coherent(dev, DMA_BIT_MASK(32)))
+			goto err_out;
+
+	mutex_init(&xdpu->mutex);
+
+	for (i = 0; i < xdpu->dpu_cnt + xdpu->sfm_cnt; i++) {
+		init_completion(&xdpu->cu[i].done);
+		mutex_init(&xdpu->cu[i].mutex);
+	}
+
+	xdpu->miscdev.minor = MISC_DYNAMIC_MINOR;
+	xdpu->miscdev.name = DEVICE_NAME;
+	xdpu->miscdev.fops = &dev_fops;
+	xdpu->miscdev.parent = dev;
+
+	if (misc_register(&xdpu->miscdev))
+		goto err_out;
+
+	xlnx_dpu_regs_init(xdpu);
+
+#ifdef CONFIG_DEBUG_FS
+	INIT_LIST_HEAD(&xdpu->client_list);
+
+	ret = dpu_debugfs_init(xdpu);
+	if (ret) {
+		dev_err(xdpu->dev, "failed to init dpu_debugfs)\n");
+		goto err_debugfs;
+	}
+#endif
+
+	platform_set_drvdata(pdev, xdpu);
+
+	dev_dbg(dev, "dpu registered as /dev/dpu successfully");
+
+	return 0;
+
+#ifdef CONFIG_DEBUG_FS
+err_debugfs:
+	misc_deregister(&xdpu->miscdev);
+#endif
+err_out:
+	clk_disable_unprepare(xdpu->dsp_clk);
+err_dspclk:
+	clk_disable_unprepare(xdpu->dpu_clk);
+err_dpuclk:
+	clk_disable_unprepare(xdpu->axi_clk);
+
+	return -EINVAL;
+}
+
+/**
+ * xlnx_dpu_remove - clean up structures
+ * @pdev:	The structure containing the device's details
+ *
+ * Return: 0 on success. -EINVAL for invalid value.
+ */
+static int xlnx_dpu_remove(struct platform_device *pdev)
+{
+	struct xdpu_dev *xdpu = platform_get_drvdata(pdev);
+	int i;
+
+	/* clean all regs */
+	for (i = 0; i < DPU_REG_END; i += 4)
+		iowrite32(0, xdpu->regs + i);
+
+#ifdef CONFIG_DEBUG_FS
+	debugfs_remove_recursive(xdpu->root);
+#endif
+	platform_set_drvdata(pdev, NULL);
+	misc_deregister(&xdpu->miscdev);
+
+	dev_dbg(xdpu->dev, "%s: device /dev/dpu unregistered\n", __func__);
+	return 0;
+}
+
+static const struct of_device_id dpu_of_match[] = {
+	{ .compatible = "xlnx,dpuczdx8g-3.4" },
+	{ .compatible = "xlnx,dpuczdx8g-4.0" },
+	{ .compatible = "xlnx,dpuczdx8g-4.1" },
+	{ /* end of table */ }
+};
+MODULE_DEVICE_TABLE(of, dpu_of_match);
+
+static struct platform_driver xlnx_dpu_drv = {
+	.probe = xlnx_dpu_probe,
+	.remove = xlnx_dpu_remove,
+
+	.driver = {
+		.name = DRV_NAME,
+		.of_match_table = dpu_of_match,
+	},
+};
+
+module_platform_driver(xlnx_dpu_drv);
+
+#ifdef CONFIG_DEBUG_FS
+
+#define dump_register(n)			\
+{						\
+	.name	= #n,				\
+	.offset	= DPU_##n,				\
+}
+
+static const struct debugfs_reg32 cu_regs[4][38] = {
+	{
+	dump_register(IPVER_INFO),
+	dump_register(IPFREQENCY),
+	dump_register(TARGETID_L),
+	dump_register(TARGETID_H),
+	dump_register(IPSTART(0)),
+	dump_register(INSADDR(0)),
+	dump_register(ADDR0_L(0)),
+	dump_register(ADDR0_H(0)),
+	dump_register(ADDR1_L(0)),
+	dump_register(ADDR1_H(0)),
+	dump_register(ADDR2_L(0)),
+	dump_register(ADDR2_H(0)),
+	dump_register(ADDR3_L(0)),
+	dump_register(ADDR3_H(0)),
+	dump_register(ADDR4_L(0)),
+	dump_register(ADDR4_H(0)),
+	dump_register(ADDR5_L(0)),
+	dump_register(ADDR5_H(0)),
+	dump_register(ADDR6_L(0)),
+	dump_register(ADDR6_H(0)),
+	dump_register(ADDR7_L(0)),
+	dump_register(ADDR7_H(0)),
+	dump_register(CYCLE_L(0)),
+	dump_register(CYCLE_H(0)),
+	dump_register(P_STA_C(0)),
+	dump_register(P_END_C(0)),
+	dump_register(C_STA_C(0)),
+	dump_register(C_END_C(0)),
+	dump_register(S_STA_C(0)),
+	dump_register(S_END_C(0)),
+	dump_register(L_STA_C(0)),
+	dump_register(L_END_C(0)),
+	dump_register(AXI_STS(0)),
+	dump_register(HPBUS(0)),
+	dump_register(INT_STS),
+	dump_register(INT_MSK),
+	dump_register(INT_RAW),
+	dump_register(INT_ICR),
+	},
+	{
+	dump_register(IPVER_INFO),
+	dump_register(IPFREQENCY),
+	dump_register(TARGETID_L),
+	dump_register(TARGETID_H),
+	dump_register(IPSTART(1)),
+	dump_register(INSADDR(1)),
+	dump_register(ADDR0_L(1)),
+	dump_register(ADDR0_H(1)),
+	dump_register(ADDR1_L(1)),
+	dump_register(ADDR1_H(1)),
+	dump_register(ADDR2_L(1)),
+	dump_register(ADDR2_H(1)),
+	dump_register(ADDR3_L(1)),
+	dump_register(ADDR3_H(1)),
+	dump_register(ADDR4_L(1)),
+	dump_register(ADDR4_H(1)),
+	dump_register(ADDR5_L(1)),
+	dump_register(ADDR5_H(1)),
+	dump_register(ADDR6_L(1)),
+	dump_register(ADDR6_H(1)),
+	dump_register(ADDR7_L(1)),
+	dump_register(ADDR7_H(1)),
+	dump_register(CYCLE_L(1)),
+	dump_register(CYCLE_H(1)),
+	dump_register(P_STA_C(1)),
+	dump_register(P_END_C(1)),
+	dump_register(C_STA_C(1)),
+	dump_register(C_END_C(1)),
+	dump_register(S_STA_C(1)),
+	dump_register(S_END_C(1)),
+	dump_register(L_STA_C(1)),
+	dump_register(L_END_C(1)),
+	dump_register(AXI_STS(1)),
+	dump_register(HPBUS(1)),
+	dump_register(INT_STS),
+	dump_register(INT_MSK),
+	dump_register(INT_RAW),
+	dump_register(INT_ICR),
+	},
+	{
+	dump_register(IPVER_INFO),
+	dump_register(IPFREQENCY),
+	dump_register(TARGETID_L),
+	dump_register(TARGETID_H),
+	dump_register(IPSTART(2)),
+	dump_register(INSADDR(2)),
+	dump_register(ADDR0_L(2)),
+	dump_register(ADDR0_H(2)),
+	dump_register(ADDR1_L(2)),
+	dump_register(ADDR1_H(2)),
+	dump_register(ADDR2_L(2)),
+	dump_register(ADDR2_H(2)),
+	dump_register(ADDR3_L(2)),
+	dump_register(ADDR3_H(2)),
+	dump_register(ADDR4_L(2)),
+	dump_register(ADDR4_H(2)),
+	dump_register(ADDR5_L(2)),
+	dump_register(ADDR5_H(2)),
+	dump_register(ADDR6_L(2)),
+	dump_register(ADDR6_H(2)),
+	dump_register(ADDR7_L(2)),
+	dump_register(ADDR7_H(2)),
+	dump_register(CYCLE_L(2)),
+	dump_register(CYCLE_H(2)),
+	dump_register(P_STA_C(2)),
+	dump_register(P_END_C(2)),
+	dump_register(C_STA_C(2)),
+	dump_register(C_END_C(2)),
+	dump_register(S_STA_C(2)),
+	dump_register(S_END_C(2)),
+	dump_register(L_STA_C(2)),
+	dump_register(L_END_C(2)),
+	dump_register(AXI_STS(2)),
+	dump_register(HPBUS(2)),
+	dump_register(INT_STS),
+	dump_register(INT_MSK),
+	dump_register(INT_RAW),
+	dump_register(INT_ICR),
+	},
+	{
+	dump_register(IPVER_INFO),
+	dump_register(IPFREQENCY),
+	dump_register(TARGETID_L),
+	dump_register(TARGETID_H),
+	dump_register(IPSTART(3)),
+	dump_register(INSADDR(3)),
+	dump_register(ADDR0_L(3)),
+	dump_register(ADDR0_H(3)),
+	dump_register(ADDR1_L(3)),
+	dump_register(ADDR1_H(3)),
+	dump_register(ADDR2_L(3)),
+	dump_register(ADDR2_H(3)),
+	dump_register(ADDR3_L(3)),
+	dump_register(ADDR3_H(3)),
+	dump_register(ADDR4_L(3)),
+	dump_register(ADDR4_H(3)),
+	dump_register(ADDR5_L(3)),
+	dump_register(ADDR5_H(3)),
+	dump_register(ADDR6_L(3)),
+	dump_register(ADDR6_H(3)),
+	dump_register(ADDR7_L(3)),
+	dump_register(ADDR7_H(3)),
+	dump_register(CYCLE_L(3)),
+	dump_register(CYCLE_H(3)),
+	dump_register(P_STA_C(3)),
+	dump_register(P_END_C(3)),
+	dump_register(C_STA_C(3)),
+	dump_register(C_END_C(3)),
+	dump_register(S_STA_C(3)),
+	dump_register(S_END_C(3)),
+	dump_register(L_STA_C(3)),
+	dump_register(L_END_C(3)),
+	dump_register(AXI_STS(3)),
+	dump_register(HPBUS(3)),
+	dump_register(INT_STS),
+	dump_register(INT_MSK),
+	dump_register(INT_RAW),
+	dump_register(INT_ICR),
+	},
+};
+
+static const struct debugfs_reg32 sfm_regs[] = {
+	dump_register(IPVER_INFO),
+	dump_register(IPFREQENCY),
+	dump_register(TARGETID_L),
+	dump_register(TARGETID_H),
+	dump_register(SFM_INT_DONE),
+	dump_register(SFM_CMD_XLEN),
+	dump_register(SFM_CMD_YLEN),
+	dump_register(SFM_SRC_ADDR),
+	dump_register(SFM_SRC_ADDR_H),
+	dump_register(SFM_DST_ADDR),
+	dump_register(SFM_DST_ADDR_H),
+	dump_register(SFM_CMD_SCAL),
+	dump_register(SFM_CMD_OFF),
+	dump_register(SFM_INT_CLR),
+	dump_register(SFM_START),
+	dump_register(SFM_RESET),
+	dump_register(SFM_MODE),
+	dump_register(INT_STS),
+	dump_register(INT_MSK),
+	dump_register(INT_RAW),
+	dump_register(INT_ICR),
+};
+
+static int dump_show(struct seq_file *seq, void *v)
+{
+	struct xdpu_client *client;
+	struct xdpu_dev *xdpu = seq->private;
+	struct dpu_buffer_block *h;
+	static const char units[] = "KMG";
+	const char *unit = units;
+	unsigned long delta = 0;
+
+	mutex_lock(&xdpu->mutex);
+	list_for_each_entry(client, &xdpu->client_list, node) {
+		if (!list_empty(&client->head)) {
+			seq_printf(seq, "Client: %px\n", client);
+			seq_puts(seq, "Virtual Address\t\t\t\t");
+			seq_puts(seq, "Request Mem\t\tPhysical Address\t\t\t");
+			seq_puts(seq, "DMA Address\n");
+			list_for_each_entry(h, &client->head, head) {
+				delta = (h->size) >> 10;
+				while (!(delta & 1023) && unit[1]) {
+					delta >>= 10;
+					unit++;
+				}
+				seq_printf(seq, "%px-%px   %9lu%c\t\t",
+					   h->cpu_addr,
+					   h->cpu_addr + h->size,
+					   delta, *unit);
+				seq_printf(seq, "   0x%010llx-0x%010llx\t\t",
+					   h->phy_addr,
+					   h->phy_addr + h->size);
+				seq_printf(seq, "0x%010llx-0x%010llx\n",
+					   h->dma_addr,
+					   (h->dma_addr + h->size));
+				delta = 0;
+				unit = units;
+			};
+		};
+	};
+	mutex_unlock(&xdpu->mutex);
+
+	return 0;
+}
+DEFINE_SHOW_ATTRIBUTE(dump);
+
+/**
+ * dpu_debugfs_init - create DPU debugfs directory.
+ * @xdpu:	dpu structure
+ *
+ * Create DPU debugfs directory. Returns zero in case of success and a negative
+ * error code in case of failure.
+ *
+ * Return:	0 if successful; otherwise -errno
+ */
+static int dpu_debugfs_init(struct xdpu_dev *xdpu)
+{
+	char buf[32];
+	struct debugfs_regset32 *regset;
+	struct dentry *dentry;
+	int i;
+
+	xdpu->root = debugfs_create_dir("dpu", NULL);
+	if (IS_ERR(xdpu->root)) {
+		dev_err(xdpu->dev, "failed to create debugfs root\n");
+		return -ENODEV;
+	}
+
+	debugfs_create_file("dma_pool", 0444, xdpu->root, xdpu, &dump_fops);
+
+	for (i = 0; i < xdpu->dpu_cnt; i++) {
+		if (snprintf(buf, 32, "cu-%d", i) < 0)
+			return -EINVAL;
+
+		dentry = debugfs_create_dir(buf, xdpu->root);
+		regset = devm_kzalloc(xdpu->dev, sizeof(*regset), GFP_KERNEL);
+		if (!regset)
+			return -ENOMEM;
+
+		regset->regs = cu_regs[i];
+		regset->nregs = ARRAY_SIZE(cu_regs[i]);
+		regset->base = xdpu->regs;
+		debugfs_create_regset32("registers", 0444, dentry, regset);
+	}
+
+	if (xdpu->sfm_cnt) {
+		dentry = debugfs_create_dir("softmax", xdpu->root);
+		regset = devm_kzalloc(xdpu->dev, sizeof(*regset), GFP_KERNEL);
+		if (!regset)
+			return -ENOMEM;
+
+		regset->regs = sfm_regs;
+		regset->nregs = ARRAY_SIZE(sfm_regs);
+		regset->base = xdpu->regs;
+		debugfs_create_regset32("registers", 0444, dentry, regset);
+	}
+	return 0;
+}
+#endif
+
+MODULE_DESCRIPTION(DRIVER_DESC);
+MODULE_AUTHOR("Ye Yang <ye.yang@xilinx.com>");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/misc/xlnx_dpu.h b/drivers/misc/xlnx_dpu.h
new file mode 100644
index 000000000..6b8481795
--- /dev/null
+++ b/drivers/misc/xlnx_dpu.h
@@ -0,0 +1,201 @@
+/* SPDX-License-Identifier: GPL-2.0 OR Apache-2.0 */
+/*
+ * Xilinx Vivado Flow Deep learning Processing Unit (DPU) Driver
+ *
+ * Copyright (C) 2022 Xilinx, Inc.
+ * Copyright (C) 2022 Advanced Micro Devices, Inc. All rights reserved.
+ *
+ * Authors:
+ *    Ye Yang <ye.yang@amd.com>
+ */
+
+#ifndef _DPU_UAPI_H_
+#define _DPU_UAPI_H_
+
+/* up to 4 dpu cores and 1 softmax core */
+#define MAX_CU_NUM		5
+#define TIMEOUT			(timeout * CONFIG_HZ)
+#define TIMEOUT_US		(timeout * 1000000)
+#define POLL_PERIOD_US		(2000)
+
+#define in_range(b, start, len) (		\
+{						\
+typeof(b) b_ = (b);				\
+typeof(start) start_ = (start);			\
+((b_) >= (start_) && (b_) < (start_) + (len));	\
+}						\
+)
+
+/* DPU fingerprint, target info */
+#define DPU_PMU_IP_RST		(0x004)
+#define DPU_IPVER_INFO		(0x1E0)
+#define DPU_IPFREQENCY		(0x1E4)
+#define DPU_TARGETID_L		(0x1F0)
+#define DPU_TARGETID_H		(0x1F4)
+
+/* DPU core0-3 registers */
+#define DPU_HPBUS(x)		(0x200 + ((x) << 8))
+#define DPU_INSADDR(x)		(0x20C + ((x) << 8))
+#define DPU_IPSTART(x)		(0x220 + ((x) << 8))
+#define DPU_ADDR0_L(x)		(0x224 + ((x) << 8))
+#define DPU_ADDR0_H(x)		(0x228 + ((x) << 8))
+#define DPU_ADDR1_L(x)		(0x22C + ((x) << 8))
+#define DPU_ADDR1_H(x)		(0x230 + ((x) << 8))
+#define DPU_ADDR2_L(x)		(0x234 + ((x) << 8))
+#define DPU_ADDR2_H(x)		(0x238 + ((x) << 8))
+#define DPU_ADDR3_L(x)		(0x23C + ((x) << 8))
+#define DPU_ADDR3_H(x)		(0x240 + ((x) << 8))
+#define DPU_ADDR4_L(x)		(0x244 + ((x) << 8))
+#define DPU_ADDR4_H(x)		(0x248 + ((x) << 8))
+#define DPU_ADDR5_L(x)		(0x24C + ((x) << 8))
+#define DPU_ADDR5_H(x)		(0x250 + ((x) << 8))
+#define DPU_ADDR6_L(x)		(0x254 + ((x) << 8))
+#define DPU_ADDR6_H(x)		(0x258 + ((x) << 8))
+#define DPU_ADDR7_L(x)		(0x25C + ((x) << 8))
+#define DPU_ADDR7_H(x)		(0x260 + ((x) << 8))
+#define DPU_P_END_C(x)		(0x264 + ((x) << 8))
+#define DPU_C_END_C(x)		(0x268 + ((x) << 8))
+#define DPU_S_END_C(x)		(0x26C + ((x) << 8))
+#define DPU_L_END_C(x)		(0x270 + ((x) << 8))
+#define DPU_P_STA_C(x)		(0x274 + ((x) << 8))
+#define DPU_C_STA_C(x)		(0x278 + ((x) << 8))
+#define DPU_S_STA_C(x)		(0x27C + ((x) << 8))
+#define DPU_L_STA_C(x)		(0x280 + ((x) << 8))
+#define DPU_AXI_STS(x)		(0x284 + ((x) << 8))
+#define DPU_CYCLE_L(x)		(0x290 + ((x) << 8))
+#define DPU_CYCLE_H(x)		(0x294 + ((x) << 8))
+
+/* DPU INT Registers */
+#define DPU_INT_STS		(0x600)
+#define DPU_INT_MSK		(0x604)
+#define DPU_INT_RAW		(0x608)
+#define DPU_INT_ICR		(0x60C)
+
+/* DPU Softmax Registers */
+#define DPU_SFM_INT_DONE	(0x700)
+#define DPU_SFM_CMD_XLEN	(0x704)
+#define DPU_SFM_CMD_YLEN	(0x708)
+#define DPU_SFM_SRC_ADDR	(0x70C)
+#define DPU_SFM_DST_ADDR	(0x710)
+#define DPU_SFM_CMD_SCAL	(0x714)
+#define DPU_SFM_CMD_OFF		(0x718)
+#define DPU_SFM_INT_CLR		(0x71C)
+#define DPU_SFM_START		(0x720)
+#define DPU_SFM_RESET		(0x730)
+#define DPU_SFM_MODE		(0x738)
+#define DPU_SFM_SRC_ADDR_H	(0x73C)
+#define DPU_SFM_DST_ADDR_H	(0x740)
+#define DPU_REG_END		(0x800)
+
+#define DPU_NUM(x)		(GENMASK(3, 0) & (x))
+#define DPU_FREQ(x)		(GENMASK(11, 0) & (x))
+#define SFM_NUM(x)		((GENMASK(7, 4) & (x)) >> 4)
+#define DPU_VER(x)		((GENMASK(31, 24) & (x)) >> 24)
+#define DPU_SUB_VER(x)		((GENMASK(23, 16) & (x)) >> 16)
+#define DPU_SAXI(x)		((GENMASK(23, 12) & (x)) >> 12)
+
+#define DPU_HPBUS_VAL		(0x07070f0f)
+#define DPU_RST_ALL_CORES	(0xF)
+#define DPU_INSTR_OFFSET	(12)
+#define DPU_IP_V3_4		(0x34)
+
+enum DPU_DMA_DIR {
+	CPU_TO_DPU = 0,
+	DPU_TO_CPU = 1
+};
+
+struct dpcma_req_free {
+	u64 dma_addr;
+	size_t capacity;
+};
+
+struct dpcma_req_alloc {
+	size_t size;
+	u64 dma_addr;
+	size_t capacity;
+};
+
+struct dpcma_req_sync {
+	u64 dma_addr;
+	size_t size;
+	int direction;
+};
+
+/**
+ * struct  ioc_kernel_run_t - describe structure for each dpu ioctl
+ * @addr_code:	the address for DPU code
+ * @addr0:	address reg0
+ * @addr1:	address reg1
+ * @addr2:	address reg2
+ * @addr3:	address reg3
+ * @addr4:	address reg4
+ * @addr5:	address reg5
+ * @addr6:	address reg6
+ * @addr7:	address reg7
+ * @time_start:	the start timestamp before running
+ * @time_end:	the end timestamp after running
+ * @counter:	nums of total cycles
+ * @core_id:	the DPU core id
+ * @pend_cnt:	finished counts of dpu misc
+ * @cend_cnt:	finished counts of dpu conv
+ * @send_cnt:	finished counts of dpu save
+ * @lend_cnt:	finished counts of dpu load
+ * @pstart_cnt:	initiated counts of dpu misc
+ * @cstart_cnt:	initiated counts of dpu conv
+ * @sstart_cnt:	initiated counts of dpu save
+ * @lstart_cnt:	initiated counts of dpu load
+ */
+struct ioc_kernel_run_t {
+	u64 addr_code;
+	u64 addr0;
+	u64 addr1;
+	u64 addr2;
+	u64 addr3;
+	u64 addr4;
+	u64 addr5;
+	u64 addr6;
+	u64 addr7;
+	u64 time_start;
+	u64 time_end;
+	u64 counter;
+	int core_id;
+	u32 pend_cnt;
+	u32 cend_cnt;
+	u32 send_cnt;
+	u32 lend_cnt;
+	u32 pstart_cnt;
+	u32 cstart_cnt;
+	u32 sstart_cnt;
+	u32 lstart_cnt;
+};
+
+/**
+ * struct  ioc_softmax_t - describe structure for each softmax ioctl
+ * @width:	width dimension of Tensor
+ * @height:	height dimension of Tensor
+ * @input:	physical address of input Tensor
+ * @output:	physical address of output Tensor
+ * @scale:	quantization info of input Tensor
+ * @offset:	offset value for input Tensor
+ */
+struct ioc_softmax_t {
+	u32 width;
+	u32 height;
+	u64 input;
+	u64 output;
+	u32 scale;
+	u32 offset;
+};
+
+#define DPU_IOC_MAGIC 'D'
+
+#define DPUIOC_CREATE_BO _IOWR(DPU_IOC_MAGIC, 1, struct dpcma_req_alloc*)
+#define DPUIOC_FREE_BO _IOWR(DPU_IOC_MAGIC, 2, struct dpcma_req_free*)
+#define DPUIOC_SYNC_BO _IOWR(DPU_IOC_MAGIC, 3, struct dpcma_req_sync*)
+#define DPUIOC_G_INFO _IOR(DPU_IOC_MAGIC, 4, u32)
+#define DPUIOC_G_TGTID _IOR(DPU_IOC_MAGIC, 5, u64)
+#define DPUIOC_RUN _IOWR(DPU_IOC_MAGIC, 6, struct ioc_kernel_run_t*)
+#define DPUIOC_RUN_SOFTMAX _IOWR(DPU_IOC_MAGIC, 7, struct ioc_softmax_t*)
+#define DPUIOC_REG_READ _IOR(DPU_IOC_MAGIC, 8, u32)
+
+#endif /* _DPU_UAPI_H_ */
diff --git a/include/linux/xlnx-ai-engine.h b/include/linux/xlnx-ai-engine.h
new file mode 100644
index 000000000..92a65cc74
--- /dev/null
+++ b/include/linux/xlnx-ai-engine.h
@@ -0,0 +1,260 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+/*
+ * xlnx-ai-engine.h - Xilinx AI engine external interface
+ *
+ * Copyright (c) 2020, Xilinx Inc.
+ */
+
+#ifndef _XLNX_AI_ENGINE_H_
+#define _XLNX_AI_ENGINE_H_
+
+#if !IS_ENABLED(CONFIG_XILINX_AIE)
+#include <linux/errno.h>
+#endif
+#include <uapi/linux/xlnx-ai-engine.h>
+
+/*
+ * Macro to classify errors into categories to provide higher-level error
+ * event abstraction.
+ */
+#define AIE_ERROR_CATEGORY_SATURATION		0U
+#define AIE_ERROR_CATEGORY_FP			1U
+#define AIE_ERROR_CATEGORY_STREAM		2U
+#define AIE_ERROR_CATEGORY_ACCESS		3U
+#define AIE_ERROR_CATEGORY_BUS			4U
+#define AIE_ERROR_CATEGORY_INSTRUCTION		5U
+#define AIE_ERROR_CATEGORY_ECC			6U
+#define AIE_ERROR_CATEGORY_LOCK			7U
+#define AIE_ERROR_CATEGORY_DMA			8U
+#define AIE_ERROR_CATEGORY_MEM_PARITY		9U
+
+/* AIE error category bit mask */
+#define AIE_ERROR_CATMASK(c)			BIT(AIE_ERROR_CATEGORY_##c)
+#define AIE_ERROR_CATEGORY_MASK_SATURATION	AIE_ERROR_CATMASK(SATURATION)
+#define AIE_ERROR_CATEGORY_MASK_FP		AIE_ERROR_CATMASK(FP)
+#define AIE_ERROR_CATEGORY_MASK_STREAM		AIE_ERROR_CATMASK(STREAM)
+#define AIE_ERROR_CATEGORY_MASK_ACCESS		AIE_ERROR_CATMASK(ACCESS)
+#define AIE_ERROR_CATEGORY_MASK_BUS		AIE_ERROR_CATMASK(BUS)
+#define AIE_ERROR_CATEGORY_MASK_INSTRUCTION	AIE_ERROR_CATMASK(INSTRUCTION)
+#define AIE_ERROR_CATEGORY_MASK_ECC		AIE_ERROR_CATMASK(ECC)
+#define AIE_ERROR_CATEGORY_MASK_LOCK		AIE_ERROR_CATMASK(LOCK)
+#define AIE_ERROR_CATEGORY_MASK_DMA		AIE_ERROR_CATMASK(DMA)
+#define AIE_ERROR_CATEGORY_MASK_MEM_PARITY	AIE_ERROR_CATMASK(MEM_PARITY)
+
+struct device;
+
+/* Data structure to capture the Tile Information */
+struct aie_tile_info {
+	u32 col_size;
+	u16 major;
+	u16 minor;
+	u16 cols;
+	u16 rows;
+	u16 core_rows;
+	u16 mem_rows;
+	u16 shim_rows;
+	u16 core_row_start;
+	u16 mem_row_start;
+	u16 shim_row_start;
+	u16 core_dma_channels;
+	u16 mem_dma_channels;
+	u16 shim_dma_channels;
+	u16 core_locks;
+	u16 mem_locks;
+	u16 shim_locks;
+	u16 core_events;
+	u16 mem_events;
+	u16 shim_events;
+	u16 padding;
+};
+
+/* Data structure to capture the dma status */
+struct aie_dma_status {
+	u32 s2mm_sts;
+	u32 mm2s_sts;
+};
+
+/* Data structure to capture the core tile status */
+struct aie_core_tile_status {
+	struct aie_dma_status *dma;
+	u32 *core_mode_event_sts;
+	u32 *mem_mode_event_sts;
+	u32 core_status;
+	u32 prg_cntr;
+	u32 stack_ptr;
+	u32 link_reg;
+	u8 *lock_value;
+};
+
+/* Data structure to capture the mem tile status */
+struct aie_mem_tile_status {
+	struct aie_dma_status *dma;
+	u32 *event_sts;
+	u8 *lock_value;
+};
+
+/* Data structure to capture the shim tile status */
+struct aie_shim_tile_status {
+	struct aie_dma_status *dma;
+	u32 *event_sts;
+	u8 *lock_value;
+};
+
+/* Data structure to capture column status */
+struct aie_col_status {
+	struct aie_core_tile_status *core_tile;
+	struct aie_mem_tile_status *mem_tile;
+	struct aie_shim_tile_status *shim_tile;
+};
+
+/**
+ * struct aie_error - AI engine error
+ * @loc: AI engine tile location of which the error is from
+ * @module: AI engine module type of which the error is from
+ * @error_id: AI engine hardware event ID
+ * @category: AI engine error category of the error
+ */
+struct aie_error {
+	struct aie_location loc;
+	enum aie_module_type module;
+	u32 error_id;
+	u32 category;
+};
+
+struct aie_errors {
+	struct device *dev;
+	struct aie_error *errors;
+	u32 num_err;
+};
+
+#if IS_ENABLED(CONFIG_XILINX_AIE)
+bool aie_partition_is_available(struct aie_partition_req *req);
+struct device *aie_partition_request(struct aie_partition_req *req);
+int aie_partition_get_fd(struct device *dev);
+void aie_partition_release(struct device *dev);
+int aie_partition_reset(struct device *dev);
+int aie_partition_post_reinit(struct device *dev);
+
+int aie_register_error_notification(struct device *dev,
+				    void (*cb)(void *priv), void *priv);
+int aie_unregister_error_notification(struct device *dev);
+struct aie_errors *aie_get_errors(struct device *dev);
+u32 aie_get_error_categories(struct aie_errors *aie_errs);
+const char *aie_get_error_string(struct aie_errors *aie_errs,
+				 struct aie_error *aie_err);
+int aie_flush_errors(struct device *dev);
+void aie_free_errors(struct aie_errors *aie_errs);
+
+int aie_partition_set_freq_req(struct device *dev, u64 freq);
+int aie_partition_get_freq(struct device *dev, u64 *freq);
+int aie_partition_get_freq_req(struct device *dev, u64 *freq);
+
+int aie_get_status_dump(struct device *dev, struct aie_col_status *status);
+int aie_get_tile_info(struct device *dev, struct aie_tile_info *tile_info);
+
+/**
+ * aie_get_error_category() - Get the category of an AIE error
+ * @err: AI engine hardware error
+ * @return: category of the error
+ */
+static inline u32 aie_get_error_category(struct aie_error *err)
+{
+	return err->category;
+}
+
+#else
+static inline bool aie_partition_is_available(struct aie_partition_req *req)
+{
+	return false;
+}
+
+static inline struct device *
+aie_partition_request(struct aie_partition_req *req)
+{
+	return NULL;
+}
+
+static inline int aie_partition_get_fd(struct device *dev)
+{
+	return -EINVAL;
+}
+
+static inline void aie_partition_release(struct device *dev) {}
+
+static inline int aie_partition_reset(struct device *dev)
+{
+	return -EINVAL;
+}
+
+static inline int aie_partition_post_reinit(struct device *dev)
+{
+	return -EINVAL;
+}
+
+static inline int
+aie_register_error_notification(struct device *dev, void (*cb)(void *priv),
+				void *priv)
+{
+	return -EINVAL;
+}
+
+static inline int aie_unregister_error_notification(struct device *dev)
+{
+	return -EINVAL;
+}
+
+static inline struct aie_errors *aie_get_errors(struct device *dev)
+{
+	return NULL;
+}
+
+static inline u32 aie_get_error_categories(struct aie_errors *aie_errs)
+{
+	return 0;
+}
+
+static inline const char *aie_get_error_string(struct aie_errors *aie_errs,
+					       struct aie_error *aie_err)
+{
+	return NULL;
+}
+
+static inline int aie_flush_errors(struct device *dev)
+{
+	return -EINVAL;
+}
+
+static inline void aie_free_errors(struct aie_errors *aie_errs) {}
+
+static inline u32 aie_get_error_category(struct aie_error *err)
+{
+	return 0;
+}
+
+static inline int aie_partition_set_freq_req(struct device *dev, u64 freq)
+{
+	return -EINVAL;
+}
+
+static inline int aie_partition_get_freq(struct device *dev, u64 *freq)
+{
+	return -EINVAL;
+}
+
+static inline int aie_partition_get_freq_req(struct device *dev, u64 *freq)
+{
+	return -EINVAL;
+}
+
+static inline int aie_get_status_dump(struct device *dev, struct aie_col_status *status)
+{
+	return -EINVAL;
+}
+
+static inline int aie_get_tile_info(struct device *dev, struct aie_tile_info *tile_info)
+{
+	return -EINVAL;
+}
+
+#endif /* CONFIG_XILINX_AIE */
+#endif
diff --git a/include/uapi/linux/xlnx-ai-engine.h b/include/uapi/linux/xlnx-ai-engine.h
new file mode 100644
index 000000000..12431cfe3
--- /dev/null
+++ b/include/uapi/linux/xlnx-ai-engine.h
@@ -0,0 +1,646 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ * Copyright (c) 2020, Xilinx Inc.
+ */
+
+#ifndef _UAPI_AI_ENGINE_H_
+#define _UAPI_AI_ENGINE_H_
+
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+
+enum aie_reg_op {
+	AIE_REG_WRITE,
+	AIE_REG_BLOCKWRITE,
+	AIE_REG_BLOCKSET,
+	AIE_REG_MASKWRITE,
+	AIE_REG_MASKPOLL,
+	AIE_CONFIG_SHIMDMA_BD,
+	AIE_CONFIG_SHIMDMA_DMABUF_BD,
+};
+
+/**
+ * enum aie_module_type - identifies different hardware modules within a
+ *			  tile type. AIE tile may have memory and core
+ *			  module. While a PL or shim tile may have PL module.
+ * @AIE_MEM_MOD: comprises of the following sub-modules,
+ *			* data memory.
+ *			* tile DMA.
+ *			* lock module.
+ *			* events, event broadcast and event actions.
+ *			* tracing and profiling.
+ * @AIE_CORE_MOD: comprises of the following sub-modules,
+ *			* AIE core.
+ *			* program Memory.
+ *			* events, event broadcast and event actions.
+ *			* tracing and profiling.
+ *			* AXI-MM and AXI-S tile interconnects.
+ * @AIE_PL_MOD: comprises of the following sub-modules,
+ *			* PL interface.
+ *			* AXI-MM and AXI-S tile interconnects.
+ *			* Level 1 interrupt controllers.
+ *			* events, event broadcast and event actions.
+ *			* tracing and profiling.
+ * @AIE_NOC_MOD: comprises of the following sub-modules,
+ *			* interface from NoC Slave Unit (NSU)
+ *			  (bridge to AXI-MM switch)
+ *			* interfaces to NoC NoC Master Unit (NMU)
+ *				* shim DMA & locks
+ *				* NoC stream interface
+ */
+enum aie_module_type {
+	AIE_MEM_MOD,
+	AIE_CORE_MOD,
+	AIE_PL_MOD,
+	AIE_NOC_MOD,
+};
+
+/**
+ * enum aie_rsc_type - defines AI engine hardware resource types
+ * @AIE_RSCTYPE_PERF: perfcounter resource
+ * @AIE_RSCTYPE_USEREVENT: user events resource
+ * @AIE_RSCTYPE_TRACECONTROL: trace controller resource
+ * @AIE_RSCTYPE_PCEVENT: PC events resource
+ * @AIE_RSCTYPE_SSSELECT: stream switch port select resource
+ * @AIE_RSCTYPE_BROADCAST: broadcast events resource
+ * @AIE_RSCTYPE_COMBOEVENT: combo events resource
+ * @AIE_RSCTYPE_GROUPEVENTS: group events resource
+ * @AIE_RSCTYPE_MAX: total number of resources
+ */
+enum aie_rsc_type {
+	AIE_RSCTYPE_PERF,
+	AIE_RSCTYPE_USEREVENT,
+	AIE_RSCTYPE_TRACECONTROL,
+	AIE_RSCTYPE_PCEVENT,
+	AIE_RSCTYPE_SSSELECT,
+	AIE_RSCTYPE_BROADCAST,
+	AIE_RSCTYPE_COMBOEVENT,
+	AIE_RSCTYPE_GROUPEVENTS,
+	AIE_RSCTYPE_MAX
+};
+
+/**
+ * enum aie_part_status - defines AI engine partition status
+ * @XAIE_PART_STATUS_IDLE: partition is idle
+ * @XAIE_PART_STATUS_INUSE: partition is in use
+ * @XAIE_PART_STATUS_INVALID: partition is invalid to the system
+ *			      that is system cannot see the partition
+ */
+enum aie_part_status {
+	XAIE_PART_STATUS_IDLE,
+	XAIE_PART_STATUS_INUSE,
+	XAIE_PART_STATUS_INVALID,
+};
+
+/*
+ * AI engine partition control flags
+ */
+/* Not reset when release AI engine partition */
+#define XAIE_PART_NOT_RST_ON_RELEASE	0x00000001U
+
+/*
+ * AI engine resource property flags
+ */
+/*
+ * For resources which needs to be allocated contiguous
+ * such as combo events, it needs to be 0, 1; 2, 3;
+ * or 0, 1, 2, 3
+ */
+#define XAIE_RSC_PATTERN_BLOCK		(1U << 0)
+
+/* Any broadcast channel id */
+#define XAIE_BROADCAST_ID_ANY		0xFFFFFFFFU
+
+/* request a channel to broadcast to the whole partition */
+#define XAIE_BROADCAST_ALL		(1U << 0)
+
+/**
+ * struct aie_location - AIE location information
+ * @col: column id
+ * @row: row id
+ */
+struct aie_location {
+	__u32 col;
+	__u32 row;
+};
+
+/**
+ * struct aie_location_byte - AIE location information with single byte for
+ *			      column and row
+ * @row: row id
+ * @col: column id
+ *
+ * This structure follows the SSW AIE row and col sequence.
+ */
+struct aie_location_byte {
+	__u8 row;
+	__u8 col;
+};
+
+/**
+ * struct aie_range - AIE range information
+ * @start: start tile location
+ * @size: size of the range, number of columns and rows
+ */
+struct aie_range {
+	struct aie_location start;
+	struct aie_location size;
+};
+
+/**
+ * struct aie_mem - AIE memory information
+ * @range: range of tiles of the memory
+ * @offset: register offset within a tile of the memory
+ * @size: of a the memory in one tile
+ * @fd: file descriptor of the memory
+ */
+struct aie_mem {
+	struct aie_range range;
+	__kernel_size_t offset;
+	__kernel_size_t size;
+	int fd;
+};
+
+/**
+ * struct aie_mem_args - AIE memory enquiry arguments
+ * @num_mems: number of "struct aie_mem" elements
+ *	      e.g. two memory information elements, one for tile core memory,
+ *	      and the other for tile data memory.
+ * @mems: array of AI engine memory information elements
+ */
+struct aie_mem_args {
+	unsigned int num_mems;
+	struct aie_mem *mems;
+};
+
+/**
+ * struct aie_reg_args - AIE access register arguments
+ * @op: if this request is to read, write or poll register
+ * @mask: mask for mask write, 0 for not mask write
+ * @offset: offset of register to the start of an AI engine partition
+ * @val: value to write or get
+ * @dataptr: pointer to data buffer for block write
+ * @len: length of the buffer pointed by data
+ */
+struct aie_reg_args {
+	enum aie_reg_op op;
+	__u32 mask;
+	__u64 offset;
+	__u32 val;
+	__u64 dataptr;
+	__u32 len;
+};
+
+/**
+ * struct aie_range_args - AIE range request arguments
+ * @partition_id: partition id. It is used to identify the
+ *		  AI engine partition in the system.
+ * @uid: image identifier loaded on the AI engine partition
+ * @range: range of AIE tiles
+ * @status: indicate if the AI engine is in use.
+ *	    0 means not in used, otherwise, in use.
+ */
+struct aie_range_args {
+	__u32 partition_id;
+	__u32 uid;
+	struct aie_range range;
+	__u32 status;
+};
+
+/**
+ * struct aie_partition_query - AIE partition query arguments
+ * @partition_cnt: number of defined partitions in the system
+ * @partitions: buffer to store defined partitions information.
+ */
+struct aie_partition_query {
+	struct aie_range_args *partitions;
+	__u32 partition_cnt;
+};
+
+#define AIE_PART_ID_START_COL_SHIFT	0U
+#define AIE_PART_ID_NUM_COLS_SHIFT	8U
+#define AIE_PART_ID_START_COL_MASK	(0xFFU << AIE_PART_ID_START_COL_SHIFT)
+#define AIE_PART_ID_NUM_COLS_MASK	(0xFFU << AIE_PART_ID_NUM_COLS_SHIFT)
+
+#define aie_part_id_get_val(part_id, F) \
+	(((part_id) & AIE_PART_ID_##F ##_MASK) >> AIE_PART_ID_##F ##_SHIFT)
+#define aie_part_id_get_start_col(part_id) \
+	aie_part_id_get_val((part_id), START_COL)
+#define aie_part_id_get_num_cols(part_id) \
+	aie_part_id_get_val((part_id), NUM_COLS)
+
+/**
+ * struct aie_partition_req - AIE request partition arguments
+ * @partition_id: partition node id. It is used to identify the AI engine
+ *		  partition. Its format is:
+ *		  Reserved_16bits | start_col_8bits | num_cols_8bits
+ * @uid: image identifier loaded on the AI engine partition
+ * @meta_data: meta data to indicate which resources used by application.
+ * @flag: used for application to indicate particular driver requirements
+ *	  application wants to have for the partition. e.g. do not clean
+ *	  resource when closing the partition.
+ */
+struct aie_partition_req {
+	__u32 partition_id;
+	__u32 uid;
+	__u64 meta_data;
+	__u32 flag;
+};
+
+/**
+ * struct aie_partition_init_args - AIE partition initialization arguments
+ * @locs: Allocated array of tile locations that will be used
+ * @num_tiles: Number of tiles to use
+ * @init_opts: Partition initialization options
+ */
+struct aie_partition_init_args {
+	struct aie_location *locs;
+	__u32 num_tiles;
+	__u32 init_opts;
+};
+
+/*
+ * AI engine partition initialize options
+ */
+#define AIE_PART_INIT_OPT_COLUMN_RST		(1U << 0)
+#define AIE_PART_INIT_OPT_SHIM_RST		(1U << 1)
+#define AIE_PART_INIT_OPT_BLOCK_NOCAXIMMERR	(1U << 2)
+#define AIE_PART_INIT_OPT_ISOLATE		(1U << 3)
+#define AIE_PART_INIT_OPT_ZEROIZEMEM		(1U << 4)
+#define AIE_PART_INIT_OPT_DEFAULT		0xFU
+
+/**
+ * struct aie_dma_bd_args - AIE DMA buffer descriptor information
+ * @bd: DMA buffer descriptor
+ * @data_va: virtual address of the data
+ * @loc: Tile location relative to the start of a partition
+ * @bd_id: buffer descriptor id
+ */
+struct aie_dma_bd_args {
+	__u32 *bd;
+	__u64 data_va;
+	struct aie_location loc;
+	__u32 bd_id;
+};
+
+/**
+ * struct aie_dmabuf_bd_args - AIE dmabuf buffer descriptor information
+ * @bd: DMA buffer descriptor, within the buffer descriptor, the address field
+ *	will be the offset to the start of the dmabuf
+ * @buf_fd: DMA buffer handler which is dmabuf file descriptor
+ * @loc: Tile location relative to the start of a partition
+ * @bd_id: buffer descriptor id
+ */
+struct aie_dmabuf_bd_args {
+	__u32 *bd;
+	struct aie_location loc;
+	int buf_fd;
+	__u32 bd_id;
+};
+
+/**
+ * struct aie_tiles_array - AIE tiles array
+ * @locs: tiles locations array
+ * @num_tiles: number of tiles in the tiles locations array
+ */
+struct aie_tiles_array {
+	struct aie_location *locs;
+	__u32 num_tiles;
+};
+
+/**
+ * struct aie_txn_inst - AIE transaction instance
+ * @num_cmds: number commands containing register ops
+ * @cmdsptr: pointer to the buffer containing register ops
+ */
+struct aie_txn_inst {
+	__u32 num_cmds;
+	__u64 cmdsptr;
+};
+
+/**
+ * struct aie_rsc_req - AIE resource request
+ * @loc: tile location
+ * @mod: module type
+ * @type: resource type
+ * @num_rscs: number of resource per request
+ * @flag: resource property, such as it needs to be in pattern block such as
+ *	  if @num_rscs is 2, it needs to be 0,1; 2,3, or 4,5
+ */
+struct aie_rsc_req {
+	struct aie_location loc;
+	__u32 mod;
+	__u32 type;
+	__u32 num_rscs;
+	__u8 flag;
+};
+
+/**
+ * struct aie_rsc - AIE resource properties
+ * @loc: tile location, single byte for column and row each
+ * @mod: module type
+ * @type: resource type
+ * @id: resource id
+ */
+struct aie_rsc {
+	struct aie_location_byte loc;
+	__u32 mod;
+	__u32 type;
+	__u32 id;
+};
+
+/**
+ * struct aie_rsc_req_rsp - AIE resource request and response structure
+ * @req: resource request per tile module
+ * @rscs: allocated resources array of `struct aie_rsc`
+ */
+struct aie_rsc_req_rsp {
+	struct aie_rsc_req req;
+	__u64 rscs;
+};
+
+/**
+ * struct aie_rsc_bc_req - AIE broadcast channel request
+ * @rscs: broadcast channel resource array for every module and every tile
+ *	  of the channel
+ * @num_rscs: number of expected broadcast channel resources on the path,
+ *	      it also indicates the number of expected modules on the path.
+ * @flag: user flag to indicate if it is to get a broadcast channel for the
+ *	  whole partition.
+ * @id: broadcast channel ID. XAIE_BROADCAST_ID_ANY, it means not particular
+ *	id is specified, driver will allocate a free one.
+ */
+struct aie_rsc_bc_req {
+	__u64 rscs;
+	__u32 num_rscs;
+	__u32 flag;
+	__u32 id;
+};
+
+/* AI engine resource statistics types */
+#define AIE_RSC_STAT_TYPE_STATIC	0U
+#define AIE_RSC_STAT_TYPE_AVAIL		1U
+#define AIE_RSC_STAT_TYPE_MAX		2U
+
+/**
+ * struct aie_rsc_user_stat - AIE user requested resource statistics
+ * @loc: tile location, single byte for column and row each
+ * @mod: module type
+ * @type: resource type
+ * @num_rscs: number of resources
+ */
+struct aie_rsc_user_stat {
+	struct aie_location_byte loc;
+	__u8 mod;
+	__u8 type;
+	__u8 num_rscs;
+} __attribute__((packed, aligned(4)));
+
+/**
+ * struct aie_rsc_user_stat_array - AIE user requested resource statistics array
+ * @stats: resource statistics array
+ * @num_stats: number of resource statistics elements
+ * @stats_type: resource statistics type
+ */
+struct aie_rsc_user_stat_array {
+	__u64 stats;
+	__u32 num_stats;
+	__u32 stats_type;
+};
+
+#define AIE_IOCTL_BASE 'A'
+
+/* AI engine device IOCTL operations */
+#define AIE_ENQUIRE_PART_IOCTL		_IOWR(AIE_IOCTL_BASE, 0x1, \
+					      struct aie_partition_query)
+#define AIE_REQUEST_PART_IOCTL		_IOR(AIE_IOCTL_BASE, 0x2, \
+					     struct aie_partition_req)
+
+/* AI engine partition IOCTL operations */
+/**
+ * DOC: AIE_PARTITION_INIT_IOCTL - initializes AI engine partition
+ *
+ * This ioctl is used initialize a partition. Options parameter can
+ * be passed for initialization options. This operation does the
+ * following steps to initialize AI engine partition:
+ * 1. Clock gate all columns
+ * 2. Enable column reset
+ * 3. Ungate all columns
+ * 4. Disable column reset
+ * 5. Reset shim tiles
+ * 6. Setup AXI MM not to return errors for AXI decode or slave
+ *    errors, raise events instead.
+ * 7. Setup partition isolation
+ * 8. Zeroize memory
+ */
+#define AIE_PARTITION_INIT_IOCTL	_IOW(AIE_IOCTL_BASE, 0x3, \
+					     struct aie_partition_init_args)
+
+/**
+ * DOC: AIE_PARTITION_TEAR_IOCTL - teardown AI engine partition
+ *
+ * This ioctl is used teardown a partition. This operation does the
+ * following steps to teardown AI engine partition:
+ * 1. Clock gate all columns
+ * 2. Enable column reset
+ * 3. Ungate all columns
+ * 4. Disable column reset
+ * 5. Reset shim tiles
+ * 6. Zeroize memory
+ * 7. Clock gate all columns
+ */
+#define AIE_PARTITION_TEAR_IOCTL	_IO(AIE_IOCTL_BASE, 0x4)
+
+/**
+ * DOC: AIE_PARTITION_CLR_CONTEXT_IOCTL - clear context of AI engine partition
+ *
+ * This ioctl is used to clear context of a partition.  This operation does the
+ * following steps to clear context of AI engine partition:
+ * - Gate all columns
+ * - Reset AI engine partition columns
+ * - Ungate all columns
+ * - Reset shim tiles
+ * - Setup axi mm to raise events
+ * - Setup partition isolation
+ * - Zeroize data memory
+ * - Setup L2 intrupt
+ */
+#define AIE_PARTITION_CLR_CONTEXT_IOCTL _IO(AIE_IOCTL_BASE, 0x5)
+
+#define AIE_REG_IOCTL			_IOWR(AIE_IOCTL_BASE, 0x8, \
+					      struct aie_reg_args)
+/**
+ * DOC: AIE_GET_MEM_IOCTL - enquire information of memories in the AI engine
+ *			    partition
+ * This ioctl is used to get the information of all the different types of
+ * memories in the AI engine partition. Application can get the memories
+ * information in two steps:
+ * 1. passing 0 as @num_mems in struct aie_mem_args to enquire the number of
+ *    different memories in the partition, the value will be returned in
+ *    @num_mems.
+ * 2. passing the number of memories in @num_mems and valid pointer as @mems of
+ *    struct aie_mem_args to store the details information of different
+ *    memories. The driver will create DMA buf for each type of memories, and
+ *    will return the memory addressing information along with the DMA buf file
+ *    descriptors in @mems.
+ * After getting the memories information, user can use mmap() with the DMA buf
+ * file descriptor to enable access the memories from userspace.
+ */
+#define AIE_GET_MEM_IOCTL		_IOWR(AIE_IOCTL_BASE, 0x9, \
+					      struct aie_mem_args)
+/**
+ * DOC: AIE_ATTACH_DMABUF_IOCTL - attach a dmabuf to AI engine partition
+ *
+ * This ioctl is used to attach a dmabuf to the AI engine partition. AI engine
+ * partition will return the number of scatter gather list elements of the
+ * dmabuf.
+ */
+#define AIE_ATTACH_DMABUF_IOCTL		_IOR(AIE_IOCTL_BASE, 0xa, int)
+
+/**
+ * DOC: AIE_DETACH_DMABUF_IOCTL - dettach a dmabuf from AI engine partition
+ *
+ * This ioctl is used to detach a dmabuf from the AI engine partition
+ */
+#define AIE_DETACH_DMABUF_IOCTL		_IOR(AIE_IOCTL_BASE, 0xb, int)
+
+/**
+ * DOC: AIE_SET_DMABUF_BD_IOCTL - set buffer descriptor to SHIM DMA
+ *
+ * This ioctl is used to set the buffer descriptor to SHIM DMA
+ */
+#define AIE_SET_SHIMDMA_BD_IOCTL	_IOW(AIE_IOCTL_BASE, 0xd, \
+					     struct aie_dma_bd_args)
+
+/**
+ * DOC: AIE_REQUEST_TILES_IOCTL - request AI engine tiles
+ *
+ * This ioctl is used to request tiles.
+ * When requested the AI engine partition, the kernel driver will scan the
+ * partition to track which tiles are enabled or not. After that, if user
+ * want to request for more tiles, it will use this ioctl to request more
+ * tiles.
+ * If the aie_tiles_array is empty, it means it will request for all tiles
+ * in the partition.
+ */
+#define AIE_REQUEST_TILES_IOCTL		_IOW(AIE_IOCTL_BASE, 0xe, \
+					     struct aie_tiles_array)
+
+/**
+ * DOC: AIE_RELEASE_TILES_IOCTL - release AI engine tiles
+ *
+ * This ioctl is used to release tiles
+ */
+#define AIE_RELEASE_TILES_IOCTL		_IOW(AIE_IOCTL_BASE, 0xf, \
+					     struct aie_tiles_array)
+
+/**
+ * DOC: AIE_SET_SHIMDMA_DMABUF_BD_IOCTL - set buffer descriptor which contains
+ *					  dmabuf to SHIM DMA
+ *
+ * This ioctl is used to set the buffer descriptor to SHIM DMA. The
+ * aie_dmabuf_bd_args contains the dmabuf fd and the buffer descriptor contents.
+ * The address field in the buffer descriptor contents should be the offset to
+ * the start of the dmabuf.
+ */
+#define AIE_SET_SHIMDMA_DMABUF_BD_IOCTL	_IOW(AIE_IOCTL_BASE, 0x10, \
+					     struct aie_dmabuf_bd_args)
+
+/**
+ * DOC: AIE_TRANSACTION_IOCTL - execute the register operations to
+ *					configure AIE partition
+ *
+ * This ioctl is used to perform multiple register operations like write,
+ * mask write, block set and block write on AIE partition. The aie_txn_inst
+ * contains the buffer with all the register operations required by the
+ * application.
+ */
+#define AIE_TRANSACTION_IOCTL		_IOWR(AIE_IOCTL_BASE, 0x11, \
+					     struct aie_txn_inst)
+
+/**
+ * DOC: AIE_RSC_REQ_IOCTL - request a type of resources of a tile
+ *
+ * This ioctl is used to request a type of resources of a tile of an AI engine
+ * partition.
+ * AI engine partitition driver will check if there are the requested number
+ * of resources available. If yes, fill in the allcoated resource IDs in the
+ * resources array provided by user.
+ */
+#define AIE_RSC_REQ_IOCTL		_IOW(AIE_IOCTL_BASE, 0x14, \
+					     struct aie_rsc_req_rsp)
+
+/**
+ * DOC: AIE_RSC_REQ_SPECIFIC_IOCTL - request statically allocated resource
+ *
+ * This ioctl is used to request to use a specified allcoated resource
+ * AI engine partitition driver will check if the resource has been allocated
+ * at compilation time. If yes, and no one else has requested it, it returns
+ * success.
+ */
+#define AIE_RSC_REQ_SPECIFIC_IOCTL	_IOW(AIE_IOCTL_BASE, 0x15, \
+					     struct aie_rsc)
+
+/**
+ * DOC: AIE_RSC_RELEASE_IOCTL - release allocated resource
+ *
+ * This ioctl is used to release a resource and returns it to the resource
+ * pool, so that next time if user want to request for a resource, it is
+ * available
+ */
+#define AIE_RSC_RELEASE_IOCTL		_IOW(AIE_IOCTL_BASE, 0x16, \
+					     struct aie_rsc)
+
+/**
+ * DOC: AIE_RSC_FREE_IOCTL - free allocated resource
+ *
+ * This ioctl is used to free an allocated resource. It will unmark the
+ * resource from runtime used. If the resource is allocated at compilation
+ * time, it will not be returned back to the resource pool.
+ */
+#define AIE_RSC_FREE_IOCTL		_IOW(AIE_IOCTL_BASE, 0x17, \
+					     struct aie_rsc)
+
+/**
+ * DOC: AIE_RSC_CHECK_AVAIL_IOCTL - check if resource is available
+ *
+ * This ioctl is used to check how many resources are available for a specified
+ * type of resource.
+ */
+#define AIE_RSC_CHECK_AVAIL_IOCTL	_IOW(AIE_IOCTL_BASE, 0x18, \
+					     struct aie_rsc_req)
+
+/**
+ * DOC: AIE_RSC_GET_COMMON_BROADCAST_IOCTL - get a common broadcast channel for
+ *					     the specified set of AI engine
+ *					     modules.
+ *
+ * This ioctl is used to get a common broadcast channel for the specified set
+ * of AI engine modules in the resources array. If the any of the input set of
+ * tiles is gated, it will return failure. This ioctl will not check the
+ * connection of the input modules set.
+ * The driver will fill in the resource ID with the assigned broadcast channel
+ * ID of the resources array.
+ * If the XAIE_BROADCAST_ALL is set in the request flag, it will get the
+ * broadcast channel for all the ungated tiles of the partition.
+ * If a particular broadcast channel id is specified in the request, if will
+ * check if the channel is available for the specified modules, or the whole
+ * partition depends on if XAIE_BROADCAST_ALL is set.
+ */
+#define AIE_RSC_GET_COMMON_BROADCAST_IOCTL	_IOW(AIE_IOCTL_BASE, 0x19, \
+						struct aie_rsc_bc_req)
+
+/**
+ * DOC: AIE_RSC_GET_STAT_IOCTL - get resource usage statistics
+ *
+ * This ioctl is used to get resource usage statistics. User passes an array of
+ * resource statistics requests and the resources statistics type that is if it
+ * is statically allocated or available resources. Each request specifies the
+ * tile, module type and the resource type. This ioctl returns the number of
+ * resources of the specified statistics type per request.
+ */
+#define AIE_RSC_GET_STAT_IOCTL		_IOW(AIE_IOCTL_BASE, 0x1a, \
+					struct aie_rsc_user_stat_array)
+
+#endif
diff --git a/include/uapi/misc/xilinx_puf.h b/include/uapi/misc/xilinx_puf.h
new file mode 100644
index 000000000..78320419e
--- /dev/null
+++ b/include/uapi/misc/xilinx_puf.h
@@ -0,0 +1,75 @@
+/* SPDX-License-Identifier: GPL-2.0+ WITH Linux-syscall-note */
+/*
+ * Driver for Xilinx PUF device.
+ *
+ * Copyright (C) 2022 - 2023, Advanced Micro Devices, Inc.
+ *
+ * Description:
+ * This driver is developed for PUF registration and regeneration support.
+ */
+
+#ifndef _PUF_UAPI_H_
+#define _PUF_UAPI_H_
+
+#define PUF_MAX_SYNDROME_DATA_LEN_IN_WORDS 140
+#define PUF_EFUSE_TRIM_SYN_DATA_IN_WORDS   127
+#define PUF_ID_LEN_IN_WORDS                8
+#define PUF_ID_LEN_IN_BYTES                32
+#define PUF_REGIS                          0
+#define PUF_REGEN                          1
+
+/**
+ * struct puf_usrparams - user parameters for PUF from user space.
+ * @pufoperation: PUF registration or regeneration operation.
+ * @globalvarfilter: global variation filter.
+ * @readoption: option to read PUF data from efuse cache or ram address.
+ * @shuttervalue: shutter value for PUF registration/regeneration.
+ * @pufdataaddr: address to store/get the puf data during registration/regeneration.
+ * @pufidaddr: puf id will be stored either during registration/regeneration.
+ * @trimsyndataaddr: used during puf registration to store trimmed data.
+ */
+struct puf_usrparams {
+	u8 pufoperation;
+	u8 globalvarfilter;
+	u8 readoption;
+	u32 shuttervalue;
+	u64 pufdataaddr;
+	u64 pufidaddr;
+	u64 trimsyndataaddr;
+};
+
+/**
+ * struct puf_helperdata - parameters for puf helper data.
+ * @syndata: PUF syndrome data.
+ * @chash: PUF chash.
+ * @aux: PUF aux.
+ */
+struct puf_helperdata {
+	u32 syndata[PUF_MAX_SYNDROME_DATA_LEN_IN_WORDS];
+	u32 chash;
+	u32 aux;
+};
+
+/**
+ * struct pufdata - parameters for puf data.
+ * @pufhd: puf helper data of type struct puf_helperdata.
+ * @pufid: PUF id.
+ * @efusesyndata: PUF efuse syndrome data.
+ */
+struct pufdata {
+	struct puf_helperdata pufhd;
+	u32 pufid[PUF_ID_LEN_IN_WORDS];
+	u32 efusesyndata[PUF_EFUSE_TRIM_SYN_DATA_IN_WORDS];
+};
+
+enum pufreadoption {
+	PUF_READ_FROM_RAM = 0,
+	PUF_READ_FROM_EFUSE_CACHE = 1
+};
+
+#define PUF_IOC_MAGIC 'P'
+
+#define PUF_REGISTRATION _IOWR(PUF_IOC_MAGIC, 1, struct xpuf_usrparams *)
+#define PUF_REGENERATION _IOWR(PUF_IOC_MAGIC, 2, struct xpuf_usrparams *)
+
+#endif /* _PUF_UAPI_H_ */
